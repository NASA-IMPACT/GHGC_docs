[
  {
    "objectID": "datausage.html",
    "href": "datausage.html",
    "title": "U.S. Greenhouse Gas Center: Data Usage Notebooks",
    "section": "",
    "text": "Welcome to the U.S. Greenhouse Gas (GHG) Center data usage notebooks, your gateway to exploring and analyzing curated datasets on greenhouse gas emissions. Our cloud-based system offers seamless access to GHG curated datasets. Dive into the data with our data usage Jupyter notebooks, which demonstrate how to explore, access, visualize, and conduct basic data analysis for each GHG Center dataset in a code notebook environment. The data usage notebooks are grouped topically. Click on a notebook to learn more about the dataset and to view the data usage code.\nJoin us in our mission to make data-driven environmental solutions. Explore, analyze, and make a difference with the US GHG Center.\nView the US GHG Center Data Catalog",
    "crumbs": [
      "Data Usage Notebooks"
    ]
  },
  {
    "objectID": "datausage.html#gridded-anthropogenic-greenhouse-gas-emissions",
    "href": "datausage.html#gridded-anthropogenic-greenhouse-gas-emissions",
    "title": "U.S. Greenhouse Gas Center: Data Usage Notebooks",
    "section": "Gridded Anthropogenic Greenhouse Gas Emissions",
    "text": "Gridded Anthropogenic Greenhouse Gas Emissions\n\nOCO-2 MIP Top-Down CO₂ Budgets\n\nBeginner level notebook to access, visualize, explore statistics, and create a time series of the OCO-2 MIP Top-Down CO₂ Budgets dataset.\nIntermediate level notebook to read and visualize National CO₂ Budgets using OCO-2 MIP Top-Down CO₂ Budget country total data. This notebook utilizes the country totals available at https://ceos.org/gst/carbon-dioxide.html, which compliment the global 1° x 1° gridded CO₂ Budget data featured in the US GHG Center.\n\nODIAC Fossil Fuel CO₂ Emissions\n\nBeginner level notebook to access, visualize, explore statistics, and create a time series of the ODIAC Fossil Fuel CO₂ Emissions dataset.\n\nTM5-4DVar Isotopic CH₄ Inverse Fluxes\n\nBeginner level notebook to access, visualize, explore statistics, and create a time series of the TM5-4DVar Isotopic CH₄ Inverse Fluxes dataset.\n\nU.S. Gridded Anthropogenic Methane Emissions Inventory\n\nBeginner level notebook to access, visualize, explore statistics, and create a time series of the U.S. Gridded Anthropogenic Methane Emissions Inventory dataset.",
    "crumbs": [
      "Data Usage Notebooks"
    ]
  },
  {
    "objectID": "datausage.html#natural-greenhouse-gas-emissions-and-sinks",
    "href": "datausage.html#natural-greenhouse-gas-emissions-and-sinks",
    "title": "U.S. Greenhouse Gas Center: Data Usage Notebooks",
    "section": "Natural Greenhouse Gas Emissions and Sinks",
    "text": "Natural Greenhouse Gas Emissions and Sinks\n\nAir-Sea CO₂ Flux, ECCO-Darwin Model v5\n\nBeginner level notebook to access, visualize, explore statistics, and create a time series of the Air-Sea CO₂ Flux, ECCO-Darwin Model v5 dataset.\n\nMiCASA Land Carbon Flux\n\nBeginner level notebook to access, visualize, explore statistics, and create a time series of the MiCASA Land Carbon Flux dataset.\n\nGOSAT-based Top-down Total and Natural Methane Emissions\n\nBeginner level notebook to access, visualize, explore statistics, and create a time series of the GOSAT-based Top-down Total and Natural Methane Emissions dataset.\n\nOCO-2 MIP Top-Down CO₂ Budgets\n\nBeginner level notebook to access, visualize, explore statistics, and create a time series of the OCO-2 MIP Top-Down CO₂ Budgets dataset.\nIntermediate level notebook to read and visualizeNational CO₂ Budgets using OCO-2 MIP Top-Down CO₂ Budget country total data. This notebook utilizes the country totals available at ceos.org/gst/carbon-dioxide, which compliment the global 1° x 1° gridded CO₂ Budget data featured in the US GHG Center.\n\nTM5-4DVar Isotopic CH₄ Inverse Fluxes\n\nBeginner level notebook to access, visualize, explore statistics, and create a time series of the TM5-4DVar Isotopic CH₄ Inverse Fluxes dataset.\n\nWetland Methane Emissions, LPJ-EOSIM model\n\nBeginner level notebook to access, visualize, explore statistics, and create a time series of the Wetland Methane Emissions, LPJ-EOSIM model dataset.",
    "crumbs": [
      "Data Usage Notebooks"
    ]
  },
  {
    "objectID": "datausage.html#large-emissions-events",
    "href": "datausage.html#large-emissions-events",
    "title": "U.S. Greenhouse Gas Center: Data Usage Notebooks",
    "section": "Large Emissions Events",
    "text": "Large Emissions Events\n\nEMIT Methane Point Source Plume Complexes\n\nBeginner level notebook to access, visualize, explore statistics, and create a time series of the EMIT Methane Point Source Plume Complexes dataset.",
    "crumbs": [
      "Data Usage Notebooks"
    ]
  },
  {
    "objectID": "datausage.html#greenhouse-gas-concentrations",
    "href": "datausage.html#greenhouse-gas-concentrations",
    "title": "U.S. Greenhouse Gas Center: Data Usage Notebooks",
    "section": "Greenhouse Gas Concentrations",
    "text": "Greenhouse Gas Concentrations\n\nAtmospheric Carbon Dioxide Concentrations from NOAA Global Monitoring Laboratory\n\nBeginner level notebook to access, visualize, explore statistics, and create a time series of the Atmospheric Carbon Dioxide Concentrations from NOAA Global Monitoring Laboratory dataset.\n\nOCO-2 GEOS Column CO₂ Concentrations\n\nBeginner level notebook to access, visualize, explore statistics, and create a time series of the OCO-2 GEOS Column CO₂ Concentrations dataset.",
    "crumbs": [
      "Data Usage Notebooks"
    ]
  },
  {
    "objectID": "datausage.html#socioeconomic",
    "href": "datausage.html#socioeconomic",
    "title": "U.S. Greenhouse Gas Center: Data Usage Notebooks",
    "section": "Socioeconomic",
    "text": "Socioeconomic\n\nSEDAC Gridded World Population Density\n\nBeginner level notebook to access, visualize, explore statistics, and create a time series of the SEDAC Gridded World Population Density dataset.",
    "crumbs": [
      "Data Usage Notebooks"
    ]
  },
  {
    "objectID": "datausage.html#contact",
    "href": "datausage.html#contact",
    "title": "U.S. Greenhouse Gas Center: Data Usage Notebooks",
    "section": "Contact",
    "text": "Contact\nFor technical help or general questions, please contact the support team using the feedback form.",
    "crumbs": [
      "Data Usage Notebooks"
    ]
  },
  {
    "objectID": "data_usage.html",
    "href": "data_usage.html",
    "title": "Introduction to U.S. Greenhouse Gas Center: Data Usage Notebooks",
    "section": "",
    "text": "The U.S. Greenhouse Gas (GHG) Center provides a cloud-based system for exploring and analyzing U.S. government and other curated greenhouse gas datasets.\nOn this site, you can find the technical documentation for the services the center provides, how to load the datasets, and how the datasets were transformed from their source formats (eg. netCDF, HDF, etc.) into cloud-optimized formats that enable efficient cloud data access and visualization."
  },
  {
    "objectID": "data_usage.html#welcome",
    "href": "data_usage.html#welcome",
    "title": "Introduction to U.S. Greenhouse Gas Center: Data Usage Notebooks",
    "section": "",
    "text": "The U.S. Greenhouse Gas (GHG) Center provides a cloud-based system for exploring and analyzing U.S. government and other curated greenhouse gas datasets.\nOn this site, you can find the technical documentation for the services the center provides, how to load the datasets, and how the datasets were transformed from their source formats (eg. netCDF, HDF, etc.) into cloud-optimized formats that enable efficient cloud data access and visualization."
  },
  {
    "objectID": "data_usage.html#contents",
    "href": "data_usage.html#contents",
    "title": "Introduction to U.S. Greenhouse Gas Center: Data Usage Notebooks",
    "section": "Contents",
    "text": "Contents\n\nDataset usage examples listed below."
  },
  {
    "objectID": "data_usage.html#explore-data-usage-notebook",
    "href": "data_usage.html#explore-data-usage-notebook",
    "title": "Introduction to U.S. Greenhouse Gas Center: Data Usage Notebooks",
    "section": "Explore Data Usage Notebook",
    "text": "Explore Data Usage Notebook\n\nCASA-GFED3 Land Carbon Flux\nAir-Sea CO₂ Flux, ECCO-Darwin Model v5\nEMIT Methane Point Source Plume Complexes\nU.S. Gridded Anthropogenic Methane Emissions Inventory\nGOSAT-based Top-down Total and Natural Methane Emissions\nWetland Methane Emissions, LPJ-wsl Model\nOCO-2 MIP Top-Down CO₂ Budgets\nOCO-2 GEOS Column CO₂ Concentrations\nODIAC Fossil Fuel CO₂ Emissions\nSEDAC Gridded World Population Density\nTM5-4DVar Isotopic CH₄ Inverse Fluxes\nAtmospheric Carbon Dioxide Concentrations from NOAA Global Monitoring Laboratory"
  },
  {
    "objectID": "services/apis.html",
    "href": "services/apis.html",
    "title": "APIs",
    "section": "",
    "text": "Please note: while some of our services are already very mature, the US GHG Center platform is currently in the beta phase and will undergo many changes in coming months.",
    "crumbs": [
      "User Services",
      "APIs"
    ]
  },
  {
    "objectID": "services/apis.html#open-source",
    "href": "services/apis.html#open-source",
    "title": "APIs",
    "section": "Open Source",
    "text": "Open Source\nMost of the US GHG Center APIs are hosted out of a single project (veda-backend) that combines multiple standalone services.",
    "crumbs": [
      "User Services",
      "APIs"
    ]
  },
  {
    "objectID": "user_data_notebooks/gosat-based-ch4budget-yeargrid-v1_User_Notebook.html",
    "href": "user_data_notebooks/gosat-based-ch4budget-yeargrid-v1_User_Notebook.html",
    "title": "GOSAT-based Top-down Total and Natural Methane Emissions",
    "section": "",
    "text": "You can launch this notebook in the US GHG Center JupyterHub by clicking the link below.\nLaunch in the US GHG Center JupyterHub (requires access)",
    "crumbs": [
      "Data Usage Notebooks",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "GOSAT-based Top-down Total and Natural Methane Emissions"
    ]
  },
  {
    "objectID": "user_data_notebooks/gosat-based-ch4budget-yeargrid-v1_User_Notebook.html#run-this-notebook",
    "href": "user_data_notebooks/gosat-based-ch4budget-yeargrid-v1_User_Notebook.html#run-this-notebook",
    "title": "GOSAT-based Top-down Total and Natural Methane Emissions",
    "section": "",
    "text": "You can launch this notebook in the US GHG Center JupyterHub by clicking the link below.\nLaunch in the US GHG Center JupyterHub (requires access)",
    "crumbs": [
      "Data Usage Notebooks",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "GOSAT-based Top-down Total and Natural Methane Emissions"
    ]
  },
  {
    "objectID": "user_data_notebooks/gosat-based-ch4budget-yeargrid-v1_User_Notebook.html#approach",
    "href": "user_data_notebooks/gosat-based-ch4budget-yeargrid-v1_User_Notebook.html#approach",
    "title": "GOSAT-based Top-down Total and Natural Methane Emissions",
    "section": "Approach",
    "text": "Approach\n\nIdentify available dates and temporal frequency of observations for the given collection using the GHGC API /stac endpoint. The collection processed in this notebook is the gridded methane emissions data product.\nPass the STAC item into the raster API /stac/tilejson.jsonendpoint.\nUsing folium.plugins.DualMap, we will visualize two tiles (side-by-side), allowing us to compare time points.\nAfter the visualization, we will perform zonal statistics for a given polygon.",
    "crumbs": [
      "Data Usage Notebooks",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "GOSAT-based Top-down Total and Natural Methane Emissions"
    ]
  },
  {
    "objectID": "user_data_notebooks/gosat-based-ch4budget-yeargrid-v1_User_Notebook.html#about-the-data",
    "href": "user_data_notebooks/gosat-based-ch4budget-yeargrid-v1_User_Notebook.html#about-the-data",
    "title": "GOSAT-based Top-down Total and Natural Methane Emissions",
    "section": "About the Data",
    "text": "About the Data\nThe NASA Carbon Monitoring System Flux (CMS-Flux) team analyzed remote sensing observations from Japan’s Greenhouse gases Observing SATellite (GOSAT) to produce the global Committee on Earth Observation Satellites (CEOS) CH₄ Emissions data product. They used an analytic Bayesian inversion approach and the GEOS-Chem global chemistry transport model to quantify annual methane (CH₄) emissions and their uncertainties at a spatial resolution of 1° by 1° and then projected these to each country for 2019.\nFor more information regarding this dataset, please visit the GOSAT-based Top-down Total and Natural Methane Emissions data overview page.",
    "crumbs": [
      "Data Usage Notebooks",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "GOSAT-based Top-down Total and Natural Methane Emissions"
    ]
  },
  {
    "objectID": "user_data_notebooks/gosat-based-ch4budget-yeargrid-v1_User_Notebook.html#querying-the-stac-api",
    "href": "user_data_notebooks/gosat-based-ch4budget-yeargrid-v1_User_Notebook.html#querying-the-stac-api",
    "title": "GOSAT-based Top-down Total and Natural Methane Emissions",
    "section": "Querying the STAC API",
    "text": "Querying the STAC API\nFirst, we are going to import the required libraries. Once imported, they allow better executing a query in the GHG Center Spatio Temporal Asset Catalog (STAC) Application Programming Interface (API) where the granules for this collection are stored.\n\n# Import the following libraries\nimport requests\nimport folium\nimport folium.plugins\nfrom folium import Map, TileLayer\nfrom pystac_client import Client\nimport branca\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n\n# Provide STAC and RASTER API endpoints\nSTAC_API_URL = \"https://earth.gov/ghgcenter/api/stac\"\nRASTER_API_URL = \"https://earth.gov/ghgcenter/api/raster\"\n\n# Please use the collection name similar to the one used in STAC collection.\n\n# Name of the collection for gosat budget methane. \ncollection_name = \"gosat-based-ch4budget-yeargrid-v1\"\n\n\n# Fetching the collection from STAC collections using appropriate endpoint.\ncollection = requests.get(f\"{STAC_API_URL}/collections/{collection_name}\").json()\ncollection\n\nExamining the contents of our collection under the temporal variable, we see that the data is available from January 2012 to December 2018. By looking at the dashboard:time density, we observe that the data is available for only one year, i.e. 2019.\n\ndef get_item_count(collection_id):\n    count = 0\n    items_url = f\"{STAC_API_URL}/collections/{collection_id}/items\"\n\n    while True:\n        response = requests.get(items_url)\n\n        if not response.ok:\n            print(\"error getting items\")\n            exit()\n\n        stac = response.json()\n        count += int(stac[\"context\"].get(\"returned\", 0))\n        next = [link for link in stac[\"links\"] if link[\"rel\"] == \"next\"]\n\n        if not next:\n            break\n        items_url = next[0][\"href\"]\n\n    return count\n\n\n# Check total number of items available\nnumber_of_items = get_item_count(collection_name)\nitems = requests.get(f\"{STAC_API_URL}/collections/{collection_name}/items?limit={number_of_items}\").json()[\"features\"]\nprint(f\"Found {len(items)} items\")\n\n\n# Examining the first item in the collection\nitems[0]\n\nBelow, we enter minimum and maximum values to provide our upper and lower bounds in rescale_values.",
    "crumbs": [
      "Data Usage Notebooks",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "GOSAT-based Top-down Total and Natural Methane Emissions"
    ]
  },
  {
    "objectID": "user_data_notebooks/gosat-based-ch4budget-yeargrid-v1_User_Notebook.html#exploring-changes-in-gosat-methane-budgets-ch4-levels-using-the-raster-api",
    "href": "user_data_notebooks/gosat-based-ch4budget-yeargrid-v1_User_Notebook.html#exploring-changes-in-gosat-methane-budgets-ch4-levels-using-the-raster-api",
    "title": "GOSAT-based Top-down Total and Natural Methane Emissions",
    "section": "Exploring Changes in GOSAT Methane budgets (CH4) Levels Using the Raster API",
    "text": "Exploring Changes in GOSAT Methane budgets (CH4) Levels Using the Raster API\nIn this notebook, we will explore the impacts of methane emissions and by examining changes over time in urban regions. We will visualize the outputs on a map using folium.\n\n# To access the year value from each item more easily, this will let us query more explicity by year and month (e.g., 2020-02)\nitems = {item[\"properties\"][\"start_datetime\"][:10]: item for item in items} \nasset_name = \"prior-total\"\n\n\n# Fetching the min and max values for a specific item\nrescale_values = {\"max\":items[list(items.keys())[0]][\"assets\"][asset_name][\"raster:bands\"][0][\"histogram\"][\"max\"], \"min\":items[list(items.keys())[0]][\"assets\"][asset_name][\"raster:bands\"][0][\"histogram\"][\"min\"]}\n\n\nitems.keys()\n\nNow, we will pass the item id, collection name, and rescaling_factor to the Raster API endpoint. We will do this for first January 2019.\n\ncolor_map = \"rainbow\" # please select the color ramp from matplotlib library.\njanuary_2019_tile = requests.get(\n    f\"{RASTER_API_URL}/stac/tilejson.json?collection={items['2019-01-01']['collection']}&item={items['2019-01-01']['id']}\"\n    f\"&assets={asset_name}\"\n    f\"&color_formula=gamma+r+1.05&colormap_name={color_map}\"\n    f\"&rescale={rescale_values['min']},{rescale_values['max']}\", \n).json()\njanuary_2019_tile",
    "crumbs": [
      "Data Usage Notebooks",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "GOSAT-based Top-down Total and Natural Methane Emissions"
    ]
  },
  {
    "objectID": "user_data_notebooks/gosat-based-ch4budget-yeargrid-v1_User_Notebook.html#visualizing-ch₄-emissions",
    "href": "user_data_notebooks/gosat-based-ch4budget-yeargrid-v1_User_Notebook.html#visualizing-ch₄-emissions",
    "title": "GOSAT-based Top-down Total and Natural Methane Emissions",
    "section": "Visualizing CH₄ Emissions",
    "text": "Visualizing CH₄ Emissions\n\n# Set initial zoom and center of map for CH₄ Layer\n# Centre of map [latitude,longitude]\nmap_ = folium.Map(location=(34, -118), zoom_start=6)\n\n# January 2019\nmap_layer_2019 = TileLayer(\n    tiles=january_2019_tile[\"tiles\"][0],\n    attr=\"GHG\",\n    opacity=0.7,\n)\nmap_layer_2019.add_to(map_)\n\n# # January 2012\n# map_layer_2012 = TileLayer(\n#     tiles=january_2012_tile[\"tiles\"][0],\n#     attr=\"GHG\",\n#     opacity=0.7,\n# )\n# map_layer_2012.add_to(map_.m2)\n\n# visualising the map\nmap_",
    "crumbs": [
      "Data Usage Notebooks",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "GOSAT-based Top-down Total and Natural Methane Emissions"
    ]
  },
  {
    "objectID": "user_data_notebooks/gosat-based-ch4budget-yeargrid-v1_User_Notebook.html#summary",
    "href": "user_data_notebooks/gosat-based-ch4budget-yeargrid-v1_User_Notebook.html#summary",
    "title": "GOSAT-based Top-down Total and Natural Methane Emissions",
    "section": "Summary",
    "text": "Summary\nIn this notebook we have successfully completed the following steps for the STAC collection for the GOSAT-based Top-down Total and Natural Methane Emissions dataset.\n\nInstall and import the necessary libraries\nFetch the collection from STAC collections using the appropriate endpoints\nCount the number of existing granules within the collection\nMap the methane emission levels\nGenerate zonal statistics for the area of interest (AOI)\n\nIf you have any questions regarding this user notebook, please contact us using the feedback form.",
    "crumbs": [
      "Data Usage Notebooks",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "GOSAT-based Top-down Total and Natural Methane Emissions"
    ]
  },
  {
    "objectID": "user_data_notebooks/lpjeosim-wetlandch4-grid-v2_User_Notebook.html",
    "href": "user_data_notebooks/lpjeosim-wetlandch4-grid-v2_User_Notebook.html",
    "title": "Wetland Methane Emissions, LPJ-EOSIM Model",
    "section": "",
    "text": "You can launch this notebook in the US GHG Center JupyterHub by clicking the link below.\nLaunch in the US GHG Center JupyterHub (requires access)",
    "crumbs": [
      "Data Usage Notebooks",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "Wetland Methane Emissions, LPJ-EOSIM Model"
    ]
  },
  {
    "objectID": "user_data_notebooks/lpjeosim-wetlandch4-grid-v2_User_Notebook.html#run-this-notebook",
    "href": "user_data_notebooks/lpjeosim-wetlandch4-grid-v2_User_Notebook.html#run-this-notebook",
    "title": "Wetland Methane Emissions, LPJ-EOSIM Model",
    "section": "",
    "text": "You can launch this notebook in the US GHG Center JupyterHub by clicking the link below.\nLaunch in the US GHG Center JupyterHub (requires access)",
    "crumbs": [
      "Data Usage Notebooks",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "Wetland Methane Emissions, LPJ-EOSIM Model"
    ]
  },
  {
    "objectID": "user_data_notebooks/lpjeosim-wetlandch4-grid-v2_User_Notebook.html#approach",
    "href": "user_data_notebooks/lpjeosim-wetlandch4-grid-v2_User_Notebook.html#approach",
    "title": "Wetland Methane Emissions, LPJ-EOSIM Model",
    "section": "Approach",
    "text": "Approach\n\nIdentify available dates and temporal frequency of observations for the given collection using the GHGC API /stac endpoint. The collection processed in this notebook is the Wetland Methane Emissions, LPJ-EOSIM Model data product.\nPass the STAC item into the raster API /stac/tilejson.jsonendpoint.\nUsing folium.plugins.DualMap, visualize two tiles (side-by-side), allowing time point comparison.\nAfter the visualization, perform zonal statistics for a given polygon.",
    "crumbs": [
      "Data Usage Notebooks",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "Wetland Methane Emissions, LPJ-EOSIM Model"
    ]
  },
  {
    "objectID": "user_data_notebooks/lpjeosim-wetlandch4-grid-v2_User_Notebook.html#about-the-data",
    "href": "user_data_notebooks/lpjeosim-wetlandch4-grid-v2_User_Notebook.html#about-the-data",
    "title": "Wetland Methane Emissions, LPJ-EOSIM Model",
    "section": "About the Data",
    "text": "About the Data\nMethane (CH₄) emissions from vegetated wetlands are estimated to be the largest natural source of methane in the global CH₄ budget, contributing to roughly one third of the total of natural and anthropogenic emissions. Wetland CH₄ is produced by microbes breaking down organic matter in the oxygen deprived environment of inundated soils. Due to limited data availability, the details of the role of wetland CH₄ emissions have thus far been underrepresented. Using the Earth Observation SIMulator version (LPJ-EOSIM) of the Lund-Potsdam-Jena Dynamic Global Vegetation Model (LPJ-DGVM) global CH₄ emissions from wetlands are estimated at 0.5° x 0.5 degree spatial resolution. By simulating wetland extent and using characteristics of inundated areas, such as wetland soil moisture, temperature, and carbon content, the model provides estimates of CH₄ quantities emitted into the atmosphere. This dataset shows concentrated methane sources from tropical and high latitude ecosystems. The LPJ-EOSIM Wetland Methane Emissions dataset consists of global daily model estimates of terrestrial wetland methane emissions from 1990 to the present, with data added bimonthly. The estimates are regularly used in conjunction with NASA’s Goddard Earth Observing System (GEOS) model to simulate the impact of wetlands and other methane sources on atmospheric methane concentrations, to compare against satellite and airborne data, and to improve understanding and prediction of wetland emissions.\nFor more information regarding this dataset, please visit the U.S. Greenhouse Gas Center.",
    "crumbs": [
      "Data Usage Notebooks",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "Wetland Methane Emissions, LPJ-EOSIM Model"
    ]
  },
  {
    "objectID": "user_data_notebooks/lpjeosim-wetlandch4-grid-v2_User_Notebook.html#query-the-stac-api",
    "href": "user_data_notebooks/lpjeosim-wetlandch4-grid-v2_User_Notebook.html#query-the-stac-api",
    "title": "Wetland Methane Emissions, LPJ-EOSIM Model",
    "section": "Query the STAC API",
    "text": "Query the STAC API\nFirst, we are going to import the required libraries. Once imported, they allow better executing a query in the GHG Center Spatio Temporal Asset Catalog (STAC) Application Programming Interface (API) where the granules for this collection are stored.\n\n# Import the following libraries\nimport requests\nimport folium\nimport folium.plugins\nfrom folium import Map, TileLayer\nfrom pystac_client import Client\nimport branca\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n\n# Provide the STAC and RASTER API endpoints\n# The endpoint is referring to a location within the API that executes a request on a data collection nesting on the server.\n\n# The STAC API is a catalog of all the existing data collections that are stored in the GHG Center.\nSTAC_API_URL = \"https://earth.gov/ghgcenter/api/stac\"\n\n# The RASTER API is used to fetch collections for visualization\nRASTER_API_URL = \"https://earth.gov/ghgcenter/api/raster\"\n\n# The collection name is used to fetch the dataset from the STAC API. First, we define the collection name as a variable\n# Name of the collection for the wetland methane emissions LPJ-EOSIM Model\ncollection_name = \"lpjeosim-wetlandch4-daygrid-v2\"\n\n# Next, we need to specify the asset name for this collection\n# The asset name is referring to the raster band containing the pixel values for the parameter of interest\nasset_name = \"ensemble-mean-ch4-wetlands-emissions\"\n\n\n# Fetch the collection from the STAC API using the appropriate endpoint\n# The 'requests' library allows a HTTP request possible\ncollection = requests.get(f\"{STAC_API_URL}/collections/{collection_name}\").json()\n\n# Print the properties of the collection to the console\ncollection\n\n{'id': 'lpjeosim-wetlandch4-daygrid-v2',\n 'type': 'Collection',\n 'links': [{'rel': 'items',\n   'type': 'application/geo+json',\n   'href': 'https://earth.gov/ghgcenter/api/stac/collections/lpjeosim-wetlandch4-daygrid-v2/items'},\n  {'rel': 'parent',\n   'type': 'application/json',\n   'href': 'https://earth.gov/ghgcenter/api/stac/'},\n  {'rel': 'root',\n   'type': 'application/json',\n   'href': 'https://earth.gov/ghgcenter/api/stac/'},\n  {'rel': 'self',\n   'type': 'application/json',\n   'href': 'https://earth.gov/ghgcenter/api/stac/collections/lpjeosim-wetlandch4-daygrid-v2'}],\n 'title': 'Wetland Methane Emissions, LPJ-EOSIM Model v2',\n 'assets': None,\n 'extent': {'spatial': {'bbox': [[-180, -90, 180, 90]]},\n  'temporal': {'interval': [['1990-01-01T00:00:00+00:00',\n     '2024-02-27T00:00:00+00:00']]}},\n 'license': 'CC0 1.0',\n 'keywords': None,\n 'providers': [{'url': None,\n   'name': 'NASA',\n   'roles': None,\n   'description': None}],\n 'summaries': {'datetime': ['1990-01-01T00:00:00Z', '2024-02-27T00:00:00Z']},\n 'description': 'Global, daily estimates of methane (CH4) emissions from terrestrial wetlands at 0.5 x 0.5 degree spatial resolution using the Earth Observation SIMulator version (LPJ-EOSIM) of the Lund-Potsdam-Jena Dynamic Global Vegetation Model (LPJ-DGVM). Methane emissions from vegetated wetlands are estimated to be the largest natural source of methane in the global CH4 budget, contributing to roughly one third of the total of natural and anthropogenic emissions. Wetland CH4 is produced by microbes breaking down organic matter in the oxygen deprived environment of inundated soils. Due to limited data availability, the details of the role of wetland CH4 emissions have thus far been underrepresented. The LPJ-EOSIM model estimates wetland methane emissions by simulating wetland extent and using characteristics of these inundated areas such as soil moisture, temperature, and carbon content to estimate CH4 quantities emitted into the atmosphere. Input climate forcing data comes from Modern-Era Retrospective analysis for Research and Applications Version 2 (MERRA-2) data and ECMWF Re-Analysis data (ERA5). An ensemble layer provides the result of the mean of the MERRA-2 and ERA5 layers.',\n 'item_assets': {'era5-ch4-wetlands-emissions': {'type': 'image/tiff; application=geotiff; profile=cloud-optimized',\n   'roles': ['data', 'layer'],\n   'title': 'Wetland Methane Emissions, ERA5 LPJ-EOSIM Model v2',\n   'description': 'Methane emissions from wetlands in units of grams of methane per meter squared per day. ECMWF Re-Analysis (ERA5) as input to LPJ-EOSIM model.'},\n  'merra2-ch4-wetlands-emissions': {'type': 'image/tiff; application=geotiff; profile=cloud-optimized',\n   'roles': ['data', 'layer'],\n   'title': 'Wetland Methane Emissions, MERRA-2 LPJ-EOSIM Model v2',\n   'description': 'Methane emissions from wetlands in units of grams of methane per meter squared per day. Modern-Era Retrospective analysis for Research and Applications Version 2 (MERRA-2) data as input to LPJ-EOSIM model.'},\n  'ensemble-mean-ch4-wetlands-emissions': {'type': 'image/tiff; application=geotiff; profile=cloud-optimized',\n   'roles': ['data', 'layer'],\n   'title': 'Wetland Methane Emissions, Ensemble Mean LPJ-EOSIM Model v2',\n   'description': 'Methane emissions from wetlands in units of grams of methane per meter squared per day. Ensemble of multiple climate forcing data sources input to LPJ-EOSIM model.'}},\n 'stac_version': '1.0.0',\n 'stac_extensions': None,\n 'dashboard:is_periodic': True,\n 'dashboard:time_density': 'day'}\n\n\nExamining the contents of our collection under summaries, we see that the data is available from January 1990 to December 2024. By looking at dashboard: time density, we can see that these observations are collected monthly.\n\n# Create a function that would search for a data collection in the US GHG Center STAC API\n\n# First, we need to define the function\n# The name of the function = \"get_item_count\"\n# The argument that will be passed through the defined function = \"collection_id\"\n\ndef get_item_count(collection_id):\n\n    # Set a counter for the number of items existing in the collection\n    count = 0\n\n    # Define the path to retrieve the granules (items) of the collection of interest in the STAC API\n    items_url = f\"{STAC_API_URL}/collections/{collection_id}/items\"\n\n    # Run a while loop to make HTTP requests until there are no more URLs associated with the collection in the STAC API\n    while True:\n\n        # Retrieve information about the granules by sending a \"get\" request to the STAC API using the defined collection path\n        response = requests.get(items_url)\n\n        # If the items do not exist, print an error message and quit the loop\n        if not response.ok:\n            print(\"error getting items\")\n            exit()\n\n        # Return the results of the HTTP response as JSON\n        stac = response.json()\n\n        # Increase the \"count\" by the number of items (granules) returned in the response\n        count += int(stac[\"context\"].get(\"returned\", 0))\n\n        # Retrieve information about the next URL associated with the collection in the STAC API (if applicable)\n        next = [link for link in stac[\"links\"] if link[\"rel\"] == \"next\"]\n\n        # Exit the loop if there are no other URLs\n        if not next:\n            break\n        \n        # Ensure the information gathered by other STAC API links associated with the collection are added to the original path\n        # \"href\" is the identifier for each of the tiles stored in the STAC API\n        items_url = next[0][\"href\"]\n        # temp = items_url.split('/')\n        # temp.insert(3, 'ghgcenter')\n        # temp.insert(4, 'api')\n        # temp.insert(5, 'stac')\n        # items_url = '/'.join(temp)\n\n    # Return the information about the total number of granules found associated with the collection\n    return count\n\n\n# Apply the function created above \"get_item_count\" to the data collection\nnumber_of_items = get_item_count(collection_name)\n\n# Get the information about the number of granules found in the collection\nitems = requests.get(f\"{STAC_API_URL}/collections/{collection_name}/items?limit=800\"\n).json()[\"features\"]\n\n# Print the total number of items (granules) found\nprint(f\"Found {len(items)} items\")\n\nFound 800 items\n\n\n\n# Examine the first item in the collection\n# Keep in mind that a list starts from 0, 1, 2... therefore items[0] is referring to the first item in the list/collection\nitems[0]\n\nBelow, we are entering the minimum and maximum values to provide our upper and lower bounds in the rescale_values.\n\n# Fetch the minimum and maximum values for rescaling\nrescale_values = {'max': 0.0003, 'min': 0.0}",
    "crumbs": [
      "Data Usage Notebooks",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "Wetland Methane Emissions, LPJ-EOSIM Model"
    ]
  },
  {
    "objectID": "user_data_notebooks/lpjeosim-wetlandch4-grid-v2_User_Notebook.html#explore-changes-in-methane-ch4-emission-levels-using-the-raster-api",
    "href": "user_data_notebooks/lpjeosim-wetlandch4-grid-v2_User_Notebook.html#explore-changes-in-methane-ch4-emission-levels-using-the-raster-api",
    "title": "Wetland Methane Emissions, LPJ-EOSIM Model",
    "section": "Explore Changes in Methane (CH4) Emission Levels Using the Raster API",
    "text": "Explore Changes in Methane (CH4) Emission Levels Using the Raster API\nIn this notebook, we will explore the temporal impacts of methane emissions. We will visualize the outputs on a map using folium.\n\n# Now we create a dictionary where the start datetime values for each granule is queried more explicitly by year and month (e.g., 2020-02)\nitems = {item[\"properties\"][\"datetime\"][:10]: item for item in items} \n\nNow, we will pass the item id, collection name, and rescaling_factor to the Raster API endpoint. We will do this twice, once for date 1 mentioned in the next cell and again for date 2, so we can visualize each event independently.\n\n# Choose a color for displaying the tiles\n# Please refer to matplotlib library if you'd prefer choosing a different color ramp.\n# For more information on Colormaps in Matplotlib, please visit https://matplotlib.org/stable/users/explain/colors/colormaps.html\ncolor_map = \"magma\" \n\n# Make a GET request to retrieve information for the date mentioned below\ndate1 = '2024-01-01'\ndate1_tile = requests.get(\n\n    # Pass the collection name, collection date, and its ID\n    # To change the year, month and date of the observed parameter, you can modify the date2 variable above\n    f\"{RASTER_API_URL}/stac/tilejson.json?collection={items[date1]['collection']}&item={items[date1]['id']}\"\n\n    # Pass the asset name\n    f\"&assets={asset_name}\"\n\n    # Pass the color formula and colormap for custom visualization\n    f\"&color_formula=gamma+r+1.05&colormap_name={color_map}\"\n\n    # Pass the minimum and maximum values for rescaling\n    f\"&rescale={rescale_values['min']},{rescale_values['max']}\", \n\n# Return response in JSON format\n).json()\n\n# Print the properties of the retrieved granule to the console\ndate1_tile\n\n{'tilejson': '2.2.0',\n 'version': '1.0.0',\n 'scheme': 'xyz',\n 'tiles': ['https://earth.gov/ghgcenter/api/raster/stac/tiles/WebMercatorQuad/{z}/{x}/{y}@1x?collection=lpjeosim-wetlandch4-daygrid-v2&item=lpjeosim-wetlandch4-daygrid-v2-20240101day&assets=ensemble-mean-ch4-wetlands-emissions&color_formula=gamma+r+1.05&colormap_name=magma&rescale=0.0%2C0.0003'],\n 'minzoom': 0,\n 'maxzoom': 24,\n 'bounds': [-180.0, -90.0, 180.0, 90.0],\n 'center': [0.0, 0.0, 0]}\n\n\n\n# Make a GET request to retrieve information for date mentioned below\ndate2 = '2024-01-30'\ndate2_tile = requests.get(\n\n    # Pass the collection name, collection date, and its ID\n    # To change the year, month and date of the observed parameter, you can modify the date2 variable above\n    f\"{RASTER_API_URL}/stac/tilejson.json?collection={items[date2]['collection']}&item={items[date2]['id']}\"\n\n    # Pass the asset name\n    f\"&assets={asset_name}\"\n\n    # Pass the color formula and colormap for custom visualization\n    f\"&color_formula=gamma+r+1.05&colormap_name={color_map}\"\n\n    # Pass the minimum and maximum values for rescaling\n    f\"&rescale={rescale_values['min']},{rescale_values['max']}\",\n\n# Return response in JSON format \n).json()\n\n# Print the properties of the retrieved granule to the console\ndate2_tile\n\n{'tilejson': '2.2.0',\n 'version': '1.0.0',\n 'scheme': 'xyz',\n 'tiles': ['https://earth.gov/ghgcenter/api/raster/stac/tiles/WebMercatorQuad/{z}/{x}/{y}@1x?collection=lpjeosim-wetlandch4-daygrid-v2&item=lpjeosim-wetlandch4-daygrid-v2-20240130day&assets=ensemble-mean-ch4-wetlands-emissions&color_formula=gamma+r+1.05&colormap_name=magma&rescale=0.0%2C0.0003'],\n 'minzoom': 0,\n 'maxzoom': 24,\n 'bounds': [-180.0, -90.0, 180.0, 90.0],\n 'center': [0.0, 0.0, 0]}",
    "crumbs": [
      "Data Usage Notebooks",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "Wetland Methane Emissions, LPJ-EOSIM Model"
    ]
  },
  {
    "objectID": "user_data_notebooks/lpjeosim-wetlandch4-grid-v2_User_Notebook.html#visualize-ch₄-emissions",
    "href": "user_data_notebooks/lpjeosim-wetlandch4-grid-v2_User_Notebook.html#visualize-ch₄-emissions",
    "title": "Wetland Methane Emissions, LPJ-EOSIM Model",
    "section": "Visualize CH₄ Emissions",
    "text": "Visualize CH₄ Emissions\n\n# For this study we are going to compare the CH₄ Emissions in date1 and date2 along the coast of California\n# To change the location, you can simply insert the latitude and longitude of the area of your interest in the \"location=(LAT, LONG)\" statement\n\n# Set initial zoom and center of map\n# 'folium.plugins' allows mapping side-by-side\nmap_ = folium.plugins.DualMap(location=(34, -118), zoom_start=6)\n\n# Define the first map layer for tile fetched for date 1\n# The TileLayer library helps in manipulating and displaying raster layers on a map\nmap_layer_date1 = TileLayer(\n    tiles=date1_tile[\"tiles\"][0], # Path to retrieve the tile\n    attr=\"GHG\", # Set the attribution\n    opacity=0.5, # Adjust the transparency of the layer\n)\n\n# Add the first layer to the Dual Map\nmap_layer_date1.add_to(map_.m1)\n\n\n# Define the second map layer for the tile fetched for date 2\nmap_layer_date2 = TileLayer(\n    tiles=date2_tile[\"tiles\"][0], # Path to retrieve the tile\n    attr=\"GHG\", # Set the attribution\n    opacity=0.5, # Adjust the transparency of the layer\n)\n\n# Add the second layer to the Dual Map\nmap_layer_date2.add_to(map_.m2)\n\n# Visualize the Dual Map\nmap_\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook",
    "crumbs": [
      "Data Usage Notebooks",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "Wetland Methane Emissions, LPJ-EOSIM Model"
    ]
  },
  {
    "objectID": "user_data_notebooks/lpjeosim-wetlandch4-grid-v2_User_Notebook.html#visualize-the-data-as-a-time-series",
    "href": "user_data_notebooks/lpjeosim-wetlandch4-grid-v2_User_Notebook.html#visualize-the-data-as-a-time-series",
    "title": "Wetland Methane Emissions, LPJ-EOSIM Model",
    "section": "Visualize the Data as a Time Series",
    "text": "Visualize the Data as a Time Series\nWe can now explore the wetland methane emissions time series (January 1990 – December 2024) available for the Texas area of the U.S. We can plot the data set using the code below:\n\n# Determine the width and height of the plot using the 'matplotlib' library\n# Figure size: 20 representing the width, 10 representing the height\nfig = plt.figure(figsize=(20, 10))\n\n# Plot the time series\nplt.plot(\n    df[\"date\"], # X-axis: date\n    df[\"max\"], # Y-axis: CH₄ value\n    color=\"red\", # Line color\n    linestyle=\"-\", # Line style\n    linewidth=0.5, # Line width\n    label=\"Max daily CH₄ emissions\", # Legend label\n)\n\n# Display legend\nplt.legend()\n\n# Insert label for the X-axis\nplt.xlabel(\"Years\")\n\n# Insert label for the Y-axis\nplt.ylabel(\"Daily CH4 emissions g/m2\")\n\n# Insert title for the plot\nplt.title(\"Daily CH4 emission Values for Texas, January 2022- March 2024\")\n\nText(0.5, 1.0, 'Daily CH4 emission Values for Texas, January 2022- March 2024')\n\n\n\n\n\n\n\n\n\nTo take a closer look at the CH4 variability across this region, we are going to retrieve and display data collected during the February, 2024 observation.\n\n# The 2024-02-25 observation is the 3rd item in the list\n# Considering that a list starts with \"0\", we need to insert \"2\" in the \"items[2]\" statement\n# Print the start Date Time of the third granule in the collection\nprint(items[2][\"properties\"][\"datetime\"])\n\n2024-02-25T00:00:00+00:00\n\n\n\n# A GET request is made for the 3rd item in the collection\nobserved_tile = requests.get(\n\n    # Pass the collection name, the item number in the list, and its ID\n    f\"{RASTER_API_URL}/stac/tilejson.json?collection={items[2]['collection']}&item={items[2]['id']}&assets={asset_name}\"\n\n    # Pass the color formula and colormap for custom visualization\n    f\"&color_formula=gamma+r+1.05&colormap_name={color_map}\"\n\n    # Pass the minimum and maximum values for rescaling\n    f\"&rescale={rescale_values['min']},{rescale_values['max']}\",\n\n# Return the response in JSON format\n).json()\n\n# Print the properties of the retrieved granule to the console\nobserved_tile\n\n{'tilejson': '2.2.0',\n 'version': '1.0.0',\n 'scheme': 'xyz',\n 'tiles': ['https://earth.gov/ghgcenter/api/raster/stac/tiles/WebMercatorQuad/{z}/{x}/{y}@1x?collection=lpjeosim-wetlandch4-daygrid-v2&item=lpjeosim-wetlandch4-daygrid-v2-20240225day&assets=ensemble-mean-ch4-wetlands-emissions&color_formula=gamma+r+1.05&colormap_name=magma&rescale=0.0%2C0.0003'],\n 'minzoom': 0,\n 'maxzoom': 24,\n 'bounds': [-180.0, -90.0, 180.0, 90.0],\n 'center': [0.0, 0.0, 0]}\n\n\n\n# Create a new map to display the CH4 variability for the Texas region for Observed tile timeframe\naoi_map_bbox = Map(\n\n    # Base map is set to OpenStreetMap\n    tiles=\"OpenStreetMap\",\n\n    # Set the center of the map\n    location=[\n        30,-100\n    ],\n\n    # Set the zoom value\n    zoom_start=8,\n)\n\n# Define the map layer\nmap_layer = TileLayer(\n    tiles=observed_tile[\"tiles\"][0], # Path to retrieve the tile\n    attr=\"GHG\", opacity = 0.5 # Set the attribution and transparency\n)\n\n# Add the layer to the map\nmap_layer.add_to(aoi_map_bbox)\n\n# Visualize the map\naoi_map_bbox\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook",
    "crumbs": [
      "Data Usage Notebooks",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "Wetland Methane Emissions, LPJ-EOSIM Model"
    ]
  },
  {
    "objectID": "user_data_notebooks/lpjeosim-wetlandch4-grid-v2_User_Notebook.html#summary",
    "href": "user_data_notebooks/lpjeosim-wetlandch4-grid-v2_User_Notebook.html#summary",
    "title": "Wetland Methane Emissions, LPJ-EOSIM Model",
    "section": "Summary",
    "text": "Summary\nIn this notebook we have successfully completed the following steps for the STAC collection for the Daily Wetland Methane Emissions, LPJ-EOSIM Model data: 1. Install and import the necessary libraries 2. Fetch the collection from STAC collections using the appropriate endpoints 3. Count the number of existing granules within the collection 4. Map and compare the CH4 levels over the Texas region for two distinctive years 5. Create a table that displays the minimum, maximum, and sum of the CH4 levels for a specified region 6. Generate a time-series graph of the CH4 levels for a specified region\nIf you have any questions regarding this user notebook, please contact us using the feedback form.",
    "crumbs": [
      "Data Usage Notebooks",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "Wetland Methane Emissions, LPJ-EOSIM Model"
    ]
  },
  {
    "objectID": "user_data_notebooks/lpjeosim-wetlandch4-monthgrid-v2_User_Notebook.html",
    "href": "user_data_notebooks/lpjeosim-wetlandch4-monthgrid-v2_User_Notebook.html",
    "title": "Wetland Methane Emissions, LPJ-EOSIM Model",
    "section": "",
    "text": "You can launch this notebook in the US GHG Center JupyterHub by clicking the link below.\nLaunch in the US GHG Center JupyterHub (requires access)"
  },
  {
    "objectID": "user_data_notebooks/lpjeosim-wetlandch4-monthgrid-v2_User_Notebook.html#run-this-notebook",
    "href": "user_data_notebooks/lpjeosim-wetlandch4-monthgrid-v2_User_Notebook.html#run-this-notebook",
    "title": "Wetland Methane Emissions, LPJ-EOSIM Model",
    "section": "",
    "text": "You can launch this notebook in the US GHG Center JupyterHub by clicking the link below.\nLaunch in the US GHG Center JupyterHub (requires access)"
  },
  {
    "objectID": "user_data_notebooks/lpjeosim-wetlandch4-monthgrid-v2_User_Notebook.html#approach",
    "href": "user_data_notebooks/lpjeosim-wetlandch4-monthgrid-v2_User_Notebook.html#approach",
    "title": "Wetland Methane Emissions, LPJ-EOSIM Model",
    "section": "Approach",
    "text": "Approach\n\nIdentify available dates and temporal frequency of observations for the given collection using the GHGC API /stac endpoint. The collection processed in this notebook is the Wetland Methane Emissions, LPJ-EOSIM Model data product.\nPass the STAC item into the raster API /stac/tilejson.jsonendpoint.\nUsing folium.plugins.DualMap, visualize two tiles (side-by-side), allowing time point comparison.\nAfter the visualization, perform zonal statistics for a given polygon."
  },
  {
    "objectID": "user_data_notebooks/lpjeosim-wetlandch4-monthgrid-v2_User_Notebook.html#about-the-data",
    "href": "user_data_notebooks/lpjeosim-wetlandch4-monthgrid-v2_User_Notebook.html#about-the-data",
    "title": "Wetland Methane Emissions, LPJ-EOSIM Model",
    "section": "About the Data",
    "text": "About the Data\nMethane (CH₄) emissions from vegetated wetlands are estimated to be the largest natural source of methane in the global CH₄ budget, contributing to roughly one third of the total of natural and anthropogenic emissions. Wetland CH₄ is produced by microbes breaking down organic matter in the oxygen deprived environment of inundated soils. Due to limited data availability, the details of the role of wetland CH₄ emissions have thus far been underrepresented. Using the Earth Observation SIMulator version (LPJ-EOSIM) of the Lund-Potsdam-Jena Dynamic Global Vegetation Model (LPJ-DGVM) global CH₄ emissions from wetlands are estimated at 0.5° x 0.5 degree spatial resolution. By simulating wetland extent and using characteristics of inundated areas, such as wetland soil moisture, temperature, and carbon content, the model provides estimates of CH₄ quantities emitted into the atmosphere. This dataset shows concentrated methane sources from tropical and high latitude ecosystems. The LPJ-EOSIM Wetland Methane Emissions dataset consists of global daily model estimates of terrestrial wetland methane emissions from 1990 to the present, with data added bimonthly. The monthly data has been curated by aggregating the daily files. The estimates are regularly used in conjunction with NASA’s Goddard Earth Observing System (GEOS) model to simulate the impact of wetlands and other methane sources on atmospheric methane concentrations, to compare against satellite and airborne data, and to improve understanding and prediction of wetland emissions.\nFor more information regarding this dataset, please visit the U.S. Greenhouse Gas Center."
  },
  {
    "objectID": "user_data_notebooks/lpjeosim-wetlandch4-monthgrid-v2_User_Notebook.html#query-the-stac-api",
    "href": "user_data_notebooks/lpjeosim-wetlandch4-monthgrid-v2_User_Notebook.html#query-the-stac-api",
    "title": "Wetland Methane Emissions, LPJ-EOSIM Model",
    "section": "Query the STAC API",
    "text": "Query the STAC API\nFirst, we are going to import the required libraries. Once imported, they allow better executing a query in the GHG Center Spatio Temporal Asset Catalog (STAC) Application Programming Interface (API) where the granules for this collection are stored.\n\n# Import the following libraries\nimport requests\nimport folium\nimport folium.plugins\nfrom folium import Map, TileLayer\nfrom pystac_client import Client\nimport branca\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n\n# Provide the STAC and RASTER API endpoints\n# The endpoint is referring to a location within the API that executes a request on a data collection nesting on the server.\n\n# The STAC API is a catalog of all the existing data collections that are stored in the GHG Center.\nSTAC_API_URL = \"https://earth.gov/ghgcenter/api/stac\"\n\n# The RASTER API is used to fetch collections for visualization\nRASTER_API_URL = \"https://earth.gov/ghgcenter/api/raster\"\n\n# The collection name is used to fetch the dataset from the STAC API. First, we define the collection name as a variable\n# Name of the collection for the wetland methane emissions LPJ-EOSIM Model\ncollection_name = \"lpjeosim-wetlandch4-monthgrid-v2\"\n\n# Next, we need to specify the asset name for this collection\n# The asset name is referring to the raster band containing the pixel values for the parameter of interest\nasset_name = \"ensemble-mean-ch4-wetlands-emissions\"\n\n\n# Fetch the collection from the STAC API using the appropriate endpoint\n# The 'requests' library allows a HTTP request possible\ncollection = requests.get(f\"{STAC_API_URL}/collections/{collection_name}\").json()\n\n# Print the properties of the collection to the console\ncollection\n\nExamining the contents of our collection under summaries, we see that the data is available from January 1990 to December 2024. By looking at dashboard: time density, we can see that these observations are collected monthly.\n\n# Create a function that would search for a data collection in the US GHG Center STAC API\n\n# First, we need to define the function\n# The name of the function = \"get_item_count\"\n# The argument that will be passed through the defined function = \"collection_id\"\n\ndef get_item_count(collection_id):\n\n    # Set a counter for the number of items existing in the collection\n    count = 0\n\n    # Define the path to retrieve the granules (items) of the collection of interest in the STAC API\n    items_url = f\"{STAC_API_URL}/collections/{collection_id}/items\"\n\n    # Run a while loop to make HTTP requests until there are no more URLs associated with the collection in the STAC API\n    while True:\n\n        # Retrieve information about the granules by sending a \"get\" request to the STAC API using the defined collection path\n        response = requests.get(items_url)\n\n        # If the items do not exist, print an error message and quit the loop\n        if not response.ok:\n            print(\"error getting items\")\n            exit()\n\n        # Return the results of the HTTP response as JSON\n        stac = response.json()\n\n        # Increase the \"count\" by the number of items (granules) returned in the response\n        count += int(stac[\"context\"].get(\"returned\", 0))\n\n        # Retrieve information about the next URL associated with the collection in the STAC API (if applicable)\n        next = [link for link in stac[\"links\"] if link[\"rel\"] == \"next\"]\n\n        # Exit the loop if there are no other URLs\n        if not next:\n            break\n        \n        # Ensure the information gathered by other STAC API links associated with the collection are added to the original path\n        # \"href\" is the identifier for each of the tiles stored in the STAC API\n        items_url = next[0][\"href\"]\n        # temp = items_url.split('/')\n        # temp.insert(3, 'ghgcenter')\n        # temp.insert(4, 'api')\n        # temp.insert(5, 'stac')\n        # items_url = '/'.join(temp)\n\n    # Return the information about the total number of granules found associated with the collection\n    return count\n\n\n# Apply the function created above \"get_item_count\" to the data collection\nnumber_of_items = get_item_count(collection_name)\n\n# Get the information about the number of granules found in the collection\nitems = requests.get(f\"{STAC_API_URL}/collections/{collection_name}/items?limit={number_of_items}\"\n).json()[\"features\"]\n\n# Print the total number of items (granules) found\nprint(f\"Found {len(items)} items\")\n\nFound 409 items\n\n\n\n# Examine the first item in the collection\n# Keep in mind that a list starts from 0, 1, 2... therefore items[0] is referring to the first item in the list/collection\nitems[0]\n\nBelow, we are entering the minimum and maximum values to provide our upper and lower bounds in the rescale_values.\n\n# Fetch the minimum and maximum values for rescaling\nrescale_values = {'max': 0.0003, 'min': 0.0}"
  },
  {
    "objectID": "user_data_notebooks/lpjeosim-wetlandch4-monthgrid-v2_User_Notebook.html#explore-changes-in-methane-ch4-emission-levels-using-the-raster-api",
    "href": "user_data_notebooks/lpjeosim-wetlandch4-monthgrid-v2_User_Notebook.html#explore-changes-in-methane-ch4-emission-levels-using-the-raster-api",
    "title": "Wetland Methane Emissions, LPJ-EOSIM Model",
    "section": "Explore Changes in Methane (CH4) Emission Levels Using the Raster API",
    "text": "Explore Changes in Methane (CH4) Emission Levels Using the Raster API\nIn this notebook, we will explore the temporal impacts of methane emissions. We will visualize the outputs on a map using folium.\n\n# Now we create a dictionary where the start datetime values for each granule is queried more explicitly by year and month (e.g., 2020-02)\nitems = {item[\"properties\"][\"start_datetime\"][:7]: item for item in items} \n\nNow, we will pass the item id, collection name, and rescaling_factor to the Raster API endpoint. We will do this twice, once for month 1 mentioned in the next cell and again for month 2, so we can visualize each event independently.\n\n# Choose a color for displaying the tiles\n# Please refer to matplotlib library if you'd prefer choosing a different color ramp.\n# For more information on Colormaps in Matplotlib, please visit https://matplotlib.org/stable/users/explain/colors/colormaps.html\ncolor_map = \"magma\" \n\n# Make a GET request to retrieve information for the date mentioned below\nmonth1 = '1990-01'\nmonth1_tile = requests.get(\n\n    # Pass the collection name, collection date, and its ID\n    # To change the year and month of the observed parameter, you can modify month mentioned above.\n    f\"{RASTER_API_URL}/stac/tilejson.json?collection={items[month1]['collection']}&item={items[month1]['id']}\"\n\n    # Pass the asset name\n    f\"&assets={asset_name}\"\n\n    # Pass the color formula and colormap for custom visualization\n    f\"&color_formula=gamma+r+1.05&colormap_name={color_map}\"\n\n    # Pass the minimum and maximum values for rescaling\n    f\"&rescale={rescale_values['min']},{rescale_values['max']}\", \n\n# Return response in JSON format\n).json()\n\n# Print the properties of the retrieved granule to the console\nmonth1_tile\n\n{'tilejson': '2.2.0',\n 'version': '1.0.0',\n 'scheme': 'xyz',\n 'tiles': ['https://earth.gov/ghgcenter/api/raster/stac/tiles/WebMercatorQuad/{z}/{x}/{y}@1x?collection=lpjeosim-wetlandch4-monthgrid-v2&item=lpjeosim-wetlandch4-monthgrid-v2-199001&assets=ensemble-mean-ch4-wetlands-emissions&color_formula=gamma+r+1.05&colormap_name=magma&rescale=0.0%2C0.0003'],\n 'minzoom': 0,\n 'maxzoom': 24,\n 'bounds': [-180.0, -90.0, 180.0, 90.0],\n 'center': [0.0, 0.0, 0]}\n\n\n\n# Make a GET request to retrieve information for date mentioned below\nmonth2 = '1990-08'\nmonth2_tile = requests.get(\n\n    # Pass the collection name, collection date, and its ID\n    # To change the year and month of the observed parameter, you can modify the month mentioned above.\n    f\"{RASTER_API_URL}/stac/tilejson.json?collection={items[month2]['collection']}&item={items[month2]['id']}\"\n\n    # Pass the asset name\n    f\"&assets={asset_name}\"\n\n    # Pass the color formula and colormap for custom visualization\n    f\"&color_formula=gamma+r+1.05&colormap_name={color_map}\"\n\n    # Pass the minimum and maximum values for rescaling\n    f\"&rescale={rescale_values['min']},{rescale_values['max']}\",\n\n# Return response in JSON format \n).json()\n\n# Print the properties of the retrieved granule to the console\nmonth2_tile\n\n{'tilejson': '2.2.0',\n 'version': '1.0.0',\n 'scheme': 'xyz',\n 'tiles': ['https://earth.gov/ghgcenter/api/raster/stac/tiles/WebMercatorQuad/{z}/{x}/{y}@1x?collection=lpjeosim-wetlandch4-monthgrid-v2&item=lpjeosim-wetlandch4-monthgrid-v2-199008&assets=ensemble-mean-ch4-wetlands-emissions&color_formula=gamma+r+1.05&colormap_name=magma&rescale=0.0%2C0.0003'],\n 'minzoom': 0,\n 'maxzoom': 24,\n 'bounds': [-180.0, -90.0, 180.0, 90.0],\n 'center': [0.0, 0.0, 0]}"
  },
  {
    "objectID": "user_data_notebooks/lpjeosim-wetlandch4-monthgrid-v2_User_Notebook.html#visualize-ch₄-emissions",
    "href": "user_data_notebooks/lpjeosim-wetlandch4-monthgrid-v2_User_Notebook.html#visualize-ch₄-emissions",
    "title": "Wetland Methane Emissions, LPJ-EOSIM Model",
    "section": "Visualize CH₄ Emissions",
    "text": "Visualize CH₄ Emissions\n\n# For this study we are going to compare the CH₄ Emissions for month1 and month2 along the coast of California\n# To change the location, you can simply insert the latitude and longitude of the area of your interest in the \"location=(LAT, LONG)\" statement\n\n# Set initial zoom and center of map\n# 'folium.plugins' allows mapping side-by-side\nmap_ = folium.plugins.DualMap(location=(34, -118), zoom_start=6)\n\n# Define the first map layer for tile fetched for month 1\n# The TileLayer library helps in manipulating and displaying raster layers on a map\nmap_layer_month1 = TileLayer(\n    tiles=month1_tile[\"tiles\"][0], # Path to retrieve the tile\n    attr=\"GHG\", # Set the attribution\n    opacity=0.5, # Adjust the transparency of the layer\n)\n\n# Add the first layer to the Dual Map\nmap_layer_month1.add_to(map_.m1)\n\n\n# Define the second map layer for the tile fetched for month 2\nmap_layer_month2 = TileLayer(\n    tiles=month2_tile[\"tiles\"][0], # Path to retrieve the tile\n    attr=\"GHG\", # Set the attribution\n    opacity=0.5, # Adjust the transparency of the layer\n)\n\n# Add the second layer to the Dual Map\nmap_layer_month2.add_to(map_.m2)\n\n# Visualize the Dual Map\nmap_\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook"
  },
  {
    "objectID": "user_data_notebooks/lpjeosim-wetlandch4-monthgrid-v2_User_Notebook.html#visualize-the-data-as-a-time-series",
    "href": "user_data_notebooks/lpjeosim-wetlandch4-monthgrid-v2_User_Notebook.html#visualize-the-data-as-a-time-series",
    "title": "Wetland Methane Emissions, LPJ-EOSIM Model",
    "section": "Visualize the Data as a Time Series",
    "text": "Visualize the Data as a Time Series\nWe can now explore the wetland methane emissions time series (January 1990 – December 2024) available for the Texas area of the U.S. We can plot the data set using the code below:\n\n# Determine the width and height of the plot using the 'matplotlib' library\n# Figure size: 20 representing the width, 10 representing the height\nfig = plt.figure(figsize=(20, 10))\n\n# Plot the time series\nplt.plot(\n    df[\"date\"], # X-axis: date\n    df[\"max\"], # Y-axis: CH₄ value\n    color=\"red\", # Line color\n    linestyle=\"-\", # Line style\n    linewidth=0.5, # Line width\n    label=\"Max monthly CH₄ emissions\", # Legend label\n)\n\n# Display legend\nplt.legend()\n\n# Insert label for the X-axis\nplt.xlabel(\"Years\")\n\n# Insert label for the Y-axis\nplt.ylabel(\"Monthly CH4 emissions g/m2\")\n\n# Insert title for the plot\nplt.title(\"Monthly CH4 emission Values for Texas, 1990-2024\")\n\nText(0.5, 1.0, 'Monthly CH4 emission Values for Texas, 1990-2024')\n\n\n\n\n\n\n\n\n\nTo take a closer look at the CH4 variability across this region, we are going to retrieve and display data collected for the observation mentioned below.\n\n# The 2023-11-01 observation is the 3rd item in the list\n# Considering that a list starts with \"0\", we need to insert \"2\" in the \"items[2]\" statement\n# Print the start Date Time of the third granule in the collection\nprint(items[2][\"properties\"][\"start_datetime\"])\n\n2023-11-01T00:00:00+00:00\n\n\n\n# A GET request is made for the 3rd item in the collection\nobserved_tile = requests.get(\n\n    # Pass the collection name, the item number in the list, and its ID\n    f\"{RASTER_API_URL}/stac/tilejson.json?collection={items[2]['collection']}&item={items[2]['id']}&assets={asset_name}\"\n\n    # Pass the color formula and colormap for custom visualization\n    f\"&color_formula=gamma+r+1.05&colormap_name={color_map}\"\n\n    # Pass the minimum and maximum values for rescaling\n    f\"&rescale={rescale_values['min']},{rescale_values['max']}\",\n\n# Return the response in JSON format\n).json()\n\n# Print the properties of the retrieved granule to the console\nobserved_tile\n\n{'tilejson': '2.2.0',\n 'version': '1.0.0',\n 'scheme': 'xyz',\n 'tiles': ['https://earth.gov/ghgcenter/api/raster/stac/tiles/WebMercatorQuad/{z}/{x}/{y}@1x?collection=lpjeosim-wetlandch4-monthgrid-v2&item=lpjeosim-wetlandch4-monthgrid-v2-202311&assets=ensemble-mean-ch4-wetlands-emissions&color_formula=gamma+r+1.05&colormap_name=magma&rescale=0.0%2C0.0003'],\n 'minzoom': 0,\n 'maxzoom': 24,\n 'bounds': [-180.0, -90.0, 180.0, 90.0],\n 'center': [0.0, 0.0, 0]}\n\n\n\n# Create a new map to display the CH4 variability for the Texas region for the time in previous cell.\naoi_map_bbox = Map(\n\n    # Base map is set to OpenStreetMap\n    tiles=\"OpenStreetMap\",\n\n    # Set the center of the map\n    location=[\n        30,-100\n    ],\n\n    # Set the zoom value\n    zoom_start=8,\n)\n\n# Define the map layer\nmap_layer = TileLayer(\n    tiles=observed_tile[\"tiles\"][0], # Path to retrieve the tile\n    attr=\"GHG\", opacity = 0.5 # Set the attribution and transparency\n)\n\n# Add the layer to the map\nmap_layer.add_to(aoi_map_bbox)\n\n# Visualize the map\naoi_map_bbox\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook"
  },
  {
    "objectID": "user_data_notebooks/lpjeosim-wetlandch4-monthgrid-v2_User_Notebook.html#summary",
    "href": "user_data_notebooks/lpjeosim-wetlandch4-monthgrid-v2_User_Notebook.html#summary",
    "title": "Wetland Methane Emissions, LPJ-EOSIM Model",
    "section": "Summary",
    "text": "Summary\nIn this notebook we have successfully completed the following steps for the STAC collection for the Monthly Wetland Methane Emissions, LPJ-EOSIM Model data: 1. Install and import the necessary libraries 2. Fetch the collection from STAC collections using the appropriate endpoints 3. Count the number of existing granules within the collection 4. Map and compare the CH4 levels over the Texas region for two distinctive years 5. Create a table that displays the minimum, maximum, and sum of the CH4 levels for a specified region 6. Generate a time-series graph of the CH4 levels for a specified region\nIf you have any questions regarding this user notebook, please contact us using the feedback form."
  },
  {
    "objectID": "user_data_notebooks/epa-ch4emission-grid-v2express_User_Notebook.html",
    "href": "user_data_notebooks/epa-ch4emission-grid-v2express_User_Notebook.html",
    "title": "U.S. Gridded Anthropogenic Methane Emissions Inventory",
    "section": "",
    "text": "You can launch this notebook in the US GHG Center JupyterHub by clicking the link below.\nLaunch in the US GHG Center JupyterHub (requires access)",
    "crumbs": [
      "Data Usage Notebooks",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "U.S. Gridded Anthropogenic Methane Emissions Inventory"
    ]
  },
  {
    "objectID": "user_data_notebooks/epa-ch4emission-grid-v2express_User_Notebook.html#run-this-notebook",
    "href": "user_data_notebooks/epa-ch4emission-grid-v2express_User_Notebook.html#run-this-notebook",
    "title": "U.S. Gridded Anthropogenic Methane Emissions Inventory",
    "section": "",
    "text": "You can launch this notebook in the US GHG Center JupyterHub by clicking the link below.\nLaunch in the US GHG Center JupyterHub (requires access)",
    "crumbs": [
      "Data Usage Notebooks",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "U.S. Gridded Anthropogenic Methane Emissions Inventory"
    ]
  },
  {
    "objectID": "user_data_notebooks/epa-ch4emission-grid-v2express_User_Notebook.html#approach",
    "href": "user_data_notebooks/epa-ch4emission-grid-v2express_User_Notebook.html#approach",
    "title": "U.S. Gridded Anthropogenic Methane Emissions Inventory",
    "section": "Approach",
    "text": "Approach\n\nIdentify available dates and temporal frequency of observations for the given collection using the GHGC API /stac endpoint. The collection processed in this notebook is the gridded methane emissions data product.\nPass the STAC item into the raster API /stac/tilejson.jsonendpoint.\nUsing folium.plugins.DualMap, we will visualize two tiles (side-by-side), allowing us to compare time points.\nAfter the visualization, we will perform zonal statistics for a given polygon.",
    "crumbs": [
      "Data Usage Notebooks",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "U.S. Gridded Anthropogenic Methane Emissions Inventory"
    ]
  },
  {
    "objectID": "user_data_notebooks/epa-ch4emission-grid-v2express_User_Notebook.html#about-the-data",
    "href": "user_data_notebooks/epa-ch4emission-grid-v2express_User_Notebook.html#about-the-data",
    "title": "U.S. Gridded Anthropogenic Methane Emissions Inventory",
    "section": "About the Data",
    "text": "About the Data\nThe gridded EPA U.S. anthropogenic methane greenhouse gas inventory (gridded GHGI) includes spatially disaggregated (0.1 deg x 0.1 deg or approximately 10 x 10 km resolution) maps of annual anthropogenic methane emissions (for the contiguous United States (CONUS), consistent with national annual U.S. anthropogenic methane emissions reported in the U.S. EPA Inventory of U.S. Greenhouse Gas Emissions and Sinks (U.S. GHGI). This V2 Express Extension dataset contains methane emissions provided as fluxes, in units of molecules of methane per square cm per second, for over 25 individual emission source categories, including those from agriculture, petroleum and natural gas systems, coal mining, and waste. The data have been converted from their original NetCDF format to Cloud-Optimized GeoTIFF (COG) for use in the US GHG Center, thereby enabling user exploration of spatial anthropogenic methane emissions and their trends.\nFor more information regarding this dataset, please visit the U.S. Gridded Anthropogenic Methane Emissions Inventory data overview page.",
    "crumbs": [
      "Data Usage Notebooks",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "U.S. Gridded Anthropogenic Methane Emissions Inventory"
    ]
  },
  {
    "objectID": "user_data_notebooks/epa-ch4emission-grid-v2express_User_Notebook.html#querying-the-stac-api",
    "href": "user_data_notebooks/epa-ch4emission-grid-v2express_User_Notebook.html#querying-the-stac-api",
    "title": "U.S. Gridded Anthropogenic Methane Emissions Inventory",
    "section": "Querying the STAC API",
    "text": "Querying the STAC API\nFirst, we are going to import the required libraries. Once imported, they allow better executing a query in the GHG Center Spatio Temporal Asset Catalog (STAC) Application Programming Interface (API) where the granules for this collection are stored.\n\n# Import the following libraries\nimport requests\nimport folium\nimport folium.plugins\nfrom folium import Map, TileLayer\nfrom pystac_client import Client\nimport branca\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n\n# Provide the STAC and RASTER API endpoints\n# The endpoint is referring to a location within the API that executes a request on a data collection nesting on the server.\n\n# The STAC API is a catalog of all the existing data collections that are stored in the GHG Center.\nSTAC_API_URL = \"https://earth.gov/ghgcenter/api/stac\"\n\n# The RASTER API is used to fetch collections for visualization\nRASTER_API_URL = \"https://earth.gov/ghgcenter/api/raster\"\n\n# The collection name is used to fetch the dataset from the STAC API. First, we define the collection name as a variable\n# Name of the collection for gridded methane dataset \ncollection_name = \"epa-ch4emission-yeargrid-v2express\"\n\n\n# Fetch the collection from the STAC API using the appropriate endpoint\n# The 'requests' library allows a HTTP request possible\ncollection = requests.get(f\"{STAC_API_URL}/collections/{collection_name}\").json()\n\n# Print the properties of the collection to the console\ncollection\n\nExamining the contents of our collection under the temporal variable, we see that the data is available from January 2012 to December 2020. By looking at the dashboard:time density, we observe that the periodic frequency of these observations is yearly.\n\n# Create a function that would search for a data collection in the US GHG Center STAC API\n\n# First, we need to define the function\n# The name of the function = \"get_item_count\"\n# The argument that will be passed through the defined function = \"collection_id\"\ndef get_item_count(collection_id):\n\n    # Set a counter for the number of items existing in the collection\n    count = 0\n\n    # Define the path to retrieve the granules (items) of the collection of interest in the STAC API\n    items_url = f\"{STAC_API_URL}/collections/{collection_id}/items\"\n\n    # Run a while loop to make HTTP requests until there are no more URLs associated with the collection in the STAC API\n    while True:\n\n        # Retrieve information about the granules by sending a \"get\" request to the STAC API using the defined collection path\n        response = requests.get(items_url)\n\n        # If the items do not exist, print an error message and quit the loop\n        if not response.ok:\n            print(\"error getting items\")\n            exit()\n\n        # Return the results of the HTTP response as JSON\n        stac = response.json()\n\n        # Increase the \"count\" by the number of items (granules) returned in the response\n        count += int(stac[\"context\"].get(\"returned\", 0))\n\n        # Retrieve information about the next URL associated with the collection in the STAC API (if applicable)\n        next = [link for link in stac[\"links\"] if link[\"rel\"] == \"next\"]\n\n        # Exit the loop if there are no other URLs\n        if not next:\n            break\n        \n        # Ensure the information gathered by other STAC API links associated with the collection are added to the original path\n        # \"href\" is the identifier for each of the tiles stored in the STAC API\n        items_url = next[0][\"href\"]\n\n    # Return the information about the total number of granules found associated with the collection\n    return count\n\n\n# Apply the function created above \"get_item_count\" to the data collection\nnumber_of_items = get_item_count(collection_name)\n\n# Get the information about the number of granules found in the collection\nitems = requests.get(f\"{STAC_API_URL}/collections/{collection_name}/items?limit={number_of_items}\").json()[\"features\"]\n\n# Print the total number of items (granules) found\nprint(f\"Found {len(items)} items\")\n\nThis makes sense as there are 9 years between 2012 - 2020, meaning 9 records in total.\n\n# Examine the first item in the collection\n# Keep in mind that a list starts from 0, 1, 2... therefore items[0] is referring to the first item in the list/collection\nitems[0]",
    "crumbs": [
      "Data Usage Notebooks",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "U.S. Gridded Anthropogenic Methane Emissions Inventory"
    ]
  },
  {
    "objectID": "user_data_notebooks/epa-ch4emission-grid-v2express_User_Notebook.html#exploring-changes-in-methane-ch4-levels-using-the-raster-api",
    "href": "user_data_notebooks/epa-ch4emission-grid-v2express_User_Notebook.html#exploring-changes-in-methane-ch4-levels-using-the-raster-api",
    "title": "U.S. Gridded Anthropogenic Methane Emissions Inventory",
    "section": "Exploring Changes in Methane (CH4) Levels Using the Raster API",
    "text": "Exploring Changes in Methane (CH4) Levels Using the Raster API\nIn this notebook, we will explore the impacts of methane emissions and by examining changes over time in urban regions. We will visualize the outputs on a map using folium.\n\n# Now we create a dictionary where the start datetime values for each granule is queried more explicitly by year and month (e.g., 2020-02)\nitems = {item[\"properties\"][\"datetime\"][:7]: item for item in items} \n\n# Next, we need to specify the asset name for this collection\n# The asset name is referring to the raster band containing the pixel values for the parameter of interest\n# For the case of the U.S. Gridded Anthropogenic Methane Emissions Inventory collection, the parameter of interest is “surface-coal”\nasset_name = \"surface-coal\"\n\nBelow, we enter minimum and maximum values to provide our upper and lower bounds in rescale_values.\n\n# Fetching the min and max values for a specific item\nrescale_values = {\"max\":items[list(items.keys())[0]][\"assets\"][asset_name][\"raster:bands\"][0][\"histogram\"][\"max\"], \"min\":items[list(items.keys())[0]][\"assets\"][asset_name][\"raster:bands\"][0][\"histogram\"][\"min\"]}\n\n\nitems\n\nNow, we will pass the item id, collection name, asset name, and the rescaling factor to the Raster API endpoint. We will do this twice, once for January 2018 and again for January 2012, so that we can visualize each event independently.\n\n# Choose a color map for displaying the first observation (event)\n# Please refer to matplotlib library if you'd prefer choosing a different color ramp.\n# For more information on Colormaps in Matplotlib, please visit https://matplotlib.org/stable/users/explain/colors/colormaps.html\ncolor_map = \"rainbow\" \n\n# Make a GET request to retrieve information for the 2018 tile \njanuary_2018_tile = requests.get(\n\n    # Pass the collection name, the item number in the list, and its ID\n    f\"{RASTER_API_URL}/stac/tilejson.json?collection={items['2018-01']['collection']}&item={items['2018-01']['id']}\"\n\n    # Pass the asset name\n    f\"&assets={asset_name}\"\n\n    # Pass the color formula and colormap for custom visualization\n    f\"&color_formula=gamma+r+1.05&colormap_name={color_map}\"\n\n    # Pass the minimum and maximum values for rescaling\n    f\"&rescale={rescale_values['min']},{rescale_values['max']}\", \n\n# Return the response in JSON format\n).json()\n\n# Print the properties of the retrieved granule to the console\njanuary_2018_tile\n\n\n# Make a GET request to retrieve information for the 2012 tile \njanuary_2012_tile = requests.get(\n\n    # Pass the collection name, the item number in the list, and its ID\n    f\"{RASTER_API_URL}/stac/tilejson.json?collection={items['2012-01']['collection']}&item={items['2012-01']['id']}\"\n\n    # Pass the asset name\n    f\"&assets={asset_name}\"\n\n    # Pass the color formula and colormap for custom visualization\n    f\"&color_formula=gamma+r+1.05&colormap_name={color_map}\"\n\n    # Pass the minimum and maximum values for rescaling\n    f\"&rescale={rescale_values['min']},{rescale_values['max']}\", \n\n# Return the response in JSON format\n).json()\n\n# Print the properties of the retrieved granule to the console\njanuary_2012_tile",
    "crumbs": [
      "Data Usage Notebooks",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "U.S. Gridded Anthropogenic Methane Emissions Inventory"
    ]
  },
  {
    "objectID": "user_data_notebooks/epa-ch4emission-grid-v2express_User_Notebook.html#visualizing-ch₄-emissions",
    "href": "user_data_notebooks/epa-ch4emission-grid-v2express_User_Notebook.html#visualizing-ch₄-emissions",
    "title": "U.S. Gridded Anthropogenic Methane Emissions Inventory",
    "section": "Visualizing CH₄ emissions",
    "text": "Visualizing CH₄ emissions\n\n# Set initial zoom and center of map for CH₄ Layer\n# Centre of map [latitude,longitude]\n# 'folium.plugins' allows mapping side-by-side\nmap_ = folium.plugins.DualMap(location=(34, -118), zoom_start=6)\n\n# Define the first map layer (January 2018)\nmap_layer_2018 = TileLayer(\n    tiles=january_2018_tile[\"tiles\"][0], # Path to retrieve the tile\n    attr=\"GHG\", # Set the attribution\n    opacity=0.7, # Adjust the transparency of the layer\n)\n\n# Add the first layer to the Dual Map\nmap_layer_2018.add_to(map_.m1)\n\n# Define the second map layer (January 2012)\nmap_layer_2012 = TileLayer(\n    tiles=january_2012_tile[\"tiles\"][0], # Path to retrieve the tile\n    attr=\"GHG\", # Set the attribution\n    opacity=0.7, # Adjust the transparency of the layer\n)\n\n# Add the second layer to the Dual Map\nmap_layer_2012.add_to(map_.m2)\n\n# Visualize the Dual Map\nmap_",
    "crumbs": [
      "Data Usage Notebooks",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "U.S. Gridded Anthropogenic Methane Emissions Inventory"
    ]
  },
  {
    "objectID": "user_data_notebooks/epa-ch4emission-grid-v2express_User_Notebook.html#visualizing-the-data-as-a-time-series",
    "href": "user_data_notebooks/epa-ch4emission-grid-v2express_User_Notebook.html#visualizing-the-data-as-a-time-series",
    "title": "U.S. Gridded Anthropogenic Methane Emissions Inventory",
    "section": "Visualizing the Data as a Time Series",
    "text": "Visualizing the Data as a Time Series\nWe can now explore the gridded methane emission (Domestic Wastewater Treatment & Discharge (5D)) time series (January 2000 -December 2021) available for the Dallas, Texas area of the U.S. We can plot the data set using the code below:\n\n# Figure size: 20 representing the width, 10 representing the height\nfig = plt.figure(figsize=(20, 10))\n\n\nplt.plot(\n    df[\"date\"], # X-axis: sorted date\n    df[\"max\"],  # Y-axis: maximum CH4 emission\n    color=\"red\", # Line color\n    linestyle=\"-\", # Line style\n    linewidth=0.5, # Line width\n    label=\"Max CH4 emissions\", # Legend label\n)\n\n# Display legend\nplt.legend()\n\n# Insert label for the X-axis\nplt.xlabel(\"Years\")\n\n# Insert label for the Y-axis\nplt.ylabel(\"CH4 emissions Molecules CH₄/cm²/s\")\n\n# Insert title for the plot\nplt.title(\"CH4 gridded methane emission from Domestic Wastewater Treatment & Discharge (5D) for Texas, Dallas (2012-202)\")\n\n\n# Print the properties for the 3rd item in the collection\nprint(items[2][\"properties\"][\"datetime\"])\n\n\n# A GET request is made for the 2016 tile\ntile_2016 = requests.get(\n\n    # Pass the collection name, the item number in the list, and its ID\n    f\"{RASTER_API_URL}/stac/tilejson.json?collection={items[2]['collection']}&item={items[2]['id']}\"\n\n    # Pass the asset name\n    f\"&assets={asset_name}\"\n\n    # Pass the color formula and colormap for custom visualization\n    f\"&color_formula=gamma+r+1.05&colormap_name={color_map}\"\n\n    # Pass the minimum and maximum values for rescaling\n    f\"&rescale={rescale_values['min']},{rescale_values['max']}\",\n\n# Return the response in JSON format\n).json()\n\n# Print the properties of the retrieved granule to the console\ntile_2016\n\n\n# Create a new map to display the 2016 tile\naoi_map_bbox = Map(\n\n    # Base map is set to OpenStreetMap\n    tiles=\"OpenStreetMap\",\n\n    # Set the center of the map\n    location=[\n        30,-100\n    ],\n\n    # Set the zoom value\n    zoom_start=8,\n)\n\n# Define the map layer\nmap_layer = TileLayer(\n\n    # Path to retrieve the tile\n    tiles=tile_2016[\"tiles\"][0],\n\n    # Set the attribution and adjust the transparency of the layer\n    attr=\"GHG\", opacity = 0.5\n)\n\n# Add the layer to the map\nmap_layer.add_to(aoi_map_bbox)\n\n# Visualize the map\naoi_map_bbox",
    "crumbs": [
      "Data Usage Notebooks",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "U.S. Gridded Anthropogenic Methane Emissions Inventory"
    ]
  },
  {
    "objectID": "user_data_notebooks/epa-ch4emission-grid-v2express_User_Notebook.html#summary",
    "href": "user_data_notebooks/epa-ch4emission-grid-v2express_User_Notebook.html#summary",
    "title": "U.S. Gridded Anthropogenic Methane Emissions Inventory",
    "section": "Summary",
    "text": "Summary\nIn this notebook we have successfully completed the following steps for the STAC collection for the U.S. Gridded Anthropogenic Methane Emissions Inventory dataset:\n\nInstall and import the necessary libraries\nFetch the collection from STAC collections using the appropriate endpoints\nCount the number of existing granules within the collection\nMap and compare the anthropogenic methane emissions for two distinctive months/years\nGenerate zonal statistics for the area of interest (AOI)\nGenerate a time-series graph of the anthropogenic methane emissions for a specified region\n\nIf you have any questions regarding this user notebook, please contact us using the feedback form.",
    "crumbs": [
      "Data Usage Notebooks",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "U.S. Gridded Anthropogenic Methane Emissions Inventory"
    ]
  },
  {
    "objectID": "user_data_notebooks/odiac-ffco2-monthgrid-v2023_User_Notebook.html",
    "href": "user_data_notebooks/odiac-ffco2-monthgrid-v2023_User_Notebook.html",
    "title": "ODIAC Fossil Fuel CO₂ Emissions",
    "section": "",
    "text": "You can launch this notebook in the US GHG Center JupyterHub by clicking the link below.\nLaunch in the US GHG Center JupyterHub (requires access)",
    "crumbs": [
      "Data Usage Notebooks",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "ODIAC Fossil Fuel CO₂ Emissions"
    ]
  },
  {
    "objectID": "user_data_notebooks/odiac-ffco2-monthgrid-v2023_User_Notebook.html#run-this-notebook",
    "href": "user_data_notebooks/odiac-ffco2-monthgrid-v2023_User_Notebook.html#run-this-notebook",
    "title": "ODIAC Fossil Fuel CO₂ Emissions",
    "section": "",
    "text": "You can launch this notebook in the US GHG Center JupyterHub by clicking the link below.\nLaunch in the US GHG Center JupyterHub (requires access)",
    "crumbs": [
      "Data Usage Notebooks",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "ODIAC Fossil Fuel CO₂ Emissions"
    ]
  },
  {
    "objectID": "user_data_notebooks/odiac-ffco2-monthgrid-v2023_User_Notebook.html#approach",
    "href": "user_data_notebooks/odiac-ffco2-monthgrid-v2023_User_Notebook.html#approach",
    "title": "ODIAC Fossil Fuel CO₂ Emissions",
    "section": "Approach",
    "text": "Approach\n\nIdentify available dates and temporal frequency of observations for the given collection using the GHGC API /stac endpoint. Collection processed in this notebook is ODIAC CO₂ emissions version 2023.\nPass the STAC item into raster API /stac/tilejson.json endpoint\nWe’ll visualize two tiles (side-by-side) allowing for comparison of each of the time points using folium.plugins.DualMap\nAfter the visualization, we’ll perform zonal statistics for a given polygon.",
    "crumbs": [
      "Data Usage Notebooks",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "ODIAC Fossil Fuel CO₂ Emissions"
    ]
  },
  {
    "objectID": "user_data_notebooks/odiac-ffco2-monthgrid-v2023_User_Notebook.html#about-the-data",
    "href": "user_data_notebooks/odiac-ffco2-monthgrid-v2023_User_Notebook.html#about-the-data",
    "title": "ODIAC Fossil Fuel CO₂ Emissions",
    "section": "About the Data",
    "text": "About the Data\nThe Open-Data Inventory for Anthropogenic Carbon dioxide (ODIAC) is a high-spatial resolution global emission data product of CO₂ emissions from fossil fuel combustion (Oda and Maksyutov, 2011). ODIAC pioneered the combined use of space-based nighttime light data and individual power plant emission/location profiles to estimate the global spatial extent of fossil fuel CO₂ emissions. With the innovative emission modeling approach, ODIAC achieved the fine picture of global fossil fuel CO₂ emissions at a 1x1km.\nFor more information regarding this dataset, please visit the ODIAC Fossil Fuel CO₂ Emissions data overview page.",
    "crumbs": [
      "Data Usage Notebooks",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "ODIAC Fossil Fuel CO₂ Emissions"
    ]
  },
  {
    "objectID": "user_data_notebooks/odiac-ffco2-monthgrid-v2023_User_Notebook.html#querying-the-stac-api",
    "href": "user_data_notebooks/odiac-ffco2-monthgrid-v2023_User_Notebook.html#querying-the-stac-api",
    "title": "ODIAC Fossil Fuel CO₂ Emissions",
    "section": "Querying the STAC API",
    "text": "Querying the STAC API\nFirst, we are going to import the required libraries. Once imported, they allow better executing a query in the GHG Center Spatio Temporal Asset Catalog (STAC) Application Programming Interface (API) where the granules for this collection are stored.\n\n# Import the following libraries\nimport requests\nimport folium\nimport folium.plugins\nfrom folium import Map, TileLayer\nfrom pystac_client import Client\nimport branca\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n\n# Provide the STAC and RASTER API endpoints\n# The endpoint is referring to a location within the API that executes a request on a data collection nesting on the server.\n\n# The STAC API is a catalog of all the existing data collections that are stored in the GHG Center.\nSTAC_API_URL = \"https://earth.gov/ghgcenter/api/stac\"\n\n# The RASTER API is used to fetch collections for visualization\nRASTER_API_URL = \"https://earth.gov/ghgcenter/api/raster\"\n\n# The collection name is used to fetch the dataset from the STAC API. First, we define the collection name as a variable\n# Name of the collection for ODIAC dataset \ncollection_name = \"odiac-ffco2-monthgrid-v2023\"\n\n\n# Fetch the collection from the STAC API using the appropriate endpoint\n# The 'requests' library allows a HTTP request possible\ncollection = requests.get(f\"{STAC_API_URL}/collections/{collection_name}\").json()\n\n# Print the properties of the collection to the console\ncollection\n\nExamining the contents of our collection under summaries we see that the data is available from January 2000 to December 2022. By looking at the dashboard:time density we observe that the periodic frequency of these observations is monthly.\n\n# Create a function that would search for a data collection in the US GHG Center STAC API\n\n# First, we need to define the function\n# The name of the function = \"get_item_count\"\n# The argument that will be passed through the defined function = \"collection_id\"\ndef get_item_count(collection_id):\n\n    # Set a counter for the number of items existing in the collection\n    count = 0\n\n    # Define the path to retrieve the granules (items) of the collection of interest in the STAC API\n    items_url = f\"{STAC_API_URL}/collections/{collection_id}/items\"\n\n    # Run a while loop to make HTTP requests until there are no more URLs associated with the collection in the STAC API\n    while True:\n\n        # Retrieve information about the granules by sending a \"get\" request to the STAC API using the defined collection path\n        response = requests.get(items_url)\n\n        # If the items do not exist, print an error message and quit the loop\n        if not response.ok:\n            print(\"error getting items\")\n            exit()\n\n        # Return the results of the HTTP response as JSON\n        stac = response.json()\n\n        # Increase the \"count\" by the number of items (granules) returned in the response\n        count += int(stac[\"context\"].get(\"returned\", 0))\n\n        # Retrieve information about the next URL associated with the collection in the STAC API (if applicable)\n        next = [link for link in stac[\"links\"] if link[\"rel\"] == \"next\"]\n\n        # Exit the loop if there are no other URLs\n        if not next:\n            break\n        \n        # Ensure the information gathered by other STAC API links associated with the collection are added to the original path\n        # \"href\" is the identifier for each of the tiles stored in the STAC API\n        items_url = next[0][\"href\"]\n\n    # Return the information about the total number of granules found associated with the collection\n    return count\n\n\n# Apply the function created above \"get_item_count\" to the data collection\nnumber_of_items = get_item_count(collection_name)\n\n# Get the information about the number of granules found in the collection\nitems = requests.get(f\"{STAC_API_URL}/collections/{collection_name}/items?limit={number_of_items}\").json()[\"features\"]\n\n# Print the total number of items (granules) found\nprint(f\"Found {len(items)} items\")\n\nThis makes sense as there are 23 years between 2000 - 2023, with 12 months per year, meaning 276 records in total.\n\n# Examine the first item in the collection\n# Keep in mind that a list starts from 0, 1, 2... therefore items[0] is referring to the first item in the list/collection\nitems[0]",
    "crumbs": [
      "Data Usage Notebooks",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "ODIAC Fossil Fuel CO₂ Emissions"
    ]
  },
  {
    "objectID": "user_data_notebooks/odiac-ffco2-monthgrid-v2023_User_Notebook.html#exploring-changes-in-carbon-dioxide-co₂-levels-using-the-raster-api",
    "href": "user_data_notebooks/odiac-ffco2-monthgrid-v2023_User_Notebook.html#exploring-changes-in-carbon-dioxide-co₂-levels-using-the-raster-api",
    "title": "ODIAC Fossil Fuel CO₂ Emissions",
    "section": "Exploring Changes in Carbon Dioxide (CO₂) levels using the Raster API",
    "text": "Exploring Changes in Carbon Dioxide (CO₂) levels using the Raster API\nWe will explore changes in fossil fuel emissions in urban egions. In this notebook, we’ll explore the impacts of these emissions and explore these changes over time. We’ll then visualize the outputs on a map using folium.\n\n# Now we create a dictionary where the start datetime values for each granule is queried more explicitly by year and month (e.g., 2020-02)\nitems = {item[\"properties\"][\"start_datetime\"][:7]: item for item in items} \n\n# Next, we need to specify the asset name for this collection\n# The asset name is referring to the raster band containing the pixel values for the parameter of interest\n# For the case of the ODIAC Fossil Fuel CO₂ Emissions collection, the parameter of interest is “co2-emissions”\nasset_name = \"co2-emissions\"\n\nBelow, we are entering the minimum and maximum values to provide our upper and lower bounds in rescale_values.\n\n# Fetching the min and max values for a specific item\nrescale_values = {\"max\":items[list(items.keys())[0]][\"assets\"][asset_name][\"raster:bands\"][0][\"histogram\"][\"max\"], \"min\":items[list(items.keys())[0]][\"assets\"][asset_name][\"raster:bands\"][0][\"histogram\"][\"min\"]}\n\nNow, we will pass the item id, collection name, asset name, and the rescaling factor to the Raster API endpoint. We will do this twice, once for January 2020 and again for January 2000, so that we can visualize each event independently.\n\n# Choose a color map for displaying the first observation (event)\n# Please refer to matplotlib library if you'd prefer choosing a different color ramp.\n# For more information on Colormaps in Matplotlib, please visit https://matplotlib.org/stable/users/explain/colors/colormaps.html\ncolor_map = \"rainbow\" \n\n# Make a GET request to retrieve information for the 2020 tile\n# 2020\njanuary_2020_tile = requests.get(\n\n    # Pass the collection name, the item number in the list, and its ID\n    f\"{RASTER_API_URL}/stac/tilejson.json?collection={items['2020-01']['collection']}&item={items['2020-01']['id']}\"\n\n    # Pass the asset name\n    f\"&assets={asset_name}\"\n\n    # Pass the color formula and colormap for custom visualization\n    f\"&color_formula=gamma+r+1.05&colormap_name={color_map}\"\n\n    # Pass the minimum and maximum values for rescaling\n    f\"&rescale={rescale_values['min']},{rescale_values['max']}\", \n\n# Return the response in JSON format\n).json()\n\n# Print the properties of the retrieved granule to the console\njanuary_2020_tile\n\n\n# Make a GET request to retrieve information for the 2000 tile\n# 2000\njanuary_2000_tile = requests.get(\n\n    # Pass the collection name, the item number in the list, and its ID\n    f\"{RASTER_API_URL}/stac/tilejson.json?collection={items['2000-01']['collection']}&item={items['2000-01']['id']}\"\n\n    # Pass the asset name\n    f\"&assets={asset_name}\"\n\n    # Pass the color formula and colormap for custom visualization\n    f\"&color_formula=gamma+r+1.05&colormap_name={color_map}\"\n\n    # Pass the minimum and maximum values for rescaling\n    f\"&rescale={rescale_values['min']},{rescale_values['max']}\", \n\n# Return the response in JSON format\n).json()\n\n# Print the properties of the retrieved granule to the console\njanuary_2000_tile",
    "crumbs": [
      "Data Usage Notebooks",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "ODIAC Fossil Fuel CO₂ Emissions"
    ]
  },
  {
    "objectID": "user_data_notebooks/odiac-ffco2-monthgrid-v2023_User_Notebook.html#visualizing-co₂-emissions",
    "href": "user_data_notebooks/odiac-ffco2-monthgrid-v2023_User_Notebook.html#visualizing-co₂-emissions",
    "title": "ODIAC Fossil Fuel CO₂ Emissions",
    "section": "Visualizing CO₂ emissions",
    "text": "Visualizing CO₂ emissions\n\n# To change the location, you can simply insert the latitude and longitude of the area of your interest in the \"location=(LAT, LONG)\" statement\n\n# Set the initial zoom level and center of map for both tiles\n# 'folium.plugins' allows mapping side-by-side\nmap_ = folium.plugins.DualMap(location=(34, -118), zoom_start=6)\n\n# Define the first map layer (January 2020)\nmap_layer_2020 = TileLayer(\n    tiles=january_2020_tile[\"tiles\"][0], # Path to retrieve the tile\n    attr=\"GHG\", # Set the attribution\n    opacity=0.8, # Adjust the transparency of the layer\n)\n\n# Add the first layer to the Dual Map\nmap_layer_2020.add_to(map_.m1)\n\n# Define the second map layer (January 2000)\nmap_layer_2000 = TileLayer(\n    tiles=january_2000_tile[\"tiles\"][0], # Path to retrieve the tile\n    attr=\"GHG\", # Set the attribution\n    opacity=0.8, # Adjust the transparency of the layer\n)\n\n# Add the second layer to the Dual Map\nmap_layer_2000.add_to(map_.m2)\n\n# Visualize the Dual Map\nmap_",
    "crumbs": [
      "Data Usage Notebooks",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "ODIAC Fossil Fuel CO₂ Emissions"
    ]
  },
  {
    "objectID": "user_data_notebooks/odiac-ffco2-monthgrid-v2023_User_Notebook.html#visualizing-the-data-as-a-time-series",
    "href": "user_data_notebooks/odiac-ffco2-monthgrid-v2023_User_Notebook.html#visualizing-the-data-as-a-time-series",
    "title": "ODIAC Fossil Fuel CO₂ Emissions",
    "section": "Visualizing the Data as a Time Series",
    "text": "Visualizing the Data as a Time Series\nWe can now explore the ODIAC fossil fuel emission time series available (January 2000 -December 2022) for the Texas, Dallas area of USA. We can plot the data set using the code below:\n\n# Figure size: 20 representing the width, 10 representing the height\nfig = plt.figure(figsize=(20, 10))\n\n\nplt.plot(\n    df[\"date\"], # X-axis: sorted datetime\n    df[\"max\"], # Y-axis: maximum CO₂ level\n    color=\"red\", # Line color\n    linestyle=\"-\", # Line style\n    linewidth=0.5, # Line width\n    label=\"Max monthly CO₂ emissions\", # Legend label\n)\n\n# Display legend\nplt.legend()\n\n# Insert label for the X-axis\nplt.xlabel(\"Years\")\n\n# Insert label for the Y-axis\nplt.ylabel(\"CO2 emissions gC/m2/d\")\n\n# Insert title for the plot\nplt.title(\"CO2 emission Values for Texas, Dallas (2000-2022)\")\n\n###\n# Add data citation\nplt.text(\n    df[\"date\"].iloc[0],           # X-coordinate of the text\n    df[\"max\"].min(),              # Y-coordinate of the text\n\n\n\n\n    # Text to be displayed\n    \"Source: NASA ODIAC Fossil Fuel CO₂ Emissions\",                  \n    fontsize=12,                             # Font size\n    horizontalalignment=\"right\",             # Horizontal alignment\n    verticalalignment=\"top\",                 # Vertical alignment\n    color=\"blue\",                            # Text color\n)\n\n# Plot the time series\nplt.show()\n\n\n# Print the properties of the 3rd item in the collection\nprint(items[2][\"properties\"][\"start_datetime\"])\n\n\n# A GET request is made for the October tile\noctober_tile = requests.get(\n\n    # Pass the collection name, the item number in the list, and its ID\n    f\"{RASTER_API_URL}/stac/tilejson.json?collection={items[2]['collection']}&item={items[2]['id']}\"\n\n    # Pass the asset name\n    f\"&assets={asset_name}\"\n\n    # Pass the color formula and colormap for custom visualization\n    f\"&color_formula=gamma+r+1.05&colormap_name={color_map}\"\n\n    # Pass the minimum and maximum values for rescaling\n    f\"&rescale={rescale_values['min']},{rescale_values['max']}\",\n\n# Return the response in JSON format\n).json()\n\n# Print the properties of the retrieved granule to the console\noctober_tile\n\n\n# Create a new map to display the October tile\naoi_map_bbox = Map(\n\n    # Base map is set to OpenStreetMap\n    tiles=\"OpenStreetMap\",\n\n    # Set the center of the map\n    location=[\n        30,-100\n    ],\n\n    # Set the zoom value\n    zoom_start=8,\n)\n\n# Define the map layer\nmap_layer = TileLayer(\n\n    # Path to retrieve the tile\n    tiles=october_tile[\"tiles\"][0],\n\n    # Set the attribution and adjust the transparency of the layer\n    attr=\"GHG\", opacity = 0.5\n)\n\n# Add the layer to the map\nmap_layer.add_to(aoi_map_bbox)\n\n# Visualize the map\naoi_map_bbox",
    "crumbs": [
      "Data Usage Notebooks",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "ODIAC Fossil Fuel CO₂ Emissions"
    ]
  },
  {
    "objectID": "user_data_notebooks/odiac-ffco2-monthgrid-v2023_User_Notebook.html#summary",
    "href": "user_data_notebooks/odiac-ffco2-monthgrid-v2023_User_Notebook.html#summary",
    "title": "ODIAC Fossil Fuel CO₂ Emissions",
    "section": "Summary",
    "text": "Summary\nIn this notebook we have successfully explored, analysed and visualized STAC collecetion for ODIAC C02 fossisl fuel emission (2023).\n\nInstall and import the necessary libraries\nFetch the collection from STAC collections using the appropriate endpoints\nCount the number of existing granules within the collection\nMap and compare the CO₂ levels for two distinctive months/years\nGenerate zonal statistics for the area of interest (AOI)\nVisualizing the Data as a Time Series\n\nIf you have any questions regarding this user notebook, please contact us using the feedback form.",
    "crumbs": [
      "Data Usage Notebooks",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "ODIAC Fossil Fuel CO₂ Emissions"
    ]
  },
  {
    "objectID": "user_data_notebooks/oco2-mip-co2budget-yeargrid-v1_User_Notebook.html",
    "href": "user_data_notebooks/oco2-mip-co2budget-yeargrid-v1_User_Notebook.html",
    "title": "OCO-2 MIP Top-Down CO₂ Budgets",
    "section": "",
    "text": "You can launch this notebook in the US GHG Center JupyterHub by clicking the link below.\nLaunch in the US GHG Center JupyterHub (requires access)",
    "crumbs": [
      "Data Usage Notebooks",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "OCO-2 MIP Top-Down CO₂ Budgets"
    ]
  },
  {
    "objectID": "user_data_notebooks/oco2-mip-co2budget-yeargrid-v1_User_Notebook.html#run-this-notebook",
    "href": "user_data_notebooks/oco2-mip-co2budget-yeargrid-v1_User_Notebook.html#run-this-notebook",
    "title": "OCO-2 MIP Top-Down CO₂ Budgets",
    "section": "",
    "text": "You can launch this notebook in the US GHG Center JupyterHub by clicking the link below.\nLaunch in the US GHG Center JupyterHub (requires access)",
    "crumbs": [
      "Data Usage Notebooks",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "OCO-2 MIP Top-Down CO₂ Budgets"
    ]
  },
  {
    "objectID": "user_data_notebooks/oco2-mip-co2budget-yeargrid-v1_User_Notebook.html#approach",
    "href": "user_data_notebooks/oco2-mip-co2budget-yeargrid-v1_User_Notebook.html#approach",
    "title": "OCO-2 MIP Top-Down CO₂ Budgets",
    "section": "Approach",
    "text": "Approach\n\nIdentify available dates and temporal frequency of observations for the given collection using the GHGC API /stac endpoint. The collection processed in this notebook is the OCO-2 MIP Top-Down CO₂ Budgets data product.\nPass the STAC item into the raster API /stac/tilejson.jsonendpoint.\nUsing folium.plugins.DualMap, we will visualize two tiles (side-by-side), allowing us to compare time points.\nAfter the visualization, we will perform zonal statistics for a given polygon.",
    "crumbs": [
      "Data Usage Notebooks",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "OCO-2 MIP Top-Down CO₂ Budgets"
    ]
  },
  {
    "objectID": "user_data_notebooks/oco2-mip-co2budget-yeargrid-v1_User_Notebook.html#about-the-data",
    "href": "user_data_notebooks/oco2-mip-co2budget-yeargrid-v1_User_Notebook.html#about-the-data",
    "title": "OCO-2 MIP Top-Down CO₂ Budgets",
    "section": "About the Data",
    "text": "About the Data\nThe Committee on Earth Observation Satellites (CEOS) Atmospheric Composition - Virtual Constellation (AC-VC) Greenhouse Gas (GHG) team has generated the CEOS CO₂ Budgets dataset, which provides annual top-down carbon dioxide (CO2) emissions and removals from 2015 - 2020 gridded globally at 1° resolution, and as national totals. Data is provided in units of grams of carbon dioxide per square meter per year (g CO2/m2/yr). Only a subset of the full dataset is displayed in the GHG Center explore view.\nFor more information regarding this dataset, please visit the OCO-2 MIP Top-Down CO₂ Budgets data overview page.",
    "crumbs": [
      "Data Usage Notebooks",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "OCO-2 MIP Top-Down CO₂ Budgets"
    ]
  },
  {
    "objectID": "user_data_notebooks/oco2-mip-co2budget-yeargrid-v1_User_Notebook.html#querying-the-stac-api",
    "href": "user_data_notebooks/oco2-mip-co2budget-yeargrid-v1_User_Notebook.html#querying-the-stac-api",
    "title": "OCO-2 MIP Top-Down CO₂ Budgets",
    "section": "Querying the STAC API",
    "text": "Querying the STAC API\nFirst, we are going to import the required libraries. Once imported, they allow better executing a query in the GHG Center Spatio Temporal Asset Catalog (STAC) Application Programming Interface (API) where the granules for this collection are stored.\n\nimport requests\nimport folium\nimport folium.plugins\nfrom folium import Map, TileLayer\nfrom pystac_client import Client\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n\n# Provide the STAC and RASTER API endpoints\n# The endpoint is referring to a location within the API that executes a request on a data collection nesting on the server.\n\n# The STAC API is a catalog of all the existing data collections that are stored in the GHG Center.\nSTAC_API_URL = \"https://earth.gov/ghgcenter/api/stac\"\n\n# The RASTER API is used to fetch collections for visualization\nRASTER_API_URL = \"https://earth.gov/ghgcenter/api/raster\"\n\n# The collection name is used to fetch the dataset from the STAC API. First, we define the collection name as a variable\n# Name of the collection for CEOS National Top-Down CO₂ Budgets dataset \ncollection_name = \"oco2-mip-co2budget-yeargrid-v1\"\n\n\n# Fetch the collection from the STAC API using the appropriate endpoint\n# The 'requests' library allows a HTTP request possible\ncollection = requests.get(f\"{STAC_API_URL}/collections/{collection_name}\").json()\n\n# Print the properties of the collection to the console\ncollection\n\nExamining the contents of our collection under the temporal variable, we see that the data is available from January 2015 to December 2020. By looking at the dashboard:time density, we observe that the periodic frequency of these observations is yearly.\n\n# Create a function that would search for a data collection in the US GHG Center STAC API\n\n# First, we need to define the function\n# The name of the function = \"get_item_count\"\n# The argument that will be passed through the defined function = \"collection_id\"\ndef get_item_count(collection_id):\n\n    # Set a counter for the number of items existing in the collection\n    count = 0\n\n    # Define the path to retrieve the granules (items) of the collection of interest in the STAC API\n    items_url = f\"{STAC_API_URL}/collections/{collection_id}/items\"\n\n    # Run a while loop to make HTTP requests until there are no more URLs associated with the collection in the STAC API\n    while True:\n\n        # Retrieve information about the granules by sending a \"get\" request to the STAC API using the defined collection path\n        response = requests.get(items_url)\n\n        # If the items do not exist, print an error message and quit the loop\n        if not response.ok:\n            print(\"error getting items\")\n            exit()\n\n        # Return the results of the HTTP response as JSON\n        stac = response.json()\n\n        # Increase the \"count\" by the number of items (granules) returned in the response\n        count += int(stac[\"context\"].get(\"returned\", 0))\n\n        # Retrieve information about the next URL associated with the collection in the STAC API (if applicable)\n        next = [link for link in stac[\"links\"] if link[\"rel\"] == \"next\"]\n\n        # Exit the loop if there are no other URLs\n        if not next:\n            break\n        \n        # Ensure the information gathered by other STAC API links associated with the collection are added to the original path\n        # \"href\" is the identifier for each of the tiles stored in the STAC API\n        items_url = next[0][\"href\"]\n\n    # Return the information about the total number of granules found associated with the collection\n    return count\n\n\n# Apply the function created above \"get_item_count\" to the data collection\nnumber_of_items = get_item_count(collection_name)\n\n# Get the information about the number of granules found in the collection\nitems = requests.get(f\"{STAC_API_URL}/collections/{collection_name}/items?limit={number_of_items}\").json()[\"features\"]\n\n# Print the total number of items (granules) found\nprint(f\"Found {len(items)} items\")\n\nFound 6 items\n\n\n\n# Examine the first item in the collection\n# Keep in mind that a list starts from 0, 1, 2... therefore items[0] is referring to the first item in the list/collection\nitems[0]",
    "crumbs": [
      "Data Usage Notebooks",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "OCO-2 MIP Top-Down CO₂ Budgets"
    ]
  },
  {
    "objectID": "user_data_notebooks/oco2-mip-co2budget-yeargrid-v1_User_Notebook.html#exploring-changes-in-co₂-levels-using-the-raster-api",
    "href": "user_data_notebooks/oco2-mip-co2budget-yeargrid-v1_User_Notebook.html#exploring-changes-in-co₂-levels-using-the-raster-api",
    "title": "OCO-2 MIP Top-Down CO₂ Budgets",
    "section": "Exploring Changes in CO₂ Levels Using the Raster API",
    "text": "Exploring Changes in CO₂ Levels Using the Raster API\nIn this notebook, we will explore the global changes of CO₂ budgets over time in urban regions. We will visualize the outputs on a map using folium.\n\n# Now we create a dictionary where the start datetime values for each granule is queried more explicitly by year and month (e.g., 2020-02)\nitems = {item[\"properties\"][\"start_datetime\"]: item for item in items} \n\n# Next, we need to specify the asset name for this collection\n# The asset name is referring to the raster band containing the pixel values for the parameter of interest\n# For the case of the OCO-2 MIP Top-Down CO₂ Budgets collection, the parameter of interest is “ff”\nasset_name = \"ff\" #fossil fuel\n\nBelow, we are entering the minimum and maximum values to provide our upper and lower bounds in the rescale_values.\n\n# Fetching the min and max values for a specific item\nrescale_values = {\"max\":items[list(items.keys())[0]][\"assets\"][asset_name][\"raster:bands\"][0][\"histogram\"][\"max\"], \"min\":items[list(items.keys())[0]][\"assets\"][asset_name][\"raster:bands\"][0][\"histogram\"][\"min\"]}\n\n# Hardcoding the min and max values to match the scale in the GHG Center dashboard\nrescale_values = {\"max\": 450, \"min\": 0}\n\nNow, we will pass the item id, collection name, asset name, and the rescaling factor to the Raster API endpoint. We will do this twice, once for 2020 and again for 2019, so that we can visualize each event independently.\n\n# Choose a color map for displaying the first observation (event)\n# Please refer to matplotlib library if you'd prefer choosing a different color ramp.\n# For more information on Colormaps in Matplotlib, please visit https://matplotlib.org/stable/users/explain/colors/colormaps.html\ncolor_map = \"purd\"\n\n# Make a GET request to retrieve information for the 2020 tile which is the 1st item in the collection\n# To retrieve the first item in the collection we use \"0\" in the \"(items.keys())[0]\" statement\n\n# 2020\nco2_flux_1 = requests.get(\n\n    # Pass the collection name, the item number in the list, and its ID\n    f\"{RASTER_API_URL}/stac/tilejson.json?collection={items[list(items.keys())[0]]['collection']}&item={items[list(items.keys())[0]]['id']}\"\n\n    # Pass the asset name\n    f\"&assets={asset_name}\"\n\n    # Pass the color formula and colormap for custom visualization\n    f\"&color_formula=gamma+r+1.05&colormap_name={color_map}\"\n\n    # Pass the minimum and maximum values for rescaling\n    f\"&rescale={rescale_values['min']},{rescale_values['max']}\", \n\n# Return the response in JSON format\n).json()\n\n# Print the properties of the retrieved granule to the console\nco2_flux_1\n\n{'tilejson': '2.2.0',\n 'version': '1.0.0',\n 'scheme': 'xyz',\n 'tiles': ['https://earth.gov/ghgcenter/api/raster/stac/tiles/WebMercatorQuad/{z}/{x}/{y}@1x?collection=oco2-mip-co2budget-yeargrid-v1&item=oco2-mip-co2budget-yeargrid-v1-2020&assets=ff&color_formula=gamma+r+1.05&colormap_name=purd&rescale=0%2C450'],\n 'minzoom': 0,\n 'maxzoom': 24,\n 'bounds': [-180.0, -90.0, 180.0, 90.0],\n 'center': [0.0, 0.0, 0]}\n\n\n\n# Make a GET request to retrieve information for the 2019 tile which is the 2st item in the collection\n# To retrieve the second item in the collection we use \"1\" in the \"(items.keys())[1]\" statement\n\n# 2019\nco2_flux_2 = requests.get(\n\n    # Pass the collection name, the item number in the list, and its ID\n    f\"{RASTER_API_URL}/stac/tilejson.json?collection={items[list(items.keys())[1]]['collection']}&item={items[list(items.keys())[1]]['id']}\"\n\n    # Pass the asset name\n    f\"&assets={asset_name}\"\n\n    # Pass the color formula and colormap for custom visualization\n    f\"&color_formula=gamma+r+1.05&colormap_name={color_map}\"\n\n    # Pass the minimum and maximum values for rescaling\n    f\"&rescale={rescale_values['min']},{rescale_values['max']}\", \n\n# Return the response in JSON format\n).json()\n\n# Print the properties of the retrieved granule to the console\nco2_flux_2\n\n{'tilejson': '2.2.0',\n 'version': '1.0.0',\n 'scheme': 'xyz',\n 'tiles': ['https://earth.gov/ghgcenter/api/raster/stac/tiles/WebMercatorQuad/{z}/{x}/{y}@1x?collection=oco2-mip-co2budget-yeargrid-v1&item=oco2-mip-co2budget-yeargrid-v1-2019&assets=ff&color_formula=gamma+r+1.05&colormap_name=purd&rescale=0%2C450'],\n 'minzoom': 0,\n 'maxzoom': 24,\n 'bounds': [-180.0, -90.0, 180.0, 90.0],\n 'center': [0.0, 0.0, 0]}",
    "crumbs": [
      "Data Usage Notebooks",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "OCO-2 MIP Top-Down CO₂ Budgets"
    ]
  },
  {
    "objectID": "user_data_notebooks/oco2-mip-co2budget-yeargrid-v1_User_Notebook.html#visualizing-co₂-emissions",
    "href": "user_data_notebooks/oco2-mip-co2budget-yeargrid-v1_User_Notebook.html#visualizing-co₂-emissions",
    "title": "OCO-2 MIP Top-Down CO₂ Budgets",
    "section": "Visualizing CO₂ Emissions",
    "text": "Visualizing CO₂ Emissions\n\n# For this study we are going to compare the CO2 budget in 2020 and 2019 along the coast of California\n# To change the location, you can simply insert the latitude and longitude of the area of your interest in the \"location=(LAT, LONG)\" statement\n\n# Set the initial zoom level and center of map for both tiles\n# 'folium.plugins' allows mapping side-by-side\nmap_ = folium.plugins.DualMap(location=(34, -118), zoom_start=6)\n\n# Define the first map layer (2020)\nmap_layer_2020 = TileLayer(\n    tiles=co2_flux_1[\"tiles\"][0], # Path to retrieve the tile\n    attr=\"GHG\", # Set the attribution\n    opacity=0.5, # Adjust the transparency of the layer\n)\n\n# Add the first layer to the Dual Map\nmap_layer_2020.add_to(map_.m1)\n\n# Define the second map layer (2019)\nmap_layer_2019 = TileLayer(\n    tiles=co2_flux_2[\"tiles\"][0], # Path to retrieve the tile\n    attr=\"GHG\", # Set the attribution\n    opacity=0.5, # Adjust the transparency of the layer\n)\n\n# Add the second layer to the Dual Map\nmap_layer_2019.add_to(map_.m2)\n\n# Visualize the Dual Map\nmap_\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook",
    "crumbs": [
      "Data Usage Notebooks",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "OCO-2 MIP Top-Down CO₂ Budgets"
    ]
  },
  {
    "objectID": "user_data_notebooks/oco2-mip-co2budget-yeargrid-v1_User_Notebook.html#visualizing-the-data-as-a-time-series",
    "href": "user_data_notebooks/oco2-mip-co2budget-yeargrid-v1_User_Notebook.html#visualizing-the-data-as-a-time-series",
    "title": "OCO-2 MIP Top-Down CO₂ Budgets",
    "section": "Visualizing the Data as a Time Series",
    "text": "Visualizing the Data as a Time Series\nWe can now explore the fossil fuel emission time series (January 2015 -December 2020) available for the Dallas, Texas area of the U.S. We can plot the data set using the code below:\n\n# Figure size: 20 representing the width, 10 representing the height\nfig = plt.figure(figsize=(20, 10))\n\nplt.plot(\n    df[\"datetime\"], # X-axis: sorted datetime\n    df[\"max\"], # Y-axis: maximum CO₂ emission\n    color=\"red\", # Line color\n    linestyle=\"-\", # Line style\n    linewidth=0.5, # Line width\n    label=\"CO2 emissions\", # Legend label\n)\n\n# Display legend\nplt.legend()\n\n# Insert label for the X-axis\nplt.xlabel(\"Years\")\n\n# Insert label for the Y-axis\nplt.ylabel(\"CO2 emissions gC/m2/year1\")\n\n# Insert title for the plot\nplt.title(\"CO2 emission Values for Texas, Dallas (2015-2020)\")\n\n# Add data citation\nplt.text(\n    df[\"datetime\"].iloc[0],           # X-coordinate of the text \n    df[\"max\"].min(),                  # Y-coordinate of the text \n\n\n    # Text to be displayed\n    \"Source: NASA/NOAA OCO-2 MIP Top-Down CO₂ Budgets\",                  \n    fontsize=12,                             # Font size\n    horizontalalignment=\"left\",              # Horizontal alignment\n    verticalalignment=\"top\",                 # Vertical alignment\n    color=\"blue\",                            # Text color\n)\n\n# Plot the time series\nplt.show()\n\nText(0.5, 1.0, 'CO2 emission Values for Texas, Dallas (2015-2020)')\n\n\n\n\n\n\n\n\n\n\n# The 2018-01-01 observation is the 3rd item in the list.\n# Considering that a list starts with \"0\", we need to insert \"2\" in the \"items[2]\" statement\nprint(items[2][\"properties\"][\"start_datetime\"])\n\n2018-01-01T00:00:00+00:00\n\n\n\n# A GET request is made for the 2018-01-01 tile\nco2_flux_3 = requests.get(\n\n    # Pass the collection name, the item number in the list, and its ID\n    f\"{RASTER_API_URL}/stac/tilejson.json?collection={items[2]['collection']}&item={items[2]['id']}\"\n\n    # Pass the asset name\n    f\"&assets={asset_name}\"\n\n    # Pass the color formula and colormap for custom visualization\n    f\"&color_formula=gamma+r+1.05&colormap_name={color_map}\"\n\n    # Pass the minimum and maximum values for rescaling\n    f\"&rescale={rescale_values['min']},{rescale_values['max']}\",\n\n# Return the response in JSON format\n).json()\n\n# Print the properties of the retrieved granule to the console\nco2_flux_3\n\n{'tilejson': '2.2.0',\n 'version': '1.0.0',\n 'scheme': 'xyz',\n 'tiles': ['https://earth.gov/ghgcenter/api/raster/stac/tiles/WebMercatorQuad/{z}/{x}/{y}@1x?collection=oco2-mip-co2budget-yeargrid-v1&item=oco2-mip-co2budget-yeargrid-v1-2018&assets=ff&color_formula=gamma+r+1.05&colormap_name=purd&rescale=0%2C450'],\n 'minzoom': 0,\n 'maxzoom': 24,\n 'bounds': [-180.0, -90.0, 180.0, 90.0],\n 'center': [0.0, 0.0, 0]}\n\n\n\n# Create a new map to display the 2018-01-01 tile\naoi_map_bbox = Map(\n\n    # Base map is set to OpenStreetMap\n    tiles=\"OpenStreetMap\",\n\n    # Set the center of the map\n    location=[\n        30,-100\n    ],\n\n    # Set the zoom value\n    zoom_start=6.8,\n)\n\n# Define the map layer\nmap_layer = TileLayer(\n\n    # Path to retrieve the tile\n    tiles=co2_flux_3[\"tiles\"][0],\n\n    # Set the attribution and adjust the transparency of the layer\n    attr=\"GHG\", opacity = 0.7\n)\n\n# Add the layer to the map\nmap_layer.add_to(aoi_map_bbox)\n\n# Visualize the map\naoi_map_bbox\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook",
    "crumbs": [
      "Data Usage Notebooks",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "OCO-2 MIP Top-Down CO₂ Budgets"
    ]
  },
  {
    "objectID": "user_data_notebooks/oco2-mip-co2budget-yeargrid-v1_User_Notebook.html#summary",
    "href": "user_data_notebooks/oco2-mip-co2budget-yeargrid-v1_User_Notebook.html#summary",
    "title": "OCO-2 MIP Top-Down CO₂ Budgets",
    "section": "Summary",
    "text": "Summary\nIn this notebook we have successfully explored, analyzed, and visualized the STAC collection for OCO-2 MIP Top-Down CO₂ Budgets.\n\nInstall and import the necessary libraries\nFetch the collection from STAC collections using the appropriate endpoints\nCount the number of existing granules within the collection\nVisualizing CO₂ Emissions for two distinctive months/years\nGenerate zonal statistics for a specified region\nGenerate a time-series graph\n\nIf you have any questions regarding this user notebook, please contact us using the feedback form.",
    "crumbs": [
      "Data Usage Notebooks",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "OCO-2 MIP Top-Down CO₂ Budgets"
    ]
  },
  {
    "objectID": "user_data_notebooks/oco2geos-co2-daygrid-v10r_User_Notebook.html",
    "href": "user_data_notebooks/oco2geos-co2-daygrid-v10r_User_Notebook.html",
    "title": "OCO-2 GEOS Column CO₂ Concentrations",
    "section": "",
    "text": "You can launch this notebook in the US GHG Center JupyterHub by clicking the link below.\nLaunch in the US GHG Center JupyterHub (requires access)",
    "crumbs": [
      "Data Usage Notebooks",
      "Greenhouse Gas Concentrations",
      "OCO-2 GEOS Column CO₂ Concentrations"
    ]
  },
  {
    "objectID": "user_data_notebooks/oco2geos-co2-daygrid-v10r_User_Notebook.html#run-this-notebook",
    "href": "user_data_notebooks/oco2geos-co2-daygrid-v10r_User_Notebook.html#run-this-notebook",
    "title": "OCO-2 GEOS Column CO₂ Concentrations",
    "section": "",
    "text": "You can launch this notebook in the US GHG Center JupyterHub by clicking the link below.\nLaunch in the US GHG Center JupyterHub (requires access)",
    "crumbs": [
      "Data Usage Notebooks",
      "Greenhouse Gas Concentrations",
      "OCO-2 GEOS Column CO₂ Concentrations"
    ]
  },
  {
    "objectID": "user_data_notebooks/oco2geos-co2-daygrid-v10r_User_Notebook.html#approach",
    "href": "user_data_notebooks/oco2geos-co2-daygrid-v10r_User_Notebook.html#approach",
    "title": "OCO-2 GEOS Column CO₂ Concentrations",
    "section": "Approach",
    "text": "Approach\n\nIdentify available dates and temporal frequency of observations for the given collection using the GHGC API /stac endpoint. The collection processed in this notebook is the OCO-2 GEOS Column CO₂ Concentrations data product.\nPass the STAC item into the raster API /stac/tilejson.json endpoint.\nUsing folium.plugins.DualMap, visualize two tiles (side-by-side), allowing time point comparison.\nAfter the visualization, perform zonal statistics for a given polygon.",
    "crumbs": [
      "Data Usage Notebooks",
      "Greenhouse Gas Concentrations",
      "OCO-2 GEOS Column CO₂ Concentrations"
    ]
  },
  {
    "objectID": "user_data_notebooks/oco2geos-co2-daygrid-v10r_User_Notebook.html#about-the-data",
    "href": "user_data_notebooks/oco2geos-co2-daygrid-v10r_User_Notebook.html#about-the-data",
    "title": "OCO-2 GEOS Column CO₂ Concentrations",
    "section": "About the Data",
    "text": "About the Data\nIn July 2014, NASA successfully launched the first dedicated Earth remote sensing satellite to study atmospheric carbon dioxide (CO₂) from space. The Orbiting Carbon Observatory-2 (OCO-2) is an exploratory science mission designed to collect space-based global measurements of atmospheric CO₂ with the precision, resolution, and coverage needed to characterize sources and sinks (fluxes) on regional scales (≥1000 km). This dataset provides global gridded, daily column-averaged carbon dioxide (XCO₂) concentrations from January 1, 2015 - February 28, 2022. The data are derived from OCO-2 observations that were input to the Goddard Earth Observing System (GEOS) Constituent Data Assimilation System (CoDAS), a modeling and data assimilation system maintained by NASA’s Global Modeling and Assimilation Office (GMAO). Concentrations are measured in moles of carbon dioxide per mole of dry air (mol CO₂/mol dry) at a spatial resolution of 0.5° x 0.625°. Data assimilation synthesizes simulations and observations, adjusting modeled atmospheric constituents like CO₂ to reflect observed values. With the support of NASA’s Carbon Monitoring System (CMS) Program and the OCO Science Team, this dataset was produced as part of the OCO-2 mission which provides the highest quality space-based XCO₂ retrievals to date.\nFor more information regarding this dataset, please visit the OCO-2 GEOS Column CO₂ Concentrations data overview page.",
    "crumbs": [
      "Data Usage Notebooks",
      "Greenhouse Gas Concentrations",
      "OCO-2 GEOS Column CO₂ Concentrations"
    ]
  },
  {
    "objectID": "user_data_notebooks/oco2geos-co2-daygrid-v10r_User_Notebook.html#install-the-required-libraries",
    "href": "user_data_notebooks/oco2geos-co2-daygrid-v10r_User_Notebook.html#install-the-required-libraries",
    "title": "OCO-2 GEOS Column CO₂ Concentrations",
    "section": "Install the Required Libraries",
    "text": "Install the Required Libraries\nRequired libraries are pre-installed on the GHG Center Hub. If you need to run this notebook elsewhere, please install them with this line in a code cell:\n%pip install requests folium rasterstats pystac_client pandas matplotlib –quiet\n\n# Import the following libraries\nimport requests\nimport folium\nimport folium.plugins\nfrom folium import Map, TileLayer\nfrom pystac_client import Client\nimport branca\nimport pandas as pd\nimport matplotlib.pyplot as plt",
    "crumbs": [
      "Data Usage Notebooks",
      "Greenhouse Gas Concentrations",
      "OCO-2 GEOS Column CO₂ Concentrations"
    ]
  },
  {
    "objectID": "user_data_notebooks/oco2geos-co2-daygrid-v10r_User_Notebook.html#querying-the-stac-api",
    "href": "user_data_notebooks/oco2geos-co2-daygrid-v10r_User_Notebook.html#querying-the-stac-api",
    "title": "OCO-2 GEOS Column CO₂ Concentrations",
    "section": "Querying the STAC API",
    "text": "Querying the STAC API\nFirst, we are going to import the required libraries. Once imported, they allow better executing a query in the GHG Center Spatio Temporal Asset Catalog (STAC) Application Programming Interface (API) where the granules for this collection are stored.\n\n# Provide STAC and RASTER API endpoints\nSTAC_API_URL = \"https://earth.gov/ghgcenter/api/stac\"\nRASTER_API_URL = \"https://earth.gov/ghgcenter/api/raster\"\n\n# Please use the collection name similar to the one used in STAC collection.\n# Name of the collection for OCO-2 GEOS Column CO₂ Concentrations. \ncollection_name = \"oco2geos-co2-daygrid-v10r\"\n\n\n# Fetching the collection from STAC collections using appropriate endpoint.\ncollection = requests.get(f\"{STAC_API_URL}/collections/{collection_name}\").json()\ncollection\n\nExamining the contents of our collection under the temporal variable, we see that the data is available from January 2015 to February 2022. By looking at the dashboard:time density, we can see that these observations are collected daily.\n\ndef get_item_count(collection_id):\n    count = 0\n    items_url = f\"{STAC_API_URL}/collections/{collection_id}/items\"\n\n    while True:\n        response = requests.get(items_url)\n\n        if not response.ok:\n            print(\"error getting items\")\n            exit()\n\n        stac = response.json()\n        count += int(stac[\"context\"].get(\"returned\", 0))\n        next = [link for link in stac[\"links\"] if link[\"rel\"] == \"next\"]\n\n        if not next:\n            break\n        items_url = next[0][\"href\"]\n\n    return count\n\n\n# Check total number of items available\nnumber_of_items = get_item_count(collection_name)\nitems = requests.get(f\"{STAC_API_URL}/collections/{collection_name}/items?limit={number_of_items}\").json()[\"features\"]\nprint(f\"Found {len(items)} items\")\n\n\n# Examining the first item in the collection\nitems[0]\n\nBelow, we enter minimum and maximum values to provide our upper and lower bounds in rescale_values.",
    "crumbs": [
      "Data Usage Notebooks",
      "Greenhouse Gas Concentrations",
      "OCO-2 GEOS Column CO₂ Concentrations"
    ]
  },
  {
    "objectID": "user_data_notebooks/oco2geos-co2-daygrid-v10r_User_Notebook.html#exploring-changes-in-column-averaged-xco₂-concentrations-levels-using-the-raster-api",
    "href": "user_data_notebooks/oco2geos-co2-daygrid-v10r_User_Notebook.html#exploring-changes-in-column-averaged-xco₂-concentrations-levels-using-the-raster-api",
    "title": "OCO-2 GEOS Column CO₂ Concentrations",
    "section": "Exploring Changes in Column-Averaged XCO₂ Concentrations Levels Using the Raster API",
    "text": "Exploring Changes in Column-Averaged XCO₂ Concentrations Levels Using the Raster API\nIn this notebook, we will explore the temporal impacts of CO₂ emissions. We will visualize the outputs on a map using folium.\n\n# To access the year value from each item more easily, this will let us query more explicitly by year and month (e.g., 2020-02)\nitems = {item[\"properties\"][\"datetime\"]: item for item in items} \nasset_name = \"xco2\" #fossil fuel\n\n\n# Fetching the min and max values for a specific item\nrescale_values = {\"max\":items[list(items.keys())[0]][\"assets\"][asset_name][\"raster:bands\"][0][\"histogram\"][\"max\"], \"min\":items[list(items.keys())[0]][\"assets\"][asset_name][\"raster:bands\"][0][\"histogram\"][\"min\"]}\n\nNow, we will pass the item id, collection name, and rescaling_factor to the Raster API endpoint. We will do this twice, once for 2022-02-08 and again for 2022-01-27, so that we can visualize each event independently.\n\ncolor_map = \"magma\"\noco2_1 = requests.get(\n    f\"{RASTER_API_URL}/stac/tilejson.json?collection={items[list(items.keys())[0]]['collection']}&item={items[list(items.keys())[0]]['id']}\"\n    f\"&assets={asset_name}\"\n    f\"&color_formula=gamma+r+1.05&colormap_name={color_map}\"\n    f\"&rescale={rescale_values['min']},{rescale_values['max']}\", \n).json()\noco2_1\n\n\noco2_2 = requests.get(\n    f\"{RASTER_API_URL}/stac/tilejson.json?collection={items[list(items.keys())[1]]['collection']}&item={items[list(items.keys())[1]]['id']}\"\n    f\"&assets={asset_name}\"\n    f\"&color_formula=gamma+r+1.05&colormap_name={color_map}\"\n    f\"&rescale={rescale_values['min']},{rescale_values['max']}\", \n).json()\noco2_2",
    "crumbs": [
      "Data Usage Notebooks",
      "Greenhouse Gas Concentrations",
      "OCO-2 GEOS Column CO₂ Concentrations"
    ]
  },
  {
    "objectID": "user_data_notebooks/oco2geos-co2-daygrid-v10r_User_Notebook.html#visualizing-daily-column-averaged-xco₂-concentrations",
    "href": "user_data_notebooks/oco2geos-co2-daygrid-v10r_User_Notebook.html#visualizing-daily-column-averaged-xco₂-concentrations",
    "title": "OCO-2 GEOS Column CO₂ Concentrations",
    "section": "Visualizing Daily Column-Averaged XCO₂ Concentrations",
    "text": "Visualizing Daily Column-Averaged XCO₂ Concentrations\n\n# Set initial zoom and center of map for XCO₂ Layer\n# Centre of map [latitude,longitude]\nmap_ = folium.plugins.DualMap(location=(34, -118), zoom_start=6)\n\n\nmap_layer_2020 = TileLayer(\n    tiles=oco2_1[\"tiles\"][0],\n    attr=\"GHG\",\n    opacity=0.5,\n)\nmap_layer_2020.add_to(map_.m1)\n\nmap_layer_2019 = TileLayer(\n    tiles=oco2_2[\"tiles\"][0],\n    attr=\"GHG\",\n    opacity=0.5,\n)\nmap_layer_2019.add_to(map_.m2)\n\n# visualising the map\nmap_",
    "crumbs": [
      "Data Usage Notebooks",
      "Greenhouse Gas Concentrations",
      "OCO-2 GEOS Column CO₂ Concentrations"
    ]
  },
  {
    "objectID": "user_data_notebooks/oco2geos-co2-daygrid-v10r_User_Notebook.html#visualizing-the-data-as-a-time-series",
    "href": "user_data_notebooks/oco2geos-co2-daygrid-v10r_User_Notebook.html#visualizing-the-data-as-a-time-series",
    "title": "OCO-2 GEOS Column CO₂ Concentrations",
    "section": "Visualizing the Data as a Time Series",
    "text": "Visualizing the Data as a Time Series\nWe can now explore the XCO₂ concentrations time series (January 1, 2015 - February 28, 2022) available for the Dallas, Texas area of the U.S. We can plot the data set using the code below:\n\nfig = plt.figure(figsize=(20, 10))\n\n\nplt.plot(\n    df[\"datetime\"],\n    df[\"max\"],\n    color=\"red\",\n    linestyle=\"-\",\n    linewidth=0.5,\n    label=\"CO₂ concentrations\",\n)\n\nplt.legend()\nplt.xlabel(\"Years\")\nplt.ylabel(\"CO2 concentrations ppm\")\nplt.title(\"CO₂ concentrations Values for Texas, Dallas (Jan 2015- Feb 2022)\")\n\n\nprint(items[2][\"properties\"][\"datetime\"])\n\n\noco2_3 = requests.get(\n    f\"{RASTER_API_URL}/stac/tilejson.json?collection={items[2]['collection']}&item={items[2]['id']}\"\n    f\"&assets={asset_name}\"\n    f\"&color_formula=gamma+r+1.05&colormap_name={color_map}\"\n    f\"&rescale={rescale_values['min']},{rescale_values['max']}\",\n).json()\noco2_3\n\n\n# Use bbox initial zoom and map\n# Set up a map located w/in event bounds\naoi_map_bbox = Map(\n    tiles=\"OpenStreetMap\",\n    location=[\n        30,-100\n    ],\n    zoom_start=6.8,\n)\n\nmap_layer = TileLayer(\n    tiles=oco2_3[\"tiles\"][0],\n    attr=\"GHG\", opacity = 0.7\n)\n\nmap_layer.add_to(aoi_map_bbox)\n\naoi_map_bbox",
    "crumbs": [
      "Data Usage Notebooks",
      "Greenhouse Gas Concentrations",
      "OCO-2 GEOS Column CO₂ Concentrations"
    ]
  },
  {
    "objectID": "user_data_notebooks/oco2geos-co2-daygrid-v10r_User_Notebook.html#summary",
    "href": "user_data_notebooks/oco2geos-co2-daygrid-v10r_User_Notebook.html#summary",
    "title": "OCO-2 GEOS Column CO₂ Concentrations",
    "section": "Summary",
    "text": "Summary\nIn this notebook, we have successfully explored, analyzed, and visualized the STAC collection for OCO-2 GEOS Column CO₂ Concentrations.\n\nInstall and import the necessary libraries\nFetch the collection from STAC collections using the appropriate endpoints\nCount the number of existing granules within the collection\nMap and compare the Column-Averaged XCO₂ Concentrations Levels for two distinctive months/years\nGenerate zonal statistics for the area of interest (AOI)\nVisualizing the Data as a Time Series\n\nIf you have any questions regarding this user notebook, please contact us using the feedback form.",
    "crumbs": [
      "Data Usage Notebooks",
      "Greenhouse Gas Concentrations",
      "OCO-2 GEOS Column CO₂ Concentrations"
    ]
  },
  {
    "objectID": "user_data_notebooks/sedac-popdensity-yeargrid5yr-v4.11_User_Notebook.html",
    "href": "user_data_notebooks/sedac-popdensity-yeargrid5yr-v4.11_User_Notebook.html",
    "title": "SEDAC Gridded World Population Density",
    "section": "",
    "text": "You can launch this notebook in the US GHG Center JupyterHub by clicking the link below.\nLaunch in the US GHG Center JupyterHub (requires access)",
    "crumbs": [
      "Data Usage Notebooks",
      "Socioeconomic",
      "SEDAC Gridded World Population Density"
    ]
  },
  {
    "objectID": "user_data_notebooks/sedac-popdensity-yeargrid5yr-v4.11_User_Notebook.html#run-this-notebook",
    "href": "user_data_notebooks/sedac-popdensity-yeargrid5yr-v4.11_User_Notebook.html#run-this-notebook",
    "title": "SEDAC Gridded World Population Density",
    "section": "",
    "text": "You can launch this notebook in the US GHG Center JupyterHub by clicking the link below.\nLaunch in the US GHG Center JupyterHub (requires access)",
    "crumbs": [
      "Data Usage Notebooks",
      "Socioeconomic",
      "SEDAC Gridded World Population Density"
    ]
  },
  {
    "objectID": "user_data_notebooks/sedac-popdensity-yeargrid5yr-v4.11_User_Notebook.html#approach",
    "href": "user_data_notebooks/sedac-popdensity-yeargrid5yr-v4.11_User_Notebook.html#approach",
    "title": "SEDAC Gridded World Population Density",
    "section": "Approach",
    "text": "Approach\n\nIdentify available dates and temporal frequency of observations for the given collection using the GHGC API /stac endpoint. Collection processed in this notebook is SEDAC gridded population density.\nPass the STAC item into raster API /stac/tilejson.json endpoint\nWe’ll visualize two tiles (side-by-side) allowing for comparison of each of the time points using folium.plugins.DualMap\nAfter the visualization, we’ll perform zonal statistics for a given polygon.",
    "crumbs": [
      "Data Usage Notebooks",
      "Socioeconomic",
      "SEDAC Gridded World Population Density"
    ]
  },
  {
    "objectID": "user_data_notebooks/sedac-popdensity-yeargrid5yr-v4.11_User_Notebook.html#about-the-data",
    "href": "user_data_notebooks/sedac-popdensity-yeargrid5yr-v4.11_User_Notebook.html#about-the-data",
    "title": "SEDAC Gridded World Population Density",
    "section": "About the Data",
    "text": "About the Data\nThe SEDAC Gridded Population of the World: Population Density, v4.11 dataset provides annual estimates of population density for the years 2000, 2005, 2010, 2015, and 2020 on a 30 arc-second (~1 km) grid. These data can be used for assessing disaster impacts, risk mapping, and any other applications that include a human dimension. This population density dataset is provided by NASA’s Socioeconomic Data and Applications Center (SEDAC) hosted by the Center for International Earth Science Information Network (CIESIN) at Columbia University. The population estimates are provided as a continuous raster for the entire globe.\nFor more information regarding this dataset, please visit the SEDAC Gridded World Population Density data overview page.",
    "crumbs": [
      "Data Usage Notebooks",
      "Socioeconomic",
      "SEDAC Gridded World Population Density"
    ]
  },
  {
    "objectID": "user_data_notebooks/sedac-popdensity-yeargrid5yr-v4.11_User_Notebook.html#querying-the-stac-api",
    "href": "user_data_notebooks/sedac-popdensity-yeargrid5yr-v4.11_User_Notebook.html#querying-the-stac-api",
    "title": "SEDAC Gridded World Population Density",
    "section": "Querying the STAC API",
    "text": "Querying the STAC API\nFirst, we are going to import the required libraries. Once imported, they allow better executing a query in the GHG Center Spatio Temporal Asset Catalog (STAC) Application Programming Interface (API) where the granules for this collection are stored.\n\n# Provide the STAC and RASTER API endpoints\n# The endpoint is referring to a location within the API that executes a request on a data collection nesting on the server.\n\n# The STAC API is a catalog of all the existing data collections that are stored in the GHG Center.\nSTAC_API_URL = \"https://earth.gov/ghgcenter/api/stac\"\n\n# The RASTER API is used to fetch collections for visualization\nRASTER_API_URL = \"https://earth.gov/ghgcenter/api/raster\"\n\n# The collection name is used to fetch the dataset from the STAC API. First, we define the collection name as a variable\n# Name of the collection for SEDAC population density dataset \ncollection_name = \"sedac-popdensity-yeargrid5yr-v4.11\"\n\n\n# Fetch the collection from the STAC API using the appropriate endpoint\n# The 'requests' library allows a HTTP request possible\ncollection = requests.get(f\"{STAC_API_URL}/collections/{collection_name}\").json()\n\n# Print the properties of the collection to the console\ncollection\n\nExamining the contents of our collection under summaries we see that the data is available from January 2000 to December 2020. By looking at the dashboard:time density we observe that the data is available for the years 2000, 2005, 2010, 2015, 2020.\n\n# Create a function that would search for a data collection in the US GHG Center STAC API\n\n# First, we need to define the function\n# The name of the function = \"get_item_count\"\n# The argument that will be passed through the defined function = \"collection_id\"\ndef get_item_count(collection_id):\n\n    # Set a counter for the number of items existing in the collection\n    count = 0\n\n    # Define the path to retrieve the granules (items) of the collection of interest in the STAC API\n    items_url = f\"{STAC_API_URL}/collections/{collection_id}/items\"\n\n    # Run a while loop to make HTTP requests until there are no more URLs associated with the collection in the STAC API\n    while True:\n\n        # Retrieve information about the granules by sending a \"get\" request to the STAC API using the defined collection path\n        response = requests.get(items_url)\n\n        # If the items do not exist, print an error message and quit the loop\n        if not response.ok:\n            print(\"error getting items\")\n            exit()\n\n        # Return the results of the HTTP response as JSON\n        stac = response.json()\n\n        # Increase the \"count\" by the number of items (granules) returned in the response\n        count += int(stac[\"context\"].get(\"returned\", 0))\n\n        # Retrieve information about the next URL associated with the collection in the STAC API (if applicable)\n        next = [link for link in stac[\"links\"] if link[\"rel\"] == \"next\"]\n\n        # Exit the loop if there are no other URLs\n        if not next:\n            break\n        \n        # Ensure the information gathered by other STAC API links associated with the collection are added to the original path\n        # \"href\" is the identifier for each of the tiles stored in the STAC API\n        items_url = next[0][\"href\"]\n\n    # Return the information about the total number of granules found associated with the collection\n    return count\n\n\n# Apply the function created above \"get_item_count\" to the data collection\nnumber_of_items = get_item_count(collection_name)\n\n# Get the information about the number of granules found in the collection\nitems = requests.get(f\"{STAC_API_URL}/collections/{collection_name}/items?limit={number_of_items}\").json()[\"features\"]\n\n# Print the total number of items (granules) found\nprint(f\"Found {len(items)} items\")\n\n\n# Examine the first item in the collection\n# Keep in mind that a list starts from 0, 1, 2... therefore items[0] is referring to the first item in the list/collection\nitems[0]",
    "crumbs": [
      "Data Usage Notebooks",
      "Socioeconomic",
      "SEDAC Gridded World Population Density"
    ]
  },
  {
    "objectID": "user_data_notebooks/sedac-popdensity-yeargrid5yr-v4.11_User_Notebook.html#exploring-changes-in-the-world-population-density-using-the-raster-api",
    "href": "user_data_notebooks/sedac-popdensity-yeargrid5yr-v4.11_User_Notebook.html#exploring-changes-in-the-world-population-density-using-the-raster-api",
    "title": "SEDAC Gridded World Population Density",
    "section": "Exploring Changes in the World Population Density using the Raster API",
    "text": "Exploring Changes in the World Population Density using the Raster API\nWe will explore changes in population density in urban regions. In this notebook, we’ll explore the changes in population density over time. We’ll then visualize the outputs on a map using folium.\n\n# Now we create a dictionary where the start datetime values for each granule is queried more explicitly by year and month (e.g., 2020-02)\nitems = {item[\"properties\"][\"start_datetime\"][:7]: item for item in items} \n\n# Next, we need to specify the asset name for this collection\n# The asset name is referring to the raster band containing the pixel values for the parameter of interest\n# For the case of the SEDAC Gridded World Population Density collection, the parameter of interest is “population-density”\nasset_name = \"population-density\"\n\nBelow, we are entering the minimum and maximum values to provide our upper and lower bounds in the rescale_values.\n\n# Fetching the min and max values\nrescale_values = {\"max\":items[list(items.keys())[0]][\"assets\"][asset_name][\"raster:bands\"][0][\"histogram\"][\"max\"], \"min\":items[list(items.keys())[0]][\"assets\"][asset_name][\"raster:bands\"][0][\"histogram\"][\"min\"]}\n\nNow, we will pass the item id, collection name, asset name, and the rescaling factor to the Raster API endpoint. We will do this twice, once for January 2000 and again for January 2020, so that we can visualize each event independently.\n\n# Choose a color map for displaying the first observation (event)\n# Please refer to matplotlib library if you'd prefer choosing a different color ramp.\n# For more information on Colormaps in Matplotlib, please visit https://matplotlib.org/stable/users/explain/colors/colormaps.html\ncolor_map = \"rainbow\" \n\n# Make a GET request to retrieve information for the 2020 tile\njanuary_2020_tile = requests.get(\n\n    # Pass the collection name, the item number in the list, and its ID\n    f\"{RASTER_API_URL}/stac/tilejson.json?collection={items['2020-01']['collection']}&item={items['2020-01']['id']}\"\n\n    # Pass the asset name\n    f\"&assets={asset_name}\"\n\n    # Pass the color formula and colormap for custom visualization\n    f\"&color_formula=gamma+r+1.05&colormap_name={color_map}\"\n\n    # Pass the minimum and maximum values for rescaling\n    f\"&rescale={rescale_values['min']},{rescale_values['max']}\",\n\n# Return the response in JSON format \n).json()\n\n# Print the properties of the retrieved granule to the console\njanuary_2020_tile\n\n\n# Make a GET request to retrieve information for the 2000 tile\njanuary_2000_tile = requests.get(\n\n    # Pass the collection name, the item number in the list, and its ID\n    f\"{RASTER_API_URL}/stac/tilejson.json?collection={items['2000-01']['collection']}&item={items['2000-01']['id']}\"\n\n    # Pass the asset name\n    f\"&assets={asset_name}\"\n\n    # Pass the color formula and colormap for custom visualization\n    f\"&color_formula=gamma+r+1.05&colormap_name={color_map}\"\n\n    # Pass the minimum and maximum values for rescaling\n    f\"&rescale={rescale_values['min']},{rescale_values['max']}\",\n\n# Return the response in JSON format  \n).json()\n\n# Print the properties of the retrieved granule to the console\njanuary_2000_tile",
    "crumbs": [
      "Data Usage Notebooks",
      "Socioeconomic",
      "SEDAC Gridded World Population Density"
    ]
  },
  {
    "objectID": "user_data_notebooks/sedac-popdensity-yeargrid5yr-v4.11_User_Notebook.html#visualizing-population-density.",
    "href": "user_data_notebooks/sedac-popdensity-yeargrid5yr-v4.11_User_Notebook.html#visualizing-population-density.",
    "title": "SEDAC Gridded World Population Density",
    "section": "Visualizing Population Density.",
    "text": "Visualizing Population Density.\n\n# Set initial zoom and center of map for population density Layer\n# 'folium.plugins' allows mapping side-by-side\nmap_ = folium.plugins.DualMap(location=(34, -118), zoom_start=6)\n\n# Define the first map layer (January 2020)\nmap_layer_2020 = TileLayer(\n    tiles=january_2020_tile[\"tiles\"][0], # Path to retrieve the tile\n    attr=\"GHG\", # Set the attribution\n    opacity=1, # Adjust the transparency of the layer\n)\n\n# Add the first layer to the Dual Map\nmap_layer_2020.add_to(map_.m1)\n\n# Define the second map layer (January 2000)\nmap_layer_2000 = TileLayer(\n    tiles=january_2000_tile[\"tiles\"][0], # Path to retrieve the tile\n    attr=\"GHG\", # Set the attribution\n    opacity=1, # Adjust the transparency of the layer\n)\n\n# Add the second layer to the Dual Map\nmap_layer_2000.add_to(map_.m2)\n\n# Visualize the Dual Map\nmap_",
    "crumbs": [
      "Data Usage Notebooks",
      "Socioeconomic",
      "SEDAC Gridded World Population Density"
    ]
  },
  {
    "objectID": "user_data_notebooks/sedac-popdensity-yeargrid5yr-v4.11_User_Notebook.html#visualizing-the-data-as-a-time-series",
    "href": "user_data_notebooks/sedac-popdensity-yeargrid5yr-v4.11_User_Notebook.html#visualizing-the-data-as-a-time-series",
    "title": "SEDAC Gridded World Population Density",
    "section": "Visualizing the Data as a Time Series",
    "text": "Visualizing the Data as a Time Series\nWe can now explore the SEDAC population density dataset time series available for the Texas, Dallas area of USA. We can plot the dataset using the code below:\n\n# Figure size: 20 representing the width, 10 representing the height\nfig = plt.figure(figsize=(20, 10))\n\nplt.plot(\n    df[\"date\"], # X-axis: sorted datetime\n    df[\"max\"], # Y-axis: maximum pop density\n    color=\"red\", # Line color\n    linestyle=\"-\", # Line style\n    linewidth=0.5, # Line width\n    label=\"Population density over the years\", # Legend label\n)\n\n# Display legend\nplt.legend()\n\n# Insert label for the X-axis\nplt.xlabel(\"Years\")\n\n# Insert label for the Y-axis\nplt.ylabel(\"Population density\")\n\n# Insert title for the plot\nplt.title(\"Population density over Texas, Dallas (2000-2020)\")\n\n###\n# Add data citation\nplt.text(\n    df[\"date\"].iloc[0],           # X-coordinate of the text\n    df[\"max\"].min(),              # Y-coordinate of the text\n\n\n\n\n    # Text to be displayed\n    \"Source: NASA SEDAC Gridded World Population Density\",                  \n    fontsize=12,                             # Font size\n    horizontalalignment=\"right\",             # Horizontal alignment\n    verticalalignment=\"bottom\",              # Vertical alignment\n    color=\"blue\",                            # Text color\n)\n\n\n# Plot the time series\nplt.show()\n\n\n# Print the properties for the 3rd item in the collection\nprint(items[2][\"properties\"][\"start_datetime\"])\n\n\n# A GET request is made for the 2010 tile\njanuary2010_tile = requests.get(\n\n    # Pass the collection name, the item number in the list, and its ID\n    f\"{RASTER_API_URL}/stac/tilejson.json?collection={items[2]['collection']}&item={items[2]['id']}\"\n\n    # Pass the asset name\n    f\"&assets={asset_name}\"\n\n    # Pass the color formula and colormap for custom visualization\n    f\"&color_formula=gamma+r+1.05&colormap_name={color_map}\"\n\n    # Pass the minimum and maximum values for rescaling\n    f\"&rescale={rescale_values['min']},{rescale_values['max']}\",\n\n# Return the response in JSON format\n).json()\n\n# Print the properties of the retrieved granule to the console\njanuary2010_tile\n\n\n# Create a new map to display the 2010 tile\naoi_map_bbox = Map(\n\n    # Base map is set to OpenStreetMap\n    tiles=\"OpenStreetMap\",\n\n    # Set the center of the map\n    location=[\n        30,-100\n    ],\n\n    # Set the zoom value\n    zoom_start=8,\n)\n\n# Define the map layer\nmap_layer = TileLayer(\n\n    # Path to retrieve the tile\n    tiles=january2010_tile[\"tiles\"][0],\n\n    # Set the attribution and adjust the transparency of the layer\n    attr=\"GHG\", opacity = 0.5\n)\n\n# Add the layer to the map\nmap_layer.add_to(aoi_map_bbox)\n\n# Visualize the map\naoi_map_bbox",
    "crumbs": [
      "Data Usage Notebooks",
      "Socioeconomic",
      "SEDAC Gridded World Population Density"
    ]
  },
  {
    "objectID": "user_data_notebooks/sedac-popdensity-yeargrid5yr-v4.11_User_Notebook.html#summary",
    "href": "user_data_notebooks/sedac-popdensity-yeargrid5yr-v4.11_User_Notebook.html#summary",
    "title": "SEDAC Gridded World Population Density",
    "section": "Summary",
    "text": "Summary\nIn this notebook we have successfully explored, analyzed and visualized the STAC collection for the SEDAC Gridded World Population Density dataset.\n\nInstall and import the necessary libraries\nFetch the collection from STAC collections using the appropriate endpoints\nCount the number of existing granules within the collection\nMap and compare population density for two distinctive months/years\nGenerate zonal statistics for the area of interest (AOI)\nVisualizing the Data as a Time Series\n\nIf you have any questions regarding this user notebook, please contact us using the feedback form.",
    "crumbs": [
      "Data Usage Notebooks",
      "Socioeconomic",
      "SEDAC Gridded World Population Density"
    ]
  },
  {
    "objectID": "datatransformationcode.html",
    "href": "datatransformationcode.html",
    "title": "U.S. Greenhouse Gas Center: Data Transformation Notebooks",
    "section": "",
    "text": "Welcome to the U.S. Greenhouse Gas (GHG) Center data transformation notebooks, where we harness the power of Cloud Optimized Geotiffs (COGs) to offer a dynamic, cloud-based platform for exploring and analyzing greenhouse gas datasets. Dive into our curated collection of GHG datasets, all optimized for seamless accessibility and analysis.\nDiscover the journey of each dataset from its original format to COGs through the below Jupyter notebooks. The transformation examples are grouped topically. The dataset product type (model output, satellite observation, etc.) is noted next to the notebook name. Click on a notebook to learn more about the dataset and to view the transformation code.\nJoin us in our mission to make data-driven environmental solutions. Explore, analyze, and make a difference with the US GHG Center.\nNote: Not all datasets have a transformation code\nView the US GHG Center Data Catalog",
    "crumbs": [
      "Data Transformation Notebooks"
    ]
  },
  {
    "objectID": "datatransformationcode.html#gridded-anthropogenic-greenhouse-gas-emissions",
    "href": "datatransformationcode.html#gridded-anthropogenic-greenhouse-gas-emissions",
    "title": "U.S. Greenhouse Gas Center: Data Transformation Notebooks",
    "section": "Gridded Anthropogenic Greenhouse Gas Emissions",
    "text": "Gridded Anthropogenic Greenhouse Gas Emissions\n\nOCO-2 MIP Top-Down CO₂ Budgets\nODIAC Fossil Fuel CO₂ Emissions\nTM5-4DVar Isotopic CH₄ Inverse Fluxes\nU.S. Gridded Anthropogenic Methane Emissions Inventory",
    "crumbs": [
      "Data Transformation Notebooks"
    ]
  },
  {
    "objectID": "datatransformationcode.html#natural-greenhouse-gas-emissions-and-sinks",
    "href": "datatransformationcode.html#natural-greenhouse-gas-emissions-and-sinks",
    "title": "U.S. Greenhouse Gas Center: Data Transformation Notebooks",
    "section": "Natural Greenhouse Gas Emissions and Sinks",
    "text": "Natural Greenhouse Gas Emissions and Sinks\n\nAir-Sea CO₂ Flux, ECCO-Darwin Model v5\nGOSAT-based Top-down Total and Natural Methane Emissions\nOCO-2 MIP Top-Down CO₂ Budgets\nTM5-4DVar Isotopic CH₄ Inverse Fluxes",
    "crumbs": [
      "Data Transformation Notebooks"
    ]
  },
  {
    "objectID": "datatransformationcode.html#large-emissions-events",
    "href": "datatransformationcode.html#large-emissions-events",
    "title": "U.S. Greenhouse Gas Center: Data Transformation Notebooks",
    "section": "Large Emissions Events",
    "text": "Large Emissions Events\n\nEMIT Methane Point Source Plume Complexes",
    "crumbs": [
      "Data Transformation Notebooks"
    ]
  },
  {
    "objectID": "datatransformationcode.html#greenhouse-gas-concentrations",
    "href": "datatransformationcode.html#greenhouse-gas-concentrations",
    "title": "U.S. Greenhouse Gas Center: Data Transformation Notebooks",
    "section": "Greenhouse Gas Concentrations",
    "text": "Greenhouse Gas Concentrations\n\nAtmospheric Carbon Dioxide and Methane Concentrations from NOAA Global Monitoring Laboratory\nOCO-2 GEOS Column CO₂ Concentrations",
    "crumbs": [
      "Data Transformation Notebooks"
    ]
  },
  {
    "objectID": "datatransformationcode.html#socioeconomic",
    "href": "datatransformationcode.html#socioeconomic",
    "title": "U.S. Greenhouse Gas Center: Data Transformation Notebooks",
    "section": "Socioeconomic",
    "text": "Socioeconomic\n\nSEDAC Gridded World Population Density",
    "crumbs": [
      "Data Transformation Notebooks"
    ]
  },
  {
    "objectID": "datatransformationcode.html#contact",
    "href": "datatransformationcode.html#contact",
    "title": "U.S. Greenhouse Gas Center: Data Transformation Notebooks",
    "section": "Contact",
    "text": "Contact\nFor technical help or general questions, please contact the support team using the feedback form.",
    "crumbs": [
      "Data Transformation Notebooks"
    ]
  },
  {
    "objectID": "cog_transformation/epa-ch4emission-monthgrid-v2.html",
    "href": "cog_transformation/epa-ch4emission-monthgrid-v2.html",
    "title": "Gridded Anthropogenic Methane Emissions Inventory",
    "section": "",
    "text": "This script was used to transform the Gridded Anthropogenic Methane Emissions Inventory dataset from netCDF to Cloud Optimized GeoTIFF (COG) format for display in the Greenhouse Gas (GHG) Center.\n\nimport os\nimport xarray\nimport re\nimport pandas as pd\nimport json\nimport tempfile\nimport boto3\nfrom datetime import datetime\nfrom dateutil.relativedelta import relativedelta\n\n\nsession = boto3.session.Session()\ns3_client = session.client(\"s3\")\nbucket_name = (\n    \"ghgc-data-store-dev\"  # S3 bucket where the COGs are stored after transformation\n)\nFOLDER_NAME = \"epa_emissions/monthly_scale\"\ns3_folder_name = \"epa-emissions-monthly-scale-factors\"\n\nfiles_processed = pd.DataFrame(\n    columns=[\"file_name\", \"COGs_created\"]\n)  # A dataframe to keep track of the files that we have transformed into COGs\n\n# Reading the raw netCDF files from local machine\nfor name in os.listdir(FOLDER_NAME):\n    xds = xarray.open_dataset(f\"{FOLDER_NAME}/{name}\", engine=\"netcdf4\")\n    xds = xds.assign_coords(lon=(((xds.lon + 180) % 360) - 180)).sortby(\"lon\")\n    variable = [var for var in xds.data_vars]\n    filename = name.split(\"/ \")[-1]\n    filename_elements = re.split(\"[_ .]\", filename)\n    start_time = datetime(int(filename_elements[-2]), 1, 1)\n\n    for time_increment in range(0, len(xds.time)):\n        for var in variable:\n            filename = name.split(\"/ \")[-1]\n            filename_elements = re.split(\"[_ .]\", filename)\n            data = getattr(xds.isel(time=time_increment), var)\n            data = data.isel(lat=slice(None, None, -1))\n            data.rio.set_spatial_dims(\"lon\", \"lat\", inplace=True)\n            data.rio.write_crs(\"epsg:4326\", inplace=True)\n            date = start_time + relativedelta(months=+time_increment)\n\n            # # insert date of generated COG into filename\n            filename_elements.pop()\n            filename_elements[-1] = date.strftime(\"%Y%m\")\n            filename_elements.insert(2, var)\n            cog_filename = \"_\".join(filename_elements)\n            # # add extension\n            cog_filename = f\"{cog_filename}.tif\"\n\n            with tempfile.NamedTemporaryFile() as temp_file:\n                data.rio.to_raster(\n                    temp_file.name,\n                    driver=\"COG\",\n                )\n                s3_client.upload_file(\n                    Filename=temp_file.name,\n                    Bucket=bucket_name,\n                    Key=f\"{s3_folder_name}/{cog_filename}\",\n                )\n\n            files_processed = files_processed._append(\n                {\"file_name\": name, \"COGs_created\": cog_filename},\n                ignore_index=True,\n            )\n\n            print(f\"Generated and saved COG: {cog_filename}\")\n\n# Generate the json file with the metadata that is present in the netCDF files.\nwith tempfile.NamedTemporaryFile(mode=\"w+\") as fp:\n    json.dump(xds.attrs, fp)\n    json.dump({\"data_dimensions\": dict(xds.dims)}, fp)\n    json.dump({\"data_variables\": list(xds.data_vars)}, fp)\n    fp.flush()\n\n    s3_client.upload_file(\n        Filename=fp.name,\n        Bucket=bucket_name,\n        Key=f\"{s3_folder_name}/metadata.json\",\n    )\n\n# creating the csv file with the names of files transformed.\nfiles_processed.to_csv(\n    f\"s3://{bucket_name}/{s3_folder_name}/files_converted.csv\",\n)\nprint(\"Done generating COGs\")\n\n\n\n\n Back to top"
  },
  {
    "objectID": "cog_transformation/lpjwsl-wetlandch4-monthgrid-v1.html",
    "href": "cog_transformation/lpjwsl-wetlandch4-monthgrid-v1.html",
    "title": "Wetland Methane Emissions, LPJ-wsl Model",
    "section": "",
    "text": "This script was used to transform the Wetland Methane Emissions, LPJ-wsl Model dataset from netCDF to Cloud Optimized GeoTIFF (COG) format for display in the Greenhouse Gas (GHG) Center.\n\nimport os\nimport xarray\nimport re\nimport pandas as pd\nimport json\nimport tempfile\nimport boto3\n\n\nsession = boto3.session.Session()\ns3_client = session.client(\"s3\")\nbucket_name = (\n    \"ghgc-data-store-dev\"  # S3 bucket where the COGs are stored after transformation\n)\nFOLDER_NAME = \"NASA_GSFC_ch4_wetlands_monthly\"\ndirectory = \"ch4_wetlands_monthly\"\n\nfiles_processed = pd.DataFrame(\n    columns=[\"file_name\", \"COGs_created\"]\n)  # A dataframe to keep track of the files that we have transformed into COGs\n\n# Reading the raw netCDF files from local machine\nfor name in os.listdir(directory):\n    xds = xarray.open_dataset(\n        f\"{directory}/{name}\", engine=\"netcdf4\", decode_times=False\n    )\n    xds = xds.assign_coords(longitude=(((xds.longitude + 180) % 360) - 180)).sortby(\n        \"longitude\"\n    )\n    variable = [var for var in xds.data_vars]\n    filename = name.split(\"/ \")[-1]\n    filename_elements = re.split(\"[_ .]\", filename)\n\n    for time_increment in range(0, len(xds.time)):\n        for var in variable:\n            filename = name.split(\"/ \")[-1]\n            filename_elements = re.split(\"[_ .]\", filename)\n            data = getattr(xds.isel(time=time_increment), var)\n            data = data.isel(latitude=slice(None, None, -1))\n            data = data * 1000\n            data.rio.set_spatial_dims(\"longitude\", \"latitude\", inplace=True)\n            data.rio.write_crs(\"epsg:4326\", inplace=True)\n\n            date = (\n                f\"0{int((data.time.item(0)/732)+1)}\"\n                if len(str(int((data.time.item(0) / 732) + 1))) == 1\n                else f\"{int((data.time.item(0)/732)+1)}\"\n            )\n            # # insert date of generated COG into filename\n            filename_elements.pop()\n            filename_elements[-1] = filename_elements[-1] + date\n            filename_elements.insert(2, var)\n            cog_filename = \"_\".join(filename_elements)\n            # # add extension\n            cog_filename = f\"{cog_filename}.tif\"\n\n            with tempfile.NamedTemporaryFile() as temp_file:\n                data.rio.to_raster(\n                    temp_file.name,\n                    driver=\"COG\",\n                )\n                s3_client.upload_file(\n                    Filename=temp_file.name,\n                    Bucket=bucket_name,\n                    Key=f\"{FOLDER_NAME}/{cog_filename}\",\n                )\n\n            files_processed = files_processed._append(\n                {\"file_name\": name, \"COGs_created\": cog_filename},\n                ignore_index=True,\n            )\n\n            print(f\"Generated and saved COG: {cog_filename}\")\n\n# Generate the json file with the metadata that is present in the netCDF files.\nwith tempfile.NamedTemporaryFile(mode=\"w+\") as fp:\n    json.dump(xds.attrs, fp)\n    json.dump({\"data_dimensions\": dict(xds.dims)}, fp)\n    json.dump({\"data_variables\": list(xds.data_vars)}, fp)\n    fp.flush()\n\n    s3_client.upload_file(\n        Filename=fp.name,\n        Bucket=bucket_name,\n        Key=f\"{FOLDER_NAME}/metadata.json\",\n    )\n\n# creating the csv file with the names of files transformed.\nfiles_processed.to_csv(\n    f\"s3://{bucket_name}/{FOLDER_NAME}/files_converted.csv\",\n)\nprint(\"Done generating COGs\")\n\n\n\n\n Back to top"
  },
  {
    "objectID": "cog_transformation/oco2-mip-co2budget-yeargrid-v1.html",
    "href": "cog_transformation/oco2-mip-co2budget-yeargrid-v1.html",
    "title": "OCO-2 MIP Top-Down CO₂ Budgets",
    "section": "",
    "text": "This script was used to transform the OCO-2 MIP Top-Down CO₂ Budgets dataset from netCDF to Cloud Optimized GeoTIFF (COG) format for display in the Greenhouse Gas (GHG) Center.\n\nimport os\nimport xarray\nimport re\nimport pandas as pd\nimport json\nimport tempfile\nimport boto3\nimport rasterio\nfrom datetime import datetime\nfrom dateutil.relativedelta import relativedelta\n\n\nsession = boto3.session.Session()\ns3_client = session.client(\"s3\")\nbucket_name = \"ghgc-data-store-dev\" # S3 bucket where the COGs are to be stored\nyear_ = datetime(2015, 1, 1)    # Initialize the starting date time of the dataset.\n\nCOG_PROFILE = {\"driver\": \"COG\", \"compress\": \"DEFLATE\"}\n\n# Reading the raw netCDF files from local machine\nfiles_processed = pd.DataFrame(columns=[\"file_name\", \"COGs_created\"])   # A dataframe to keep track of the files that are converted into COGs\nfor name in os.listdir(\"new_data\"):\n    ds = xarray.open_dataset(\n        f\"new_data/{name}\",\n        engine=\"netcdf4\",\n    )\n    ds = ds.rename({\"latitude\": \"lat\", \"longitude\": \"lon\"})\n    # assign coords from dimensions\n    ds = ds.assign_coords(lon=(((ds.lon + 180) % 360) - 180)).sortby(\"lon\")\n    ds = ds.assign_coords(lat=list(ds.lat))\n\n    variable = [var for var in ds.data_vars]\n\n    for time_increment in range(0, len(ds.year)):\n        for var in variable[2:]:\n            filename = name.split(\"/ \")[-1]\n            filename_elements = re.split(\"[_ .]\", filename)\n            try:\n                data = ds[var].sel(year=time_increment)\n                date = year_ + relativedelta(years=+time_increment)\n                filename_elements[-1] = date.strftime(\"%Y\")\n                # # insert date of generated COG into filename\n                filename_elements.insert(2, var)\n                cog_filename = \"_\".join(filename_elements)\n                # # add extension\n                cog_filename = f\"{cog_filename}.tif\"\n            except KeyError:\n                data = ds[var]\n                date = year_ + relativedelta(years=+(len(ds.year) - 1))\n                filename_elements.pop()\n                filename_elements.append(year_.strftime(\"%Y\"))\n                filename_elements.append(date.strftime(\"%Y\"))\n                filename_elements.insert(2, var)\n                cog_filename = \"_\".join(filename_elements)\n                # # add extension\n                cog_filename = f\"{cog_filename}.tif\"\n\n            data = data.reindex(lat=list(reversed(data.lat)))\n\n            data.rio.set_spatial_dims(\"lon\", \"lat\")\n            data.rio.write_crs(\"epsg:4326\", inplace=True)\n\n            # generate COG\n            COG_PROFILE = {\"driver\": \"COG\", \"compress\": \"DEFLATE\"}\n            with tempfile.NamedTemporaryFile() as temp_file:\n                data.rio.to_raster(temp_file.name, **COG_PROFILE)\n                s3_client.upload_file(\n                    Filename=temp_file.name,\n                    Bucket=bucket_name,\n                    Key=f\"ceos_co2_flux/{cog_filename}\",\n                )\n\n            files_processed = files_processed._append(\n                {\"file_name\": name, \"COGs_created\": cog_filename},\n                ignore_index=True,\n            )\n\n            print(f\"Generated and saved COG: {cog_filename}\")\n\n# creating the csv file with the names of files transformed.\nfiles_processed.to_csv(\n    f\"s3://{bucket_name}/ceos_co2_flux/files_converted.csv\",\n)\nprint(\"Done generating COGs\")\n\n\n\n\n Back to top",
    "crumbs": [
      "Data Transformation Notebooks",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "OCO-2 MIP Top-Down CO₂ Budgets"
    ]
  },
  {
    "objectID": "cog_transformation/emit-ch4plume-v1.html",
    "href": "cog_transformation/emit-ch4plume-v1.html",
    "title": "EMIT Methane Point Source Plume Complexes",
    "section": "",
    "text": "This script was used to read the EMIT Methane Point Source Plume Complexes dataset provided in Cloud Optimized GeoTIFF (COG) format for display in the Greenhouse Gas (GHG) Center.\n\nimport re\nimport pandas as pd\nimport json\nimport tempfile\nimport boto3\n\n\nsession_ghgc = boto3.session.Session(profile_name=\"ghg_user\")\ns3_client_ghgc = session_ghgc.client(\"s3\")\nsession_veda_smce = boto3.session.Session()\ns3_client_veda_smce = session_veda_smce.client(\"s3\")\n\n# Since the plume emissions were already COGs, we just had to transform their naming convention to be stored in the STAC collection.\nSOURCE_BUCKET_NAME = \"ghgc-data-staging-uah\"\nTARGET_BUCKET_NAME = \"ghgc-data-store-dev\"\n\n\nkeys = []\nresp = s3_client_ghgc.list_objects_v2(Bucket=SOURCE_BUCKET_NAME)\nfor obj in resp[\"Contents\"]:\n    if \"l3\" in obj[\"Key\"]:\n        keys.append(obj[\"Key\"])\n\nfor key in keys:\n    s3_obj = s3_client_ghgc.get_object(Bucket=SOURCE_BUCKET_NAME, Key=key)[\n        \"Body\"\n    ]\n    filename = key.split(\"/\")[-1]\n    filename_elements = re.split(\"[_ .]\", filename)\n\n    date = re.search(\"t\\d\\d\\d\\d\\d\\d\\d\\dt\", key).group(0)\n    filename_elements.insert(-1, date[1:-1])\n    filename_elements.pop()\n\n    cog_filename = \"_\".join(filename_elements)\n    # # add extension\n    cog_filename = f\"{cog_filename}.tif\"\n    s3_client_veda_smce.upload_fileobj(\n        Fileobj=s3_obj,\n        Bucket=TARGET_BUCKET_NAME,\n        Key=f\"plum_data/{cog_filename}\",\n    )\n\n\n\n\n Back to top",
    "crumbs": [
      "Data Transformation Notebooks",
      "Large Emissions Events",
      "EMIT Methane Point Source Plume Complexes"
    ]
  },
  {
    "objectID": "cog_transformation/epa-ch4emission-grid-v2express.html",
    "href": "cog_transformation/epa-ch4emission-grid-v2express.html",
    "title": "U.S. Gridded Anthropogenic Methane Emissions Inventory",
    "section": "",
    "text": "This script was used to transform the Gridded Anthropogenic Methane Emissions Inventory monthly dataset from netCDF to Cloud Optimized GeoTIFF (COG) format for display in the Greenhouse Gas (GHG) Center.\n\nimport os\nimport xarray\nimport re\nimport pandas as pd\nimport json\nimport tempfile\nimport boto3\nfrom datetime import datetime\nimport numpy as np\n\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\nTrue\n\n\n\n# session = boto3.session.Session()\nsession = boto3.Session(\n    aws_access_key_id=os.environ.get(\"AWS_ACCESS_KEY_ID\"),\n    aws_secret_access_key=os.environ.get(\"AWS_SECRET_ACCESS_KEY\"),\n    aws_session_token=os.environ.get(\"AWS_SESSION_TOKEN\"),\n)\ns3_client = session.client(\"s3\")\nbucket_name = \"ghgc-data-store-dev\" # S3 bucket where the COGs are stored after transformation\nFOLDER_NAME = \"../data/epa_emissions_express_extension\"\ns3_folder_name = \"epa_express_extension_Mg_km2_yr\"\n# raw gridded data [molec/cm2/s] * 1/6.022x10^23 [molec/mol] * 16.04x10^-6 [ Mg/mol] * 366 [days/yr] * 1x10^10 [cm2/km2]\n\nfiles_processed = pd.DataFrame(columns=[\"file_name\", \"COGs_created\"])   # A dataframe to keep track of the files that we have transformed into COGs\n\n# Reading the raw netCDF files from local machine\nfor name in os.listdir(FOLDER_NAME):\n    xds = xarray.open_dataset(f\"{FOLDER_NAME}/{name}\", engine=\"netcdf4\")\n    xds = xds.assign_coords(lon=(((xds.lon + 180) % 360) - 180)).sortby(\"lon\")\n    variable = [var for var in xds.data_vars]\n    filename = name.split(\"/ \")[-1]\n    filename_elements = re.split(\"[_ .]\", filename)\n    start_time = datetime(int(filename_elements[-2]), 1, 1)\n\n    for time_increment in range(0, len(xds.time)):\n        for var in variable:\n            filename = name.split(\"/ \")[-1]\n            filename_elements = re.split(\"[_ .]\", filename)\n            data = getattr(xds.isel(time=time_increment), var)\n            data.values[data.values==0] = np.nan\n            data = data*((1/(6.022*pow(10,23)))*(16.04*pow(10,-6))*366*pow(10,10)*86400)\n            data = data.fillna(-9999)\n            data = data.isel(lat=slice(None, None, -1))\n            data.rio.set_spatial_dims(\"lon\", \"lat\", inplace=True)\n            data.rio.write_crs(\"epsg:4326\", inplace=True)\n\n            # # insert date of generated COG into filename\n            filename_elements.pop()\n            filename_elements[-1] = start_time.strftime(\"%Y\")\n            filename_elements.insert(2, var)\n            cog_filename = \"_\".join(filename_elements)\n            # # add extension\n            cog_filename = f\"{cog_filename}.tif\"\n\n            with tempfile.NamedTemporaryFile() as temp_file:\n                data.rio.to_raster(\n                    temp_file.name,\n                    driver=\"COG\",\n                )\n                s3_client.upload_file(\n                    Filename=temp_file.name,\n                    Bucket=bucket_name,\n                    Key=f\"{s3_folder_name}/{cog_filename}\",\n                )\n\n            files_processed = files_processed._append(\n                {\"file_name\": name, \"COGs_created\": cog_filename},\n                ignore_index=True,\n            )\n\n            print(f\"Generated and saved COG: {cog_filename}\")\n\n# Generate the json file with the metadata that is present in the netCDF files.\nwith tempfile.NamedTemporaryFile(mode=\"w+\") as fp:\n    json.dump(xds.attrs, fp)\n    json.dump({\"data_dimensions\": dict(xds.dims)}, fp)\n    json.dump({\"data_variables\": list(xds.data_vars)}, fp)\n    fp.flush()\n\n    s3_client.upload_file(\n        Filename=fp.name,\n        Bucket=bucket_name,\n        Key=f\"{s3_folder_name}/metadata.json\",\n    )\n\n# creating the csv file with the names of files transformed.\nfiles_processed.to_csv(\n    f\"s3://{bucket_name}/{s3_folder_name}/files_converted.csv\",\n)\nprint(\"Done generating COGs\")\n\nGenerated and saved COG: Express_Extension_emi_ch4_1A_Combustion_Mobile_Gridded_GHGI_Methane_v2_2015.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1A_Combustion_Stationary_Gridded_GHGI_Methane_v2_2015.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B1a_Abandoned_Coal_Gridded_GHGI_Methane_v2_2015.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B1a_Surface_Coal_Gridded_GHGI_Methane_v2_2015.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B1a_Underground_Coal_Gridded_GHGI_Methane_v2_2015.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2a_Petroleum_Systems_Exploration_Gridded_GHGI_Methane_v2_2015.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2a_Petroleum_Systems_Production_Gridded_GHGI_Methane_v2_2015.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2a_Petroleum_Systems_Refining_Gridded_GHGI_Methane_v2_2015.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2a_Petroleum_Systems_Transport_Gridded_GHGI_Methane_v2_2015.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2ab_Abandoned_Oil_Gas_Gridded_GHGI_Methane_v2_2015.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_Distribution_Gridded_GHGI_Methane_v2_2015.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_Exploration_Gridded_GHGI_Methane_v2_2015.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_Processing_Gridded_GHGI_Methane_v2_2015.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_Production_Gridded_GHGI_Methane_v2_2015.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_TransmissionStorage_Gridded_GHGI_Methane_v2_2015.tif\nGenerated and saved COG: Express_Extension_emi_ch4_2B8_Industry_Petrochemical_Gridded_GHGI_Methane_v2_2015.tif\nGenerated and saved COG: Express_Extension_emi_ch4_2C2_Industry_Ferroalloy_Gridded_GHGI_Methane_v2_2015.tif\nGenerated and saved COG: Express_Extension_emi_ch4_3A_Enteric_Fermentation_Gridded_GHGI_Methane_v2_2015.tif\nGenerated and saved COG: Express_Extension_emi_ch4_3B_Manure_Management_Gridded_GHGI_Methane_v2_2015.tif\nGenerated and saved COG: Express_Extension_emi_ch4_3C_Rice_Cultivation_Gridded_GHGI_Methane_v2_2015.tif\nGenerated and saved COG: Express_Extension_emi_ch4_3F_Field_Burning_Gridded_GHGI_Methane_v2_2015.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5A1_Landfills_Industrial_Gridded_GHGI_Methane_v2_2015.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5A1_Landfills_MSW_Gridded_GHGI_Methane_v2_2015.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5B1_Composting_Gridded_GHGI_Methane_v2_2015.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5D_Wastewater_Treatment_Domestic_Gridded_GHGI_Methane_v2_2015.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5D_Wastewater_Treatment_Industrial_Gridded_GHGI_Methane_v2_2015.tif\nGenerated and saved COG: Express_Extension_emi_ch4_Supp_1B2b_PostMeter_Gridded_GHGI_Methane_v2_2015.tif\nGenerated and saved COG: Express_Extension_grid_cell_area_Gridded_GHGI_Methane_v2_2015.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1A_Combustion_Mobile_Gridded_GHGI_Methane_v2_2020.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1A_Combustion_Stationary_Gridded_GHGI_Methane_v2_2020.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B1a_Abandoned_Coal_Gridded_GHGI_Methane_v2_2020.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B1a_Surface_Coal_Gridded_GHGI_Methane_v2_2020.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B1a_Underground_Coal_Gridded_GHGI_Methane_v2_2020.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2a_Petroleum_Systems_Exploration_Gridded_GHGI_Methane_v2_2020.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2a_Petroleum_Systems_Production_Gridded_GHGI_Methane_v2_2020.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2a_Petroleum_Systems_Refining_Gridded_GHGI_Methane_v2_2020.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2a_Petroleum_Systems_Transport_Gridded_GHGI_Methane_v2_2020.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2ab_Abandoned_Oil_Gas_Gridded_GHGI_Methane_v2_2020.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_Distribution_Gridded_GHGI_Methane_v2_2020.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_Exploration_Gridded_GHGI_Methane_v2_2020.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_Processing_Gridded_GHGI_Methane_v2_2020.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_Production_Gridded_GHGI_Methane_v2_2020.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_TransmissionStorage_Gridded_GHGI_Methane_v2_2020.tif\nGenerated and saved COG: Express_Extension_emi_ch4_2B8_Industry_Petrochemical_Gridded_GHGI_Methane_v2_2020.tif\nGenerated and saved COG: Express_Extension_emi_ch4_2C2_Industry_Ferroalloy_Gridded_GHGI_Methane_v2_2020.tif\nGenerated and saved COG: Express_Extension_emi_ch4_3A_Enteric_Fermentation_Gridded_GHGI_Methane_v2_2020.tif\nGenerated and saved COG: Express_Extension_emi_ch4_3B_Manure_Management_Gridded_GHGI_Methane_v2_2020.tif\nGenerated and saved COG: Express_Extension_emi_ch4_3C_Rice_Cultivation_Gridded_GHGI_Methane_v2_2020.tif\nGenerated and saved COG: Express_Extension_emi_ch4_3F_Field_Burning_Gridded_GHGI_Methane_v2_2020.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5A1_Landfills_Industrial_Gridded_GHGI_Methane_v2_2020.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5A1_Landfills_MSW_Gridded_GHGI_Methane_v2_2020.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5B1_Composting_Gridded_GHGI_Methane_v2_2020.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5D_Wastewater_Treatment_Domestic_Gridded_GHGI_Methane_v2_2020.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5D_Wastewater_Treatment_Industrial_Gridded_GHGI_Methane_v2_2020.tif\nGenerated and saved COG: Express_Extension_emi_ch4_Supp_1B2b_PostMeter_Gridded_GHGI_Methane_v2_2020.tif\nGenerated and saved COG: Express_Extension_grid_cell_area_Gridded_GHGI_Methane_v2_2020.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1A_Combustion_Mobile_Gridded_GHGI_Methane_v2_2014.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1A_Combustion_Stationary_Gridded_GHGI_Methane_v2_2014.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B1a_Abandoned_Coal_Gridded_GHGI_Methane_v2_2014.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B1a_Surface_Coal_Gridded_GHGI_Methane_v2_2014.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B1a_Underground_Coal_Gridded_GHGI_Methane_v2_2014.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2a_Petroleum_Systems_Exploration_Gridded_GHGI_Methane_v2_2014.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2a_Petroleum_Systems_Production_Gridded_GHGI_Methane_v2_2014.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2a_Petroleum_Systems_Refining_Gridded_GHGI_Methane_v2_2014.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2a_Petroleum_Systems_Transport_Gridded_GHGI_Methane_v2_2014.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2ab_Abandoned_Oil_Gas_Gridded_GHGI_Methane_v2_2014.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_Distribution_Gridded_GHGI_Methane_v2_2014.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_Exploration_Gridded_GHGI_Methane_v2_2014.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_Processing_Gridded_GHGI_Methane_v2_2014.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_Production_Gridded_GHGI_Methane_v2_2014.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_TransmissionStorage_Gridded_GHGI_Methane_v2_2014.tif\nGenerated and saved COG: Express_Extension_emi_ch4_2B8_Industry_Petrochemical_Gridded_GHGI_Methane_v2_2014.tif\nGenerated and saved COG: Express_Extension_emi_ch4_2C2_Industry_Ferroalloy_Gridded_GHGI_Methane_v2_2014.tif\nGenerated and saved COG: Express_Extension_emi_ch4_3A_Enteric_Fermentation_Gridded_GHGI_Methane_v2_2014.tif\nGenerated and saved COG: Express_Extension_emi_ch4_3B_Manure_Management_Gridded_GHGI_Methane_v2_2014.tif\nGenerated and saved COG: Express_Extension_emi_ch4_3C_Rice_Cultivation_Gridded_GHGI_Methane_v2_2014.tif\nGenerated and saved COG: Express_Extension_emi_ch4_3F_Field_Burning_Gridded_GHGI_Methane_v2_2014.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5A1_Landfills_Industrial_Gridded_GHGI_Methane_v2_2014.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5A1_Landfills_MSW_Gridded_GHGI_Methane_v2_2014.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5B1_Composting_Gridded_GHGI_Methane_v2_2014.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5D_Wastewater_Treatment_Domestic_Gridded_GHGI_Methane_v2_2014.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5D_Wastewater_Treatment_Industrial_Gridded_GHGI_Methane_v2_2014.tif\nGenerated and saved COG: Express_Extension_emi_ch4_Supp_1B2b_PostMeter_Gridded_GHGI_Methane_v2_2014.tif\nGenerated and saved COG: Express_Extension_grid_cell_area_Gridded_GHGI_Methane_v2_2014.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1A_Combustion_Mobile_Gridded_GHGI_Methane_v2_2013.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1A_Combustion_Stationary_Gridded_GHGI_Methane_v2_2013.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B1a_Abandoned_Coal_Gridded_GHGI_Methane_v2_2013.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B1a_Surface_Coal_Gridded_GHGI_Methane_v2_2013.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B1a_Underground_Coal_Gridded_GHGI_Methane_v2_2013.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2a_Petroleum_Systems_Exploration_Gridded_GHGI_Methane_v2_2013.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2a_Petroleum_Systems_Production_Gridded_GHGI_Methane_v2_2013.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2a_Petroleum_Systems_Refining_Gridded_GHGI_Methane_v2_2013.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2a_Petroleum_Systems_Transport_Gridded_GHGI_Methane_v2_2013.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2ab_Abandoned_Oil_Gas_Gridded_GHGI_Methane_v2_2013.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_Distribution_Gridded_GHGI_Methane_v2_2013.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_Exploration_Gridded_GHGI_Methane_v2_2013.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_Processing_Gridded_GHGI_Methane_v2_2013.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_Production_Gridded_GHGI_Methane_v2_2013.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_TransmissionStorage_Gridded_GHGI_Methane_v2_2013.tif\nGenerated and saved COG: Express_Extension_emi_ch4_2B8_Industry_Petrochemical_Gridded_GHGI_Methane_v2_2013.tif\nGenerated and saved COG: Express_Extension_emi_ch4_2C2_Industry_Ferroalloy_Gridded_GHGI_Methane_v2_2013.tif\nGenerated and saved COG: Express_Extension_emi_ch4_3A_Enteric_Fermentation_Gridded_GHGI_Methane_v2_2013.tif\nGenerated and saved COG: Express_Extension_emi_ch4_3B_Manure_Management_Gridded_GHGI_Methane_v2_2013.tif\nGenerated and saved COG: Express_Extension_emi_ch4_3C_Rice_Cultivation_Gridded_GHGI_Methane_v2_2013.tif\nGenerated and saved COG: Express_Extension_emi_ch4_3F_Field_Burning_Gridded_GHGI_Methane_v2_2013.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5A1_Landfills_Industrial_Gridded_GHGI_Methane_v2_2013.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5A1_Landfills_MSW_Gridded_GHGI_Methane_v2_2013.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5B1_Composting_Gridded_GHGI_Methane_v2_2013.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5D_Wastewater_Treatment_Domestic_Gridded_GHGI_Methane_v2_2013.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5D_Wastewater_Treatment_Industrial_Gridded_GHGI_Methane_v2_2013.tif\nGenerated and saved COG: Express_Extension_emi_ch4_Supp_1B2b_PostMeter_Gridded_GHGI_Methane_v2_2013.tif\nGenerated and saved COG: Express_Extension_grid_cell_area_Gridded_GHGI_Methane_v2_2013.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1A_Combustion_Mobile_Gridded_GHGI_Methane_v2_2017.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1A_Combustion_Stationary_Gridded_GHGI_Methane_v2_2017.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B1a_Abandoned_Coal_Gridded_GHGI_Methane_v2_2017.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B1a_Surface_Coal_Gridded_GHGI_Methane_v2_2017.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B1a_Underground_Coal_Gridded_GHGI_Methane_v2_2017.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2a_Petroleum_Systems_Exploration_Gridded_GHGI_Methane_v2_2017.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2a_Petroleum_Systems_Production_Gridded_GHGI_Methane_v2_2017.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2a_Petroleum_Systems_Refining_Gridded_GHGI_Methane_v2_2017.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2a_Petroleum_Systems_Transport_Gridded_GHGI_Methane_v2_2017.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2ab_Abandoned_Oil_Gas_Gridded_GHGI_Methane_v2_2017.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_Distribution_Gridded_GHGI_Methane_v2_2017.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_Exploration_Gridded_GHGI_Methane_v2_2017.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_Processing_Gridded_GHGI_Methane_v2_2017.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_Production_Gridded_GHGI_Methane_v2_2017.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_TransmissionStorage_Gridded_GHGI_Methane_v2_2017.tif\nGenerated and saved COG: Express_Extension_emi_ch4_2B8_Industry_Petrochemical_Gridded_GHGI_Methane_v2_2017.tif\nGenerated and saved COG: Express_Extension_emi_ch4_2C2_Industry_Ferroalloy_Gridded_GHGI_Methane_v2_2017.tif\nGenerated and saved COG: Express_Extension_emi_ch4_3A_Enteric_Fermentation_Gridded_GHGI_Methane_v2_2017.tif\nGenerated and saved COG: Express_Extension_emi_ch4_3B_Manure_Management_Gridded_GHGI_Methane_v2_2017.tif\nGenerated and saved COG: Express_Extension_emi_ch4_3C_Rice_Cultivation_Gridded_GHGI_Methane_v2_2017.tif\nGenerated and saved COG: Express_Extension_emi_ch4_3F_Field_Burning_Gridded_GHGI_Methane_v2_2017.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5A1_Landfills_Industrial_Gridded_GHGI_Methane_v2_2017.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5A1_Landfills_MSW_Gridded_GHGI_Methane_v2_2017.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5B1_Composting_Gridded_GHGI_Methane_v2_2017.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5D_Wastewater_Treatment_Domestic_Gridded_GHGI_Methane_v2_2017.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5D_Wastewater_Treatment_Industrial_Gridded_GHGI_Methane_v2_2017.tif\nGenerated and saved COG: Express_Extension_emi_ch4_Supp_1B2b_PostMeter_Gridded_GHGI_Methane_v2_2017.tif\nGenerated and saved COG: Express_Extension_grid_cell_area_Gridded_GHGI_Methane_v2_2017.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1A_Combustion_Mobile_Gridded_GHGI_Methane_v2_2016.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1A_Combustion_Stationary_Gridded_GHGI_Methane_v2_2016.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B1a_Abandoned_Coal_Gridded_GHGI_Methane_v2_2016.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B1a_Surface_Coal_Gridded_GHGI_Methane_v2_2016.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B1a_Underground_Coal_Gridded_GHGI_Methane_v2_2016.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2a_Petroleum_Systems_Exploration_Gridded_GHGI_Methane_v2_2016.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2a_Petroleum_Systems_Production_Gridded_GHGI_Methane_v2_2016.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2a_Petroleum_Systems_Refining_Gridded_GHGI_Methane_v2_2016.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2a_Petroleum_Systems_Transport_Gridded_GHGI_Methane_v2_2016.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2ab_Abandoned_Oil_Gas_Gridded_GHGI_Methane_v2_2016.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_Distribution_Gridded_GHGI_Methane_v2_2016.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_Exploration_Gridded_GHGI_Methane_v2_2016.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_Processing_Gridded_GHGI_Methane_v2_2016.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_Production_Gridded_GHGI_Methane_v2_2016.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_TransmissionStorage_Gridded_GHGI_Methane_v2_2016.tif\nGenerated and saved COG: Express_Extension_emi_ch4_2B8_Industry_Petrochemical_Gridded_GHGI_Methane_v2_2016.tif\nGenerated and saved COG: Express_Extension_emi_ch4_2C2_Industry_Ferroalloy_Gridded_GHGI_Methane_v2_2016.tif\nGenerated and saved COG: Express_Extension_emi_ch4_3A_Enteric_Fermentation_Gridded_GHGI_Methane_v2_2016.tif\nGenerated and saved COG: Express_Extension_emi_ch4_3B_Manure_Management_Gridded_GHGI_Methane_v2_2016.tif\nGenerated and saved COG: Express_Extension_emi_ch4_3C_Rice_Cultivation_Gridded_GHGI_Methane_v2_2016.tif\nGenerated and saved COG: Express_Extension_emi_ch4_3F_Field_Burning_Gridded_GHGI_Methane_v2_2016.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5A1_Landfills_Industrial_Gridded_GHGI_Methane_v2_2016.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5A1_Landfills_MSW_Gridded_GHGI_Methane_v2_2016.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5B1_Composting_Gridded_GHGI_Methane_v2_2016.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5D_Wastewater_Treatment_Domestic_Gridded_GHGI_Methane_v2_2016.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5D_Wastewater_Treatment_Industrial_Gridded_GHGI_Methane_v2_2016.tif\nGenerated and saved COG: Express_Extension_emi_ch4_Supp_1B2b_PostMeter_Gridded_GHGI_Methane_v2_2016.tif\nGenerated and saved COG: Express_Extension_grid_cell_area_Gridded_GHGI_Methane_v2_2016.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1A_Combustion_Mobile_Gridded_GHGI_Methane_v2_2012.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1A_Combustion_Stationary_Gridded_GHGI_Methane_v2_2012.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B1a_Abandoned_Coal_Gridded_GHGI_Methane_v2_2012.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B1a_Surface_Coal_Gridded_GHGI_Methane_v2_2012.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B1a_Underground_Coal_Gridded_GHGI_Methane_v2_2012.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2a_Petroleum_Systems_Exploration_Gridded_GHGI_Methane_v2_2012.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2a_Petroleum_Systems_Production_Gridded_GHGI_Methane_v2_2012.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2a_Petroleum_Systems_Refining_Gridded_GHGI_Methane_v2_2012.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2a_Petroleum_Systems_Transport_Gridded_GHGI_Methane_v2_2012.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2ab_Abandoned_Oil_Gas_Gridded_GHGI_Methane_v2_2012.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_Distribution_Gridded_GHGI_Methane_v2_2012.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_Exploration_Gridded_GHGI_Methane_v2_2012.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_Processing_Gridded_GHGI_Methane_v2_2012.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_Production_Gridded_GHGI_Methane_v2_2012.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_TransmissionStorage_Gridded_GHGI_Methane_v2_2012.tif\nGenerated and saved COG: Express_Extension_emi_ch4_2B8_Industry_Petrochemical_Gridded_GHGI_Methane_v2_2012.tif\nGenerated and saved COG: Express_Extension_emi_ch4_2C2_Industry_Ferroalloy_Gridded_GHGI_Methane_v2_2012.tif\nGenerated and saved COG: Express_Extension_emi_ch4_3A_Enteric_Fermentation_Gridded_GHGI_Methane_v2_2012.tif\nGenerated and saved COG: Express_Extension_emi_ch4_3B_Manure_Management_Gridded_GHGI_Methane_v2_2012.tif\nGenerated and saved COG: Express_Extension_emi_ch4_3C_Rice_Cultivation_Gridded_GHGI_Methane_v2_2012.tif\nGenerated and saved COG: Express_Extension_emi_ch4_3F_Field_Burning_Gridded_GHGI_Methane_v2_2012.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5A1_Landfills_Industrial_Gridded_GHGI_Methane_v2_2012.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5A1_Landfills_MSW_Gridded_GHGI_Methane_v2_2012.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5B1_Composting_Gridded_GHGI_Methane_v2_2012.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5D_Wastewater_Treatment_Domestic_Gridded_GHGI_Methane_v2_2012.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5D_Wastewater_Treatment_Industrial_Gridded_GHGI_Methane_v2_2012.tif\nGenerated and saved COG: Express_Extension_emi_ch4_Supp_1B2b_PostMeter_Gridded_GHGI_Methane_v2_2012.tif\nGenerated and saved COG: Express_Extension_grid_cell_area_Gridded_GHGI_Methane_v2_2012.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1A_Combustion_Mobile_Gridded_GHGI_Methane_v2_2019.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1A_Combustion_Stationary_Gridded_GHGI_Methane_v2_2019.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B1a_Abandoned_Coal_Gridded_GHGI_Methane_v2_2019.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B1a_Surface_Coal_Gridded_GHGI_Methane_v2_2019.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B1a_Underground_Coal_Gridded_GHGI_Methane_v2_2019.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2a_Petroleum_Systems_Exploration_Gridded_GHGI_Methane_v2_2019.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2a_Petroleum_Systems_Production_Gridded_GHGI_Methane_v2_2019.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2a_Petroleum_Systems_Refining_Gridded_GHGI_Methane_v2_2019.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2a_Petroleum_Systems_Transport_Gridded_GHGI_Methane_v2_2019.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2ab_Abandoned_Oil_Gas_Gridded_GHGI_Methane_v2_2019.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_Distribution_Gridded_GHGI_Methane_v2_2019.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_Exploration_Gridded_GHGI_Methane_v2_2019.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_Processing_Gridded_GHGI_Methane_v2_2019.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_Production_Gridded_GHGI_Methane_v2_2019.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_TransmissionStorage_Gridded_GHGI_Methane_v2_2019.tif\nGenerated and saved COG: Express_Extension_emi_ch4_2B8_Industry_Petrochemical_Gridded_GHGI_Methane_v2_2019.tif\nGenerated and saved COG: Express_Extension_emi_ch4_2C2_Industry_Ferroalloy_Gridded_GHGI_Methane_v2_2019.tif\nGenerated and saved COG: Express_Extension_emi_ch4_3A_Enteric_Fermentation_Gridded_GHGI_Methane_v2_2019.tif\nGenerated and saved COG: Express_Extension_emi_ch4_3B_Manure_Management_Gridded_GHGI_Methane_v2_2019.tif\nGenerated and saved COG: Express_Extension_emi_ch4_3C_Rice_Cultivation_Gridded_GHGI_Methane_v2_2019.tif\nGenerated and saved COG: Express_Extension_emi_ch4_3F_Field_Burning_Gridded_GHGI_Methane_v2_2019.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5A1_Landfills_Industrial_Gridded_GHGI_Methane_v2_2019.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5A1_Landfills_MSW_Gridded_GHGI_Methane_v2_2019.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5B1_Composting_Gridded_GHGI_Methane_v2_2019.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5D_Wastewater_Treatment_Domestic_Gridded_GHGI_Methane_v2_2019.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5D_Wastewater_Treatment_Industrial_Gridded_GHGI_Methane_v2_2019.tif\nGenerated and saved COG: Express_Extension_emi_ch4_Supp_1B2b_PostMeter_Gridded_GHGI_Methane_v2_2019.tif\nGenerated and saved COG: Express_Extension_grid_cell_area_Gridded_GHGI_Methane_v2_2019.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1A_Combustion_Mobile_Gridded_GHGI_Methane_v2_2018.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1A_Combustion_Stationary_Gridded_GHGI_Methane_v2_2018.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B1a_Abandoned_Coal_Gridded_GHGI_Methane_v2_2018.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B1a_Surface_Coal_Gridded_GHGI_Methane_v2_2018.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B1a_Underground_Coal_Gridded_GHGI_Methane_v2_2018.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2a_Petroleum_Systems_Exploration_Gridded_GHGI_Methane_v2_2018.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2a_Petroleum_Systems_Production_Gridded_GHGI_Methane_v2_2018.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2a_Petroleum_Systems_Refining_Gridded_GHGI_Methane_v2_2018.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2a_Petroleum_Systems_Transport_Gridded_GHGI_Methane_v2_2018.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2ab_Abandoned_Oil_Gas_Gridded_GHGI_Methane_v2_2018.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_Distribution_Gridded_GHGI_Methane_v2_2018.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_Exploration_Gridded_GHGI_Methane_v2_2018.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_Processing_Gridded_GHGI_Methane_v2_2018.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_Production_Gridded_GHGI_Methane_v2_2018.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_TransmissionStorage_Gridded_GHGI_Methane_v2_2018.tif\nGenerated and saved COG: Express_Extension_emi_ch4_2B8_Industry_Petrochemical_Gridded_GHGI_Methane_v2_2018.tif\nGenerated and saved COG: Express_Extension_emi_ch4_2C2_Industry_Ferroalloy_Gridded_GHGI_Methane_v2_2018.tif\nGenerated and saved COG: Express_Extension_emi_ch4_3A_Enteric_Fermentation_Gridded_GHGI_Methane_v2_2018.tif\nGenerated and saved COG: Express_Extension_emi_ch4_3B_Manure_Management_Gridded_GHGI_Methane_v2_2018.tif\nGenerated and saved COG: Express_Extension_emi_ch4_3C_Rice_Cultivation_Gridded_GHGI_Methane_v2_2018.tif\nGenerated and saved COG: Express_Extension_emi_ch4_3F_Field_Burning_Gridded_GHGI_Methane_v2_2018.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5A1_Landfills_Industrial_Gridded_GHGI_Methane_v2_2018.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5A1_Landfills_MSW_Gridded_GHGI_Methane_v2_2018.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5B1_Composting_Gridded_GHGI_Methane_v2_2018.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5D_Wastewater_Treatment_Domestic_Gridded_GHGI_Methane_v2_2018.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5D_Wastewater_Treatment_Industrial_Gridded_GHGI_Methane_v2_2018.tif\nGenerated and saved COG: Express_Extension_emi_ch4_Supp_1B2b_PostMeter_Gridded_GHGI_Methane_v2_2018.tif\nGenerated and saved COG: Express_Extension_grid_cell_area_Gridded_GHGI_Methane_v2_2018.tif\nDone generating COGs\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Data Transformation Notebooks",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "U.S. Gridded Anthropogenic Methane Emissions Inventory"
    ]
  },
  {
    "objectID": "cog_transformation/odiac-ffco2-monthgrid-v2023.html",
    "href": "cog_transformation/odiac-ffco2-monthgrid-v2023.html",
    "title": "ODIAC Fossil Fuel CO₂ Emissions",
    "section": "",
    "text": "This script was used to transform the ODIAC Fossil Fuel CO₂ Emissions dataset from GeoTIFF to Cloud Optimized GeoTIFF (COG) format for display in the Greenhouse Gas (GHG) Center.\n\nimport xarray\nimport re\nimport tempfile\nimport numpy as np\nimport boto3\nimport os\nimport gzip,shutil, wget\nimport s3fs\nimport hashlib\nimport json\n\n\n\nsession = boto3.session.Session()\ns3_client = session.client(\"s3\")\nfs = s3fs.S3FileSystem()\n\ndata_dir = \"data/\"\ndataset_name = \"odiac-ffco2-monthgrid-v2023\"\ncog_data_bucket = \"ghgc-data-store-develop\"\ncog_data_prefix= f\"transformed_cogs/{dataset_name}\"\ncog_checksum_prefix= \"checksum\"\n\n\n# Retrieve the checksum of raw files\nchecksum_dict ={}\nfor year in range(2000,2023):\n    checksum_url = f\"https://db.cger.nies.go.jp/nies_data/10.17595/20170411.001/odiac2023/1km_tiff/{year}/odiac2023_1km_checksum_{year}.md5.txt\"\n    response = requests.get(checksum_url)\n    content = response.text\n    tmp={}\n    \n    # Split the content into lines\n    lines = content.splitlines()\n    \n    for line in lines:\n        checksum, filename = line.split()\n        tmp[filename[:-3]] = checksum\n    checksum_dict.update(tmp)\nchecksum_dict = {k: v for k, v in checksum_dict.items() if k.endswith('.tif')}\n\n\n\ndef calculate_md5(file_path):\n    \"\"\"\n    Calculate the MD5 hash of a file.\n\n    Parameters:\n    file_path (str): The path to the file.\n\n    Returns:\n    str: The MD5 hash of the file.\n    \"\"\"\n    hash_md5 = hashlib.md5()\n    with open(file_path, \"rb\") as f:\n        for chunk in iter(lambda: f.read(4096), b\"\"):\n            hash_md5.update(chunk)\n    return hash_md5.hexdigest()\n\n\n#Code to download raw ODIAC data in your local machine\n\n# Creating  a base directory for ODIAC data\nif not os.path.exists(data_dir):\n        os.makedirs(data_dir)\n\nchecksum_dict_local={}\n# Download and unzip data for the years you want\nfor year in range(2000,2023):\n    year_dir = os.path.join(data_dir, str(year))\n    checksum_download_link = f\"https://db.cger.nies.go.jp/nies_data/10.17595/20170411.001/odiac2023/1km_tiff/{year}/odiac2023_1km_checksum_{year}.md5.txt\"\n    wget.download(checksum_download_link, year_dir)\n    # Make a subfolder for each year\n    if not os.path.exists(year_dir):\n        os.makedirs(year_dir)\n\n    for month in range(1,13):\n        month = f\"{month:02d}\"\n        download_link = f\"https://db.cger.nies.go.jp/nies_data/10.17595/20170411.001/odiac2023/1km_tiff/{year}/odiac2023_1km_excl_intl_{str(year)[-2:]}{month}.tif.gz\"\n        target_folder = f\"{data_dir}/{year}/\"\n        fname = os.path.basename(download_link)\n        target_path = os.path.join(target_folder, fname)\n\n        # Download the file\n        wget.download(download_link, target_path)\n\n        # Unzip the file\n        with gzip.open(target_path, 'rb') as f_in:\n            with open(target_path[:-3], 'wb') as f_out:\n                shutil.copyfileobj(f_in, f_out)\n                \n        # Calculate checksum of the .gz file \n        checksum_dict_local[target_path.split(\"/\")[-1][:-3]]=calculate_md5(target_path)\n        \n        # Remove the zip file\n        os.remove(target_path)\n    \n\n\n# check if the checksums match\nchecksum_dict_local == checksum_dict\n\n\n# List of years you want to run the transformation on\nfold_names=[str(i) for i in range(2020,2023)]\n\nfor fol_ in fold_names:\n    names= os.listdir(f\"{data_dir}{fol_}\")\n    names= [name for name in names if name.endswith('.tif')]\n    print(\"For year: \" ,fol_)\n    for name in names:\n        xds = xarray.open_dataarray(f\"{data_dir}{fol_}/{name}\")\n        filename = name.split(\"/ \")[-1]\n        filename_elements = re.split(\"[_ .]\", filename)\n        \n        # Remove the extension\n        filename_elements.pop()\n        # Extract and insert date of generated COG into filename\n        filename_elements[-1] = fol_ + filename_elements[-1][-2:]\n\n        # Replace 0 values  with -9999\n        xds = xds.where(xds!=0, -9999)\n        xds.rio.set_spatial_dims(\"x\", \"y\", inplace=True)\n        xds.rio.write_nodata(-9999, inplace=True)\n        xds.rio.write_crs(\"epsg:4326\", inplace=True)\n\n        cog_filename = \"_\".join(filename_elements)\n        cog_filename = f\"{cog_filename}.tif\"\n\n        # Write the cog file to s3 \n        with tempfile.NamedTemporaryFile() as temp_file:\n            xds.rio.to_raster(\n                temp_file.name,\n                driver=\"COG\",\n                compress=\"DEFLATE\"\n            )\n            s3_client.upload_file(\n                Filename=temp_file.name,\n                Bucket=cog_data_bucket,\n                Key=f\"{cog_data_prefix}/{cog_filename}\",\n            )\n\n        print(f\"Generated and saved COG: {cog_filename}\")\n\nprint(\"ODIAC COGs generation completed!!!\")\n\n\n# This block is used to calculate the SHA for each COG file and store in a JSON.\n\ndef get_all_s3_keys(bucket, model_name, ext):\n    \"\"\"Get a list of all keys in an S3 bucket.\"\"\"\n    keys = []\n\n    kwargs = {\"Bucket\": bucket, \"Prefix\": f\"{model_name}/\"}\n    while True:\n        resp = s3_client.list_objects_v2(**kwargs)\n        for obj in resp[\"Contents\"]:\n            if obj[\"Key\"].endswith(ext) and \"historical\" not in obj[\"Key\"]:\n                keys.append(obj[\"Key\"])\n\n        try:\n            kwargs[\"ContinuationToken\"] = resp[\"NextContinuationToken\"]\n        except KeyError:\n            break\n\n    return keys\n\nkeys = get_all_s3_keys(cog_data_bucket, cog_data_prefix,\".tif\")\n\n\ndef compute_sha256(url):\n    \"\"\"Compute SHA-256 checksum for a given file.\"\"\"\n    sha256_hash = hashlib.sha256()\n    with fs.open(url) as f:\n        for byte_block in iter(lambda: f.read(4096), b\"\"):\n            sha256_hash.update(byte_block)\n    return sha256_hash.hexdigest()\n\nsha_mapping = {}\nfor key in keys:\n    sha_mapping[key.split(\"/\")[-1]]=compute_sha256(f\"s3://{cog_data_bucket}/{key}\")\n\n\njson_data = json.dumps(sha_mapping, indent=4)\ns3_client.put_object(Bucket=cog_data_bucket, Key=f\"{cog_checksum_prefix}/{dataset_name}.json\", Body=json_data)\n\nprint(\"Checksums created for ODIAC!!!\")\n\n\n\n\n Back to top",
    "crumbs": [
      "Data Transformation Notebooks",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "ODIAC Fossil Fuel CO₂ Emissions"
    ]
  },
  {
    "objectID": "cog_transformation/eccodarwin-co2flux-monthgrid-v5.html",
    "href": "cog_transformation/eccodarwin-co2flux-monthgrid-v5.html",
    "title": "Air-Sea CO₂ Flux, ECCO-Darwin Model v5",
    "section": "",
    "text": "This script was used to transform the Air-Sea CO₂ Flux, ECCO-Darwin Mode dataset from netCDF to Cloud Optimized GeoTIFF (COG) format for display in the Greenhouse Gas (GHG) Center.\n\nimport os\nimport xarray\nimport re\nimport pandas as pd\nimport json\nimport tempfile\nimport boto3\nimport rasterio\nfrom datetime import datetime\nfrom dateutil.relativedelta import relativedelta\n\n\nsession = boto3.session.Session()\ns3_client = session.client(\"s3\")\n\nbucket_name = (\n    \"ghgc-data-store-dev\"  # S3 bucket where the COGs are stored after transformation\n)\nFOLDER_NAME = \"ecco-darwin\"\ns3_fol_name = \"ecco_darwin\"\n\n# Reading the raw netCDF files from local machine\nfiles_processed = pd.DataFrame(\n    columns=[\"file_name\", \"COGs_created\"]\n)  # A dataframe to keep track of the files that we have transformed into COGs\nfor name in os.listdir(FOLDER_NAME):\n    xds = xarray.open_dataset(\n        f\"{FOLDER_NAME}/{name}\",\n        engine=\"netcdf4\",\n    )\n    xds = xds.rename({\"y\": \"latitude\", \"x\": \"longitude\"})\n    xds = xds.assign_coords(longitude=((xds.longitude / 1440) * 360) - 180).sortby(\n        \"longitude\"\n    )\n    xds = xds.assign_coords(latitude=((xds.latitude / 721) * 180) - 90).sortby(\n        \"latitude\"\n    )\n\n    variable = [var for var in xds.data_vars]\n\n    for time_increment in xds.time.values:\n        for var in variable[2:]:\n            filename = name.split(\"/ \")[-1]\n            filename_elements = re.split(\"[_ .]\", filename)\n            data = xds[var]\n\n            data = data.reindex(latitude=list(reversed(data.latitude)))\n            data.rio.set_spatial_dims(\"longitude\", \"latitude\", inplace=True)\n            data.rio.write_crs(\"epsg:4326\", inplace=True)\n\n            # generate COG\n            COG_PROFILE = {\"driver\": \"COG\", \"compress\": \"DEFLATE\"}\n\n            filename_elements.pop()\n            filename_elements[-1] = filename_elements[-2] + filename_elements[-1]\n            filename_elements.pop(-2)\n            # # insert date of generated COG into filename\n            cog_filename = \"_\".join(filename_elements)\n            # # add extension\n            cog_filename = f\"{cog_filename}.tif\"\n\n            with tempfile.NamedTemporaryFile() as temp_file:\n                data.rio.to_raster(temp_file.name, **COG_PROFILE)\n                s3_client.upload_file(\n                    Filename=temp_file.name,\n                    Bucket=bucket_name,\n                    Key=f\"{s3_fol_name}/{cog_filename}\",\n                )\n\n            files_processed = files_processed._append(\n                {\"file_name\": name, \"COGs_created\": cog_filename},\n                ignore_index=True,\n            )\n            del data\n\n            print(f\"Generated and saved COG: {cog_filename}\")\n\n# Generate the json file with the metadata that is present in the netCDF files.\nwith tempfile.NamedTemporaryFile(mode=\"w+\") as fp:\n    json.dump(xds.attrs, fp)\n    json.dump({\"data_dimensions\": dict(xds.dims)}, fp)\n    json.dump({\"data_variables\": list(xds.data_vars)}, fp)\n    fp.flush()\n\n    s3_client.upload_file(\n        Filename=fp.name,\n        Bucket=bucket_name,\n        Key=\"s3_fol_name/metadata.json\",\n    )\n\n# A csv file to store the names of all the files converted.\nfiles_processed.to_csv(\n    f\"s3://{bucket_name}/{s3_fol_name}/files_converted.csv\",\n)\nprint(\"Done generating COGs\")\n\n\n\n\n Back to top",
    "crumbs": [
      "Data Transformation Notebooks",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "Air-Sea CO₂ Flux, ECCO-Darwin Model v5"
    ]
  },
  {
    "objectID": "cog_transformation/noaa-gggrn-concentrations.html",
    "href": "cog_transformation/noaa-gggrn-concentrations.html",
    "title": "Atmospheric Carbon Dioxide and Methane Concentrations from NOAA Global Monitoring Laboratory",
    "section": "",
    "text": "This script was used to transform the CO₂ and CH₄ datasets in txt format with hourly granularity to JSON in daily and monthly granularity for visualization in the Greenhouse Gas (GHG) Center.\n\nimport sys\nimport json\nimport pandas as pd\n\n\ndef daily_aggregate(filepath):\n    \"\"\"\n    Reads hourly data from a .txt file, aggregates it to daily, and returns a list of JSON objects that can be readily visualized in chart.\n\n    Parameters:\n        filepath (str): The path to the file containing the data to be aggregated.\n\n    Returns:\n        list: A list of dictionaries representing aggregated data, with each dictionary containing\n              'date' and 'value' keys.\n\n    Description:\n        This function reads data from the specified file, aggregates it, and returns a list of JSON objects.\n        The function performs the following steps:\n        - Reads the content of the file.\n        - Extracts the header lines from the file to determine the structure of the data.\n        - Processes the data into a DataFrame.\n        - Filters and aggregates the data.\n        - Converts the aggregated data into a list of JSON objects, where each object contains 'date' and 'value' keys.\n\n    Exceptions:\n        - FileNotFoundError: If the specified file is not found.\n        - Exception: If any other exception occurs during the processing, the exception message is returned.\n\n    Note:\n        - The input file is expected to have a .txt format with header lines indicating the structure of the data.\n        - The function aggregates data from hourly to daily intervals.\n        - The returned JSON list is suitable for use in frontend applications to visualize the aggregated data.\n\n    Example:\n        aggregated_data = daily_aggregate(\"/path/to/data_file.txt\")\n    \"\"\"\n    try:\n        with open(filepath, \"r\", encoding=\"utf-8\") as file:\n            file_content_str = file.read()\n            # split the string text based on new line\n            file_content_list = file_content_str.split(\"\\n\")\n            # get the header lines. its mentioned in the file's first line.\n            header_lines = file_content_list[0].split(\":\")[-1]\n            header_lines = int(header_lines)\n            # Slice the non header part of the data. and the last empty element\n            str_datas = file_content_list[header_lines - 1: -1]\n            data = [data.replace(\"\\n\", \"\").split(\" \") for data in str_datas]\n            # seperate table body and head to form dataframe\n            table_head = data[0]\n            table_body = data[1:]\n            dataframe = pd.DataFrame(table_body, columns=table_head)\n            dataframe['value'] = dataframe['value'].astype(float)\n            # Filter data\n            mask = (dataframe[\"qcflag\"] == \"...\") & (dataframe[\"value\"] != 0) & (dataframe[\"value\"] != -999)\n            filtered_df = dataframe[mask].reset_index(drop=True)\n            # Aggregate data (hourly into daily)\n            aggregated_df = filtered_df.groupby(['year', 'month', 'day'])['value'].mean().reset_index()\n            aggregated_df['value'] = aggregated_df['value'].round(2)\n            # necessary columns, processed df\n            aggregated_df['datetime'] = pd.to_datetime(aggregated_df[['year', 'month', 'day']])\n            aggregated_df['datetime'] = aggregated_df['datetime'].dt.strftime('%Y-%m-%dT%H:%M:%SZ')\n            processed_df = aggregated_df[['datetime', 'value']]\n            processed_df = processed_df.sort_values(by='datetime')\n            # dict formation, needed for frontend [{date: , value: }]\n            json_list = []\n            for _, row in processed_df.iterrows():\n                json_obj = {'date': row['datetime'], 'value': row['value']}\n                json_list.append(json_obj)\n            return json_list\n    except FileNotFoundError:\n        return \"File not found\"\n    except Exception as e:\n        return f\"Exception occured {e}\"\n\n\ndef monthly_aggregate(filepath):\n    \"\"\"\n    Reads hourly data from a .txt file, aggregates it to monthly, and returns a list of JSON objects that can be readily visualized in chart.\n\n    Parameters:\n        filepath (str): The path to the file containing the data to be aggregated.\n\n    Returns:\n        list: A list of dictionaries representing aggregated data, with each dictionary containing\n              'date' and 'value' keys.\n\n    Description:\n        This function reads data from the specified file, aggregates it, and returns a list of JSON objects.\n        The function performs the following steps:\n        - Reads the content of the file.\n        - Extracts the header lines from the file to determine the structure of the data.\n        - Processes the data into a DataFrame.\n        - Filters and aggregates the data.\n        - Converts the aggregated data into a list of JSON objects, where each object contains 'date' and 'value' keys.\n\n    Exceptions:\n        - FileNotFoundError: If the specified file is not found.\n        - Exception: If any other exception occurs during the processing, the exception message is returned.\n\n    Note:\n        - The input file is expected to have a .txt format with header lines indicating the structure of the data.\n        - The function aggregates data from hourly to daily intervals.\n        - The returned JSON list is suitable for use in frontend applications to visualize the aggregated data.\n\n    Example:\n        aggregated_data = monthly_aggregate(\"/path/to/data_file.txt\")\n    \"\"\"\n    try:\n        with open(filepath, \"r\", encoding=\"utf-8\") as file:\n            file_content_str = file.read()\n            # split the string text based on new line\n            file_content_list = file_content_str.split(\"\\n\")\n            # get the header lines. its mentioned in the file's first line.\n            header_lines = file_content_list[0].split(\":\")[-1]\n            header_lines = int(header_lines)\n            # Slice the non header part of the data. and the last empty element\n            str_datas = file_content_list[header_lines - 1: -1]\n            data = [data.replace(\"\\n\", \"\").split(\" \") for data in str_datas]\n            # seperate table body and head to form dataframe\n            table_head = data[0]\n            table_body = data[1:]\n            dataframe = pd.DataFrame(table_body, columns=table_head)\n            dataframe['value'] = dataframe['value'].astype(float)\n            # Filter data\n            mask = (dataframe[\"qcflag\"] == \"...\") & (dataframe[\"value\"] != 0) & (dataframe[\"value\"] != -999)\n            filtered_df = dataframe[mask].reset_index(drop=True)\n            # Aggregate data (hourly into monthly)\n            aggregated_df = filtered_df.groupby(['year', 'month'])['value'].mean().reset_index()\n            aggregated_df['value'] = aggregated_df['value'].round(2)\n            # necessary columns, processed df\n            aggregated_df['datetime'] = pd.to_datetime(aggregated_df[['year', 'month']].assign(day=1))\n            aggregated_df['datetime'] = aggregated_df['datetime'].dt.strftime('%Y-%m-%dT%H:%M:%SZ')\n            processed_df = aggregated_df[['datetime', 'value']]\n            processed_df = processed_df.sort_values(by='datetime')\n            # dict formation, needed for frontend [{date: , value: }]\n            json_list = []\n            for _, row in processed_df.iterrows():\n                json_obj = {'date': row['datetime'], 'value': row['value']}\n                json_list.append(json_obj)\n            return json_list\n    except FileNotFoundError:\n        return \"File not found\"\n    except Exception as e:\n        return f\"Exception occured {e}\"\n\n\nif __name__ == \"__main__\":\n    # Check if filepath argument is provided\n    if len(sys.argv) != 2:\n        print(\"Usage: python aggregrate.py &lt;daily|monthly&gt; &lt;filepath&gt;\")\n        sys.exit(1)\n\n    # Get the filepath from command line argument\n    frequency = sys.argv[1]\n    hourly_data_filepath = sys.argv[2]\n\n    # Call the aggregate function with the provided filepath\n    if (frequency == \"daily\"):\n        result = daily_aggregate(hourly_data_filepath)\n    elif (frequency == \"monthly\"):\n        result = monthly_aggregate(hourly_data_filepath)\n    else:\n        print(\"Usage: python aggregrate.py &lt;daily|monthly&gt; &lt;filepath&gt;\")\n        sys.exit(1)\n\n    if result is not None:\n        print(result)\n        # save the json file for reference\n        out_path = f\"{hourly_data_filepath.split(\"/\")[-1]}.json\"\n        with open(out_path, \"w\", encoding=\"utf-8\") as file:\n            json.dump(result, file)\n\n\n\n\n Back to top",
    "crumbs": [
      "Data Transformation Notebooks",
      "Greenhouse Gas Concentrations",
      "Atmospheric Carbon Dioxide and Methane Concentrations from NOAA Global Monitoring Laboratory"
    ]
  },
  {
    "objectID": "data_workflow/odiac-ffco2-monthgrid-v2023_Data_Flow.html",
    "href": "data_workflow/odiac-ffco2-monthgrid-v2023_Data_Flow.html",
    "title": "ODIAC Fossil Fuel CO₂ Emissions",
    "section": "",
    "text": "ODIAC Fossil Fuel CO₂ Emissions\n\n\n\nData Flow Diagram Extending From Acquisition/Creation to User Delivery\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Data Flow Diagrams",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "ODIAC Fossil Fuel CO₂ Emissions"
    ]
  },
  {
    "objectID": "data_workflow/oco2-mip-co2budget-yeargrid-v1_Data_Flow.html",
    "href": "data_workflow/oco2-mip-co2budget-yeargrid-v1_Data_Flow.html",
    "title": "OCO-2 MIP Top-Down CO₂ Budgets",
    "section": "",
    "text": "OCO-2 MIP Top-Down CO₂ Budgets\n\n\n\nData Flow Diagram Extending From Acquisition/Creation to User Delivery\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Data Flow Diagrams",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "OCO-2 MIP Top-Down CO₂ Budgets"
    ]
  },
  {
    "objectID": "data_workflow/eccodarwin-co2flux-monthgrid-v5_Data_Flow.html",
    "href": "data_workflow/eccodarwin-co2flux-monthgrid-v5_Data_Flow.html",
    "title": "Air-Sea CO₂ Flux, ECCO-Darwin Model v5",
    "section": "",
    "text": "Air-Sea CO₂ Flux, ECCO-Darwin Model v5\n\n\n\nData Flow Diagram Extending From Acquisition/Creation to User Delivery\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Data Flow Diagrams",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "Air-Sea CO₂ Flux, ECCO-Darwin Model v5"
    ]
  },
  {
    "objectID": "data_workflow/micasa-carbonflux-daygrid-v1_Data_Flow.html",
    "href": "data_workflow/micasa-carbonflux-daygrid-v1_Data_Flow.html",
    "title": "MiCASA Land Carbon Flux - Data Workflow",
    "section": "",
    "text": "MiCASA Land Carbon Flux - Data Workflow\n\n\n\nData Flow Diagram Extending From Acquisition/Creation to User Delivery\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Data Flow Diagrams",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "MiCASA Land Carbon Flux - Data Workflow"
    ]
  },
  {
    "objectID": "data_workflow/noaa-gggrn-ch4-concentrations_Data_Flow.html",
    "href": "data_workflow/noaa-gggrn-ch4-concentrations_Data_Flow.html",
    "title": "Atmospheric Methane Concentrations from the NOAA Global Monitoring Laboratory",
    "section": "",
    "text": "Atmospheric Methane Concentrations from the NOAA Global Monitoring Laboratory\n\n\n\nData Flow Diagram Extending From Acquisition/Creation to User Delivery\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Data Flow Diagrams",
      "Greenhouse Gas Concentrations",
      "Atmospheric Methane Concentrations from the NOAA Global Monitoring Laboratory"
    ]
  },
  {
    "objectID": "data_workflow/lpjeosim-wetlandch4-grid-v2_Data_Flow.html",
    "href": "data_workflow/lpjeosim-wetlandch4-grid-v2_Data_Flow.html",
    "title": "Wetland Methane Emissions, LPJ-EOSIM Model",
    "section": "",
    "text": "Wetland Methane Emissions, LPJ-EOSIM Model\n\n\n\nData Flow Diagram Extending From Acquisition/Creation to User Delivery\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Data Flow Diagrams",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "Wetland Methane Emissions, LPJ-EOSIM Model"
    ]
  },
  {
    "objectID": "data_workflow/tm54dvar-ch4flux-monthgrid-v1_Data_Flow.html",
    "href": "data_workflow/tm54dvar-ch4flux-monthgrid-v1_Data_Flow.html",
    "title": "TM5-4DVar Isotopic CH₄ Inverse Fluxes",
    "section": "",
    "text": "TM5-4DVar Isotopic CH₄ Inverse Fluxes\n\n\n\nData Flow Diagram Extending From Acquisition/Creation to User Delivery\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Data Flow Diagrams",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "TM5-4DVar Isotopic CH₄ Inverse Fluxes"
    ]
  },
  {
    "objectID": "processing_and_verification_reports/oco2geos-co2-daygrid-v10r_Processing and Verification Report.html",
    "href": "processing_and_verification_reports/oco2geos-co2-daygrid-v10r_Processing and Verification Report.html",
    "title": "OCO-2 GEOS Column CO₂ Concentrations",
    "section": "",
    "text": "This browser does not support PDFs. Please download the PDF to view it: Download PDF.\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Processing and Verification Reports",
      "Greenhouse Gas Concentrations",
      "OCO-2 GEOS Column CO₂ Concentrations"
    ]
  },
  {
    "objectID": "processing_and_verification_reports/micasa-carbonflux-daygrid-v1_Processing and Verification Report.html",
    "href": "processing_and_verification_reports/micasa-carbonflux-daygrid-v1_Processing and Verification Report.html",
    "title": "MiCASA Land Carbon Flux",
    "section": "",
    "text": "This browser does not support PDFs. Please download the PDF to view it: Download PDF.\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Processing and Verification Reports",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "MiCASA Land Carbon Flux"
    ]
  },
  {
    "objectID": "processing_and_verification_reports/gosat-based-ch4budget-yeargrid-v1_Processing and Verification Report.html",
    "href": "processing_and_verification_reports/gosat-based-ch4budget-yeargrid-v1_Processing and Verification Report.html",
    "title": "GOSAT-based Top-down Total and Natural Methane Emissions",
    "section": "",
    "text": "This browser does not support PDFs. Please download the PDF to view it: Download PDF.\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Processing and Verification Reports",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "GOSAT-based Top-down Total and Natural Methane Emissions"
    ]
  },
  {
    "objectID": "processing_and_verification_reports/oco2-mip-co2budget-yeargrid-v1_Processing and Verification Report.html",
    "href": "processing_and_verification_reports/oco2-mip-co2budget-yeargrid-v1_Processing and Verification Report.html",
    "title": "OCO-2 MIP Top-Down CO₂ Budgets",
    "section": "",
    "text": "This browser does not support PDFs. Please download the PDF to view it: Download PDF.\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Processing and Verification Reports",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "OCO-2 MIP Top-Down CO₂ Budgets"
    ]
  },
  {
    "objectID": "processing_and_verification_reports/emit-ch4plume-v1_Processing and Verification Report.html",
    "href": "processing_and_verification_reports/emit-ch4plume-v1_Processing and Verification Report.html",
    "title": "EMIT Methane Point Source Plume Complexes",
    "section": "",
    "text": "This browser does not support PDFs. Please download the PDF to view it: Download PDF.\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Processing and Verification Reports",
      "Large Emissions Events",
      "EMIT Methane Point Source Plume Complexes"
    ]
  },
  {
    "objectID": "processing_and_verification_reports/lpjeosim-wetlandch4-grid-v2_Processing and Verification Report.html",
    "href": "processing_and_verification_reports/lpjeosim-wetlandch4-grid-v2_Processing and Verification Report.html",
    "title": "Wetland Methane Emissions, LPJ-EOSIM Model",
    "section": "",
    "text": "This browser does not support PDFs. Please download the PDF to view it: Download PDF.\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Processing and Verification Reports",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "Wetland Methane Emissions, LPJ-EOSIM Model"
    ]
  },
  {
    "objectID": "advanceduser.html",
    "href": "advanceduser.html",
    "title": "U.S. Greenhouse Gas Center: Advanced User Notebook",
    "section": "",
    "text": "Welcome to the U.S. Greenhouse Gas (GHG) Center: Advanced User Notebook, your gateway to exploring and analyzing curated datasets on greenhouse gas emissions. Our cloud-based system offers seamless access to Greenhouse Gas curated datasets. Dive into the data with our data usage Jupyter notebooks, designed for efficient exploration, visualization, and analysis. Whether you are focused on specific focus areas or product types, our dataset usage notebooks provide invaluable insights to drive informed decision-making.\nJoin us in our mission to make data-driven environmental solutions. Explore, analyze, and make a difference with the US GHG Center.\nView the US GHG Center Data Catalogg"
  },
  {
    "objectID": "advanceduser.html#gridded-anthropogenic-greenhouse-gas-emissions",
    "href": "advanceduser.html#gridded-anthropogenic-greenhouse-gas-emissions",
    "title": "U.S. Greenhouse Gas Center: Advanced User Notebook",
    "section": "Gridded Anthropogenic Greenhouse Gas Emissions",
    "text": "Gridded Anthropogenic Greenhouse Gas Emissions\n\nOCO-2 MIP Top-Down National CO₂ Budgets - Model Output"
  },
  {
    "objectID": "advanceduser.html#natural-greenhouse-gas-emissions-and-sinks",
    "href": "advanceduser.html#natural-greenhouse-gas-emissions-and-sinks",
    "title": "U.S. Greenhouse Gas Center: Advanced User Notebook",
    "section": "Natural Greenhouse Gas Emissions and Sinks",
    "text": "Natural Greenhouse Gas Emissions and Sinks"
  },
  {
    "objectID": "advanceduser.html#large-emissions-events",
    "href": "advanceduser.html#large-emissions-events",
    "title": "U.S. Greenhouse Gas Center: Advanced User Notebook",
    "section": "Large Emissions Events",
    "text": "Large Emissions Events"
  },
  {
    "objectID": "advanceduser.html#greenhouse-gas-concentrations",
    "href": "advanceduser.html#greenhouse-gas-concentrations",
    "title": "U.S. Greenhouse Gas Center: Advanced User Notebook",
    "section": "Greenhouse Gas Concentrations",
    "text": "Greenhouse Gas Concentrations"
  },
  {
    "objectID": "advanceduser.html#socioeconomic",
    "href": "advanceduser.html#socioeconomic",
    "title": "U.S. Greenhouse Gas Center: Advanced User Notebook",
    "section": "Socioeconomic",
    "text": "Socioeconomic"
  },
  {
    "objectID": "advanceduser.html#contact",
    "href": "advanceduser.html#contact",
    "title": "U.S. Greenhouse Gas Center: Advanced User Notebook",
    "section": "Contact",
    "text": "Contact\nFor technical help or general questions, please contact the support team using the feedback form."
  },
  {
    "objectID": "generating_statistics_for_validation/odiac-stats-2023/generate_odiac_stats.html",
    "href": "generating_statistics_for_validation/odiac-stats-2023/generate_odiac_stats.html",
    "title": "U.S. Greenhouse Gas Center Documentation",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\nimport rasterio\nfrom glob import glob\nimport pathlib\nimport boto3\nimport pandas as pd\nimport calendar\nimport seaborn as sns\nimport json\nimport re\n\n\n# Enter the year you want to run validation on\nvyear=2022 # summary json files will be later generated for the year you provide here\ndata_dir=\"data/\" # make sure you have the data for vyear in your data directory\n\n\nsession = boto3.session.Session()\ns3_client = session.client(\"s3\")\n\ndataset_name= \"odiac-ffco2-monthgrid-v2023\"\ncog_data_bucket=\"ghgc-data-store-develop\"\ncog_data_prefix = f\"transformed_cogs/{dataset_name}\"\n\n\ndef get_all_s3_keys(bucket, model_name, ext):\n    \"\"\"Get a list of all keys in an S3 bucket.\"\"\"\n    keys = []\n\n    kwargs = {\"Bucket\": bucket, \"Prefix\": f\"{model_name}/\"}\n    while True:\n        resp = s3_client.list_objects_v2(**kwargs)\n        for obj in resp[\"Contents\"]:\n            if obj[\"Key\"].endswith(ext) and \"historical\" not in obj[\"Key\"]:\n                keys.append(obj[\"Key\"])\n\n        try:\n            kwargs[\"ContinuationToken\"] = resp[\"NextContinuationToken\"]\n        except KeyError:\n            break\n\n    return keys\n\nkeys = get_all_s3_keys(cog_data_bucket, cog_data_prefix, \".tif\")\n\n# Extract only the COGs for selected year\npattern = re.compile(rf'{vyear}(0[1-9]|1[0-2])')\nkeys = [path for path in keys if pattern.search(path)]\n\n\n# Initialize the summary variables\nsummary_dict_netcdf, summary_dict_cog = {}, {}\noverall_stats_netcdf, overall_stats_cog = {}, {}\nfull_data_df_netcdf, full_data_df_cog = pd.DataFrame(), pd.DataFrame()\n\n\n# Process the COGs to get the statistics\nfor key in keys:\n    url=f\"s3://{cog_data_bucket}/{key}\"\n    with rasterio.open(url) as src:\n        filename_elements = re.split(\"[_ ? . ]\", url)\n        for band in src.indexes:\n            print(\"_\".join(filename_elements[1:6]))\n            idx = pd.MultiIndex.from_product(\n                    [\n                        [\"_\".join(filename_elements[1:6])],\n                        [filename_elements[5]],\n                        [x for x in np.arange(1, src.height + 1)],\n                    ]\n                )\n            raster_data = src.read(band)\n            raster_data[raster_data == -9999] = 0 # because we did that in the transformation script\n            temp = pd.DataFrame(index=idx, data=raster_data)\n            full_data_df_cog = full_data_df_cog._append(temp, ignore_index=False)\n\n            # Calculate summary statistics\n            min_value = np.float64(temp.values.min())\n            max_value = np.float64(temp.values.max())\n            mean_value = np.float64(temp.values.mean())\n            std_value = np.float64(temp.values.std())\n\n            summary_dict_cog[\n                    f'{\"_\".join(filename_elements[1:5])}_{filename_elements[5][:4]}_{calendar.month_name[int(filename_elements[5][4:])]}'\n                ] = {\n                    \"min_value\": min_value,\n                    \"max_value\": max_value,\n                    \"mean_value\": mean_value,\n                    \"std_value\": std_value,\n                }\n\n\n# Process the raw files for selected year to get the statistics \ntif_files = glob(f\"{data_dir}{vyear}/*.tif\", recursive=True)\nfor tif_file in tif_files:\n    file_name = pathlib.Path(tif_file).name[:-4]\n    print(file_name)\n    with rasterio.open(tif_file) as src:\n        for band in src.indexes:\n            idx = pd.MultiIndex.from_product(\n                [\n                    [pathlib.Path(tif_file).name[:-9]],\n                    [pathlib.Path(tif_file).name[-8:-4]],\n                    [x for x in np.arange(1, src.height + 1)],\n                ]\n            )\n            # Read the raster data\n            raster_data = src.read(band)\n            #raster_data[raster_data == -9999] = np.nan\n            temp = pd.DataFrame(index=idx, data=raster_data)\n            full_data_df_netcdf = full_data_df_netcdf._append(temp, ignore_index=False)\n\n            # Calculate summary statistics\n            min_value = np.float64(temp.values.min())\n            max_value = np.float64(temp.values.max())\n            mean_value = np.float64(temp.values.mean())\n            std_value = np.float64(temp.values.std())\n\n            summary_dict_netcdf[\n                f'{tif_file.split(\"/\")[-1][:-9]}_{calendar.month_name[int(tif_file.split(\"/\")[-1][-6:-4])]}'\n            ] = {\n                \"min_value\": min_value,\n                \"max_value\": max_value,\n                \"mean_value\": mean_value,\n                \"std_value\": std_value,\n            }\n            \n\n\n# Merge monthly stats for COGs and raw files in a csv file \ncog_df = pd.DataFrame(summary_dict_cog).T.reset_index()\nraw_df = pd.DataFrame(summary_dict_netcdf).T.reset_index()\ncog_df['date']= cog_df[\"index\"].apply(lambda x: (x.split(\"_\")[-1]+x.split(\"_\")[-2]) )\nraw_df['date']= raw_df[\"index\"].apply(lambda x: (x.split(\"_\")[-1]+str(vyear)) )\ncheck_df=pd.merge(cog_df, raw_df[[\"min_value\",\"max_value\",\"mean_value\",\"std_value\",\"date\"]], how='inner', on='date',suffixes=('', '_raw'))\ncheck_df.to_csv(f\"monthly_stats_{vyear}.csv\")\n\n\n# Calculate the overall data stat for that year\noverall_stats_netcdf[\"min_value\"] = np.float64(full_data_df_netcdf.values.min())\noverall_stats_netcdf[\"max_value\"] = np.float64(full_data_df_netcdf.values.max())\noverall_stats_netcdf[\"mean_value\"] = np.float64(full_data_df_netcdf.values.mean())\noverall_stats_netcdf[\"std_value\"] = np.float64(full_data_df_netcdf.values.std())\n\noverall_stats_cog[\"min_value\"] = np.float64(full_data_df_cog.values.min())\noverall_stats_cog[\"max_value\"] = np.float64(full_data_df_cog.values.max())\noverall_stats_cog[\"mean_value\"] = np.float64(full_data_df_cog.values.mean())\noverall_stats_cog[\"std_value\"] = np.float64(full_data_df_cog.values.std())\n\n\n\ndata = {\n    \"Stats for raw netCDF files.\": summary_dict_netcdf,\n    \"Stats for transformed COG files.\": summary_dict_cog\n}\n\n# Writing to JSON file\nwith open(f\"monthly_stats_{vyear}.json\", \"w\") as fp:\n    json.dump(data, fp, indent=4) \n\ndata = {\n    \"Stats for raw netCDF files.\": overall_stats_netcdf,\n    \"Stats for transformed COG files.\": overall_stats_cog\n}\n\n# Writing to JSON file\nwith open(f\"overall_stats_{vyear}.json\", \"w\") as fp:\n    json.dump(data, fp, indent=4) \n\n\n\n\n Back to top"
  },
  {
    "objectID": "processing_and_verification_reports/tm54dvar-ch4flux-monthgrid-v1_Processing and Verification Report.html",
    "href": "processing_and_verification_reports/tm54dvar-ch4flux-monthgrid-v1_Processing and Verification Report.html",
    "title": "TM5-4DVar Isotopic CH₄ Inverse Fluxes",
    "section": "",
    "text": "This browser does not support PDFs. Please download the PDF to view it: Download PDF.\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Processing and Verification Reports",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "TM5-4DVar Isotopic CH₄ Inverse Fluxes"
    ]
  },
  {
    "objectID": "processing_and_verification_reports/eccodarwin-co2flux-monthgrid-v5_Processing and Verification Report.html",
    "href": "processing_and_verification_reports/eccodarwin-co2flux-monthgrid-v5_Processing and Verification Report.html",
    "title": "Air-Sea CO₂ Flux, ECCO-Darwin Model v5",
    "section": "",
    "text": "This browser does not support PDFs. Please download the PDF to view it: Download PDF.\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Processing and Verification Reports",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "Air-Sea CO₂ Flux, ECCO-Darwin Model v5"
    ]
  },
  {
    "objectID": "processing_and_verification_reports/sedac-popdensity-yeargrid5yr-v4.11_Processing and Verification Report.html",
    "href": "processing_and_verification_reports/sedac-popdensity-yeargrid5yr-v4.11_Processing and Verification Report.html",
    "title": "SEDAC Gridded World Population Density",
    "section": "",
    "text": "This browser does not support PDFs. Please download the PDF to view it: Download PDF.\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Processing and Verification Reports",
      "Socioeconomic",
      "SEDAC Gridded World Population Density"
    ]
  },
  {
    "objectID": "processing_and_verification_reports/epa-ch4emission-grid-v2express_Processing and Verification Report.html",
    "href": "processing_and_verification_reports/epa-ch4emission-grid-v2express_Processing and Verification Report.html",
    "title": "Gridded Anthropogenic Methane Emissions Inventory",
    "section": "",
    "text": "This browser does not support PDFs. Please download the PDF to view it: Download PDF.\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Processing and Verification Reports",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "Gridded Anthropogenic Methane Emissions Inventory"
    ]
  },
  {
    "objectID": "processing_and_verification_reports/noaa-gggrn-ch4-concentrations_Processing and Verification Report.html",
    "href": "processing_and_verification_reports/noaa-gggrn-ch4-concentrations_Processing and Verification Report.html",
    "title": "Atmospheric Methane Concentrations from the NOAA Global Monitoring Laboratory",
    "section": "",
    "text": "This browser does not support PDFs. Please download the PDF to view it: Download PDF.\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Processing and Verification Reports",
      "Greenhouse Gas Concentrations",
      "Atmospheric Methane Concentrations from the NOAA Global Monitoring Laboratory"
    ]
  },
  {
    "objectID": "processing_and_verification_reports/odiac-ffco2-monthgrid-v2023_Processing and Verification Report.html",
    "href": "processing_and_verification_reports/odiac-ffco2-monthgrid-v2023_Processing and Verification Report.html",
    "title": "ODIAC Fossil Fuel CO₂ Emissions",
    "section": "",
    "text": "This browser does not support PDFs. Please download the PDF to view it: Download PDF.\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Processing and Verification Reports",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "ODIAC Fossil Fuel CO₂ Emissions"
    ]
  },
  {
    "objectID": "processing_and_verification_reports/noaa-gggrn-co2-concentrations_Processing and Verification Report.html",
    "href": "processing_and_verification_reports/noaa-gggrn-co2-concentrations_Processing and Verification Report.html",
    "title": "Atmospheric Carbon Dioxide Concentrations from NOAA Global Monitoring Laboratory",
    "section": "",
    "text": "This browser does not support PDFs. Please download the PDF to view it: Download PDF.\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Processing and Verification Reports",
      "Greenhouse Gas Concentrations",
      "Atmospheric Carbon Dioxide Concentrations from NOAA Global Monitoring Laboratory"
    ]
  },
  {
    "objectID": "data_workflow/gosat-based-ch4budget-yeargrid-v1_Data_Flow.html",
    "href": "data_workflow/gosat-based-ch4budget-yeargrid-v1_Data_Flow.html",
    "title": "GOSAT-based Top-down Total and Natural Methane Emissions",
    "section": "",
    "text": "GOSAT-based Top-down Total and Natural Methane Emissions\n\n\n\nData Flow Diagram Extending From Acquisition/Creation to User Delivery\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Data Flow Diagrams",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "GOSAT-based Top-down Total and Natural Methane Emissions"
    ]
  },
  {
    "objectID": "data_workflow/noaa-gggrn-co2-concentrations_Data_Flow.html",
    "href": "data_workflow/noaa-gggrn-co2-concentrations_Data_Flow.html",
    "title": "Atmospheric Carbon Dioxide Concentrations from the NOAA Global Monitoring Laboratory",
    "section": "",
    "text": "Atmospheric Carbon Dioxide Concentrations from the NOAA Global Monitoring Laboratory\n\n\n\nData Flow Diagram Extending From Acquisition/Creation to User Delivery\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Data Flow Diagrams",
      "Greenhouse Gas Concentrations",
      "Atmospheric Carbon Dioxide Concentrations from the NOAA Global Monitoring Laboratory"
    ]
  },
  {
    "objectID": "data_workflow/sedac-popdensity-yeargrid5yr-v4.11_Data_Flow.html",
    "href": "data_workflow/sedac-popdensity-yeargrid5yr-v4.11_Data_Flow.html",
    "title": "SEDAC Gridded World Population Data",
    "section": "",
    "text": "SEDAC Gridded World Population Data\n\n\n\nData Flow Diagram Extending From Acquisition/Creation to User Delivery\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Data Flow Diagrams",
      "Socioeconomic",
      "SEDAC Gridded World Population Data"
    ]
  },
  {
    "objectID": "data_workflow/emit-ch4plume-v1_Data_Flow.html",
    "href": "data_workflow/emit-ch4plume-v1_Data_Flow.html",
    "title": "EMIT Methane Point Source Plume Complexes",
    "section": "",
    "text": "EMIT Methane Point Source Plume Complexes\n\n\n\nData Flow Diagram Extending From Acquisition/Creation to User Delivery\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Data Flow Diagrams",
      "Large Emissions Events",
      "EMIT Methane Point Source Plume Complexes"
    ]
  },
  {
    "objectID": "data_workflow/epa-ch4emission-grid-v2express_Data_Flow.html",
    "href": "data_workflow/epa-ch4emission-grid-v2express_Data_Flow.html",
    "title": "Gridded Anthropogenic Methane Emissions Inventory",
    "section": "",
    "text": "Gridded Anthropogenic Methane Emissions Inventory\n\n\n\nData Flow Diagram Extending From Acquisition/Creation to User Delivery\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Data Flow Diagrams",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "Gridded Anthropogenic Methane Emissions Inventory"
    ]
  },
  {
    "objectID": "data_workflow/oco2geos-co2-daygrid-v10r_Data_Flow.html",
    "href": "data_workflow/oco2geos-co2-daygrid-v10r_Data_Flow.html",
    "title": "OCO-2 GEOS Column CO₂ Concentrations",
    "section": "",
    "text": "OCO-2 GEOS Column CO₂ Concentrations\n\n\n\nData Flow Diagram Extending From Acquisition/Creation to User Delivery\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Data Flow Diagrams",
      "Greenhouse Gas Concentrations",
      "OCO-2 GEOS Column CO₂ Concentrations"
    ]
  },
  {
    "objectID": "cog_transformation/epa-ch4emission-grid-v2express_layers_update.html",
    "href": "cog_transformation/epa-ch4emission-grid-v2express_layers_update.html",
    "title": "Gridded Anthropogenic Methane Emissions Inventory",
    "section": "",
    "text": "This script was used to add concatenated layers and transform Gridded Anthropogenic Methane Emissions Inventory dataset from netCDF to Cloud Optimized GeoTIFF (COG) format for display in the Greenhouse Gas (GHG) Center.\n\nimport os\nimport xarray\nimport re\nimport pandas as pd\nimport json\nimport tempfile\nimport boto3\nfrom datetime import datetime\nimport numpy as np\n\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\nTrue\n\n\n\n# session = boto3.session.Session()\nsession = boto3.Session(\n    aws_access_key_id=os.environ.get(\"AWS_ACCESS_KEY_ID\"),\n    aws_secret_access_key=os.environ.get(\"AWS_SECRET_ACCESS_KEY\"),\n    aws_session_token=os.environ.get(\"AWS_SESSION_TOKEN\"),\n)\ns3_client = session.client(\"s3\")\nbucket_name = (\n    \"ghgc-data-store-dev\"  # S3 bucket where the COGs are stored after transformation\n)\nFOLDER_NAME = \"../data/epa_emissions_express_extension\"\ns3_folder_name = \"epa_express_extension_Mg_km2_yr\"\n\nfiles_processed = pd.DataFrame(\n    columns=[\"file_name\", \"COGs_created\"]\n)  # A dataframe to keep track of the files that we have transformed into COGs\n\n# Reading the raw netCDF files from local machine\nfor name in os.listdir(FOLDER_NAME):\n    xds = xarray.open_dataset(f\"{FOLDER_NAME}/{name}\", engine=\"netcdf4\")\n    xds = xds.assign_coords(lon=(((xds.lon + 180) % 360) - 180)).sortby(\"lon\")\n    variable = [var for var in xds.data_vars]\n    new_variables = {\n        \"all-variables\": variable[:-1],\n        \"agriculture\": variable[17:21],\n        \"natural-gas-systems\": variable[10:15] + [variable[26]],\n        \"petroleum-systems\": variable[5:9],\n        \"waste\": variable[21:26],\n        \"coal-mines\": variable[2:5],\n        \"other\": variable[:2] + [variable[9]] + variable[15:17],\n    }\n    filename = name.split(\"/ \")[-1]\n    filename_elements = re.split(\"[_ .]\", filename)\n    start_time = datetime(int(filename_elements[-2]), 1, 1)\n\n    for time_increment in range(0, len(xds.time)):\n        for key, value in new_variables.items():\n            data = np.zeros(dtype=np.float32, shape=(len(xds.lat), len(xds.lon)))\n            filename = name.split(\"/ \")[-1]\n            filename_elements = re.split(\"[_ .]\", filename)\n            for var in value:\n                data = data + getattr(xds.isel(time=time_increment), var)\n            # data = np.round(data / pow(10, 9), 2)\n            data.values[data.values==0] = np.nan\n            data = data*((1/(6.022*pow(10,23)))*(16.04*pow(10,-6))*366*pow(10,10)*86400)\n            data = data.fillna(-9999)\n            data = data.isel(lat=slice(None, None, -1))\n            data.rio.set_spatial_dims(\"lon\", \"lat\", inplace=True)\n            data.rio.write_crs(\"epsg:4326\", inplace=True)\n\n            # # insert date of generated COG into filename\n            filename_elements.pop()\n            filename_elements[-1] = start_time.strftime(\"%Y\")\n            filename_elements.insert(2, key)\n            cog_filename = \"_\".join(filename_elements)\n            # # add extension\n            cog_filename = f\"{cog_filename}.tif\"\n\n            with tempfile.NamedTemporaryFile() as temp_file:\n                data.rio.to_raster(\n                    temp_file.name,\n                    driver=\"COG\",\n                )\n                s3_client.upload_file(\n                    Filename=temp_file.name,\n                    Bucket=bucket_name,\n                    Key=f\"{s3_folder_name}/{cog_filename}\",\n                )\n\n                files_processed = files_processed._append(\n                    {\"file_name\": name, \"COGs_created\": cog_filename},\n                    ignore_index=True,\n                )\n\n                print(f\"Generated and saved COG: {cog_filename}\")\nprint(\"Done generating COGs\")\n\nTraceback (most recent call last):\n  File \"_pydevd_bundle/pydevd_cython.pyx\", line 1078, in _pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\n  File \"_pydevd_bundle/pydevd_cython.pyx\", line 297, in _pydevd_bundle.pydevd_cython.PyDBFrame.do_wait_suspend\n  File \"/Users/vgaur/miniconda3/envs/cmip6/lib/python3.9/site-packages/debugpy/_vendored/pydevd/pydevd.py\", line 1976, in do_wait_suspend\n    keep_suspended = self._do_wait_suspend(thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\n  File \"/Users/vgaur/miniconda3/envs/cmip6/lib/python3.9/site-packages/debugpy/_vendored/pydevd/pydevd.py\", line 2011, in _do_wait_suspend\n    time.sleep(0.01)\nKeyboardInterrupt\n\n\n\n---------------------------------------------------------------------------\nKeyboardInterrupt                         Traceback (most recent call last)\n/Users/vgaur/ghgc-docs/cog_transformation/epa-ch4emission-grid-v2express_layers_update.ipynb Cell 4 line 4\n     &lt;a href='vscode-notebook-cell:/Users/vgaur/ghgc-docs/cog_transformation/epa-ch4emission-grid-v2express_layers_update.ipynb#W3sZmlsZQ%3D%3D?line=45'&gt;46&lt;/a&gt; # data = data*(9.74*pow(10,-11))\n     &lt;a href='vscode-notebook-cell:/Users/vgaur/ghgc-docs/cog_transformation/epa-ch4emission-grid-v2express_layers_update.ipynb#W3sZmlsZQ%3D%3D?line=46'&gt;47&lt;/a&gt; # data.values[data.values&lt;=np.nanpercentile(data.values, 50)] = np.nan\n     &lt;a href='vscode-notebook-cell:/Users/vgaur/ghgc-docs/cog_transformation/epa-ch4emission-grid-v2express_layers_update.ipynb#W3sZmlsZQ%3D%3D?line=47'&gt;48&lt;/a&gt; data = data.fillna(-9999)\n---&gt; &lt;a href='vscode-notebook-cell:/Users/vgaur/ghgc-docs/cog_transformation/epa-ch4emission-grid-v2express_layers_update.ipynb#W3sZmlsZQ%3D%3D?line=48'&gt;49&lt;/a&gt; data = data.isel(lat=slice(None, None, -1))\n     &lt;a href='vscode-notebook-cell:/Users/vgaur/ghgc-docs/cog_transformation/epa-ch4emission-grid-v2express_layers_update.ipynb#W3sZmlsZQ%3D%3D?line=49'&gt;50&lt;/a&gt; data.rio.set_spatial_dims(\"lon\", \"lat\", inplace=True)\n     &lt;a href='vscode-notebook-cell:/Users/vgaur/ghgc-docs/cog_transformation/epa-ch4emission-grid-v2express_layers_update.ipynb#W3sZmlsZQ%3D%3D?line=50'&gt;51&lt;/a&gt; data.rio.write_crs(\"epsg:4326\", inplace=True)\n\n/Users/vgaur/ghgc-docs/cog_transformation/epa-ch4emission-grid-v2express_layers_update.ipynb Cell 4 line 4\n     &lt;a href='vscode-notebook-cell:/Users/vgaur/ghgc-docs/cog_transformation/epa-ch4emission-grid-v2express_layers_update.ipynb#W3sZmlsZQ%3D%3D?line=45'&gt;46&lt;/a&gt; # data = data*(9.74*pow(10,-11))\n     &lt;a href='vscode-notebook-cell:/Users/vgaur/ghgc-docs/cog_transformation/epa-ch4emission-grid-v2express_layers_update.ipynb#W3sZmlsZQ%3D%3D?line=46'&gt;47&lt;/a&gt; # data.values[data.values&lt;=np.nanpercentile(data.values, 50)] = np.nan\n     &lt;a href='vscode-notebook-cell:/Users/vgaur/ghgc-docs/cog_transformation/epa-ch4emission-grid-v2express_layers_update.ipynb#W3sZmlsZQ%3D%3D?line=47'&gt;48&lt;/a&gt; data = data.fillna(-9999)\n---&gt; &lt;a href='vscode-notebook-cell:/Users/vgaur/ghgc-docs/cog_transformation/epa-ch4emission-grid-v2express_layers_update.ipynb#W3sZmlsZQ%3D%3D?line=48'&gt;49&lt;/a&gt; data = data.isel(lat=slice(None, None, -1))\n     &lt;a href='vscode-notebook-cell:/Users/vgaur/ghgc-docs/cog_transformation/epa-ch4emission-grid-v2express_layers_update.ipynb#W3sZmlsZQ%3D%3D?line=49'&gt;50&lt;/a&gt; data.rio.set_spatial_dims(\"lon\", \"lat\", inplace=True)\n     &lt;a href='vscode-notebook-cell:/Users/vgaur/ghgc-docs/cog_transformation/epa-ch4emission-grid-v2express_layers_update.ipynb#W3sZmlsZQ%3D%3D?line=50'&gt;51&lt;/a&gt; data.rio.write_crs(\"epsg:4326\", inplace=True)\n\nFile _pydevd_bundle/pydevd_cython.pyx:1363, in _pydevd_bundle.pydevd_cython.SafeCallWrapper.__call__()\n\nFile _pydevd_bundle/pydevd_cython.pyx:662, in _pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch()\n\nFile _pydevd_bundle/pydevd_cython.pyx:1087, in _pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch()\n\nFile _pydevd_bundle/pydevd_cython.pyx:1078, in _pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch()\n\nFile _pydevd_bundle/pydevd_cython.pyx:297, in _pydevd_bundle.pydevd_cython.PyDBFrame.do_wait_suspend()\n\nFile ~/miniconda3/envs/cmip6/lib/python3.9/site-packages/debugpy/_vendored/pydevd/pydevd.py:1976, in PyDB.do_wait_suspend(self, thread, frame, event, arg, exception_type)\n   1973             from_this_thread.append(frame_custom_thread_id)\n   1975     with self._threads_suspended_single_notification.notify_thread_suspended(thread_id, stop_reason):\n-&gt; 1976         keep_suspended = self._do_wait_suspend(thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\n   1978 frames_list = None\n   1980 if keep_suspended:\n   1981     # This means that we should pause again after a set next statement.\n\nFile ~/miniconda3/envs/cmip6/lib/python3.9/site-packages/debugpy/_vendored/pydevd/pydevd.py:2011, in PyDB._do_wait_suspend(self, thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\n   2008         self._call_mpl_hook()\n   2010     self.process_internal_commands()\n-&gt; 2011     time.sleep(0.01)\n   2013 self.cancel_async_evaluation(get_current_thread_id(thread), str(id(frame)))\n   2015 # process any stepping instructions\n\nKeyboardInterrupt: \n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "cog_transformation/oco2geos-co2-daygrid-v10r.html",
    "href": "cog_transformation/oco2geos-co2-daygrid-v10r.html",
    "title": "OCO-2 GEOS Column CO₂ Concentrations",
    "section": "",
    "text": "This script was used to transform the OCO-2 GEOS Column CO₂ Concentrations dataset from netCDF to Cloud Optimized GeoTIFF (COG) format for display in the Greenhouse Gas (GHG) Center.\n\nimport xarray\nimport re\nimport pandas as pd\nimport json\nimport tempfile\nimport boto3\nimport os\n\n\nsession = boto3.Session()\ns3_client = session.client(\"s3\")\nbucket_name = (\n    \"ghgc-data-store-dev\"  # S3 bucket where the COGs are stored after transformation\n)\nFOLDER_NAME = \"earth_data/geos_oco2\"\ns3_folder_name = \"geos-oco2\"\n\nerror_files = []\ncount = 0\nfiles_processed = pd.DataFrame(\n    columns=[\"file_name\", \"COGs_created\"]\n)  # A dataframe to keep track of the files that we have transformed into COGs\n\n# Reading the raw netCDF files from local machine\nfor name in os.listdir(FOLDER_NAME):\n    try:\n        xds = xarray.open_dataset(f\"{FOLDER_NAME}/{name}\", engine=\"netcdf4\")\n        xds = xds.assign_coords(lon=(((xds.lon + 180) % 360) - 180)).sortby(\"lon\")\n        variable = [var for var in xds.data_vars]\n        filename = name.split(\"/ \")[-1]\n        filename_elements = re.split(\"[_ .]\", filename)\n\n        for time_increment in range(0, len(xds.time)):\n            for var in variable:\n                filename = name.split(\"/ \")[-1]\n                filename_elements = re.split(\"[_ .]\", filename)\n                data = getattr(xds.isel(time=time_increment), var)\n                data = data.isel(lat=slice(None, None, -1))\n                data.rio.set_spatial_dims(\"lon\", \"lat\", inplace=True)\n                data.rio.write_crs(\"epsg:4326\", inplace=True)\n\n                # # insert date of generated COG into filename\n                filename_elements[-1] = filename_elements[-3]\n                filename_elements.insert(2, var)\n                filename_elements.pop(-3)\n                cog_filename = \"_\".join(filename_elements)\n                # # add extension\n                cog_filename = f\"{cog_filename}.tif\"\n\n                with tempfile.NamedTemporaryFile() as temp_file:\n                    data.rio.to_raster(\n                        temp_file.name,\n                        driver=\"COG\",\n                    )\n                    s3_client.upload_file(\n                        Filename=temp_file.name,\n                        Bucket=bucket_name,\n                        Key=f\"{s3_folder_name}/{cog_filename}\",\n                    )\n\n                files_processed = files_processed._append(\n                    {\"file_name\": name, \"COGs_created\": cog_filename},\n                    ignore_index=True,\n                )\n        count += 1\n        print(f\"Generated and saved COG: {cog_filename}\")\n    except OSError:\n        error_files.append(name)\n        pass\n\n# Generate the json file with the metadata that is present in the netCDF files.\nwith tempfile.NamedTemporaryFile(mode=\"w+\") as fp:\n    json.dump(xds.attrs, fp)\n    json.dump({\"data_dimensions\": dict(xds.dims)}, fp)\n    json.dump({\"data_variables\": list(xds.data_vars)}, fp)\n    fp.flush()\n\n    s3_client.upload_file(\n        Filename=fp.name,\n        Bucket=bucket_name,\n        Key=f\"{s3_folder_name}/metadata.json\",\n    )\n\n# creating the csv file with the names of files transformed.\nfiles_processed.to_csv(\n    f\"s3://{bucket_name}/{s3_folder_name}/files_converted.csv\",\n)\nprint(\"Done generating COGs\")\n\n\n\n\n Back to top",
    "crumbs": [
      "Data Transformation Notebooks",
      "Greenhouse Gas Concentrations",
      "OCO-2 GEOS Column CO₂ Concentrations"
    ]
  },
  {
    "objectID": "cog_transformation/casagfed-carbonflux-monthgrid-v3.html",
    "href": "cog_transformation/casagfed-carbonflux-monthgrid-v3.html",
    "title": "CASA-GFED3 Land Carbon Flux",
    "section": "",
    "text": "Code used to transform CASA-GFED3 Land Carbon Flux data from netcdf to Cloud Optimized Geotiff.\n\nimport os\nimport xarray\nimport re\nimport pandas as pd\nimport json\nimport tempfile\nimport boto3\n\n\nsession = boto3.session.Session()\ns3_client = session.client(\"s3\")\nbucket_name = \"ghgc-data-store-dev\"\ndate_fmt = \"%Y%m\"\n\nfiles_processed = pd.DataFrame(columns=[\"file_name\", \"COGs_created\"])\nfor name in os.listdir(\"geoscarb\"):\n    xds = xarray.open_dataset(\n        f\"geoscarb/{name}\",\n        engine=\"netcdf4\",\n    )\n    xds = xds.assign_coords(\n        longitude=(((xds.longitude + 180) % 360) - 180)\n    ).sortby(\"longitude\")\n    variable = [var for var in xds.data_vars]\n\n    for time_increment in range(0, len(xds.time)):\n        for var in variable[:-1]:\n            filename = name.split(\"/ \")[-1]\n            filename_elements = re.split(\"[_ .]\", filename)\n            data = getattr(xds.isel(time=time_increment), var)\n            data = data.isel(latitude=slice(None, None, -1))\n            data.rio.set_spatial_dims(\"longitude\", \"latitude\", inplace=True)\n            data.rio.write_crs(\"epsg:4326\", inplace=True)\n\n            date = data.time.dt.strftime(date_fmt).item(0)\n            # # insert date of generated COG into filename\n            filename_elements.pop()\n            filename_elements[-1] = date\n            filename_elements.insert(2, var)\n            cog_filename = \"_\".join(filename_elements)\n            # # add extension\n            cog_filename = f\"{cog_filename}.tif\"\n\n            with tempfile.NamedTemporaryFile() as temp_file:\n                data.rio.to_raster(\n                    temp_file.name,\n                    driver=\"COG\",\n                )\n                s3_client.upload_file(\n                    Filename=temp_file.name,\n                    Bucket=bucket_name,\n                    Key=f\"GEOS-Carbs/{cog_filename}\",\n                )\n\n            files_processed = files_processed._append(\n                {\"file_name\": name, \"COGs_created\": cog_filename},\n                ignore_index=True,\n            )\n\n            print(f\"Generated and saved COG: {cog_filename}\")\n\nwith tempfile.NamedTemporaryFile(mode=\"w+\") as fp:\n    json.dump(xds.attrs, fp)\n    json.dump({\"data_dimensions\": dict(xds.dims)}, fp)\n    json.dump({\"data_variables\": list(xds.data_vars)}, fp)\n    fp.flush()\n\n    s3_client.upload_file(\n        Filename=fp.name,\n        Bucket=bucket_name,\n        Key=\"GEOS-Carbs/metadata.json\",\n    )\nfiles_processed.to_csv(\n    f\"s3://{bucket_name}/GEOS-Carbs/files_converted.csv\",\n)\nprint(\"Done generating COGs\")\n\n\n\n\n Back to top"
  },
  {
    "objectID": "cog_transformation/sedac-popdensity-yeargrid5yr-v4.11.html",
    "href": "cog_transformation/sedac-popdensity-yeargrid5yr-v4.11.html",
    "title": "SEDAC Gridded World Population Data",
    "section": "",
    "text": "This script was used to transform SEDAC Gridded World Population Data from netCDF to Cloud Optimized GeoTIFF (COG) format for display in the Greenhouse Gas (GHG) Center.\n\nimport os\nimport xarray\nimport re\nimport pandas as pd\n\nimport tempfile\nimport boto3\n\n\nsession = boto3.session.Session()\ns3_client = session.client(\"s3\")\nbucket_name = (\n    \"ghgc-data-store-dev\"  # S3 bucket where the COGs are stored after transformation\n)\n\nfold_names = os.listdir(\"gpw\")\n\nfiles_processed = pd.DataFrame(\n    columns=[\"file_name\", \"COGs_created\"]\n)  # A dataframe to keep track of the files that we have transformed into COGs\n\n# Reading the raw netCDF files from local machine\nfor fol_ in fold_names:\n    for name in os.listdir(f\"gpw/{fol_}\"):\n        if name.endswith(\".tif\"):\n            xds = xarray.open_dataarray(f\"gpw/{fol_}/{name}\")\n\n            filename = name.split(\"/ \")[-1]\n            filename_elements = re.split(\"[_ .]\", filename)\n            # # insert date of generated COG into filename\n            filename_elements.pop()\n            filename_elements.append(filename_elements[-3])\n\n            xds.rio.set_spatial_dims(\"x\", \"y\", inplace=True)\n            xds.rio.write_crs(\"epsg:4326\", inplace=True)\n\n            cog_filename = \"_\".join(filename_elements)\n            # # add extension\n            cog_filename = f\"{cog_filename}.tif\"\n\n            with tempfile.NamedTemporaryFile() as temp_file:\n                xds.rio.to_raster(temp_file.name, driver=\"COG\")\n                s3_client.upload_file(\n                    Filename=temp_file.name,\n                    Bucket=bucket_name,\n                    Key=f\"gridded_population_cog/{cog_filename}\",\n                )\n\n            files_processed = files_processed._append(\n                {\"file_name\": name, \"COGs_created\": cog_filename},\n                ignore_index=True,\n            )\n\n            print(f\"Generated and saved COG: {cog_filename}\")\n\n\n# creating the csv file with the names of files transformed.\nfiles_processed.to_csv(\n    f\"s3://{bucket_name}/gridded_population_cog/files_converted.csv\",\n)\nprint(\"Done generating COGs\")\n\n\n\n\n Back to top",
    "crumbs": [
      "Data Transformation Notebooks",
      "Socioeconomic",
      "SEDAC Gridded World Population Data"
    ]
  },
  {
    "objectID": "cog_transformation/gosat-based-ch4budget-yeargrid-v1.html",
    "href": "cog_transformation/gosat-based-ch4budget-yeargrid-v1.html",
    "title": "GOSAT-based Top-down Methane Budgets",
    "section": "",
    "text": "This script was used to transform the GOSAT-based Top-down Methane Budgets dataset from netCDF to Cloud Optimized GeoTIFF (COG) format for display in the Greenhouse Gas (GHG) Center.\n\nimport os\nimport xarray\nimport re\nimport pandas as pd\nimport json\nimport tempfile\nimport boto3\nimport rasterio\nfrom datetime import datetime\nfrom dateutil.relativedelta import relativedelta\n\n\nsession = boto3.session.Session()\ns3_client = session.client(\"s3\")\nbucket_name = (\n    \"ghgc-data-store-dev\"  # S3 bucket where the COGs are stored after transformation\n)\nyear_ = datetime(2019, 1, 1)\nfolder_name = \"new_data/CH4-inverse-flux\"\n\nCOG_PROFILE = {\"driver\": \"COG\", \"compress\": \"DEFLATE\"}\n\nfiles_processed = pd.DataFrame(\n    columns=[\"file_name\", \"COGs_created\"]\n)  # A dataframe to keep track of the files that we have transformed into COGs\n\n# Reading the raw netCDF files from local machine\nfor name in os.listdir(folder_name):\n    ds = xarray.open_dataset(\n        f\"{folder_name}/{name}\",\n        engine=\"netcdf4\",\n    )\n\n    ds = ds.rename({\"dimy\": \"lat\", \"dimx\": \"lon\"})\n    # assign coords from dimensions\n    ds = ds.assign_coords(lon=(((ds.lon + 180) % 360) - 180)).sortby(\"lon\")\n    ds = ds.assign_coords(lat=((ds.lat / 180) * 180) - 90).sortby(\"lat\")\n\n    variable = [var for var in ds.data_vars]\n\n    for var in variable[2:]:\n        filename = name.split(\"/ \")[-1]\n        filename_elements = re.split(\"[_ .]\", filename)\n        data = ds[var]\n        filename_elements.pop()\n        filename_elements.insert(2, var)\n        cog_filename = \"_\".join(filename_elements)\n        # # add extension\n        cog_filename = f\"{cog_filename}.tif\"\n\n        data = data.reindex(lat=list(reversed(data.lat)))\n\n        data.rio.set_spatial_dims(\"lon\", \"lat\")\n        data.rio.write_crs(\"epsg:4326\", inplace=True)\n\n        # generate COG\n        COG_PROFILE = {\"driver\": \"COG\", \"compress\": \"DEFLATE\"}\n\n        with tempfile.NamedTemporaryFile() as temp_file:\n            data.rio.to_raster(temp_file.name, **COG_PROFILE)\n            s3_client.upload_file(\n                Filename=temp_file.name,\n                Bucket=bucket_name,\n                Key=f\"ch4_inverse_flux/{cog_filename}\",\n            )\n\n        files_processed = files_processed._append(\n            {\"file_name\": name, \"COGs_created\": cog_filename},\n            ignore_index=True,\n        )\n\n        print(f\"Generated and saved COG: {cog_filename}\")\n\n# Generate the json file with the metadata that is present in the netCDF files.\nwith tempfile.NamedTemporaryFile(mode=\"w+\") as fp:\n    json.dump(ds.attrs, fp)\n    json.dump({\"data_dimensions\": dict(ds.dims)}, fp)\n    json.dump({\"data_variables\": list(ds.data_vars)}, fp)\n    fp.flush()\n\n    s3_client.upload_file(\n        Filename=fp.name,\n        Bucket=bucket_name,\n        Key=\"ch4_inverse_flux/metadata.json\",\n    )\n\n# creating the csv file with the names of files transformed.\nfiles_processed.to_csv(\n    f\"s3://{bucket_name}/ch4_inverse_flux/files_converted.csv\",\n)\nprint(\"Done generating COGs\")\n\n\n\n\n Back to top",
    "crumbs": [
      "Data Transformation Notebooks",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "GOSAT-based Top-down Methane Budgets"
    ]
  },
  {
    "objectID": "cog_transformation/odiac-ffco2-monthgrid-v2022.html",
    "href": "cog_transformation/odiac-ffco2-monthgrid-v2022.html",
    "title": "ODIAC Fossil Fuel CO₂ Emissions",
    "section": "",
    "text": "This script was used to transform the ODIAC Fossil Fuel CO₂ Emissions dataset from GeoTIFF to Cloud Optimized GeoTIFF (COG) format for display in the Greenhouse Gas (GHG) Center.\n\nimport os\nimport xarray\nimport re\nimport pandas as pd\n\nimport tempfile\nimport boto3\n\n\nsession = boto3.session.Session()\ns3_client = session.client(\"s3\")\nbucket_name = \"ghgc-data-store-dev\" # S3 bucket where the COGs are stored after transformation\n\nfold_names = os.listdir(\"ODIAC\")\n\nfiles_processed = pd.DataFrame(columns=[\"file_name\", \"COGs_created\"])   # A dataframe to keep track of the files that we have transformed into COGs\n\n# Reading the raw netCDF files from local machine\nfor fol_ in fold_names:\n    for name in os.listdir(f\"ODIAC/{fol_}\"):\n        xds = xarray.open_dataarray(f\"ODIAC/{fol_}/{name}\")\n\n        filename = name.split(\"/ \")[-1]\n        filename_elements = re.split(\"[_ .]\", filename)\n        # # insert date of generated COG into filename\n        filename_elements.pop()\n        filename_elements[-1] = fol_ + filename_elements[-1][-2:]\n\n        xds.rio.set_spatial_dims(\"x\", \"y\", inplace=True)\n        xds.rio.write_nodata(-9999, inplace=True)\n        xds.rio.write_crs(\"epsg:4326\", inplace=True)\n\n        cog_filename = \"_\".join(filename_elements)\n        # # add extension\n        cog_filename = f\"{cog_filename}.tif\"\n\n        with tempfile.NamedTemporaryFile() as temp_file:\n            xds.rio.to_raster(\n                temp_file.name,\n                driver=\"COG\",\n            )\n            s3_client.upload_file(\n                Filename=temp_file.name,\n                Bucket=bucket_name,\n                Key=f\"ODIAC_geotiffs_COGs/{cog_filename}\",\n            )\n\n        files_processed = files_processed._append(\n            {\"file_name\": name, \"COGs_created\": cog_filename},\n            ignore_index=True,\n        )\n\n        print(f\"Generated and saved COG: {cog_filename}\")\n\n\n# creating the csv file with the names of files transformed.\nfiles_processed.to_csv(\n    f\"s3://{bucket_name}/ODIAC_COGs/files_converted.csv\",\n)\nprint(\"Done generating COGs\")\n\n\n\n\n Back to top"
  },
  {
    "objectID": "cog_transformation/tm54dvar-ch4flux-monthgrid-v1.html",
    "href": "cog_transformation/tm54dvar-ch4flux-monthgrid-v1.html",
    "title": "TM5-4DVar Isotopic CH₄ Inverse Fluxes",
    "section": "",
    "text": "This script was used to transform the TM5-4DVar Isotopic CH₄ Inverse Fluxes dataset from netCDF to Cloud Optimized GeoTIFF (COG) format for display in the Greenhouse Gas (GHG) Center.\n\nimport os\nimport xarray\nimport re\nimport pandas as pd\nimport json\nimport tempfile\nimport boto3\nfrom datetime import datetime\n\n\nsession = boto3.session.Session()\ns3_client = session.client(\"s3\")\nbucket_name = (\n    \"ghgc-data-store-dev\"  # S3 bucket where the COGs are stored after transformation\n)\nFOLDER_NAME = \"tm5-ch4-inverse-flux\"\n\nfiles_processed = pd.DataFrame(\n    columns=[\"file_name\", \"COGs_created\"]\n)  # A dataframe to keep track of the files that we have transformed into COGs\n\n# Reading the raw netCDF files from local machine\nfor name in os.listdir(FOLDER_NAME):\n    xds = xarray.open_dataset(f\"{FOLDER_NAME}/{name}\", engine=\"netcdf4\")\n    xds = xds.rename({\"latitude\": \"lat\", \"longitude\": \"lon\"})\n    xds = xds.assign_coords(lon=(((xds.lon + 180) % 360) - 180)).sortby(\"lon\")\n    variable = [var for var in xds.data_vars if \"global\" not in var]\n\n    for time_increment in range(0, len(xds.months)):\n        filename = name.split(\"/ \")[-1]\n        filename_elements = re.split(\"[_ .]\", filename)\n        start_time = datetime(int(filename_elements[-2]), time_increment + 1, 1)\n        for var in variable:\n            data = getattr(xds.isel(months=time_increment), var)\n            data = data.isel(lat=slice(None, None, -1))\n            data.rio.set_spatial_dims(\"lon\", \"lat\", inplace=True)\n            data.rio.write_crs(\"epsg:4326\", inplace=True)\n\n            # # insert date of generated COG into filename\n            filename_elements.pop()\n            filename_elements[-1] = start_time.strftime(\"%Y%m\")\n            filename_elements.insert(2, var)\n            cog_filename = \"_\".join(filename_elements)\n            # # add extension\n            cog_filename = f\"{cog_filename}.tif\"\n\n            with tempfile.NamedTemporaryFile() as temp_file:\n                data.rio.to_raster(\n                    temp_file.name,\n                    driver=\"COG\",\n                )\n                s3_client.upload_file(\n                    Filename=temp_file.name,\n                    Bucket=bucket_name,\n                    Key=f\"{FOLDER_NAME}/{cog_filename}\",\n                )\n\n            files_processed = files_processed._append(\n                {\"file_name\": name, \"COGs_created\": cog_filename},\n                ignore_index=True,\n            )\n\n            print(f\"Generated and saved COG: {cog_filename}\")\n\n# Generate the json file with the metadata that is present in the netCDF files.\nwith tempfile.NamedTemporaryFile(mode=\"w+\") as fp:\n    json.dump(xds.attrs, fp)\n    json.dump({\"data_dimensions\": dict(xds.dims)}, fp)\n    json.dump({\"data_variables\": list(xds.data_vars)}, fp)\n    fp.flush()\n\n    s3_client.upload_file(\n        Filename=fp.name,\n        Bucket=bucket_name,\n        Key=f\"{FOLDER_NAME}/metadata.json\",\n    )\n\n# creating the csv file with the names of files transformed.\nfiles_processed.to_csv(\n    f\"s3://{bucket_name}/{FOLDER_NAME}/files_converted.csv\",\n)\nprint(\"Done generating COGs\")\n\n\n\n\n Back to top",
    "crumbs": [
      "Data Transformation Notebooks",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "TM5-4DVar Isotopic CH₄ Inverse Fluxes"
    ]
  },
  {
    "objectID": "cog_transformation/lpjwsl-wetlandch4-daygrid-v1.html",
    "href": "cog_transformation/lpjwsl-wetlandch4-daygrid-v1.html",
    "title": "Wetland Methane Emissions, LPJ-wsl Model",
    "section": "",
    "text": "This script was used to transform the Wetland Methane Emissions, LPJ-wsl Model dataset from netCDF to Cloud Optimized GeoTIFF (COG) format for display in the Greenhouse Gas (GHG) Center.\n\nimport os\nimport xarray\nimport re\nimport pandas as pd\nimport json\nimport tempfile\nimport boto3\nfrom datetime import datetime, timedelta\n\n\nsession = boto3.session.Session()\ns3_client = session.client(\"s3\")\nbucket_name = (\n    \"ghgc-data-store-dev\"  # S3 bucket where the COGs are stored after transformation\n)\nFOLDER_NAME = \"NASA_GSFC_ch4_wetlands_daily\"\ndirectory = \"ch4_wetlands_daily\"\n\nfiles_processed = pd.DataFrame(\n    columns=[\"file_name\", \"COGs_created\"]\n)  # A dataframe to keep track of the files that we have transformed into COGs\n\n# Reading the raw netCDF files from local machine\nfor name in os.listdir(directory):\n    xds = xarray.open_dataset(\n        f\"{directory}/{name}\", engine=\"netcdf4\", decode_times=False\n    )\n    xds = xds.assign_coords(longitude=(((xds.longitude + 180) % 360) - 180)).sortby(\n        \"longitude\"\n    )\n    variable = [var for var in xds.data_vars]\n    filename = name.split(\"/ \")[-1]\n    filename_elements = re.split(\"[_ .]\", filename)\n    start_time = datetime(int(filename_elements[-2]), 1, 1)\n\n    for time_increment in range(0, len(xds.time)):\n        for var in variable:\n            filename = name.split(\"/ \")[-1]\n            filename_elements = re.split(\"[_ .]\", filename)\n            data = getattr(xds.isel(time=time_increment), var)\n            data = data.isel(latitude=slice(None, None, -1))\n            data = data * 1000\n            data.rio.set_spatial_dims(\"longitude\", \"latitude\", inplace=True)\n            data.rio.write_crs(\"epsg:4326\", inplace=True)\n            date = start_time + timedelta(hours=data.time.item(0))\n\n            # # insert date of generated COG into filename\n            filename_elements.pop()\n            filename_elements[-1] = date.strftime(\"%Y%m%d\")\n            filename_elements.insert(2, var)\n            cog_filename = \"_\".join(filename_elements)\n            # # add extension\n            cog_filename = f\"{cog_filename}.tif\"\n\n            with tempfile.NamedTemporaryFile() as temp_file:\n                data.rio.to_raster(\n                    temp_file.name,\n                    driver=\"COG\",\n                )\n                s3_client.upload_file(\n                    Filename=temp_file.name,\n                    Bucket=bucket_name,\n                    Key=f\"{FOLDER_NAME}/{cog_filename}\",\n                )\n\n            files_processed = files_processed._append(\n                {\"file_name\": name, \"COGs_created\": cog_filename},\n                ignore_index=True,\n            )\n\n            print(f\"Generated and saved COG: {cog_filename}\")\n\n# Generate the json file with the metadata that is present in the netCDF files.\nwith tempfile.NamedTemporaryFile(mode=\"w+\") as fp:\n    json.dump(xds.attrs, fp)\n    json.dump({\"data_dimensions\": dict(xds.dims)}, fp)\n    json.dump({\"data_variables\": list(xds.data_vars)}, fp)\n    fp.flush()\n\n    s3_client.upload_file(\n        Filename=fp.name,\n        Bucket=bucket_name,\n        Key=f\"{FOLDER_NAME}/metadata.json\",\n    )\n\n# creating the csv file with the names of files transformed.\nfiles_processed.to_csv(\n    f\"s3://{bucket_name}/{FOLDER_NAME}/files_converted.csv\",\n)\nprint(\"Done generating COGs\")\n\n\n\n\n Back to top"
  },
  {
    "objectID": "processingreport.html",
    "href": "processingreport.html",
    "title": "U.S. Greenhouse Gas Center: Processing and Verification Reports",
    "section": "",
    "text": "Welcome to the U.S. Greenhouse Gas (GHG) Center processing and verification reports. These reports verify that the accuracy and integrity of each dataset in the US GHG Center is maintained once it is processed into the Center.\nThe reports are grouped topically and labeled by dataset name. Click on a dataset name to view the processing and verification report for that dataset.\nExamples of processing that may occur include transforming data from its source format into a could-optimized format, converting the units of the source data into a more common or standard unit, and flagging “nodata” values to ensure accurate data visualization. We strive to handle all data with extreme care, and share these reports to provide transparency and insight into any processing that is applied, while ensuring accuracy and reliability every step of the way.\nJoin us in our mission to make data-driven environmental solutions accessible. Explore, analyze, and make a difference with the US GHG Center.\nView the US GHG Center Data Catalog",
    "crumbs": [
      "Processing and Verification Reports"
    ]
  },
  {
    "objectID": "processingreport.html#gridded-anthropogenic-greenhouse-gas-emissions",
    "href": "processingreport.html#gridded-anthropogenic-greenhouse-gas-emissions",
    "title": "U.S. Greenhouse Gas Center: Processing and Verification Reports",
    "section": "Gridded Anthropogenic Greenhouse Gas Emissions",
    "text": "Gridded Anthropogenic Greenhouse Gas Emissions\n\nOCO-2 MIP Top-Down CO₂ Budgets Processing and Verification Report\nODIAC Fossil Fuel CO₂ Emissions Processing and Verification Report\nTM5-4DVar Isotopic CH₄ Inverse Fluxes Processing and Verification Report\nU.S. Gridded Anthropogenic Methane Emissions Inventory Processing and Verification Report",
    "crumbs": [
      "Processing and Verification Reports"
    ]
  },
  {
    "objectID": "processingreport.html#natural-greenhouse-gas-emissions-and-sinks",
    "href": "processingreport.html#natural-greenhouse-gas-emissions-and-sinks",
    "title": "U.S. Greenhouse Gas Center: Processing and Verification Reports",
    "section": "Natural Greenhouse Gas Emissions and Sinks",
    "text": "Natural Greenhouse Gas Emissions and Sinks\n\nAir-Sea CO₂ Flux, ECCO-Darwin Model v5 Processing and Verification Report\nMiCASA Land Carbon Flux Processing and Verification Report\nGOSAT-based Top-down Total and Natural Methane Emissions Processing and Verification Report\nOCO-2 MIP Top-Down CO₂ Budgets Processing and Verification Report\nTM5-4DVar Isotopic CH₄ Inverse Fluxes Processing and Verification Report\nWetland Methane Emissions, LPJ-EOSIM model Processing and Verification Report",
    "crumbs": [
      "Processing and Verification Reports"
    ]
  },
  {
    "objectID": "processingreport.html#large-emissions-events",
    "href": "processingreport.html#large-emissions-events",
    "title": "U.S. Greenhouse Gas Center: Processing and Verification Reports",
    "section": "Large Emissions Events",
    "text": "Large Emissions Events\n\nEMIT Methane Point Source Plume Complexes Processing and Verification Report",
    "crumbs": [
      "Processing and Verification Reports"
    ]
  },
  {
    "objectID": "processingreport.html#greenhouse-gas-concentrations",
    "href": "processingreport.html#greenhouse-gas-concentrations",
    "title": "U.S. Greenhouse Gas Center: Processing and Verification Reports",
    "section": "Greenhouse Gas Concentrations",
    "text": "Greenhouse Gas Concentrations\n\nAtmospheric Carbon Dioxide Concentrations from NOAA Global Monitoring Laboratory Processing and Verification Report\nAtmospheric Methane Concentrations from NOAA Global Monitoring Laboratory Processing and Verification Report\nOCO-2 GEOS Column CO₂ Concentrations Processing and Verification Report",
    "crumbs": [
      "Processing and Verification Reports"
    ]
  },
  {
    "objectID": "processingreport.html#socioeconomic",
    "href": "processingreport.html#socioeconomic",
    "title": "U.S. Greenhouse Gas Center: Processing and Verification Reports",
    "section": "Socioeconomic",
    "text": "Socioeconomic\n\nSEDAC Gridded World Population Density Processing and Verification Report",
    "crumbs": [
      "Processing and Verification Reports"
    ]
  },
  {
    "objectID": "processingreport.html#contact",
    "href": "processingreport.html#contact",
    "title": "U.S. Greenhouse Gas Center: Processing and Verification Reports",
    "section": "Contact",
    "text": "Contact\nFor technical help or general questions, please contact the support team using the feedback form.",
    "crumbs": [
      "Processing and Verification Reports"
    ]
  },
  {
    "objectID": "workflow.html",
    "href": "workflow.html",
    "title": "U.S. Greenhouse Gas Center: Data Flow Diagrams",
    "section": "",
    "text": "Welcome to the homepage for U.S. Greenhouse Gas (GHG) Center data workflow diagrams. Use these diagrams to discover the journey of each dataset from acquisition to integration in the US GHG Center.\nData flow diagrams are grouped topically and labeled by dataset name. Click on a dataset name to view the data flow diagram for that dataset, which summarizes the process followed to bring the dataset into the US GHG Center.\nJoin us in our mission to make data-driven environmental solutions accessible. Explore, analyze, and make a difference with the US GHG Center.\nView the US GHG Center Data Catalog",
    "crumbs": [
      "Data Flow Diagrams"
    ]
  },
  {
    "objectID": "workflow.html#gridded-anthropogenic-greenhouse-gas-emissions",
    "href": "workflow.html#gridded-anthropogenic-greenhouse-gas-emissions",
    "title": "U.S. Greenhouse Gas Center: Data Flow Diagrams",
    "section": "Gridded Anthropogenic Greenhouse Gas Emissions",
    "text": "Gridded Anthropogenic Greenhouse Gas Emissions\n\nOCO-2 MIP Top-Down CO₂ Budgets Data Flow Diagram\nODIAC Fossil Fuel CO₂ Emissions Data Flow Diagram\nTM5-4DVar Isotopic CH₄ Inverse Fluxes Data Flow Diagram\nU.S. Gridded Anthropogenic Methane Emissions Inventory Data Flow Diagram",
    "crumbs": [
      "Data Flow Diagrams"
    ]
  },
  {
    "objectID": "workflow.html#natural-greenhouse-gas-sources-emissions-and-sinks",
    "href": "workflow.html#natural-greenhouse-gas-sources-emissions-and-sinks",
    "title": "U.S. Greenhouse Gas Center: Data Flow Diagrams",
    "section": "Natural Greenhouse Gas Sources Emissions and Sinks",
    "text": "Natural Greenhouse Gas Sources Emissions and Sinks\n\nAir-Sea CO₂ Flux, ECCO-Darwin Model v5 Data Flow Diagram\nMiCASA Land Carbon Flux Data Flow Diagram\nGOSAT-based Top-down Total and Natural Methane Emissions Data Flow Diagram\nOCO-2 MIP Top-Down CO₂ Budgets Data Flow Diagram\nTM5-4DVar Isotopic CH₄ Inverse Fluxes Data Flow Diagram\nWetland Methane Emissions, LPJ-EOSIM Model Data Flow Diagram",
    "crumbs": [
      "Data Flow Diagrams"
    ]
  },
  {
    "objectID": "workflow.html#large-emissions-events",
    "href": "workflow.html#large-emissions-events",
    "title": "U.S. Greenhouse Gas Center: Data Flow Diagrams",
    "section": "Large Emissions Events",
    "text": "Large Emissions Events\n\nEMIT Methane Point Source Plume Complexes Data Flow Diagram",
    "crumbs": [
      "Data Flow Diagrams"
    ]
  },
  {
    "objectID": "workflow.html#greenhouse-gas-concentrations",
    "href": "workflow.html#greenhouse-gas-concentrations",
    "title": "U.S. Greenhouse Gas Center: Data Flow Diagrams",
    "section": "Greenhouse Gas Concentrations",
    "text": "Greenhouse Gas Concentrations\n\nAtmospheric Carbon Dioxide Concentrations from NOAA Global Monitoring Laboratory Data Flow Diagram\nAtmospheric Methane Concentrations from NOAA Global Monitoring Laboratory Data Flow Diagram\nOCO-2 GEOS Column CO₂ Concentrations Data Flow Diagram",
    "crumbs": [
      "Data Flow Diagrams"
    ]
  },
  {
    "objectID": "workflow.html#socioeconomic",
    "href": "workflow.html#socioeconomic",
    "title": "U.S. Greenhouse Gas Center: Data Flow Diagrams",
    "section": "Socioeconomic",
    "text": "Socioeconomic\n\nSEDAC Gridded World Population Density Data Flow Diagram",
    "crumbs": [
      "Data Flow Diagrams"
    ]
  },
  {
    "objectID": "workflow.html#contact",
    "href": "workflow.html#contact",
    "title": "U.S. Greenhouse Gas Center: Data Flow Diagrams",
    "section": "Contact",
    "text": "Contact\nFor technical help or general questions, please contact the support team using the feedback form.",
    "crumbs": [
      "Data Flow Diagrams"
    ]
  },
  {
    "objectID": "user_data_notebooks/emit-ch4plume-v1_User_Notebook.html",
    "href": "user_data_notebooks/emit-ch4plume-v1_User_Notebook.html",
    "title": "EMIT Methane Point Source Plume Complexes",
    "section": "",
    "text": "You can launch this notebook in the US GHG Center JupyterHub by clicking the link below.\nLaunch in the US GHG Center JupyterHub (requires access)",
    "crumbs": [
      "Data Usage Notebooks",
      "Large Emissions Events",
      "EMIT Methane Point Source Plume Complexes"
    ]
  },
  {
    "objectID": "user_data_notebooks/emit-ch4plume-v1_User_Notebook.html#run-this-notebook",
    "href": "user_data_notebooks/emit-ch4plume-v1_User_Notebook.html#run-this-notebook",
    "title": "EMIT Methane Point Source Plume Complexes",
    "section": "",
    "text": "You can launch this notebook in the US GHG Center JupyterHub by clicking the link below.\nLaunch in the US GHG Center JupyterHub (requires access)",
    "crumbs": [
      "Data Usage Notebooks",
      "Large Emissions Events",
      "EMIT Methane Point Source Plume Complexes"
    ]
  },
  {
    "objectID": "user_data_notebooks/emit-ch4plume-v1_User_Notebook.html#approach",
    "href": "user_data_notebooks/emit-ch4plume-v1_User_Notebook.html#approach",
    "title": "EMIT Methane Point Source Plume Complexes",
    "section": "Approach",
    "text": "Approach\n\nIdentify available dates and temporal frequency of observations for the given collection using the GHGC API /stac endpoint. The collection processed in this notebook is the Earth Surface Mineral Dust Source Investigation (EMIT) methane emission plumes data product.\nPass the STAC item into the raster API /stac/tilejson.json endpoint.\nUsing folium.Map, visualize the plumes.\nAfter the visualization, perform zonal statistics for a given polygon.",
    "crumbs": [
      "Data Usage Notebooks",
      "Large Emissions Events",
      "EMIT Methane Point Source Plume Complexes"
    ]
  },
  {
    "objectID": "user_data_notebooks/emit-ch4plume-v1_User_Notebook.html#about-the-data",
    "href": "user_data_notebooks/emit-ch4plume-v1_User_Notebook.html#about-the-data",
    "title": "EMIT Methane Point Source Plume Complexes",
    "section": "About the Data",
    "text": "About the Data\nThe Earth Surface Mineral Dust Source Investigation (EMIT) instrument builds upon NASA’s long history of developing advanced imaging spectrometers for new science and applications. EMIT launched to the International Space Station (ISS) on July 14, 2022. The data shows high-confidence research grade methane plumes from point source emitters - updated as they are identified - in keeping with Jet Propulsion Laboratory (JPL) Open Science and Open Data policy. For more information regarding this dataset, please visit the EMIT Methane Point Source Plume Complexes data overview page.",
    "crumbs": [
      "Data Usage Notebooks",
      "Large Emissions Events",
      "EMIT Methane Point Source Plume Complexes"
    ]
  },
  {
    "objectID": "user_data_notebooks/emit-ch4plume-v1_User_Notebook.html#querying-the-stac-api",
    "href": "user_data_notebooks/emit-ch4plume-v1_User_Notebook.html#querying-the-stac-api",
    "title": "EMIT Methane Point Source Plume Complexes",
    "section": "Querying the STAC API",
    "text": "Querying the STAC API\nFirst, we are going to import the required libraries. Once imported, they allow better executing a query in the GHG Center Spatio Temporal Asset Catalog (STAC) Application Programming Interface (API) where the granules for this collection are stored.\n\n# Import the following libraries\nimport requests\nimport folium\nimport folium.plugins\nfrom folium import Map, TileLayer\nfrom pystac_client import Client\nimport branca\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport branca.colormap as cm\nimport seaborn as sns\n\n\n# Provide the STAC and RASTER API endpoints\n# The endpoint is referring to a location within the API that executes a request on a data collection nesting on the server.\n\n# The STAC API is a catalog of all the existing data collections that are stored in the GHG Center.\nSTAC_API_URL = \"https://earth.gov/ghgcenter/api/stac\"\n\n# The RASTER API is used to fetch collections for visualization\nRASTER_API_URL = \"https://earth.gov/ghgcenter/api/raster\"\n\n# The collection name is used to fetch the dataset from the STAC API. First, we define the collection name as a variable\n# Name of the collection for methane emission plumes \ncollection_name = \"emit-ch4plume-v1\"\n\n\n# Fetch the collection from the STAC API using the appropriate endpoint\n# The 'requests' library allows a HTTP request possible\ncollection = requests.get(f\"{STAC_API_URL}/collections/{collection_name}\").json()\n\n# Print the properties of the collection to the console\ncollection\n\n{'id': 'emit-ch4plume-v1',\n 'type': 'Collection',\n 'links': [{'rel': 'items',\n   'type': 'application/geo+json',\n   'href': 'https://ghg.center/api/stac/collections/emit-ch4plume-v1/items'},\n  {'rel': 'parent',\n   'type': 'application/json',\n   'href': 'https://ghg.center/api/stac/'},\n  {'rel': 'root',\n   'type': 'application/json',\n   'href': 'https://ghg.center/api/stac/'},\n  {'rel': 'self',\n   'type': 'application/json',\n   'href': 'https://ghg.center/api/stac/collections/emit-ch4plume-v1'}],\n 'title': 'CH4 plumes emissions',\n 'assets': None,\n 'extent': {'spatial': {'bbox': [[-121.90662384033203,\n     -46.6751594543457,\n     151.46371459960938,\n     51.3333740234375]]},\n  'temporal': {'interval': [['2022-08-10T06:49:57+00:00',\n     '2024-04-23T06:09:58+00:00']]}},\n 'license': 'CC0-1.0',\n 'keywords': None,\n 'providers': None,\n 'summaries': {'datetime': ['2022-08-10T06:49:57Z',\n   '2022-08-10T06:50:21Z',\n   '2022-08-10T06:51:32Z',\n   '2022-08-11T04:26:30Z',\n   '2022-08-14T05:14:12Z',\n   '2022-08-15T04:28:26Z',\n   '2022-08-15T04:28:38Z',\n   '2022-08-15T07:46:45Z',\n   '2022-08-15T14:08:23Z',\n   '2022-08-16T03:44:09Z',\n   '2022-08-16T10:10:35Z',\n   '2022-08-16T10:10:58Z',\n   '2022-08-16T11:45:05Z',\n   '2022-08-17T04:32:35Z',\n   '2022-08-17T09:20:38Z',\n   '2022-08-18T03:42:31Z',\n   '2022-08-18T07:01:05Z',\n   '2022-08-18T08:35:06Z',\n   '2022-08-18T11:44:40Z',\n   '2022-08-19T09:22:31Z',\n   '2022-08-19T12:30:47Z',\n   '2022-08-20T05:28:04Z',\n   '2022-08-20T08:33:24Z',\n   '2022-08-20T10:07:39Z',\n   '2022-08-20T10:09:37Z',\n   '2022-08-20T10:10:39Z',\n   '2022-08-21T11:00:23Z',\n   '2022-08-22T06:57:13Z',\n   '2022-08-22T10:06:53Z',\n   '2022-08-23T07:45:04Z',\n   '2022-08-26T06:54:35Z',\n   '2022-08-26T08:29:15Z',\n   '2022-08-26T17:46:42Z',\n   '2022-08-27T06:07:30Z',\n   '2022-08-27T06:07:53Z',\n   '2022-08-27T07:40:30Z',\n   '2022-08-27T10:49:27Z',\n   '2022-08-28T05:18:53Z',\n   '2022-08-28T05:19:05Z',\n   '2022-08-28T05:19:17Z',\n   '2022-08-28T05:19:29Z',\n   '2022-08-28T05:19:41Z',\n   '2022-08-28T06:53:00Z',\n   '2022-08-28T06:53:24Z',\n   '2022-08-28T06:55:50Z',\n   '2022-08-28T06:56:26Z',\n   '2022-08-28T08:28:47Z',\n   '2022-08-29T06:06:27Z',\n   '2022-08-29T06:09:13Z',\n   '2022-08-29T16:55:53Z',\n   '2022-08-30T06:52:44Z',\n   '2022-08-31T06:07:02Z',\n   '2022-08-31T06:07:38Z',\n   '2022-09-01T03:43:18Z',\n   '2022-09-01T05:17:09Z',\n   '2022-09-01T05:17:20Z',\n   '2022-09-01T05:19:20Z',\n   '2022-09-01T08:25:25Z',\n   '2022-09-03T05:19:24Z',\n   '2022-09-03T06:50:56Z',\n   '2022-09-03T06:52:42Z',\n   '2022-09-03T08:25:37Z',\n   '2022-09-09T07:02:54Z',\n   '2022-09-09T07:03:06Z',\n   '2023-01-07T14:38:18Z',\n   '2023-01-11T13:01:07Z',\n   '2023-01-11T13:02:18Z',\n   '2023-01-19T04:02:23Z',\n   '2023-01-21T16:18:34Z',\n   '2023-01-22T15:31:51Z',\n   '2023-01-23T08:53:11Z',\n   '2023-01-25T00:47:44Z',\n   '2023-01-26T06:27:16Z',\n   '2023-01-26T12:43:35Z',\n   '2023-01-27T16:21:04Z',\n   '2023-01-28T12:41:18Z',\n   '2023-01-28T12:41:30Z',\n   '2023-01-29T08:46:11Z',\n   '2023-01-29T13:03:21Z',\n   '2023-01-29T13:03:33Z',\n   '2023-01-30T09:35:55Z',\n   '2023-01-30T18:49:23Z',\n   '2023-01-31T05:39:24Z',\n   '2023-01-31T05:39:36Z',\n   '2023-01-31T05:43:17Z',\n   '2023-01-31T05:43:40Z',\n   '2023-01-31T08:49:13Z',\n   '2023-02-01T07:53:26Z',\n   '2023-02-02T07:08:03Z',\n   '2023-02-02T19:38:21Z',\n   '2023-02-03T06:22:56Z',\n   '2023-02-03T06:26:29Z',\n   '2023-02-03T17:14:34Z',\n   '2023-02-04T04:06:49Z',\n   '2023-02-04T04:10:09Z',\n   '2023-02-04T07:07:01Z',\n   '2023-02-04T07:11:17Z',\n   '2023-02-04T07:11:44Z',\n   '2023-02-04T08:41:39Z',\n   '2023-02-04T08:42:03Z',\n   '2023-02-05T17:12:44Z',\n   '2023-02-05T17:12:55Z',\n   '2023-02-06T16:25:14Z',\n   '2023-02-14T07:24:57Z',\n   '2023-02-14T08:57:15Z',\n   '2023-02-14T10:34:22Z',\n   '2023-02-14T10:34:57Z',\n   '2023-02-15T06:36:26Z',\n   '2023-02-15T11:19:33Z',\n   '2023-02-15T20:33:54Z',\n   '2023-02-16T13:36:26Z',\n   '2023-02-16T13:37:01Z',\n   '2023-02-17T06:32:21Z',\n   '2023-02-17T11:16:03Z',\n   '2023-02-17T20:31:34Z',\n   '2023-02-17T20:34:32Z',\n   '2023-02-18T08:56:51Z',\n   '2023-02-18T08:57:03Z',\n   '2023-02-18T08:57:39Z',\n   '2023-02-18T10:27:23Z',\n   '2023-02-18T12:02:10Z',\n   '2023-02-18T18:10:54Z',\n   '2023-02-19T06:31:55Z',\n   '2023-02-19T08:05:03Z',\n   '2023-02-19T08:05:27Z',\n   '2023-02-19T08:05:39Z',\n   '2023-02-19T09:39:08Z',\n   '2023-02-19T09:39:43Z',\n   '2023-02-19T09:41:18Z',\n   '2023-02-19T09:41:30Z',\n   '2023-02-19T19:05:39Z',\n   '2023-02-20T05:45:40Z',\n   '2023-02-20T07:15:30Z',\n   '2023-02-20T10:32:20Z',\n   '2023-02-20T19:43:24Z',\n   '2023-02-20T19:45:46Z',\n   '2023-02-20T19:47:23Z',\n   '2023-02-21T04:56:04Z',\n   '2023-02-21T06:30:01Z',\n   '2023-02-21T09:39:54Z',\n   '2023-02-22T08:51:06Z',\n   '2023-02-23T04:56:45Z',\n   '2023-02-23T04:57:20Z',\n   '2023-02-23T06:30:22Z',\n   '2023-02-23T06:30:33Z',\n   '2023-02-23T06:30:57Z',\n   '2023-02-23T06:31:09Z',\n   '2023-02-23T08:04:47Z',\n   '2023-02-23T08:04:59Z',\n   '2023-02-24T04:11:58Z',\n   '2023-02-24T08:58:31Z',\n   '2023-02-24T10:22:19Z',\n   '2023-02-24T18:10:00Z',\n   '2023-02-24T18:14:29Z',\n   '2023-02-24T18:14:41Z',\n   '2023-02-25T05:06:19Z',\n   '2023-02-25T08:05:31Z',\n   '2023-02-25T08:05:43Z',\n   '2023-02-26T04:10:22Z',\n   '2023-02-26T05:47:14Z',\n   '2023-02-27T15:57:14Z',\n   '2023-03-11T12:59:54Z',\n   '2023-03-18T04:52:50Z',\n   '2023-03-23T14:48:05Z',\n   '2023-03-24T09:49:19Z',\n   '2023-03-24T09:49:43Z',\n   '2023-03-25T12:11:18Z',\n   '2023-03-25T13:41:23Z',\n   '2023-03-25T13:41:35Z',\n   '2023-03-25T13:41:47Z',\n   '2023-03-25T15:17:28Z',\n   '2023-03-26T08:19:20Z',\n   '2023-03-26T08:19:55Z',\n   '2023-03-26T11:25:21Z',\n   '2023-03-26T14:28:44Z',\n   '2023-03-26T14:30:19Z',\n   '2023-03-27T07:33:31Z',\n   '2023-03-27T12:08:58Z',\n   '2023-03-27T12:09:57Z',\n   '2023-03-30T09:49:34Z',\n   '2023-03-30T09:50:33Z',\n   '2023-03-30T09:50:45Z',\n   '2023-03-30T12:52:50Z',\n   '2023-03-30T12:53:02Z',\n   '2023-03-31T07:23:49Z',\n   '2023-03-31T12:06:33Z',\n   '2023-03-31T19:49:37Z',\n   '2023-04-03T08:10:31Z',\n   '2023-04-03T08:12:19Z',\n   '2023-04-03T08:14:57Z',\n   '2023-04-03T09:45:39Z',\n   '2023-04-03T11:18:37Z',\n   '2023-04-03T11:18:49Z',\n   '2023-04-04T08:58:44Z',\n   '2023-04-04T08:59:08Z',\n   '2023-04-04T09:00:19Z',\n   '2023-04-04T09:00:31Z',\n   '2023-04-04T09:00:42Z',\n   '2023-04-05T06:35:43Z',\n   '2023-04-05T08:12:46Z',\n   '2023-04-13T09:57:29Z',\n   '2023-04-16T12:22:03Z',\n   '2023-04-16T21:37:35Z',\n   '2023-04-17T09:58:36Z',\n   '2023-04-17T09:58:48Z',\n   '2023-04-18T06:06:02Z',\n   '2023-04-18T06:06:25Z',\n   '2023-04-18T09:09:31Z',\n   '2023-04-18T09:10:53Z',\n   '2023-04-18T09:11:17Z',\n   '2023-04-18T09:11:28Z',\n   '2023-04-18T09:11:52Z',\n   '2023-04-18T09:12:16Z',\n   '2023-04-18T20:01:18Z',\n   '2023-04-19T08:23:52Z',\n   '2023-04-19T11:33:35Z',\n   '2023-04-19T13:06:50Z',\n   '2023-04-20T06:01:48Z',\n   '2023-04-20T07:35:54Z',\n   '2023-04-20T10:45:34Z',\n   '2023-04-21T08:23:29Z',\n   '2023-04-21T08:26:38Z',\n   '2023-04-21T10:00:17Z',\n   '2023-04-21T19:14:23Z',\n   '2023-04-22T07:34:37Z',\n   '2023-04-22T09:10:58Z',\n   '2023-04-22T09:11:10Z',\n   '2023-04-23T05:15:16Z',\n   '2023-04-23T06:44:21Z',\n   '2023-04-23T08:22:23Z',\n   '2023-04-23T10:01:36Z',\n   '2023-04-23T11:26:19Z',\n   '2023-04-23T11:29:08Z',\n   '2023-04-23T19:12:32Z',\n   '2023-04-24T04:24:44Z',\n   '2023-04-24T06:08:59Z',\n   '2023-04-24T09:08:18Z',\n   '2023-04-24T16:49:49Z',\n   '2023-04-24T18:25:45Z',\n   '2023-04-25T03:40:28Z',\n   '2023-04-25T03:40:40Z',\n   '2023-04-25T05:12:16Z',\n   '2023-04-25T06:46:28Z',\n   '2023-04-25T08:19:23Z',\n   '2023-04-26T02:53:02Z',\n   '2023-04-26T05:57:03Z',\n   '2023-04-26T07:31:30Z',\n   '2023-04-26T18:22:39Z',\n   '2023-04-27T06:44:04Z',\n   '2023-04-27T06:44:16Z',\n   '2023-04-27T17:36:30Z',\n   '2023-04-27T17:51:26Z',\n   '2023-04-28T02:49:00Z',\n   '2023-04-28T05:55:24Z',\n   '2023-04-28T05:55:36Z',\n   '2023-04-28T09:03:09Z',\n   '2023-04-29T05:08:11Z',\n   '2023-04-29T05:08:23Z',\n   '2023-04-29T05:08:35Z',\n   '2023-04-29T06:44:05Z',\n   '2023-04-30T05:55:56Z',\n   '2023-04-30T05:56:08Z',\n   '2023-04-30T07:28:53Z',\n   '2023-04-30T16:44:07Z',\n   '2023-05-02T04:22:34Z',\n   '2023-05-02T04:22:58Z',\n   '2023-05-02T07:27:54Z',\n   '2023-05-04T13:54:42Z',\n   '2023-05-04T13:54:54Z',\n   '2023-05-26T14:21:26Z',\n   '2023-05-27T13:32:35Z',\n   '2023-05-29T08:55:41Z',\n   '2023-05-29T08:56:40Z',\n   '2023-05-29T11:57:40Z',\n   '2023-05-30T06:35:04Z',\n   '2023-05-30T09:37:28Z',\n   '2023-05-30T09:40:18Z',\n   '2023-05-30T18:57:54Z',\n   '2023-05-31T10:23:16Z',\n   '2023-05-31T10:24:39Z',\n   '2023-06-01T09:36:23Z',\n   '2023-06-01T09:39:03Z',\n   '2023-06-02T07:19:17Z',\n   '2023-06-02T08:47:26Z',\n   '2023-06-03T07:59:14Z',\n   '2023-06-03T07:59:26Z',\n   '2023-06-03T08:03:27Z',\n   '2023-06-03T09:32:09Z',\n   '2023-06-04T07:06:41Z',\n   '2023-06-04T18:02:05Z',\n   '2023-06-04T18:02:17Z',\n   '2023-06-04T18:02:29Z',\n   '2023-06-05T08:00:26Z',\n   '2023-06-06T05:34:48Z',\n   '2023-06-06T05:35:23Z',\n   '2023-06-06T07:09:19Z',\n   '2023-06-06T08:50:11Z',\n   '2023-06-06T10:14:59Z',\n   '2023-06-07T06:20:35Z',\n   '2023-06-07T06:23:33Z',\n   '2023-06-07T06:24:32Z',\n   '2023-06-07T08:00:31Z',\n   '2023-06-07T09:26:29Z',\n   '2023-06-07T09:26:41Z',\n   '2023-06-08T05:31:35Z',\n   '2023-06-08T16:23:22Z',\n   '2023-06-08T16:23:34Z',\n   '2023-06-08T16:23:46Z',\n   '2023-06-09T04:51:06Z',\n   '2023-06-09T07:50:16Z',\n   '2023-06-09T17:10:10Z',\n   '2023-06-09T17:11:33Z',\n   '2023-06-10T03:57:59Z',\n   '2023-06-10T05:30:19Z',\n   '2023-06-10T16:21:55Z',\n   '2023-06-11T04:44:27Z',\n   '2023-06-11T04:45:26Z',\n   '2023-06-11T06:16:38Z',\n   '2023-06-12T02:24:18Z',\n   '2023-06-12T05:32:53Z',\n   '2023-06-12T16:21:03Z',\n   '2023-06-13T04:43:14Z',\n   '2023-06-13T11:13:48Z',\n   '2023-06-14T10:24:03Z',\n   '2023-06-14T10:24:15Z',\n   '2023-06-14T10:24:39Z',\n   '2023-06-14T10:24:51Z',\n   '2023-06-14T19:37:06Z',\n   '2023-06-16T11:59:49Z',\n   '2023-06-16T21:13:43Z',\n   '2023-06-16T21:14:19Z',\n   '2023-06-16T21:14:31Z',\n   '2023-06-17T11:00:03Z',\n   '2023-06-19T08:03:47Z',\n   '2023-06-19T11:07:48Z',\n   '2023-06-20T08:44:14Z',\n   '2023-06-20T08:44:26Z',\n   '2023-06-22T11:50:37Z',\n   '2023-06-22T19:32:01Z',\n   '2023-06-22T19:32:13Z',\n   '2023-06-24T05:29:00Z',\n   '2023-06-24T05:30:36Z',\n   '2023-06-25T03:13:55Z',\n   '2023-06-25T06:16:49Z',\n   '2023-06-25T06:18:46Z',\n   '2023-06-25T07:52:48Z',\n   '2023-06-26T08:40:04Z',\n   '2023-06-26T10:12:32Z',\n   '2023-06-26T10:13:43Z',\n   '2023-06-27T03:08:22Z',\n   '2023-06-27T04:42:31Z',\n   '2023-06-27T07:52:01Z',\n   '2023-06-28T05:29:39Z',\n   '2023-06-28T05:32:36Z',\n   '2023-06-28T05:33:24Z',\n   '2023-06-28T16:19:24Z',\n   '2023-06-29T01:34:53Z',\n   '2023-06-29T01:35:16Z',\n   '2023-06-29T04:40:14Z',\n   '2023-06-29T04:40:50Z',\n   '2023-06-29T04:41:01Z',\n   '2023-06-29T06:14:16Z',\n   '2023-06-29T06:15:03Z',\n   '2023-06-29T06:16:26Z',\n   '2023-06-29T06:16:38Z',\n   '2023-06-29T06:16:50Z',\n   '2023-06-29T06:17:27Z',\n   '2023-06-29T06:18:50Z',\n   '2023-06-29T15:40:42Z',\n   '2023-06-30T07:06:49Z',\n   '2023-06-30T10:23:58Z',\n   '2023-06-30T16:17:28Z',\n   '2023-07-23T17:36:39Z',\n   '2023-07-25T10:05:32Z',\n   '2023-07-25T11:39:04Z',\n   '2023-07-25T11:39:27Z',\n   '2023-07-26T10:53:55Z',\n   '2023-07-26T10:54:54Z',\n   '2023-07-26T13:57:14Z',\n   '2023-07-28T10:53:13Z',\n   '2023-07-29T10:02:52Z',\n   '2023-07-29T10:06:30Z',\n   '2023-07-29T13:08:54Z',\n   '2023-07-29T13:10:41Z',\n   '2023-07-29T20:53:42Z',\n   '2023-07-30T09:14:51Z',\n   '2023-07-30T10:48:00Z',\n   '2023-07-30T12:20:47Z',\n   '2023-07-31T06:58:04Z',\n   '2023-07-31T13:07:06Z',\n   '2023-07-31T13:08:04Z',\n   '2023-07-31T19:18:10Z',\n   '2023-07-31T19:18:22Z',\n   '2023-07-31T19:18:34Z',\n   '2023-07-31T19:18:46Z',\n   '2023-08-01T09:16:36Z',\n   '2023-08-01T09:16:48Z',\n   '2023-08-02T08:25:47Z',\n   '2023-08-02T08:26:10Z',\n   '2023-08-02T08:29:53Z',\n   '2023-08-02T11:34:11Z',\n   '2023-08-03T09:21:03Z',\n   '2023-08-03T10:48:31Z',\n   '2023-08-04T05:22:23Z',\n   '2023-08-04T08:25:59Z',\n   '2023-08-04T08:29:48Z',\n   '2023-08-04T11:31:11Z',\n   '2023-08-04T17:41:29Z',\n   '2023-08-04T17:41:41Z',\n   '2023-08-05T06:08:27Z',\n   '2023-08-05T07:38:38Z',\n   '2023-08-05T07:40:37Z',\n   '2023-08-05T09:08:52Z',\n   '2023-08-05T09:09:04Z',\n   '2023-08-05T09:09:15Z',\n   '2023-08-06T03:46:59Z',\n   '2023-08-06T03:48:03Z',\n   '2023-08-06T06:52:31Z',\n   '2023-08-07T07:35:11Z',\n   '2023-08-07T07:36:22Z',\n   '2023-08-07T09:06:55Z',\n   '2023-08-09T04:30:25Z',\n   '2023-08-09T06:01:51Z',\n   '2023-08-09T06:03:49Z',\n   '2023-08-09T07:32:00Z',\n   '2023-08-09T07:32:12Z',\n   '2023-08-09T16:50:40Z',\n   '2023-08-10T05:15:16Z',\n   '2023-08-10T05:15:28Z',\n   '2023-08-10T05:15:52Z',\n   '2023-08-10T06:51:40Z',\n   '2023-08-14T06:57:12Z',\n   '2023-08-14T10:07:20Z',\n   '2023-08-14T10:08:19Z',\n   '2023-08-14T10:10:18Z',\n   '2023-08-14T10:11:06Z',\n   '2023-08-14T10:11:17Z',\n   '2023-08-14T10:11:29Z',\n   '2023-08-14T14:51:26Z',\n   '2023-08-14T14:52:25Z',\n   '2023-08-15T09:20:26Z',\n   '2023-08-16T10:10:38Z',\n   '2023-08-16T10:10:50Z',\n   '2023-08-16T11:48:56Z',\n   '2023-08-16T13:17:16Z',\n   '2023-08-17T09:24:16Z',\n   '2023-08-17T10:58:03Z',\n   '2023-08-17T20:15:10Z',\n   '2023-08-18T21:00:19Z',\n   '2023-08-19T12:29:33Z',\n   '2023-08-20T05:28:22Z',\n   '2023-08-20T10:14:14Z',\n   '2023-08-20T11:42:34Z',\n   '2023-08-20T19:28:43Z',\n   '2023-08-20T19:28:55Z',\n   '2023-08-21T09:25:16Z',\n   '2023-08-21T18:41:29Z',\n   '2023-08-22T07:00:14Z',\n   '2023-08-22T10:11:03Z',\n   '2023-08-22T17:52:29Z',\n   '2023-08-22T19:25:19Z',\n   '2023-08-22T19:25:31Z',\n   '2023-08-23T09:17:47Z',\n   '2023-08-23T09:23:49Z',\n   '2023-08-23T10:56:29Z',\n   '2023-08-23T17:06:09Z',\n   '2023-08-24T07:00:37Z',\n   '2023-08-24T07:00:49Z',\n   '2023-08-24T07:01:01Z',\n   '2023-08-24T08:39:07Z',\n   '2023-08-24T08:39:31Z',\n   '2023-08-24T17:53:37Z',\n   '2023-08-24T17:54:01Z',\n   '2023-08-25T06:13:13Z',\n   '2023-08-25T07:47:43Z',\n   '2023-08-25T07:50:05Z',\n   '2023-08-25T17:05:57Z',\n   '2023-08-25T17:06:09Z',\n   '2023-08-26T05:26:20Z',\n   '2023-08-26T08:35:22Z',\n   '2023-08-26T08:35:46Z',\n   '2023-08-26T10:06:04Z',\n   '2023-08-26T10:07:35Z',\n   '2023-08-26T10:08:34Z',\n   '2023-08-28T07:02:35Z',\n   '2023-08-28T07:03:10Z',\n   '2023-08-28T08:34:09Z',\n   '2023-08-28T08:34:21Z',\n   '2023-09-01T03:52:04Z',\n   '2023-09-03T01:00:49Z',\n   '2023-09-08T14:10:43Z',\n   '2023-09-15T04:26:28Z',\n   '2023-09-22T14:48:28Z',\n   '2023-09-24T11:42:53Z',\n   '2023-09-24T11:44:13Z',\n   '2023-09-24T20:57:57Z',\n   '2023-09-25T14:01:34Z',\n   '2023-09-26T11:41:19Z',\n   '2023-09-26T11:41:31Z',\n   '2023-09-26T11:41:43Z',\n   '2023-09-26T11:43:17Z',\n   '2023-09-27T20:11:14Z',\n   '2023-09-28T08:36:55Z',\n   '2023-09-28T10:01:33Z',\n   '2023-09-28T10:10:11Z',\n   '2023-09-28T11:45:38Z',\n   '2023-09-29T07:44:59Z',\n   '2023-09-29T12:25:34Z',\n   '2023-09-30T10:05:31Z',\n   '2023-09-30T10:05:43Z',\n   '2023-09-30T11:37:06Z',\n   '2023-09-30T11:37:30Z',\n   '2023-10-01T07:42:25Z',\n   '2023-10-02T08:24:55Z',\n   '2023-10-02T08:31:11Z',\n   '2023-10-02T11:39:07Z',\n   '2023-10-03T06:09:28Z',\n   '2023-10-03T07:42:03Z',\n   '2023-10-03T07:46:17Z',\n   '2023-10-03T07:46:41Z',\n   '2023-10-03T07:47:04Z',\n   '2023-10-03T07:47:16Z',\n   '2023-10-03T07:47:52Z',\n   '2023-10-03T09:16:35Z',\n   '2023-10-04T08:31:36Z',\n   '2023-10-04T17:47:32Z',\n   '2023-10-04T17:47:44Z',\n   '2023-10-05T06:07:19Z',\n   '2023-10-05T09:21:54Z',\n   '2023-10-06T05:21:52Z',\n   '2023-10-06T06:55:21Z',\n   '2023-10-06T06:55:45Z',\n   '2023-10-06T06:55:57Z',\n   '2023-10-06T08:27:35Z',\n   '2023-10-06T10:02:06Z',\n   '2023-10-07T09:14:07Z',\n   '2023-10-08T05:15:36Z',\n   '2023-10-08T05:15:48Z',\n   '2023-10-08T05:26:16Z',\n   '2023-10-08T10:02:18Z',\n   '2023-10-08T16:11:15Z',\n   '2023-10-08T16:11:27Z',\n   '2023-10-08T17:46:13Z',\n   '2023-10-09T04:30:59Z',\n   '2023-10-09T06:09:56Z',\n   '2023-10-09T09:14:23Z',\n   '2023-10-09T17:00:25Z',\n   '2023-10-10T02:16:46Z',\n   '2023-10-10T05:18:48Z',\n   '2023-10-10T05:19:00Z',\n   '2023-10-10T05:19:23Z',\n   '2023-10-11T01:31:58Z',\n   '2023-10-11T04:35:57Z',\n   '2023-10-13T11:00:35Z',\n   '2023-10-14T10:15:54Z',\n   '2023-10-14T10:16:06Z',\n   '2023-10-15T07:55:58Z',\n   '2023-10-15T09:24:58Z',\n   '2023-10-16T10:10:18Z',\n   '2023-10-16T11:48:53Z',\n   '2023-10-18T10:14:46Z',\n   '2023-10-19T11:04:39Z',\n   '2023-10-20T05:27:29Z',\n   '2023-10-20T05:27:52Z',\n   '2023-10-23T03:09:00Z',\n   '2023-10-23T07:50:04Z',\n   '2023-10-23T09:21:42Z',\n   '2023-10-24T07:01:57Z',\n   '2023-10-24T07:02:09Z',\n   '2023-10-24T10:09:47Z',\n   '2023-10-25T06:15:31Z',\n   '2023-10-25T17:03:54Z',\n   '2023-10-25T17:04:06Z',\n   '2023-10-26T02:21:45Z',\n   '2023-10-26T05:26:47Z',\n   '2023-10-26T05:27:35Z',\n   '2023-10-26T07:01:06Z',\n   '2023-10-26T08:36:53Z',\n   '2023-10-27T06:14:34Z',\n   '2023-10-27T15:32:39Z',\n   '2023-10-27T17:07:14Z',\n   '2023-10-28T07:01:04Z',\n   '2023-10-28T07:01:16Z',\n   '2023-10-28T08:34:16Z',\n   '2023-10-28T16:19:00Z',\n   '2023-10-29T03:10:32Z',\n   '2023-10-29T06:15:39Z',\n   '2023-10-30T03:55:33Z',\n   '2023-10-30T03:55:45Z',\n   '2023-11-01T08:34:38Z',\n   '2023-11-05T15:00:54Z',\n   '2023-11-07T00:53:49Z',\n   '2023-11-21T12:25:43Z',\n   '2023-11-24T14:26:42Z',\n   '2023-11-25T09:22:20Z',\n   '2023-11-25T10:55:40Z',\n   '2023-11-27T09:19:46Z',\n   '2023-11-27T14:00:54Z',\n   '2023-11-28T23:44:58Z',\n   '2023-11-29T09:20:12Z',\n   '2023-11-29T09:20:24Z',\n   '2023-11-30T08:31:54Z',\n   '2023-11-30T10:05:25Z',\n   '2023-11-30T10:07:36Z',\n   '2023-11-30T11:38:00Z',\n   '2023-11-30T22:06:44Z',\n   '2023-12-01T07:43:07Z',\n   '2023-12-01T18:35:25Z',\n   '2023-12-01T18:35:49Z',\n   '2023-12-01T18:36:01Z',\n   '2023-12-02T08:32:56Z',\n   '2023-12-02T08:33:39Z',\n   '2023-12-02T08:33:51Z',\n   '2023-12-04T08:30:32Z',\n   '2023-12-04T10:00:51Z',\n   '2023-12-04T17:46:18Z',\n   '2023-12-05T06:05:47Z',\n   '2023-12-05T07:43:38Z',\n   '2023-12-05T16:58:25Z',\n   '2023-12-05T16:58:37Z',\n   '2023-12-06T08:25:58Z',\n   '2023-12-06T08:26:10Z',\n   '2023-12-17T09:26:25Z',\n   '2023-12-17T12:30:27Z',\n   '2023-12-17T20:13:38Z',\n   '2023-12-20T08:35:56Z',\n   '2023-12-22T03:50:01Z',\n   '2023-12-23T09:21:03Z',\n   '2023-12-23T12:30:06Z',\n   '2023-12-24T06:57:46Z',\n   '2023-12-24T17:45:33Z',\n   '2023-12-24T17:45:45Z',\n   '2023-12-25T04:37:07Z',\n   '2023-12-26T06:54:26Z',\n   '2023-12-26T06:54:50Z',\n   '2023-12-26T06:55:14Z',\n   '2023-12-26T08:27:57Z',\n   '2023-12-26T18:07:07Z',\n   '2024-01-05T14:51:50Z',\n   '2024-01-09T13:14:23Z',\n   '2024-01-20T15:49:41Z',\n   '2024-01-22T15:47:54Z',\n   '2024-01-25T09:08:38Z',\n   '2024-01-26T13:01:03Z',\n   '2024-01-27T19:59:04Z',\n   '2024-01-28T19:11:03Z',\n   '2024-01-28T19:11:15Z',\n   '2024-01-29T04:22:45Z',\n   '2024-01-29T09:07:50Z',\n   '2024-01-29T15:05:16Z',\n   '2024-01-29T18:26:22Z',\n   '2024-01-29T19:58:57Z',\n   '2024-01-29T19:59:56Z',\n   '2024-01-30T05:17:28Z',\n   '2024-01-30T05:17:52Z',\n   '2024-01-30T05:18:16Z',\n   '2024-01-30T11:26:10Z',\n   '2024-01-30T12:36:34Z',\n   '2024-01-30T17:39:50Z',\n   '2024-01-31T09:08:23Z',\n   '2024-01-31T18:24:59Z',\n   '2024-02-01T08:23:10Z',\n   '2024-02-01T08:24:18Z',\n   '2024-02-01T14:24:58Z',\n   '2024-02-02T16:51:56Z',\n   '2024-02-03T09:51:38Z',\n   '2024-02-05T05:16:01Z',\n   '2024-02-05T05:16:13Z',\n   '2024-02-12T21:48:20Z',\n   '2024-02-12T21:48:32Z',\n   '2024-02-12T21:48:43Z',\n   '2024-02-13T11:45:18Z',\n   '2024-02-14T09:20:53Z',\n   '2024-02-14T09:21:17Z',\n   '2024-02-14T10:56:58Z',\n   '2024-02-14T10:57:33Z',\n   '2024-02-15T19:34:25Z',\n   '2024-02-15T19:34:36Z',\n   '2024-02-16T10:57:41Z',\n   '2024-02-17T05:27:07Z',\n   '2024-02-17T05:27:31Z',\n   '2024-02-17T10:08:16Z',\n   '2024-02-18T12:26:51Z',\n   '2024-02-23T16:10:50Z',\n   '2024-02-24T06:09:41Z',\n   '2024-02-24T06:10:16Z',\n   '2024-02-24T09:14:58Z',\n   '2024-02-24T16:58:01Z',\n   '2024-02-25T02:13:02Z',\n   '2024-02-25T02:13:14Z',\n   '2024-02-25T02:13:26Z',\n   '2024-02-25T02:13:38Z',\n   '2024-02-25T02:13:49Z',\n   '2024-02-25T02:14:01Z',\n   '2024-02-25T04:11:08Z',\n   '2024-02-25T16:09:27Z',\n   '2024-02-26T07:40:44Z',\n   '2024-02-26T07:40:56Z',\n   '2024-02-26T07:41:43Z',\n   '2024-02-26T07:43:53Z',\n   '2024-02-26T09:13:16Z',\n   '2024-02-26T09:14:15Z',\n   '2024-02-26T09:14:27Z',\n   '2024-02-26T17:01:04Z',\n   '2024-02-26T17:01:40Z',\n   '2024-02-27T06:53:31Z',\n   '2024-02-27T18:06:46Z',\n   '2024-02-28T03:02:50Z',\n   '2024-02-28T04:35:21Z',\n   '2024-02-28T04:35:57Z',\n   '2024-03-02T16:32:57Z',\n   '2024-03-03T06:10:34Z',\n   '2024-03-05T12:33:25Z',\n   '2024-03-19T19:12:59Z',\n   '2024-03-21T08:34:55Z',\n   '2024-03-22T21:45:09Z',\n   '2024-03-27T07:05:05Z',\n   '2024-03-27T10:07:08Z',\n   '2024-03-27T13:13:34Z',\n   '2024-03-28T06:17:14Z',\n   '2024-03-28T06:17:26Z',\n   '2024-03-28T10:53:28Z',\n   '2024-03-28T10:54:36Z',\n   '2024-03-28T12:25:08Z',\n   '2024-03-28T20:10:48Z',\n   '2024-03-31T11:37:53Z',\n   '2024-04-01T09:18:51Z',\n   '2024-04-01T09:19:15Z',\n   '2024-04-01T09:19:38Z',\n   '2024-04-03T07:46:26Z',\n   '2024-04-03T16:58:15Z',\n   '2024-04-04T06:53:36Z',\n   '2024-04-04T06:57:51Z',\n   '2024-04-04T08:28:37Z',\n   '2024-04-04T16:08:51Z',\n   '2024-04-05T07:41:39Z',\n   '2024-04-05T17:01:29Z',\n   '2024-04-08T05:22:54Z',\n   '2024-04-09T07:44:35Z',\n   '2024-04-13T07:53:28Z',\n   '2024-04-14T14:54:36Z',\n   '2024-04-15T06:21:41Z',\n   '2024-04-15T06:21:52Z',\n   '2024-04-15T09:25:56Z',\n   '2024-04-20T08:35:01Z',\n   '2024-04-20T10:14:48Z',\n   '2024-04-21T07:46:19Z',\n   '2024-04-21T18:42:22Z',\n   '2024-04-22T06:58:49Z',\n   '2024-04-23T03:04:34Z',\n   '2024-04-23T03:04:46Z',\n   '2024-04-23T06:09:58Z']},\n 'description': 'Methane plumes from point source emitters',\n 'item_assets': {'ch4-plume-emissions': {'type': 'image/tiff; application=geotiff; profile=cloud-optimized',\n   'roles': ['data', 'layer'],\n   'title': 'CH4 Plumes Emissions',\n   'description': 'Methane plumes from point source emitters.'}},\n 'stac_version': '1.0.0',\n 'stac_extensions': None,\n 'dashboard:is_periodic': False,\n 'dashboard:time_density': 'day'}\n\n\nExamining the contents of our collection under the temporal variable, we note that data is available from August 2022 to May 2023. By looking at the dashboard: time density, we can see that observations are conducted daily and non-periodically (i.e., there are plumes emissions for multiple places on the same dates).\n\ndef get_item_count(collection_id):\n    count = 0\n    items_url = f\"{STAC_API_URL}/collections/{collection_id}/items\"\n\n    while True:\n        response = requests.get(items_url)\n\n        if not response.ok:\n            print(\"error getting items\")\n            exit()\n\n        stac = response.json()\n        count += int(stac[\"context\"].get(\"returned\", 0))\n        next = [link for link in stac[\"links\"] if link[\"rel\"] == \"next\"]\n\n        if not next:\n            break\n        items_url = next[0][\"href\"]\n\n    return count\n\n\n# Check total number of items available\nnumber_of_items = get_item_count(collection_name)\nitems = requests.get(f\"{STAC_API_URL}/collections/{collection_name}/items?limit={number_of_items}\").json()[\"features\"]\nprint(f\"Found {len(items)} items\")\n\nFound 1266 items\n\n\n\n# Examining the first item in the collection\nitems[0]\n\nBelow, we are entering the minimum and maximum values to provide our upper and lower bounds in rescale_values.",
    "crumbs": [
      "Data Usage Notebooks",
      "Large Emissions Events",
      "EMIT Methane Point Source Plume Complexes"
    ]
  },
  {
    "objectID": "user_data_notebooks/emit-ch4plume-v1_User_Notebook.html#exploring-methane-emission-plumes-ch₄-using-the-raster-api",
    "href": "user_data_notebooks/emit-ch4plume-v1_User_Notebook.html#exploring-methane-emission-plumes-ch₄-using-the-raster-api",
    "title": "EMIT Methane Point Source Plume Complexes",
    "section": "Exploring Methane Emission Plumes (CH₄) using the Raster API",
    "text": "Exploring Methane Emission Plumes (CH₄) using the Raster API\nIn this notebook, we will explore global methane emission plumes from point sources. We will visualize the outputs on a map using folium.\n\n# To access the year value from each item more easily, this will let us query more explicitly by year and month (e.g., 2020-02)\nplume_complexes = {items[\"id\"]: items for items in items} \n\n\n# Set the asset value to the appropriate parameter \nasset_name = \"ch4-plume-emissions\"\n\n\n# Fetching the min and max values for a specific item\nrescale_values = {\"max\":plume_complexes[list(plume_complexes.keys())[0]][\"assets\"][asset_name][\"raster:bands\"][0][\"histogram\"][\"max\"], \"min\":plume_complexes[list(plume_complexes.keys())[0]][\"assets\"][asset_name][\"raster:bands\"][0][\"histogram\"][\"min\"]}\n\nNow we will pass the item id, collection name, and rescaling_factor to the Raster API endpoint. We will do this for only one item so that we can visualize the event.\n\n# Select the item ID which you want to visualize. Item ID is in the format yyyymmdd followed by the timestamp. This ID can be extracted from the COG name as well.\nitem_id = \"EMIT_L2B_CH4PLM_001_20230418T200118_000829\"\ncolor_map = \"magma\"\nmethane_plume_tile = requests.get(\n    f\"{RASTER_API_URL}/stac/tilejson.json?collection={plume_complexes[item_id]['collection']}&item={plume_complexes[item_id]['id']}\"\n    f\"&assets={asset_name}\"\n    f\"&color_formula=gamma+r+1.05&colormap_name={color_map}\"\n    f\"&rescale={rescale_values['min']},{rescale_values['max']}\", \n).json()\nmethane_plume_tile\n\n{'tilejson': '2.2.0',\n 'version': '1.0.0',\n 'scheme': 'xyz',\n 'tiles': ['https://ghg.center/api/raster/stac/tiles/WebMercatorQuad/{z}/{x}/{y}@1x?collection=emit-ch4plume-v1&item=EMIT_L2B_CH4PLM_001_20230418T200118_000829&assets=ch4-plume-emissions&color_formula=gamma+r+1.05&colormap_name=magma&rescale=-775.518310546875%2C2180.226318359375'],\n 'minzoom': 0,\n 'maxzoom': 24,\n 'bounds': [-104.76285251117253,\n  39.85322425220504,\n  -104.74658553556483,\n  39.86515336765068],\n 'center': [-104.75471902336868, 39.85918880992786, 0]}",
    "crumbs": [
      "Data Usage Notebooks",
      "Large Emissions Events",
      "EMIT Methane Point Source Plume Complexes"
    ]
  },
  {
    "objectID": "user_data_notebooks/emit-ch4plume-v1_User_Notebook.html#visualizing-ch₄-emission-plume",
    "href": "user_data_notebooks/emit-ch4plume-v1_User_Notebook.html#visualizing-ch₄-emission-plume",
    "title": "EMIT Methane Point Source Plume Complexes",
    "section": "Visualizing CH₄ Emission Plume",
    "text": "Visualizing CH₄ Emission Plume\n\n# Set a colormap for the granule\n# Please refer to matplotlib library if you'd prefer choosing a different color ramp (https://matplotlib.org/stable/users/explain/colors/colormaps.html)\ncolormap = \"magma\" \n\n\n#Defining the breaks in the colormap \ncolor_map = cm.LinearColormap(colors = ['#310597', '#4C02A1', '#6600A7', '#7E03A8', '#9511A1', '#AA2395', '#BC3587', '#CC4778', '#DA5A6A', '#E66C5C', '#F0804E', '#F89540','#FDAC33', '#FDC527', '#F8DF25'], vmin = 0, vmax = 1500 )\n\n\n# Add an appropriate caption, in this case it would be Parts per million meter\ncolor_map.caption = 'ppm-m'\n\n# Set initial zoom and center of map for plume Layer\nmap_ = folium.Map(location=(methane_plume_tile[\"center\"][1], methane_plume_tile[\"center\"][0]), zoom_start=13, tiles=None, tooltip = 'test tool tip')\nfolium.TileLayer(tiles='https://server.arcgisonline.com/ArcGIS/rest/services/World_Imagery/MapServer/tile/{z}/{y}/{x}.png', name='ESRI World Imagery', attr='Tiles &copy; Esri &mdash; Source: Esri, i-cubed, USDA, USGS, AEX, GeoEye, Getmapping, Aerogrid, IGN, IGP, UPR-EGP, and the GIS User Community',overlay='True').add_to(map_)\n\n\n# Use the 'TileLayer' library to display the raster layer, add an appropriate caption, and adjust the transparency of the layer on the map\nmap_layer = TileLayer(\n    tiles=methane_plume_tile[\"tiles\"][0],\n    name='Plume Complex Landfill',\n    overlay='True',\n    attr=\"GHG\",\n    opacity=1,\n)\nmap_layer.add_to(map_)\n\n\n# Adjust map elements \nfolium.LayerControl(collapsed=False, position='bottomleft').add_to(map_)\nmap_.add_child(color_map)\nsvg_style = '&lt;style&gt;svg#legend {font-size: 14px; background-color: white;}&lt;/style&gt;'\nmap_.get_root().header.add_child(folium.Element(svg_style))\n\n\n# Visualizing the map\nmap_\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook",
    "crumbs": [
      "Data Usage Notebooks",
      "Large Emissions Events",
      "EMIT Methane Point Source Plume Complexes"
    ]
  },
  {
    "objectID": "user_data_notebooks/emit-ch4plume-v1_User_Notebook.html#summary",
    "href": "user_data_notebooks/emit-ch4plume-v1_User_Notebook.html#summary",
    "title": "EMIT Methane Point Source Plume Complexes",
    "section": "Summary",
    "text": "Summary\nIn this notebook we have successfully completed the following steps for the STAC collection for the EMIT Methane Point Source Plume Complexes dataset: 1. Install and import the necessary libraries 2. Fetch the collection from STAC collections using the appropriate endpoints 3. Count the number of existing granules within the collection 4. Map the methane emission plumes 5. Generate statistics for the area of interest (AOI)\nIf you have any questions regarding this user notebook, please contact us using the feedback form.",
    "crumbs": [
      "Data Usage Notebooks",
      "Large Emissions Events",
      "EMIT Methane Point Source Plume Complexes"
    ]
  },
  {
    "objectID": "user_data_notebooks/casagfed-carbonflux-monthgrid-v3_User_Notebook.html",
    "href": "user_data_notebooks/casagfed-carbonflux-monthgrid-v3_User_Notebook.html",
    "title": "CASA-GFED3 Land Carbon Flux",
    "section": "",
    "text": "You can launch this notebook in the US GHG Center JupyterHub by clicking the link below.\nLaunch in the US GHG Center JupyterHub (requires access)"
  },
  {
    "objectID": "user_data_notebooks/casagfed-carbonflux-monthgrid-v3_User_Notebook.html#run-this-notebook",
    "href": "user_data_notebooks/casagfed-carbonflux-monthgrid-v3_User_Notebook.html#run-this-notebook",
    "title": "CASA-GFED3 Land Carbon Flux",
    "section": "",
    "text": "You can launch this notebook in the US GHG Center JupyterHub by clicking the link below.\nLaunch in the US GHG Center JupyterHub (requires access)"
  },
  {
    "objectID": "user_data_notebooks/casagfed-carbonflux-monthgrid-v3_User_Notebook.html#approach",
    "href": "user_data_notebooks/casagfed-carbonflux-monthgrid-v3_User_Notebook.html#approach",
    "title": "CASA-GFED3 Land Carbon Flux",
    "section": "Approach",
    "text": "Approach\n\nIdentify available dates and temporal frequency of observations for a given collection using the GHGC API /stac endpoint. The collection processed in this notebook is the Land-Atmosphere Carbon Flux data product.\nPass the STAC item into the raster API /stac/tilejson.jsonendpoint.\nUsing folium.plugins.DualMap, visualize two tiles (side-by-side), allowing time point comparison.\nAfter the visualization, perform zonal statistics for a given polygon."
  },
  {
    "objectID": "user_data_notebooks/casagfed-carbonflux-monthgrid-v3_User_Notebook.html#about-the-data",
    "href": "user_data_notebooks/casagfed-carbonflux-monthgrid-v3_User_Notebook.html#about-the-data",
    "title": "CASA-GFED3 Land Carbon Flux",
    "section": "About the Data",
    "text": "About the Data\nThis dataset presents a variety of carbon flux parameters derived from the Carnegie-Ames-Stanford-Approach – Global Fire Emissions Database version 3 (CASA-GFED3) model. The model’s input data includes air temperature, precipitation, incident solar radiation, a soil classification map, and a number of satellite derived products. All model calculations are driven by analyzed meteorological data from NASA’s Modern-Era Retrospective analysis for Research and Application, Version 2 (MERRA-2). The resulting product provides monthly, global data at 0.5 degree resolution from January 2003 through December 2017. It includes the following carbon flux variables expressed in units of kilograms of carbon per square meter per month (kg Carbon m²/mon) from the following sources: net primary production (NPP), net ecosystem exchange (NEE), heterotrophic respiration (Rh), wildfire emissions (FIRE), and fuel wood burning emissions (FUEL). This product and earlier versions of MERRA-driven CASA-GFED carbon fluxes have been used in a number of atmospheric CO₂ transport studies, and through the support of NASA’s Carbon Monitoring System (CMS), it helps characterize, quantify, understand and predict the evolution of global carbon sources and sinks."
  },
  {
    "objectID": "user_data_notebooks/casagfed-carbonflux-monthgrid-v3_User_Notebook.html#querying-the-stac-api",
    "href": "user_data_notebooks/casagfed-carbonflux-monthgrid-v3_User_Notebook.html#querying-the-stac-api",
    "title": "CASA-GFED3 Land Carbon Flux",
    "section": "Querying the STAC API",
    "text": "Querying the STAC API\nPlease run the next cell to import the required libraries.\n\nimport requests\nimport folium\nimport folium.plugins\nfrom folium import Map, TileLayer \nfrom pystac_client import Client \nimport branca \nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n\n# Provide STAC and RASTER API endpoints\nSTAC_API_URL = \"https://earth.gov/ghgcenter/api/stac\"\nRASTER_API_URL = \"https://earth.gov/ghgcenter/api/raster\"\n\n# Please use the collection name similar to the one used in the STAC collection.\n# Name of the collection for CASA GFED Land-Atmosphere Carbon Flux monthly emissions. \ncollection_name = \"casagfed-carbonflux-monthgrid-v3\"\n\n\n# Fetch the collection from STAC collections using the appropriate endpoint\n# the 'requests' library allows a HTTP request possible\ncollection = requests.get(f\"{STAC_API_URL}/collections/{collection_name}\").json()\ncollection\n\nExamining the contents of our collection under the temporal variable, we see that the data is available from January 2003 to December 2017. By looking at the dashboard:time density, we observe that the periodic frequency of these observations is monthly.\n\n# Create a function that would search for the above data collection in the STAC API\ndef get_item_count(collection_id):\n    count = 0\n    items_url = f\"{STAC_API_URL}/collections/{collection_id}/items\"\n\n    while True:\n        response = requests.get(items_url)\n\n        if not response.ok:\n            print(\"error getting items\")\n            exit()\n\n        stac = response.json()\n        count += int(stac[\"context\"].get(\"returned\", 0))\n        next = [link for link in stac[\"links\"] if link[\"rel\"] == \"next\"]\n\n        if not next:\n            break\n        items_url = next[0][\"href\"]\n\n    return count\n\n\n# Apply the above function and check the total number of items available within the collection\nnumber_of_items = get_item_count(collection_name)\nitems = requests.get(f\"{STAC_API_URL}/collections/{collection_name}/items?limit={number_of_items}\").json()[\"features\"]\nprint(f\"Found {len(items)} items\")\n\n\n# Examine the first item in the collection\nitems[0]"
  },
  {
    "objectID": "user_data_notebooks/casagfed-carbonflux-monthgrid-v3_User_Notebook.html#exploring-changes-in-carbon-flux-levels-using-the-raster-api",
    "href": "user_data_notebooks/casagfed-carbonflux-monthgrid-v3_User_Notebook.html#exploring-changes-in-carbon-flux-levels-using-the-raster-api",
    "title": "CASA-GFED3 Land Carbon Flux",
    "section": "Exploring Changes in Carbon Flux Levels Using the Raster API",
    "text": "Exploring Changes in Carbon Flux Levels Using the Raster API\nWe will explore changes in the land atmosphere Carbon flux Heterotrophic Respiration and examine their impacts over time. We’ll then visualize the outputs on a map using folium.\n\n# To access the year value from each item more easily, this will let us query more explicitly by year and month (e.g., 2020-02)\nitems = {item[\"properties\"][\"start_datetime\"][:7]: item for item in items} \n# rh = Heterotrophic Respiration\nasset_name = \"rh\"\n\nBelow, we are entering the minimum and maximum values to provide our upper and lower bounds in rescale_values.\n\nrescale_values = {\"max\":items[list(items.keys())[0]][\"assets\"][asset_name][\"raster:bands\"][0][\"histogram\"][\"max\"], \"min\":items[list(items.keys())[0]][\"assets\"][asset_name][\"raster:bands\"][0][\"histogram\"][\"min\"]}\n\nNow, we will pass the item id, collection name, and rescaling_factor to the Raster API endpoint. We will do this twice, once for December 2003 and again for December 2017, so that we can visualize each event independently.\n\ncolor_map = \"purd\" # please refer to matplotlib library if you'd prefer choosing a different color ramp.\n# For more information on Colormaps in Matplotlib, please visit https://matplotlib.org/stable/users/explain/colors/colormaps.html\n\n# To change the year and month of the observed parameter, you can modify the \"items['YYYY-MM']\" statement\n# For example, you can change the current statement \"items['2003-12']\" to \"items['2016-10']\" \ndecember_2003_tile = requests.get(\n    f\"{RASTER_API_URL}/stac/tilejson.json?collection={items['2003-12']['collection']}&item={items['2003-12']['id']}\"\n    f\"&assets={asset_name}\"\n    f\"&color_formula=gamma+r+1.05&colormap_name={color_map}\"\n    f\"&rescale={rescale_values['min']},{rescale_values['max']}\", \n).json()\ndecember_2003_tile\n\n\n# Now we apply the same process used in the previous step for the December 2017 tile\ndecember_2017_tile = requests.get(\n    f\"{RASTER_API_URL}/stac/tilejson.json?collection={items['2017-12']['collection']}&item={items['2017-12']['id']}\"\n    f\"&assets={asset_name}\"\n    f\"&color_formula=gamma+r+1.05&colormap_name={color_map}\"\n    f\"&rescale={rescale_values['min']},{rescale_values['max']}\", \n).json()\ndecember_2017_tile"
  },
  {
    "objectID": "user_data_notebooks/casagfed-carbonflux-monthgrid-v3_User_Notebook.html#visualizing-land-atmosphere-carbon-flux-heterotrophic-respiration",
    "href": "user_data_notebooks/casagfed-carbonflux-monthgrid-v3_User_Notebook.html#visualizing-land-atmosphere-carbon-flux-heterotrophic-respiration",
    "title": "CASA-GFED3 Land Carbon Flux",
    "section": "Visualizing Land-Atmosphere Carbon Flux (Heterotrophic Respiration)",
    "text": "Visualizing Land-Atmosphere Carbon Flux (Heterotrophic Respiration)\n\n# For this study we are going to compare the RH level in 2003 and 2017 over the State of Texas \n# To change the location, you can simply insert the latitude and longitude of the area of your interest in the \"location=(LAT, LONG)\" statement\n# For example, you can change the current statement \"location=(31.9, -99.9)\" to \"location=(34, -118)\" to monitor the RH level in California instead of Texas\n\n# Set initial zoom and center of map for CO₂ Layer\n# 'folium.plugins' allows mapping side-by-side\nmap_ = folium.plugins.DualMap(location=(31.9, -99.9), zoom_start=6)\n\n# The TileLayer library helps in manipulating and displaying raster layers on a map\n# December 2003\nmap_layer_2003 = TileLayer(\n    tiles=december_2003_tile[\"tiles\"][0],\n    attr=\"GHG\",\n    opacity=0.8,\n    name=\"December 2003 RH Level\",\n    overlay= True,\n    legendEnabled = True\n)\nmap_layer_2003.add_to(map_.m1)\n\n\n# December 2017\nmap_layer_2017 = TileLayer(\n    tiles=december_2017_tile[\"tiles\"][0],\n    attr=\"GHG\",\n    opacity=0.8,\n    name=\"December 2017 RH Level\",\n    overlay= True,\n    legendEnabled = True\n)\nmap_layer_2017.add_to(map_.m2)\n\n\n# Display data markers (titles) on both maps\nfolium.Marker((40, 5.0), tooltip=\"both\").add_to(map_)\nfolium.LayerControl(collapsed=False).add_to(map_)\n\n\n# Add a legend to the dual map using the 'branca' library. \n# Note: the inserted legend is representing the minimum and maximum values for both tiles.\ncolormap = branca.colormap.linear.PuRd_09.scale(0, 0.3) # minimum value = 0, maximum value = 0.3 (kg Carbon/m2/month)\ncolormap = colormap.to_step(index=[0, 0.07, 0.15, 0.22, 0.3])\ncolormap.caption = 'Rh Values (kg Carbon/m2/month)'\n\ncolormap.add_to(map_.m1)\n\n\n# Visualizing the map\nmap_"
  },
  {
    "objectID": "user_data_notebooks/casagfed-carbonflux-monthgrid-v3_User_Notebook.html#visualizing-the-data-as-a-time-series",
    "href": "user_data_notebooks/casagfed-carbonflux-monthgrid-v3_User_Notebook.html#visualizing-the-data-as-a-time-series",
    "title": "CASA-GFED3 Land Carbon Flux",
    "section": "Visualizing the Data as a Time Series",
    "text": "Visualizing the Data as a Time Series\nWe can now explore the Heterotrophic Respiration time series (January 2003 -December 2017) available for the Dallas, Texas area. We can plot the data set using the code below:\n\nfig = plt.figure(figsize=(20, 10)) #determine the width and height of the plot using the 'matplotlib' library\n\nplt.plot(\n    df[\"date\"],\n    df[\"max\"],\n    color=\"purple\",\n    linestyle=\"-\",\n    linewidth=0.5,\n    label=\"Max monthly Carbon emissions\",\n)\n\nplt.legend()\nplt.xlabel(\"Years\")\nplt.ylabel(\"kg Carbon/m2/month\")\nplt.title(\"Heterotrophic Respiration Values for Dallas, Texas (2003-2017)\")\n\n\n# Now let's examine the Rh level for the 3rd item in the collection for Dallas, Texas area\n# Keep in mind that a list starts from 0, 1, 2,... therefore items[2] is referring to the third item in the list/collection\nprint(items[2][\"properties\"][\"start_datetime\"]) #print the start Date Time of the third granule in the collection!\n\n\n# Fetch the third granule in the collection and set the color scheme and rescale values. \noctober_tile = requests.get(\n    f\"{RASTER_API_URL}/stac/tilejson.json?collection={items[2]['collection']}&item={items[2]['id']}\"\n    f\"&assets={asset_name}\"\n    f\"&color_formula=gamma+r+1.05&colormap_name={color_map}\"\n    f\"&rescale={rescale_values['min']},{rescale_values['max']}\",\n).json()\noctober_tile\n\n\n# Map the Rh level for the Dallas, Texas area for the October, 2017 timeframe\naoi_map_bbox = Map(\n    tiles=\"OpenStreetMap\",\n    location=[\n        32.8, # latitude\n        -96.79, # longitude\n    ],\n    zoom_start=9,\n)\n\nmap_layer = TileLayer(\n    tiles=october_tile[\"tiles\"][0],\n    attr=\"GHG\", opacity = 0.7, name=\"October 2017 RH Level\", overlay= True, legendEnabled = True\n)\n\nmap_layer.add_to(aoi_map_bbox)\n\n# Display data marker (title) on the map\nfolium.Marker((40, 5.9), tooltip=\"both\").add_to(aoi_map_bbox)\nfolium.LayerControl(collapsed=False).add_to(aoi_map_bbox)\n\n# Add a legend\ncolormap = branca.colormap.linear.PuRd_09.scale(0, 0.3) # minimum value = 0, maximum value = 0.3 (kg Carbon/m2/month)\ncolormap = colormap.to_step(index=[0, 0.07, 0.15, 0.22, 0.3])\ncolormap.caption = 'Rh Values (kg Carbon/m2/month)'\n\ncolormap.add_to(aoi_map_bbox)\n\naoi_map_bbox"
  },
  {
    "objectID": "user_data_notebooks/casagfed-carbonflux-monthgrid-v3_User_Notebook.html#summary",
    "href": "user_data_notebooks/casagfed-carbonflux-monthgrid-v3_User_Notebook.html#summary",
    "title": "CASA-GFED3 Land Carbon Flux",
    "section": "Summary",
    "text": "Summary\nIn this notebook we have successfully completed the following steps for the STAC collection for CASA GFED Land-Atmosphere Carbon Flux data: 1. Install and import the necessary libraries 2. Fetch the collection from STAC collections using the appropriate endpoints 3. Count the number of existing granules within the collection 4. Map and compare the Heterotrophic Respiration (Rh) levels over the Dallas, Texas area for two distinctive years 5. Create a table that displays the minimum, maximum, and sum of the Rh values for a specified region 6. Generate a time-series graph of the Rh values for a specified region\nIf you have any questions regarding this user notebook, please contact us using the feedback form."
  },
  {
    "objectID": "user_data_notebooks/odiac-ffco2-monthgrid-v2022_User_Notebook.html",
    "href": "user_data_notebooks/odiac-ffco2-monthgrid-v2022_User_Notebook.html",
    "title": "ODIAC Fossil Fuel CO₂ Emissions",
    "section": "",
    "text": "You can launch this notebook in the US GHG Center JupyterHub by clicking the link below.\nLaunch in the US GHG Center JupyterHub (requires access)"
  },
  {
    "objectID": "user_data_notebooks/odiac-ffco2-monthgrid-v2022_User_Notebook.html#run-this-notebook",
    "href": "user_data_notebooks/odiac-ffco2-monthgrid-v2022_User_Notebook.html#run-this-notebook",
    "title": "ODIAC Fossil Fuel CO₂ Emissions",
    "section": "",
    "text": "You can launch this notebook in the US GHG Center JupyterHub by clicking the link below.\nLaunch in the US GHG Center JupyterHub (requires access)"
  },
  {
    "objectID": "user_data_notebooks/odiac-ffco2-monthgrid-v2022_User_Notebook.html#approach",
    "href": "user_data_notebooks/odiac-ffco2-monthgrid-v2022_User_Notebook.html#approach",
    "title": "ODIAC Fossil Fuel CO₂ Emissions",
    "section": "Approach",
    "text": "Approach\n\nIdentify available dates and temporal frequency of observations for the given collection using the GHGC API /stac endpoint. Collection processed in this notebook is ODIAC CO₂ emissions version 2022.\nPass the STAC item into raster API /stac/tilejson.json endpoint\nWe’ll visualize two tiles (side-by-side) allowing for comparison of each of the time points using folium.plugins.DualMap\nAfter the visualization, we’ll perform zonal statistics for a given polygon."
  },
  {
    "objectID": "user_data_notebooks/odiac-ffco2-monthgrid-v2022_User_Notebook.html#about-the-data",
    "href": "user_data_notebooks/odiac-ffco2-monthgrid-v2022_User_Notebook.html#about-the-data",
    "title": "ODIAC Fossil Fuel CO₂ Emissions",
    "section": "About the Data",
    "text": "About the Data\nThe Open-Data Inventory for Anthropogenic Carbon dioxide (ODIAC) is a high-spatial resolution global emission data product of CO₂ emissions from fossil fuel combustion (Oda and Maksyutov, 2011). ODIAC pioneered the combined use of space-based nighttime light data and individual power plant emission/location profiles to estimate the global spatial extent of fossil fuel CO₂ emissions. With the innovative emission modeling approach, ODIAC achieved the fine picture of global fossil fuel CO₂ emissions at a 1x1km.\nFor more information regarding this dataset, please visit the ODIAC Fossil Fuel CO₂ Emissions data overview page."
  },
  {
    "objectID": "user_data_notebooks/odiac-ffco2-monthgrid-v2022_User_Notebook.html#querying-the-stac-api",
    "href": "user_data_notebooks/odiac-ffco2-monthgrid-v2022_User_Notebook.html#querying-the-stac-api",
    "title": "ODIAC Fossil Fuel CO₂ Emissions",
    "section": "Querying the STAC API",
    "text": "Querying the STAC API\nFirst, we are going to import the required libraries. Once imported, they allow better executing a query in the GHG Center Spatio Temporal Asset Catalog (STAC) Application Programming Interface (API) where the granules for this collection are stored.\n\n# Import the following libraries\nimport requests\nimport folium\nimport folium.plugins\nfrom folium import Map, TileLayer\nfrom pystac_client import Client\nimport branca\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n\n# Provide the STAC and RASTER API endpoints\n# The endpoint is referring to a location within the API that executes a request on a data collection nesting on the server.\n\n# The STAC API is a catalog of all the existing data collections that are stored in the GHG Center.\nSTAC_API_URL = \"https://earth.gov/ghgcenter/api/stac\"\n\n# The RASTER API is used to fetch collections for visualization\nRASTER_API_URL = \"https://earth.gov/ghgcenter/api/raster\"\n\n# The collection name is used to fetch the dataset from the STAC API. First, we define the collection name as a variable\n# Name of the collection for ODIAC dataset \ncollection_name = \"odiac-ffco2-monthgrid-v2022\"\n\n\n# Fetch the collection from the STAC API using the appropriate endpoint\n# The 'requests' library allows a HTTP request possible\ncollection = requests.get(f\"{STAC_API_URL}/collections/{collection_name}\").json()\n\n# Print the properties of the collection to the console\ncollection\n\nExamining the contents of our collection under summaries we see that the data is available from January 2000 to December 2021. By looking at the dashboard:time density we observe that the periodic frequency of these observations is monthly.\n\n# Create a function that would search for a data collection in the US GHG Center STAC API\n\n# First, we need to define the function\n# The name of the function = \"get_item_count\"\n# The argument that will be passed through the defined function = \"collection_id\"\ndef get_item_count(collection_id):\n\n    # Set a counter for the number of items existing in the collection\n    count = 0\n\n    # Define the path to retrieve the granules (items) of the collection of interest in the STAC API\n    items_url = f\"{STAC_API_URL}/collections/{collection_id}/items\"\n\n    # Run a while loop to make HTTP requests until there are no more URLs associated with the collection in the STAC API\n    while True:\n\n        # Retrieve information about the granules by sending a \"get\" request to the STAC API using the defined collection path\n        response = requests.get(items_url)\n\n        # If the items do not exist, print an error message and quit the loop\n        if not response.ok:\n            print(\"error getting items\")\n            exit()\n\n        # Return the results of the HTTP response as JSON\n        stac = response.json()\n\n        # Increase the \"count\" by the number of items (granules) returned in the response\n        count += int(stac[\"context\"].get(\"returned\", 0))\n\n        # Retrieve information about the next URL associated with the collection in the STAC API (if applicable)\n        next = [link for link in stac[\"links\"] if link[\"rel\"] == \"next\"]\n\n        # Exit the loop if there are no other URLs\n        if not next:\n            break\n        \n        # Ensure the information gathered by other STAC API links associated with the collection are added to the original path\n        # \"href\" is the identifier for each of the tiles stored in the STAC API\n        items_url = next[0][\"href\"]\n\n    # Return the information about the total number of granules found associated with the collection\n    return count\n\n\n# Apply the function created above \"get_item_count\" to the data collection\nnumber_of_items = get_item_count(collection_name)\n\n# Get the information about the number of granules found in the collection\nitems = requests.get(f\"{STAC_API_URL}/collections/{collection_name}/items?limit={number_of_items}\").json()[\"features\"]\n\n# Print the total number of items (granules) found\nprint(f\"Found {len(items)} items\")\n\nThis makes sense as there are 22 years between 2000 - 2021, with 12 months per year, meaning 264 records in total.\n\n# Examine the first item in the collection\n# Keep in mind that a list starts from 0, 1, 2... therefore items[0] is referring to the first item in the list/collection\nitems[0]"
  },
  {
    "objectID": "user_data_notebooks/odiac-ffco2-monthgrid-v2022_User_Notebook.html#exploring-changes-in-carbon-dioxide-co₂-levels-using-the-raster-api",
    "href": "user_data_notebooks/odiac-ffco2-monthgrid-v2022_User_Notebook.html#exploring-changes-in-carbon-dioxide-co₂-levels-using-the-raster-api",
    "title": "ODIAC Fossil Fuel CO₂ Emissions",
    "section": "Exploring Changes in Carbon Dioxide (CO₂) levels using the Raster API",
    "text": "Exploring Changes in Carbon Dioxide (CO₂) levels using the Raster API\nWe will explore changes in fossil fuel emissions in urban egions. In this notebook, we’ll explore the impacts of these emissions and explore these changes over time. We’ll then visualize the outputs on a map using folium.\n\n# Now we create a dictionary where the start datetime values for each granule is queried more explicitly by year and month (e.g., 2020-02)\nitems = {item[\"properties\"][\"start_datetime\"][:7]: item for item in items} \n\n# Next, we need to specify the asset name for this collection\n# The asset name is referring to the raster band containing the pixel values for the parameter of interest\n# For the case of the ODIAC Fossil Fuel CO₂ Emissions collection, the parameter of interest is “co2-emissions”\nasset_name = \"co2-emissions\"\n\nBelow, we are entering the minimum and maximum values to provide our upper and lower bounds in rescale_values.\n\n# Fetching the min and max values for a specific item\nrescale_values = {\"max\":items[list(items.keys())[0]][\"assets\"][asset_name][\"raster:bands\"][0][\"histogram\"][\"max\"], \"min\":items[list(items.keys())[0]][\"assets\"][asset_name][\"raster:bands\"][0][\"histogram\"][\"min\"]}\n\nNow, we will pass the item id, collection name, asset name, and the rescaling factor to the Raster API endpoint. We will do this twice, once for January 2020 and again for January 2000, so that we can visualize each event independently.\n\n# Choose a color map for displaying the first observation (event)\n# Please refer to matplotlib library if you'd prefer choosing a different color ramp.\n# For more information on Colormaps in Matplotlib, please visit https://matplotlib.org/stable/users/explain/colors/colormaps.html\ncolor_map = \"rainbow\" \n\n# Make a GET request to retrieve information for the 2020 tile\n# 2020\njanuary_2020_tile = requests.get(\n\n    # Pass the collection name, the item number in the list, and its ID\n    f\"{RASTER_API_URL}/stac/tilejson.json?collection={items['2020-01']['collection']}&item={items['2020-01']['id']}\"\n\n    # Pass the asset name\n    f\"&assets={asset_name}\"\n\n    # Pass the color formula and colormap for custom visualization\n    f\"&color_formula=gamma+r+1.05&colormap_name={color_map}\"\n\n    # Pass the minimum and maximum values for rescaling\n    f\"&rescale={rescale_values['min']},{rescale_values['max']}\", \n\n# Return the response in JSON format\n).json()\n\n# Print the properties of the retrieved granule to the console\njanuary_2020_tile\n\n\n# Make a GET request to retrieve information for the 2000 tile\n# 2000\njanuary_2000_tile = requests.get(\n\n    # Pass the collection name, the item number in the list, and its ID\n    f\"{RASTER_API_URL}/stac/tilejson.json?collection={items['2000-01']['collection']}&item={items['2000-01']['id']}\"\n\n    # Pass the asset name\n    f\"&assets={asset_name}\"\n\n    # Pass the color formula and colormap for custom visualization\n    f\"&color_formula=gamma+r+1.05&colormap_name={color_map}\"\n\n    # Pass the minimum and maximum values for rescaling\n    f\"&rescale={rescale_values['min']},{rescale_values['max']}\", \n\n# Return the response in JSON format\n).json()\n\n# Print the properties of the retrieved granule to the console\njanuary_2000_tile"
  },
  {
    "objectID": "user_data_notebooks/odiac-ffco2-monthgrid-v2022_User_Notebook.html#visualizing-co₂-emissions",
    "href": "user_data_notebooks/odiac-ffco2-monthgrid-v2022_User_Notebook.html#visualizing-co₂-emissions",
    "title": "ODIAC Fossil Fuel CO₂ Emissions",
    "section": "Visualizing CO₂ emissions",
    "text": "Visualizing CO₂ emissions\n\n# To change the location, you can simply insert the latitude and longitude of the area of your interest in the \"location=(LAT, LONG)\" statement\n\n# Set the initial zoom level and center of map for both tiles\n# 'folium.plugins' allows mapping side-by-side\nmap_ = folium.plugins.DualMap(location=(34, -118), zoom_start=6)\n\n# Define the first map layer (January 2020)\nmap_layer_2020 = TileLayer(\n    tiles=january_2020_tile[\"tiles\"][0], # Path to retrieve the tile\n    attr=\"GHG\", # Set the attribution\n    opacity=0.8, # Adjust the transparency of the layer\n)\n\n# Add the first layer to the Dual Map\nmap_layer_2020.add_to(map_.m1)\n\n# Define the second map layer (January 2000)\nmap_layer_2000 = TileLayer(\n    tiles=january_2000_tile[\"tiles\"][0], # Path to retrieve the tile\n    attr=\"GHG\", # Set the attribution\n    opacity=0.8, # Adjust the transparency of the layer\n)\n\n# Add the second layer to the Dual Map\nmap_layer_2000.add_to(map_.m2)\n\n# Visualize the Dual Map\nmap_"
  },
  {
    "objectID": "user_data_notebooks/odiac-ffco2-monthgrid-v2022_User_Notebook.html#visualizing-the-data-as-a-time-series",
    "href": "user_data_notebooks/odiac-ffco2-monthgrid-v2022_User_Notebook.html#visualizing-the-data-as-a-time-series",
    "title": "ODIAC Fossil Fuel CO₂ Emissions",
    "section": "Visualizing the Data as a Time Series",
    "text": "Visualizing the Data as a Time Series\nWe can now explore the ODIAC fossil fuel emission time series available (January 2000 -December 2021) for the Texas, Dallas area of USA. We can plot the data set using the code below:\n\n# Figure size: 20 representing the width, 10 representing the height\nfig = plt.figure(figsize=(20, 10))\n\n\nplt.plot(\n    df[\"date\"], # X-axis: sorted datetime\n    df[\"max\"], # Y-axis: maximum CO₂ level\n    color=\"red\", # Line color\n    linestyle=\"-\", # Line style\n    linewidth=0.5, # Line width\n    label=\"Max monthly CO₂ emissions\", # Legend label\n)\n\n# Display legend\nplt.legend()\n\n# Insert label for the X-axis\nplt.xlabel(\"Years\")\n\n# Insert label for the Y-axis\nplt.ylabel(\"CO2 emissions gC/m2/d\")\n\n# Insert title for the plot\nplt.title(\"CO2 emission Values for Texas, Dallas (2000-2021)\")\n\n###\n# Add data citation\nplt.text(\n    df[\"date\"].iloc[0],           # X-coordinate of the text\n    df[\"max\"].min(),              # Y-coordinate of the text\n\n\n\n\n    # Text to be displayed\n    \"Source: NASA ODIAC Fossil Fuel CO₂ Emissions\",                  \n    fontsize=12,                             # Font size\n    horizontalalignment=\"right\",             # Horizontal alignment\n    verticalalignment=\"top\",                 # Vertical alignment\n    color=\"blue\",                            # Text color\n)\n\n# Plot the time series\nplt.show()\n\n\n# Print the properties of the 3rd item in the collection\nprint(items[2][\"properties\"][\"start_datetime\"])\n\n\n# A GET request is made for the October tile\noctober_tile = requests.get(\n\n    # Pass the collection name, the item number in the list, and its ID\n    f\"{RASTER_API_URL}/stac/tilejson.json?collection={items[2]['collection']}&item={items[2]['id']}\"\n\n    # Pass the asset name\n    f\"&assets={asset_name}\"\n\n    # Pass the color formula and colormap for custom visualization\n    f\"&color_formula=gamma+r+1.05&colormap_name={color_map}\"\n\n    # Pass the minimum and maximum values for rescaling\n    f\"&rescale={rescale_values['min']},{rescale_values['max']}\",\n\n# Return the response in JSON format\n).json()\n\n# Print the properties of the retrieved granule to the console\noctober_tile\n\n\n# Create a new map to display the October tile\naoi_map_bbox = Map(\n\n    # Base map is set to OpenStreetMap\n    tiles=\"OpenStreetMap\",\n\n    # Set the center of the map\n    location=[\n        30,-100\n    ],\n\n    # Set the zoom value\n    zoom_start=8,\n)\n\n# Define the map layer\nmap_layer = TileLayer(\n\n    # Path to retrieve the tile\n    tiles=october_tile[\"tiles\"][0],\n\n    # Set the attribution and adjust the transparency of the layer\n    attr=\"GHG\", opacity = 0.5\n)\n\n# Add the layer to the map\nmap_layer.add_to(aoi_map_bbox)\n\n# Visualize the map\naoi_map_bbox"
  },
  {
    "objectID": "user_data_notebooks/odiac-ffco2-monthgrid-v2022_User_Notebook.html#summary",
    "href": "user_data_notebooks/odiac-ffco2-monthgrid-v2022_User_Notebook.html#summary",
    "title": "ODIAC Fossil Fuel CO₂ Emissions",
    "section": "Summary",
    "text": "Summary\nIn this notebook we have successfully explored, analysed and visualized STAC collecetion for ODIAC C02 fossisl fuel emission (2022).\n\nInstall and import the necessary libraries\nFetch the collection from STAC collections using the appropriate endpoints\nCount the number of existing granules within the collection\nMap and compare the CO₂ levels for two distinctive months/years\nGenerate zonal statistics for the area of interest (AOI)\nVisualizing the Data as a Time Series\n\nIf you have any questions regarding this user notebook, please contact us using the feedback form."
  },
  {
    "objectID": "user_data_notebooks/tm54dvar-ch4flux-monthgrid-v1_User_Notebook.html",
    "href": "user_data_notebooks/tm54dvar-ch4flux-monthgrid-v1_User_Notebook.html",
    "title": "TM5-4DVar Isotopic CH₄ Inverse Fluxes",
    "section": "",
    "text": "You can launch this notebook in the US GHG Center JupyterHub by clicking the link below.\nLaunch in the US GHG Center JupyterHub (requires access)",
    "crumbs": [
      "Data Usage Notebooks",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "TM5-4DVar Isotopic CH₄ Inverse Fluxes"
    ]
  },
  {
    "objectID": "user_data_notebooks/tm54dvar-ch4flux-monthgrid-v1_User_Notebook.html#run-this-notebook",
    "href": "user_data_notebooks/tm54dvar-ch4flux-monthgrid-v1_User_Notebook.html#run-this-notebook",
    "title": "TM5-4DVar Isotopic CH₄ Inverse Fluxes",
    "section": "",
    "text": "You can launch this notebook in the US GHG Center JupyterHub by clicking the link below.\nLaunch in the US GHG Center JupyterHub (requires access)",
    "crumbs": [
      "Data Usage Notebooks",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "TM5-4DVar Isotopic CH₄ Inverse Fluxes"
    ]
  },
  {
    "objectID": "user_data_notebooks/tm54dvar-ch4flux-monthgrid-v1_User_Notebook.html#approach",
    "href": "user_data_notebooks/tm54dvar-ch4flux-monthgrid-v1_User_Notebook.html#approach",
    "title": "TM5-4DVar Isotopic CH₄ Inverse Fluxes",
    "section": "Approach",
    "text": "Approach\n\nIdentify available dates and temporal frequency of observations for the given collection using the GHGC API /stac endpoint. The collection processed in this notebook is the TM5-4DVar Isotopic CH₄ Inverse Fluxes Data product.\nPass the STAC item into the raster API /stac/tilejson.jsonendpoint.\nUsing folium.plugins.DualMap, we will visualize two tiles (side-by-side), allowing us to compare time points.\nAfter the visualization, we will perform zonal statistics for a given polygon.",
    "crumbs": [
      "Data Usage Notebooks",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "TM5-4DVar Isotopic CH₄ Inverse Fluxes"
    ]
  },
  {
    "objectID": "user_data_notebooks/tm54dvar-ch4flux-monthgrid-v1_User_Notebook.html#about-the-data",
    "href": "user_data_notebooks/tm54dvar-ch4flux-monthgrid-v1_User_Notebook.html#about-the-data",
    "title": "TM5-4DVar Isotopic CH₄ Inverse Fluxes",
    "section": "About the Data",
    "text": "About the Data\nSurface methane (CH₄) emissions are derived from atmospheric measurements of methane and its ¹³C carbon isotope content. Different sources of methane contain different ratios of the two stable isotopologues, ¹²CH₄ and ¹³CH₄. This makes normally indistinguishable collocated sources of methane, say from agriculture and oil and gas exploration, distinguishable. The National Oceanic and Atmospheric Administration (NOAA) collects whole air samples from its global cooperative network of flasks (https://gml.noaa.gov/ccgg/about.html), which are then analyzed for methane and other trace gasses. A subset of those flasks are also analyzed for ¹³C of methane in collaboration with the Institute of Arctic and Alpine Research at the University of Colorado Boulder. Scientists at the National Aeronautics and Space Administration (NASA) and NOAA used those measurements of methane and ¹³C of methane in conjunction with a model of atmospheric circulation to estimate emissions of methane separated by three source types, microbial, fossil and pyrogenic.\nFor more information regarding this dataset, please visit the TM5-4DVar Isotopic CH₄ Inverse Fluxes data overview page.",
    "crumbs": [
      "Data Usage Notebooks",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "TM5-4DVar Isotopic CH₄ Inverse Fluxes"
    ]
  },
  {
    "objectID": "user_data_notebooks/tm54dvar-ch4flux-monthgrid-v1_User_Notebook.html#querying-the-stac-api",
    "href": "user_data_notebooks/tm54dvar-ch4flux-monthgrid-v1_User_Notebook.html#querying-the-stac-api",
    "title": "TM5-4DVar Isotopic CH₄ Inverse Fluxes",
    "section": "Querying the STAC API",
    "text": "Querying the STAC API\nFirst, we are going to import the required libraries. Once imported, they allow better executing a query in the GHG Center Spatio Temporal Asset Catalog (STAC) Application Programming Interface (API) where the granules for this collection are stored.\n\n# Provide the STAC and RASTER API endpoints\n# The endpoint is referring to a location within the API that executes a request on a data collection nesting on the server.\n\n# The STAC API is a catalog of all the existing data collections that are stored in the GHG Center.\nSTAC_API_URL = \"https://earth.gov/ghgcenter/api/stac\"\n\n# The RASTER API is used to fetch collections for visualization\nRASTER_API_URL = \"https://earth.gov/ghgcenter/api/raster\"\n\n# The collection name is used to fetch the dataset from the STAC API. First, we define the collection name as a variable\n# Name of the collection for TM5 CH₄ inverse flux dataset \ncollection_name = \"tm54dvar-ch4flux-monthgrid-v1\"\n\n\n# Fetch the collection from the STAC API using the appropriate endpoint\n# The 'requests' library allows a HTTP request possible\ncollection = requests.get(f\"{STAC_API_URL}/collections/{collection_name}\").json()\n\n# Print the properties of the collection to the console\ncollection\n\nExamining the contents of our collection under the temporal variable, we see that the data is available from January 1999 to December 2016. By looking at the dashboard:time density, we observe that the data is periodic with monthly time density.\n\n# Create a function that would search for a data collection in the US GHG Center STAC API\n\n# First, we need to define the function\n# The name of the function = \"get_item_count\"\n# The argument that will be passed through the defined function = \"collection_id\"\ndef get_item_count(collection_id):\n\n    # Set a counter for the number of items existing in the collection\n    count = 0\n\n    # Define the path to retrieve the granules (items) of the collection of interest in the STAC API\n    items_url = f\"{STAC_API_URL}/collections/{collection_id}/items\"\n\n    # Run a while loop to make HTTP requests until there are no more URLs associated with the collection in the STAC API\n    while True:\n\n        # Retrieve information about the granules by sending a \"get\" request to the STAC API using the defined collection path\n        response = requests.get(items_url)\n\n        # If the items do not exist, print an error message and quit the loop\n        if not response.ok:\n            print(\"error getting items\")\n            exit()\n\n        # Return the results of the HTTP response as JSON\n        stac = response.json()\n\n        # Increase the \"count\" by the number of items (granules) returned in the response\n        count += int(stac[\"context\"].get(\"returned\", 0))\n\n        # Retrieve information about the next URL associated with the collection in the STAC API (if applicable)\n        next = [link for link in stac[\"links\"] if link[\"rel\"] == \"next\"]\n\n        # Exit the loop if there are no other URLs\n        if not next:\n            break\n        \n        # Ensure the information gathered by other STAC API links associated with the collection are added to the original path\n        # \"href\" is the identifier for each of the tiles stored in the STAC API\n        items_url = next[0][\"href\"]\n\n    # Return the information about the total number of granules found associated with the collection\n    return count\n\n\n# Apply the function created above \"get_item_count\" to the data collection\nnumber_of_items = get_item_count(collection_name)\n\n# Get the information about the number of granules found in the collection\nitems = requests.get(f\"{STAC_API_URL}/collections/{collection_name}/items?limit={number_of_items}\").json()[\"features\"]\n\n# Print the total number of items (granules) found\nprint(f\"Found {len(items)} items\")\n\n\n# Examine the first item in the collection\n# Keep in mind that a list starts from 0, 1, 2... therefore items[0] is referring to the first item in the list/collection\nitems[0]",
    "crumbs": [
      "Data Usage Notebooks",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "TM5-4DVar Isotopic CH₄ Inverse Fluxes"
    ]
  },
  {
    "objectID": "user_data_notebooks/tm54dvar-ch4flux-monthgrid-v1_User_Notebook.html#exploring-changes-in-ch₄-flux-levels-using-the-raster-api",
    "href": "user_data_notebooks/tm54dvar-ch4flux-monthgrid-v1_User_Notebook.html#exploring-changes-in-ch₄-flux-levels-using-the-raster-api",
    "title": "TM5-4DVar Isotopic CH₄ Inverse Fluxes",
    "section": "Exploring Changes in CH₄ flux Levels Using the Raster API",
    "text": "Exploring Changes in CH₄ flux Levels Using the Raster API\nIn this notebook, we will explore the global changes of CH₄ flux over time in urban regions. We will visualize the outputs on a map using folium.\n\n# Now we create a dictionary where the start datetime values for each granule is queried more explicitly by year and month (e.g., 2020-02)\nitems = {item[\"properties\"][\"start_datetime\"][:10]: item for item in items} \n\n# Next, we need to specify the asset name for this collection\n# The asset name is referring to the raster band containing the pixel values for the parameter of interest\n# For the case of the TM5-4DVar Isotopic CH₄ Inverse Fluxes collection, the parameter of interest is “fossil”\nasset_name = \"fossil\" #fossil fuel\n\nBelow, we are entering the minimum and maximum values to provide our upper and lower bounds in the rescale_values.\n\n# Fetching the min and max values for a specific item\nrescale_values = {\"max\":items[list(items.keys())[0]][\"assets\"][asset_name][\"raster:bands\"][0][\"histogram\"][\"max\"], \"min\":items[list(items.keys())[0]][\"assets\"][asset_name][\"raster:bands\"][0][\"histogram\"][\"min\"]}\n\nNow, we will pass the item id, collection name, asset name, and the rescaling factor to the Raster API endpoint. We will do this twice, once for 2016 and again for 1999, so that we can visualize each event independently.\n\n# Choose a color map for displaying the first observation (event)\n# Please refer to matplotlib library if you'd prefer choosing a different color ramp.\n# For more information on Colormaps in Matplotlib, please visit https://matplotlib.org/stable/users/explain/colors/colormaps.html\ncolor_map = \"purd\"\n\n# Make a GET request to retrieve information for the 2016 tile\nch4_flux_1 = requests.get(\n\n    # Pass the collection name, the item number in the list, and its ID\n    f\"{RASTER_API_URL}/stac/tilejson.json?collection={items['2016-12-01']['collection']}&item={items['2016-12-01']['id']}\"\n\n    # Pass the asset name\n    f\"&assets={asset_name}\"\n\n    # Pass the color formula and colormap for custom visualization\n    f\"&color_formula=gamma+r+1.05&colormap_name={color_map}\"\n\n    # Pass the minimum and maximum values for rescaling\n    f\"&rescale={rescale_values['min']},{rescale_values['max']}\", \n\n# Return the response in JSON format\n).json()\n\n# Print the properties of the retrieved granule to the console\nch4_flux_1\n\n\n# Make a GET request to retrieve information for the 1999 tile\nch4_flux_2 = requests.get(\n\n    # Pass the collection name, the item number in the list, and its ID\n    f\"{RASTER_API_URL}/stac/tilejson.json?collection={items['1999-12-01']['collection']}&item={items['1999-12-01']['id']}\"\n\n    # Pass the asset name\n    f\"&assets={asset_name}\"\n\n    # Pass the color formula and colormap for custom visualization\n    f\"&color_formula=gamma+r+1.05&colormap_name={color_map}\"\n\n    # Pass the minimum and maximum values for rescaling\n    f\"&rescale={rescale_values['min']},{rescale_values['max']}\", \n\n# Return the response in JSON format\n).json()\n\n# Print the properties of the retrieved granule to the console\nch4_flux_2",
    "crumbs": [
      "Data Usage Notebooks",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "TM5-4DVar Isotopic CH₄ Inverse Fluxes"
    ]
  },
  {
    "objectID": "user_data_notebooks/tm54dvar-ch4flux-monthgrid-v1_User_Notebook.html#visualizing-ch₄-flux-emissions-from-fossil-fuel",
    "href": "user_data_notebooks/tm54dvar-ch4flux-monthgrid-v1_User_Notebook.html#visualizing-ch₄-flux-emissions-from-fossil-fuel",
    "title": "TM5-4DVar Isotopic CH₄ Inverse Fluxes",
    "section": "Visualizing CH₄ flux Emissions from Fossil Fuel",
    "text": "Visualizing CH₄ flux Emissions from Fossil Fuel\n\n# For this study we are going to compare CH4 fluxes from fossil fuels in 2016 and 1999 along the coast of California\n# To change the location, you can simply insert the latitude and longitude of the area of your interest in the \"location=(LAT, LONG)\" statement\n\n# Set the initial zoom level and center of map for both tiles\n# 'folium.plugins' allows mapping side-by-side\nmap_ = folium.plugins.DualMap(location=(34, -118), zoom_start=6)\n\n# Define the first map layer (2016)\nmap_layer_2016 = TileLayer(\n    tiles=ch4_flux_1[\"tiles\"][0], # Path to retrieve the tile\n    attr=\"GHG\", # Set the attribution\n    opacity=0.8, # Adjust the transparency of the layer\n)\n# Add the first layer to the Dual Map\nmap_layer_2016.add_to(map_.m1)\n\n\n# Define the second map layer (1999)\nmap_layer_1999 = TileLayer(\n    tiles=ch4_flux_2[\"tiles\"][0], # Path to retrieve the tile\n    attr=\"GHG\", # Set the attribution\n    opacity=0.8, # Adjust the transparency of the layer\n)\n\n# Add the second layer to the Dual Map\nmap_layer_1999.add_to(map_.m2)\n\n# Visualize the Dual Map\nmap_",
    "crumbs": [
      "Data Usage Notebooks",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "TM5-4DVar Isotopic CH₄ Inverse Fluxes"
    ]
  },
  {
    "objectID": "user_data_notebooks/tm54dvar-ch4flux-monthgrid-v1_User_Notebook.html#visualizing-the-data-as-a-time-series",
    "href": "user_data_notebooks/tm54dvar-ch4flux-monthgrid-v1_User_Notebook.html#visualizing-the-data-as-a-time-series",
    "title": "TM5-4DVar Isotopic CH₄ Inverse Fluxes",
    "section": "Visualizing the Data as a Time Series",
    "text": "Visualizing the Data as a Time Series\nWe can now explore the fossil fuel emission time series (January 1999 -December 2016) available for the Dallas, Texas area of the U.S. We can plot the data set using the code below:\n\n# Figure size: 20 representing the width, 10 representing the height\nfig = plt.figure(figsize=(20, 10))\n\nplt.plot(\n    df[\"datetime\"], # X-axis: sorted datetime\n    df[\"max\"], # Y-axis: maximum CH4 flux\n    color=\"red\", # Line color\n    linestyle=\"-\", # Line style\n    linewidth=0.5, # Line width\n    label=\"CH4 emissions\", # Legend label\n)\n\n# Display legend\nplt.legend()\n\n# Insert label for the X-axis\nplt.xlabel(\"Years\")\n\n# Insert label for the Y-axis\nplt.ylabel(\"g CH₄/m²/year\")\nplt.xticks(rotation = 90)\n\n# Insert title for the plot\nplt.title(\"CH4 emission Values for Texas, Dallas (1999-2016)\")\n\n# Add data citation\nplt.text(\n    df[\"datetime\"].iloc[0],           # X-coordinate of the text\n    df[\"max\"].min(),                  # Y-coordinate of the text\n\n\n\n\n    # Text to be displayed\n    \"Source: NASA/NOAA TM5-4DVar Isotopic CH₄ Inverse Fluxes\",                  \n    fontsize=12,                             # Font size\n    horizontalalignment=\"left\",              # Horizontal alignment\n    verticalalignment=\"top\",                 # Vertical alignment\n    color=\"blue\",                            # Text color\n)\n\n\n# Plot the time series\nplt.show()\n\n\n# Print the properties for the 3rd item in the collection\nprint(items[2][\"properties\"][\"start_datetime\"])\n\n\n# A GET request is made for the 3rd granule\nch4_flux_3 = requests.get(\n\n    # Pass the collection name, the item number in the list, and its ID\n    f\"{RASTER_API_URL}/stac/tilejson.json?collection={items[2]['collection']}&item={items[2]['id']}\"\n\n    # Pass the asset name\n    f\"&assets={asset_name}\"\n\n    # Pass the color formula and colormap for custom visualization\n    f\"&color_formula=gamma+r+1.05&colormap_name={color_map}\"\n\n    # Pass the minimum and maximum values for rescaling\n    f\"&rescale={rescale_values['min']},{rescale_values['max']}\",\n\n# Return the response in JSON format\n).json()\n\n# Print the properties of the retrieved granule to the console\nch4_flux_3\n\n\n# Create a new map to display the tile\naoi_map_bbox = Map(\n\n    # Base map is set to OpenStreetMap\n    tiles=\"OpenStreetMap\",\n\n    # Set the center of the map\n    location=[\n        30,-100\n    ],\n\n    # Set the zoom value\n    zoom_start=6.8,\n)\n\n# Define the map layer\nmap_layer = TileLayer(\n\n    # Path to retrieve the tile\n    tiles=ch4_flux_3[\"tiles\"][0],\n\n    # Set the attribution and adjust the transparency of the layer\n    attr=\"GHG\", opacity = 0.7\n)\n\n# Add the layer to the map\nmap_layer.add_to(aoi_map_bbox)\n\n# Visualize the map\naoi_map_bbox",
    "crumbs": [
      "Data Usage Notebooks",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "TM5-4DVar Isotopic CH₄ Inverse Fluxes"
    ]
  },
  {
    "objectID": "user_data_notebooks/tm54dvar-ch4flux-monthgrid-v1_User_Notebook.html#summary",
    "href": "user_data_notebooks/tm54dvar-ch4flux-monthgrid-v1_User_Notebook.html#summary",
    "title": "TM5-4DVar Isotopic CH₄ Inverse Fluxes",
    "section": "Summary",
    "text": "Summary\nIn this notebook we have successfully explored, analyzed, and visualized the STAC collection for TM5-4DVar Isotopic CH₄ Inverse Fluxes dataset.\n\nInstall and import the necessary libraries\nFetch the collection from STAC collections using the appropriate endpoints\nCount the number of existing granules within the collection\nMap and compare the CH₄ inverse fluxes for two distinctive months/years\nGenerate zonal statistics for the area of interest (AOI)\nVisualizing the Data as a Time Series\n\nIf you have any questions regarding this user notebook, please contact us using the feedback form.",
    "crumbs": [
      "Data Usage Notebooks",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "TM5-4DVar Isotopic CH₄ Inverse Fluxes"
    ]
  },
  {
    "objectID": "user_data_notebooks/micasa-carbonflux-daygrid-v1_User_Notebook.html",
    "href": "user_data_notebooks/micasa-carbonflux-daygrid-v1_User_Notebook.html",
    "title": "MiCASA Land Carbon Flux",
    "section": "",
    "text": "You can launch this notebook in the US GHG Center JupyterHub by clicking the link below.\nLaunch in the US GHG Center JupyterHub (requires access)",
    "crumbs": [
      "Data Usage Notebooks",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "MiCASA Land Carbon Flux"
    ]
  },
  {
    "objectID": "user_data_notebooks/micasa-carbonflux-daygrid-v1_User_Notebook.html#run-this-notebook",
    "href": "user_data_notebooks/micasa-carbonflux-daygrid-v1_User_Notebook.html#run-this-notebook",
    "title": "MiCASA Land Carbon Flux",
    "section": "",
    "text": "You can launch this notebook in the US GHG Center JupyterHub by clicking the link below.\nLaunch in the US GHG Center JupyterHub (requires access)",
    "crumbs": [
      "Data Usage Notebooks",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "MiCASA Land Carbon Flux"
    ]
  },
  {
    "objectID": "user_data_notebooks/micasa-carbonflux-daygrid-v1_User_Notebook.html#approach",
    "href": "user_data_notebooks/micasa-carbonflux-daygrid-v1_User_Notebook.html#approach",
    "title": "MiCASA Land Carbon Flux",
    "section": "Approach",
    "text": "Approach\n\nIdentify available dates and temporal frequency of observations for a given collection using the GHGC API /stac endpoint. The collection processed in this notebook is the Land-Atmosphere Carbon Flux data product\nPass the STAC item into the raster API /stac/tilejson.json endpoint\nUsing folium.plugins.DualMap, visualize two tiles (side-by-side), allowing time point comparison\nAfter the visualization, perform zonal statistics for a given polygon",
    "crumbs": [
      "Data Usage Notebooks",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "MiCASA Land Carbon Flux"
    ]
  },
  {
    "objectID": "user_data_notebooks/micasa-carbonflux-daygrid-v1_User_Notebook.html#about-the-data",
    "href": "user_data_notebooks/micasa-carbonflux-daygrid-v1_User_Notebook.html#about-the-data",
    "title": "MiCASA Land Carbon Flux",
    "section": "About the Data",
    "text": "About the Data\nThis dataset presents a variety of carbon flux parameters derived from the Más Informada Carnegie-Ames-Stanford-Approach (MiCASA) model. The model’s input data includes air temperature, precipitation, incident solar radiation, a soil classification map, and several satellite derived products. All model calculations are driven by analyzed meteorological data from NASA’s Modern-Era Retrospective analysis for Research and Application, Version 2 (MERRA-2). The resulting product provides global, daily data at 0.1 degree resolution from January 2001 through December 2023. It includes carbon flux variables expressed in units of kilograms of carbon per square meter per day (kg Carbon/m²/day) from net primary production (NPP), heterotrophic respiration (Rh), wildfire emissions (FIRE), fuel wood burning emissions (FUEL), net ecosystem exchange (NEE), and net biosphere exchange (NBE). The latter two are derived from the first four (see Scientific Details below). MiCASA is an extensive revision of the CASA – Global Fire Emissions Database, version 3 (CASA-GFED3) product. CASA-GFED3 and earlier versions of MERRA-driven CASA-GFED carbon fluxes have been used in several atmospheric carbon dioxide (CO₂) transport studies, serve as a community standard for priors of flux inversion systems, and through the support of NASA’s Carbon Monitoring System (CMS), help characterize, quantify, understand and predict the evolution of global carbon sources and sinks.\nFor more information regarding this dataset, please visit the U.S. Greenhouse Gas Center.",
    "crumbs": [
      "Data Usage Notebooks",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "MiCASA Land Carbon Flux"
    ]
  },
  {
    "objectID": "user_data_notebooks/micasa-carbonflux-daygrid-v1_User_Notebook.html#query-the-stac-api",
    "href": "user_data_notebooks/micasa-carbonflux-daygrid-v1_User_Notebook.html#query-the-stac-api",
    "title": "MiCASA Land Carbon Flux",
    "section": "Query the STAC API",
    "text": "Query the STAC API\nFirst, we are going to import the required libraries. Once imported, they allow better executing a query in the GHG Center Spatio Temporal Asset Catalog (STAC) Application Programming Interface (API) where the granules for this collection are stored.\n\n# Import the following libraries\nimport requests\nimport folium\nimport folium.plugins\nfrom folium import Map, TileLayer\nfrom pystac_client import Client\nimport branca\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n\n# Provide the STAC and RASTER API endpoints\n# The endpoint is referring to a location within the API that executes a request on a data collection nesting on the server.\n\n# The STAC API is a catalog of all the existing data collections that are stored in the GHG Center.\nSTAC_API_URL = \"https://earth.gov/ghgcenter/api/stac\"\n\n# The RASTER API is used to fetch collections for visualization\nRASTER_API_URL = \"https://earth.gov/ghgcenter/api/raster\"\n\n# The collection name is used to fetch the dataset from the STAC API. First, we define the collection name as a variable\n# Name of the collection for MiCASA Land Carbon Flux\ncollection_name = \"micasa-carbonflux-daygrid-v1\"\n\n# Next, we need to specify the asset name for this collection\n# The asset name is referring to the raster band containing the pixel values for the parameter of interest\n# For the case of the MiCASA Land Carbon Flux collection, the parameter of interest is “rh”\n# rh = Heterotrophic Respiration\nasset_name = \"rh\"\n\n\n# Fetch the collection from the STAC API using the appropriate endpoint\n# The 'requests' library allows a HTTP request possible\ncollection = requests.get(f\"{STAC_API_URL}/collections/{collection_name}\").json()\n\n# Print the properties of the collection to the console\ncollection\n\nExamining the contents of our collection under the temporal variable, we see that the data is available from January 2003 to December 2017. By looking at the dashboard:time density, we observe that the periodic frequency of these observations is monthly.\n\n# Create a function that would search for a data collection in the US GHG Center STAC API\n\n# First, we need to define the function\n# The name of the function = \"get_item_count\"\n# The argument that will be passed through the defined function = \"collection_id\"\ndef get_item_count(collection_id):\n   \n    # Set a counter for the number of items existing in the collection\n    count = 0\n\n    # Define the path to retrieve the granules (items) of the collection of interest (MiCASA Land Carbon Flux) in the STAC API\n    items_url = f\"{STAC_API_URL}/collections/{collection_id}/items\"\n\n    # Run a while loop to make HTTP requests until there are no more URLs associated with the collection in the STAC API\n    while True:\n\n        # Retrieve information about the granules by sending a \"get\" request to the STAC API using the defined collection path\n        response = requests.get(items_url)\n\n        # If the items do not exist, print an error message and quit the loop\n        if not response.ok:\n            print(\"error getting items\")\n            exit()\n\n        # Return the results of the HTTP response as JSON\n        stac = response.json()\n       \n        # Increase the \"count\" by the number of items (granules) returned in the response\n        count += int(stac[\"context\"].get(\"returned\", 0))\n\n        # Retrieve information about the next URL associated with the collection (MiCASA Land Carbon Flux) in the STAC API (if applicable)\n        next = [link for link in stac[\"links\"] if link[\"rel\"] == \"next\"]\n\n        # Exit the loop if there are no other URLs\n        if not next:\n            break\n       \n        # Ensure the information gathered by other STAC API links associated with the collection are added to the original path\n        # \"href\" is the identifier for each of the tiles stored in the STAC API\n        items_url = next[0][\"href\"]\n        # temp = items_url.split('/')\n        # temp.insert(3, 'ghgcenter')\n        # temp.insert(4, 'api')\n        # temp.insert(5, 'stac')\n        # items_url = '/'.join(temp)\n\n    # Return the information about the total number of granules found associated with the collection (MiCASA Land Carbon Flux)\n    return count\n\n\n# Apply the function created above \"get_item_count\" to the data collection\nnumber_of_items = get_item_count(collection_name)\n\n# Get the information about the number of granules found in the collection\nitems = requests.get(f\"{STAC_API_URL}/collections/{collection_name}/items?limit=800\").json()[\"features\"]\n\n# Print the total number of items (granules) found\nprint(f\"Found {len(items)} items\")\n\nFound 800 items\n\n\n\n# Examine the first item in the collection\n# Keep in mind that a list starts from 0, 1, 2... therefore items[0] is referring to the first item in the list/collection\nitems[0]",
    "crumbs": [
      "Data Usage Notebooks",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "MiCASA Land Carbon Flux"
    ]
  },
  {
    "objectID": "user_data_notebooks/micasa-carbonflux-daygrid-v1_User_Notebook.html#explore-changes-in-carbon-flux-levels-using-the-raster-api",
    "href": "user_data_notebooks/micasa-carbonflux-daygrid-v1_User_Notebook.html#explore-changes-in-carbon-flux-levels-using-the-raster-api",
    "title": "MiCASA Land Carbon Flux",
    "section": "Explore Changes in Carbon Flux Levels Using the Raster API",
    "text": "Explore Changes in Carbon Flux Levels Using the Raster API\nWe will explore changes in the land atmosphere Carbon flux Heterotrophic Respiration and examine their impacts over time. We’ll then visualize the outputs on a map using folium.\n\n# Now we create a dictionary where the start datetime values for each granule is queried more explicitly by year and month (e.g., 2020-02)\nitems = {item[\"properties\"][\"datetime\"][:10]: item for item in items}\n\nBelow, we are entering the minimum and maximum values to provide our upper and lower bounds in the rescale_values.\n\n# Fetch the minimum and maximum values for rescaling\nrescale_values = {\"max\":items[list(items.keys())[0]][\"assets\"][asset_name][\"raster:bands\"][0][\"histogram\"][\"max\"], \"min\":items[list(items.keys())[0]][\"assets\"][asset_name][\"raster:bands\"][0][\"histogram\"][\"min\"]}\n\nNow, we will pass the item id, collection name, asset name, and the rescaling factor to the Raster API endpoint. This step is done twice, once for December 2003 and again for December 2017, so that we can visualize each event independently.\n\n# Choose a color for displaying the tiles\n# Please refer to matplotlib library if you'd prefer choosing a different color ramp.\n# For more information on Colormaps in Matplotlib, please visit https://matplotlib.org/stable/users/explain/colors/colormaps.html\ncolor_map = \"purd\"\n\n# Make a GET request to retrieve information for the date mentioned below\ndate1 = '2023-01-01'\ndate1_tile = requests.get(\n\n    # Pass the collection name, collection date, and its ID\n    # To change the year, month and date of the observed parameter, you can modify the date mentioned above.\n    f\"{RASTER_API_URL}/stac/tilejson.json?collection={items[date1]['collection']}&item={items[date1]['id']}\"\n\n    # Pass the asset name\n    f\"&assets={asset_name}\"\n\n    # Pass the color formula and colormap for custom visualization\n    f\"&color_formula=gamma+r+1.05&colormap_name={color_map}\"\n\n    # Pass the minimum and maximum values for rescaling\n    f\"&rescale={rescale_values['min']},{rescale_values['max']}\",\n\n# Return response in JSON format\n).json()\n\n# Print the properties of the retrieved granule to the console\ndate1_tile\n\n{'tilejson': '2.2.0',\n 'version': '1.0.0',\n 'scheme': 'xyz',\n 'tiles': ['https://earth.gov/ghgcenter/api/raster/stac/tiles/WebMercatorQuad/{z}/{x}/{y}@1x?collection=micasa-carbonflux-daygrid-v1&item=micasa-carbonflux-daygrid-v1-20230101&assets=rh&color_formula=gamma+r+1.05&colormap_name=purd&rescale=-0.32319876551628113%2C5.9415082931518555'],\n 'minzoom': 0,\n 'maxzoom': 24,\n 'bounds': [-180.0, -90.0, 179.99999999999994, 90.0],\n 'center': [-2.842170943040401e-14, 0.0, 0]}\n\n\n\n# Make a GET request to retrieve information for the date mentioned below\ndate2 = '2023-01-31'\ndate2_tile = requests.get(\n\n    # Pass the collection name, collection date, and its ID\n    # To change the year, month and date of the observed parameter, you can modify the date mentioned above.\n    f\"{RASTER_API_URL}/stac/tilejson.json?collection={items[date2]['collection']}&item={items[date2]['id']}\"\n\n    # Pass the asset name\n    f\"&assets={asset_name}\"\n\n    # Pass the color formula and colormap for custom visualization\n    f\"&color_formula=gamma+r+1.05&colormap_name={color_map}\"\n\n    # Pass the minimum and maximum values for rescaling\n    f\"&rescale={rescale_values['min']},{rescale_values['max']}\", \n\n# Return response in JSON format\n).json()\n\n# Print the properties of the retrieved granule to the console\ndate2_tile\n\n{'tilejson': '2.2.0',\n 'version': '1.0.0',\n 'scheme': 'xyz',\n 'tiles': ['https://earth.gov/ghgcenter/api/raster/stac/tiles/WebMercatorQuad/{z}/{x}/{y}@1x?collection=micasa-carbonflux-daygrid-v1&item=micasa-carbonflux-daygrid-v1-20230131&assets=rh&color_formula=gamma+r+1.05&colormap_name=purd&rescale=-0.32319876551628113%2C5.9415082931518555'],\n 'minzoom': 0,\n 'maxzoom': 24,\n 'bounds': [-180.0, -90.0, 179.99999999999994, 90.0],\n 'center': [-2.842170943040401e-14, 0.0, 0]}",
    "crumbs": [
      "Data Usage Notebooks",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "MiCASA Land Carbon Flux"
    ]
  },
  {
    "objectID": "user_data_notebooks/micasa-carbonflux-daygrid-v1_User_Notebook.html#visualize-land-atmosphere-carbon-flux-heterotrophic-respiration",
    "href": "user_data_notebooks/micasa-carbonflux-daygrid-v1_User_Notebook.html#visualize-land-atmosphere-carbon-flux-heterotrophic-respiration",
    "title": "MiCASA Land Carbon Flux",
    "section": "Visualize Land-Atmosphere Carbon Flux (Heterotrophic Respiration)",
    "text": "Visualize Land-Atmosphere Carbon Flux (Heterotrophic Respiration)\n\n# For this study we are going to compare the Rh level for date1 and date2 over the State of Texas \n# To change the location, you can simply insert the latitude and longitude of the area of your interest in the \"location=(LAT, LONG)\" statement\n# For example, you can change the current statement \"location=(31.9, -99.9)\" to \"location=(34, -118)\" to monitor the Rh level in California instead of Texas\n\n# Set initial zoom and center of map for CO₂ Layer\n# 'folium.plugins' allows mapping side-by-side\nmap_ = folium.plugins.DualMap(location=(31.9, -99.9), zoom_start=6)\n\n\n# Define the first map layer with Rh level for the tile fetched for date 1\n# The TileLayer library helps in manipulating and displaying raster layers on a map\nmap_layer_date1 = TileLayer(\n    tiles=date1_tile[\"tiles\"][0], # Path to retrieve the tile\n    attr=\"GHG\", # Set the attribution\n    opacity=0.8, # Adjust the transparency of the layer\n    name=f\"{date1} Rh Level\", # Title for the layer\n    overlay= True, # The layer can be overlaid on the map\n    legendEnabled = True # Enable displaying the legend on the map\n)\n\n# Add the first layer to the Dual Map\nmap_layer_date1.add_to(map_.m1)\n\n\n# Define the first map layer with Rh level for the tile fetched for date 2\nmap_layer_date2 = TileLayer(\n    tiles=date2_tile[\"tiles\"][0], # Path to retrieve the tile\n    attr=\"GHG\", # Set the attribution\n    opacity=0.8, # Adjust the transparency of the layer\n    name=f\"{date2} RH Level\", # Title for the layer\n    overlay= True, # The layer can be overlaid on the map\n    legendEnabled = True # Enable displaying the legend on the map\n)\n\n# Add the second layer to the Dual Map\nmap_layer_date2.add_to(map_.m2)\n\n# Display data markers (titles) on both maps\nfolium.Marker((40, 5.0), tooltip=\"both\").add_to(map_)\n\n# Add a layer control to switch between map layers\nfolium.LayerControl(collapsed=False).add_to(map_)\n\n# Add a legend to the dual map using the 'branca' library. \n# Note: the inserted legend is representing the minimum and maximum values for both tiles.\ncolormap = branca.colormap.linear.PuRd_09.scale(0, 0.3) # minimum value = 0, maximum value = 0.3 (kg Carbon/m2/daily)\n\n# Classify the colormap according to specified Rh values \ncolormap = colormap.to_step(index=[0, 0.07, 0.15, 0.22, 0.3])\n\n# Add the data unit as caption\ncolormap.caption = 'Rh Values (gm Carbon/m2/daily)'\n\n# Display the legend and caption on the map\ncolormap.add_to(map_.m1)\n\n# Visualize the Dual Map\nmap_\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook",
    "crumbs": [
      "Data Usage Notebooks",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "MiCASA Land Carbon Flux"
    ]
  },
  {
    "objectID": "user_data_notebooks/micasa-carbonflux-daygrid-v1_User_Notebook.html#generate-the-statistics-for-the-aoi",
    "href": "user_data_notebooks/micasa-carbonflux-daygrid-v1_User_Notebook.html#generate-the-statistics-for-the-aoi",
    "title": "MiCASA Land Carbon Flux",
    "section": "Generate the statistics for the AOI",
    "text": "Generate the statistics for the AOI\n\n%%time\n# %%time = Wall time (execution time) for running the code below\n\n# Generate statistics using the created function \"generate_stats\" within the bounding box defined by the \"texas_dallas_aoi\" polygon\nstats = [generate_stats(item, texas_dallas_aoi) for item in items]\n\n\n# Print the stats for the first item in the collection\nstats[0]\n\n{'statistics': {'b1': {'min': 0.11864250898361206,\n   'max': 1.3311004638671875,\n   'mean': 0.7455709838867187,\n   'count': 150.0,\n   'sum': 111.83564758300781,\n   'std': 0.2550486573615515,\n   'median': 0.7395486831665039,\n   'majority': 0.11864250898361206,\n   'minority': 0.11864250898361206,\n   'unique': 150.0,\n   'histogram': [[3.0, 4.0, 17.0, 22.0, 24.0, 29.0, 20.0, 18.0, 7.0, 6.0],\n    [0.11864250898361206,\n     0.23988831043243408,\n     0.3611341118812561,\n     0.48237988352775574,\n     0.6036257147789001,\n     0.7248715162277222,\n     0.8461172580718994,\n     0.9673630595207214,\n     1.0886088609695435,\n     1.2098547220230103,\n     1.3311004638671875]],\n   'valid_percent': 100.0,\n   'masked_pixels': 0.0,\n   'valid_pixels': 150.0,\n   'percentile_2': 0.24085583359003068,\n   'percentile_98': 1.2310137295722965}},\n 'datetime': '2023-12-31'}\n\n\nCreate a function that goes through every single item in the collection and populates their properties - including the minimum, maximum, and sum of their values - in a table.\n\n# Create a function that converts statistics in JSON format into a pandas DataFrame\ndef clean_stats(stats_json) -&gt; pd.DataFrame:\n\n    # Normalize the JSON data\n    df = pd.json_normalize(stats_json)\n\n    # Replace the naming \"statistics.b1\" in the columns\n    df.columns = [col.replace(\"statistics.b1.\", \"\") for col in df.columns]\n\n    # Set the datetime format\n    df[\"date\"] = pd.to_datetime(df[\"datetime\"])\n\n    # Return the cleaned format\n    return df\n\n# Apply the generated function on the stats data\ndf = clean_stats(stats)\n\n# Display the stats for the first 5 granules in the collection in the table\n# Change the value in the parenthesis to show more or a smaller number of rows in the table\ndf.head(5)\n\n\n\n\n\n\n\n\ndatetime\nmin\nmax\nmean\ncount\nsum\nstd\nmedian\nmajority\nminority\nunique\nhistogram\nvalid_percent\nmasked_pixels\nvalid_pixels\npercentile_2\npercentile_98\ndate\n\n\n\n\n0\n2023-12-31\n0.118643\n1.331100\n0.745571\n150.0\n111.835648\n0.255049\n0.739549\n0.118643\n0.118643\n150.0\n[[3.0, 4.0, 17.0, 22.0, 24.0, 29.0, 20.0, 18.0...\n100.0\n0.0\n150.0\n0.240856\n1.231014\n2023-12-31\n\n\n1\n2023-12-30\n0.118560\n1.329713\n0.744604\n150.0\n111.690636\n0.254805\n0.738541\n0.118560\n0.118560\n150.0\n[[3.0, 4.0, 17.0, 22.0, 24.0, 29.0, 20.0, 18.0...\n100.0\n0.0\n150.0\n0.240662\n1.229237\n2023-12-30\n\n\n2\n2023-12-29\n0.118470\n1.328249\n0.743593\n150.0\n111.538979\n0.254547\n0.737490\n0.118470\n0.118470\n150.0\n[[3.0, 4.0, 17.0, 22.0, 24.0, 29.0, 20.0, 19.0...\n100.0\n0.0\n150.0\n0.240456\n1.227379\n2023-12-29\n\n\n3\n2023-12-28\n0.118373\n1.326706\n0.742537\n150.0\n111.380539\n0.254277\n0.736633\n0.118373\n0.118373\n150.0\n[[3.0, 4.0, 17.0, 23.0, 23.0, 29.0, 20.0, 19.0...\n100.0\n0.0\n150.0\n0.240238\n1.225439\n2023-12-28\n\n\n4\n2023-12-27\n0.118268\n1.325084\n0.741434\n150.0\n111.215126\n0.253992\n0.735755\n0.118268\n0.118268\n150.0\n[[3.0, 4.0, 17.0, 23.0, 23.0, 29.0, 20.0, 19.0...\n100.0\n0.0\n150.0\n0.240007\n1.223415\n2023-12-27",
    "crumbs": [
      "Data Usage Notebooks",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "MiCASA Land Carbon Flux"
    ]
  },
  {
    "objectID": "user_data_notebooks/micasa-carbonflux-daygrid-v1_User_Notebook.html#visualize-the-data-as-a-time-series",
    "href": "user_data_notebooks/micasa-carbonflux-daygrid-v1_User_Notebook.html#visualize-the-data-as-a-time-series",
    "title": "MiCASA Land Carbon Flux",
    "section": "Visualize the Data as a Time Series",
    "text": "Visualize the Data as a Time Series\nWe can now explore the Heterotrophic Respiration time series (October 2021 - January 2024) available for the Dallas, Texas area. We can plot the data set using the code below:\n\n# Determine the width and height of the plot using the 'matplotlib' library\n# Figure size: 20 representing the width, 10 representing the height\nfig = plt.figure(figsize=(20, 10)) \n\n# Plot the time series analysis of the daily Heterotrophic Respiration changes in Dallas, Texas\nplt.plot(\n    df[\"date\"], # X-axis: date\n    df[\"max\"], # Y-axis: Rh value\n    color=\"purple\", # Line color\n    linestyle=\"-\", # Line style\n    linewidth=0.5, # Line width\n    label=\"RH Level\", # Legend label\n)\n\n# Display legend\nplt.legend()\n\n# Insert label for the X-axis\nplt.xlabel(\"Years\")\n\n# Insert label for the Y-axis\nplt.ylabel(\"gm Carbon/m2/day\")\n\n# Insert title for the plot\nplt.title(\"Heterotrophic Respiration Values for Dallas, Texas (October 2021 to January 2024)\")\n\nText(0.5, 1.0, 'Heterotrophic Respiration Values for Dallas, Texas (October 2021 to January 2024)')\n\n\n\n\n\n\n\n\n\nTo take a closer look at the daily Heterotrophic Respiration variability across this region, we are going to retrieve and display data collected during the December, 2023 observation.\n\n# Fetch the third item in the list as the observation item.\n# Considering that a list starts with \"0\", we need to insert \"2\" in the \"items[2]\" statement\n# Print the start Date Time of the third granule in the collection\nprint(items[2][\"properties\"][\"datetime\"]) \n\n2023-12-29T00:00:00+00:00\n\n\n\n# A GET request is made for the observed tile\nobserved_tile = requests.get(\n\n    # Pass the collection name, the item number in the list, and its ID\n    f\"{RASTER_API_URL}/stac/tilejson.json?collection={items[2]['collection']}&item={items[2]['id']}\"\n\n    # Pass the asset name\n    f\"&assets={asset_name}\"\n\n    # Pass the color formula and colormap for custom visualization\n    f\"&color_formula=gamma+r+1.05&colormap_name={color_map}\"\n\n    # Pass the minimum and maximum values for rescaling\n    f\"&rescale={rescale_values['min']},{rescale_values['max']}\",\n\n# Return the response in JSON format\n).json()\n\n# Print the properties of the retrieved granule to the console \nobserved_tile\n\n{'tilejson': '2.2.0',\n 'version': '1.0.0',\n 'scheme': 'xyz',\n 'tiles': ['https://earth.gov/ghgcenter/api/raster/stac/tiles/WebMercatorQuad/{z}/{x}/{y}@1x?collection=micasa-carbonflux-daygrid-v1&item=micasa-carbonflux-daygrid-v1-20231229&assets=rh&color_formula=gamma+r+1.05&colormap_name=purd&rescale=-0.32319876551628113%2C5.9415082931518555'],\n 'minzoom': 0,\n 'maxzoom': 24,\n 'bounds': [-180.0, -90.0, 179.99999999999994, 90.0],\n 'center': [-2.842170943040401e-14, 0.0, 0]}\n\n\n\n# Create a new map to display the Rh level for the Dallas, Texas area for the observed tile timeframe.\naoi_map_bbox = Map(\n\n    # Base map is set to OpenStreetMap\n    tiles=\"OpenStreetMap\",\n\n    # Set the center of the map\n    location=[\n        32.8, # latitude\n        -96.79, # longitude\n    ],\n\n    # Set the zoom value\n    zoom_start=9,\n)\n\n# Define the map layer with the Rh level for observed tile\nmap_layer = TileLayer(\n    tiles=observed_tile[\"tiles\"][0], # Path to retrieve the tile\n\n    # Set the attribution, transparency, and the title along with enabling the visualization of the legend on the map \n    attr=\"GHG\", opacity = 0.7, name=\" Observed tile RH Level\", overlay= True, legendEnabled = True\n)\n\n# Add the layer to the map\nmap_layer.add_to(aoi_map_bbox)\n\n# Display data marker (title) on the map\nfolium.Marker((40, 5.9), tooltip=\"both\").add_to(aoi_map_bbox)\n\n# Add a layer control\nfolium.LayerControl(collapsed=False).add_to(aoi_map_bbox)\n\n# Add a legend using the 'branca' library\ncolormap = branca.colormap.linear.PuRd_09.scale(0, 0.3) # minimum value = 0, maximum value = 0.3 (gm Carbon/m2/daily)\n\n# Classify the colormap according to the specified Rh values\ncolormap = colormap.to_step(index=[0, 0.07, 0.15, 0.22, 0.3])\n\n# Add the data unit as caption\ncolormap.caption = 'Rh Values (gm Carbon/m2/daily)'\n\n# Display the legend and caption on the map\ncolormap.add_to(aoi_map_bbox)\n\n# Visualize the map\naoi_map_bbox\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook",
    "crumbs": [
      "Data Usage Notebooks",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "MiCASA Land Carbon Flux"
    ]
  },
  {
    "objectID": "user_data_notebooks/micasa-carbonflux-daygrid-v1_User_Notebook.html#summary",
    "href": "user_data_notebooks/micasa-carbonflux-daygrid-v1_User_Notebook.html#summary",
    "title": "MiCASA Land Carbon Flux",
    "section": "Summary",
    "text": "Summary\nIn this notebook we have successfully completed the following steps for the STAC collection for MiCASA Land Carbon Flux data: 1. Install and import the necessary libraries 2. Fetch the collection from STAC collections using the appropriate endpoints 3. Count the number of existing granules within the collection 4. Map and compare the Heterotrophic Respiration (Rh) levels over the Dallas, Texas area for two distinctive years 5. Create a table that displays the minimum, maximum, and sum of the Rh values for a specified region 6. Generate a time-series graph of the Rh values for a specified region\nIf you have any questions regarding this user notebook, please contact us using the feedback form.",
    "crumbs": [
      "Data Usage Notebooks",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "MiCASA Land Carbon Flux"
    ]
  },
  {
    "objectID": "user_data_notebooks/eccodarwin-co2flux-monthgrid-v5_User_Notebook.html",
    "href": "user_data_notebooks/eccodarwin-co2flux-monthgrid-v5_User_Notebook.html",
    "title": "Air-Sea CO₂ Flux, ECCO-Darwin Model v5",
    "section": "",
    "text": "You can launch this notebook in the US GHG Center JupyterHub by clicking the link below.\nLaunch in the US GHG Center JupyterHub (requires access)",
    "crumbs": [
      "Data Usage Notebooks",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "Air-Sea CO₂ Flux, ECCO-Darwin Model v5"
    ]
  },
  {
    "objectID": "user_data_notebooks/eccodarwin-co2flux-monthgrid-v5_User_Notebook.html#run-this-notebook",
    "href": "user_data_notebooks/eccodarwin-co2flux-monthgrid-v5_User_Notebook.html#run-this-notebook",
    "title": "Air-Sea CO₂ Flux, ECCO-Darwin Model v5",
    "section": "",
    "text": "You can launch this notebook in the US GHG Center JupyterHub by clicking the link below.\nLaunch in the US GHG Center JupyterHub (requires access)",
    "crumbs": [
      "Data Usage Notebooks",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "Air-Sea CO₂ Flux, ECCO-Darwin Model v5"
    ]
  },
  {
    "objectID": "user_data_notebooks/eccodarwin-co2flux-monthgrid-v5_User_Notebook.html#approach",
    "href": "user_data_notebooks/eccodarwin-co2flux-monthgrid-v5_User_Notebook.html#approach",
    "title": "Air-Sea CO₂ Flux, ECCO-Darwin Model v5",
    "section": "Approach",
    "text": "Approach\n\nIdentify available dates and temporal frequency of observations for the given collection using the GHGC API /stac endpoint. The collection processed in this notebook is the Air-Sea CO₂ Flux, ECCO-Darwin Model v5 Data product.\nPass the STAC item into the raster API /stac/tilejson.json endpoint.\nUsing folium.plugins.DualMap, we will visualize two tiles (side-by-side), allowing us to compare time points.\nAfter the visualization, we will perform zonal statistics for a given polygon.",
    "crumbs": [
      "Data Usage Notebooks",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "Air-Sea CO₂ Flux, ECCO-Darwin Model v5"
    ]
  },
  {
    "objectID": "user_data_notebooks/eccodarwin-co2flux-monthgrid-v5_User_Notebook.html#about-the-data",
    "href": "user_data_notebooks/eccodarwin-co2flux-monthgrid-v5_User_Notebook.html#about-the-data",
    "title": "Air-Sea CO₂ Flux, ECCO-Darwin Model v5",
    "section": "About the Data",
    "text": "About the Data\nThe ocean is a major sink for atmospheric carbon dioxide (CO2), largely due to the presence of phytoplankton that use the CO₂ to grow. Studies have shown that global ocean CO₂ uptake has increased over recent decades, however there is uncertainty in the various mechanisms that affect ocean CO₂ flux and storage and how the ocean carbon sink will respond to future climate change. Because CO₂ fluxes can vary significantly across space and time, combined with deficiencies in ocean and atmosphere CO₂ observations, there is a need for models that can thoroughly represent these processes. Ocean biogeochemical models (OBMs) have the ability to resolve the physical and biogeochemical mechanisms contributing to spatial and temporal variations in air-sea CO₂ fluxes but previous OBMs do not integrate observations to improve model accuracy and have not been able to operate on the seasonal and multi-decadal timescales needed to adequately characterize these processes. The ECCO-Darwin model is an OBM that assimilates Estimating the Circulation and Climate of the Ocean (ECCO) consortium ocean circulation estimates and biogeochemical processes from the Massachusetts Institute of Technology (MIT) Darwin Project. A pilot study using ECCO-Darwin was completed by Brix et al. (2015) however an improved version of the model was developed by Carroll et al. (2020) in which issues present in the first model were addressed using data assimilation and adjustments were made to initial conditions and biogeochemical parameters. The updated ECCO-Darwin model was compared with interpolation-based products to estimate surface ocean partial pressure (pCO2) and air-sea CO₂ flux. This dataset contains the gridded global, monthly mean air-sea CO₂ fluxes from version 5 of the ECCO-Darwin model. The data are available at ~1/3° horizontal resolution at the equator (~18 km at high latitudes) from January 2020 through December 2022.\nFor more information regarding this dataset, please visit the Air-Sea CO₂ Flux ECCO-Darwin Model data overview page.",
    "crumbs": [
      "Data Usage Notebooks",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "Air-Sea CO₂ Flux, ECCO-Darwin Model v5"
    ]
  },
  {
    "objectID": "user_data_notebooks/eccodarwin-co2flux-monthgrid-v5_User_Notebook.html#querying-the-stac-api",
    "href": "user_data_notebooks/eccodarwin-co2flux-monthgrid-v5_User_Notebook.html#querying-the-stac-api",
    "title": "Air-Sea CO₂ Flux, ECCO-Darwin Model v5",
    "section": "Querying the STAC API",
    "text": "Querying the STAC API\nFirst, we are going to import the required libraries. Once imported, they allow better executing a query in the GHG Center Spatio Temporal Asset Catalog (STAC) Application Programming Interface (API) where the granules for this collection are stored.\n\n# Import the following libraries\nimport requests\nimport folium\nimport folium.plugins\nfrom folium import Map, TileLayer \nfrom pystac_client import Client \nimport branca \nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n\n# Provide the STAC and RASTER API endpoints\n# The endpoint is referring to a location within the API that executes a request on a data collection nesting on the server.\n\n# The STAC API is a catalog of all the existing data collections that are stored in the GHG Center.\nSTAC_API_URL = \"https://earth.gov/ghgcenter/api/stac\"\n\n# The RASTER API is used to fetch collections for visualization\nRASTER_API_URL = \"https://earth.gov/ghgcenter/api/raster\"\n\n# The collection name is used to fetch the dataset from the STAC API. First, we define the collection name as a variable \n# Name of the collection for ECCO Darwin CO₂ flux monthly emissions\ncollection_name = \"eccodarwin-co2flux-monthgrid-v5\"\n\n\n# Fetch the collection from the STAC API using the appropriate endpoint\n# The 'requests' library allows a HTTP request possible\ncollection = requests.get(f\"{STAC_API_URL}/collections/{collection_name}\").json()\n\n# Print the properties of the collection to the console\ncollection\n\nExamining the contents of our collection under the temporal variable, we see that the data is available from January 2020 to December 2022. By looking at the dashboard:time density, we observe that the data is periodic with monthly time density.\n\n# Create a function that would search for a data collection in the US GHG Center STAC API\n\n# First, we need to define the function\n# The name of the function is \"get_item_count\" \n# The argument that will be passed to the defined function is \"collection_id\"\ndef get_item_count(collection_id):\n\n    # Set a counter for the number of items existing in the collection \n    count = 0 \n\n    # Define the path to retrieve the granules (items) of the collection of interest (Air-Sea CO2 Flux ECCO-Darwin model) in the STAC API\n    items_url = f\"{STAC_API_URL}/collections/{collection_id}/items\" \n\n    # Run a while loop to make HTTP requests until there are no more URLs associated with the collection (Air-Sea CO2 Flux ECCO-Darwin model) in the STAC API\n    while True:\n\n        # Retrieve information about the granules by sending a \"get\" request to the STAC API using the defined collection path \n        response = requests.get(items_url) \n\n        # If the items do not exist, print an error message and quit the loop\n        if not response.ok:\n            print(\"error getting items\")\n            exit()\n\n        # Return the results of the HTTP response as JSON\n        stac = response.json()\n\n        # Increase the \"count\" by the number of items (granules) returned in the response\n        count += int(stac[\"context\"].get(\"returned\", 0))\n\n        # Retrieve information about the next URL associated with the collection (Air-Sea CO2 Flux ECCO-Darwin model) in the STAC API (if applicable)\n        next = [link for link in stac[\"links\"] if link[\"rel\"] == \"next\"]\n\n        # Exit the loop if there are no other URLs\n        if not next:\n            break\n        \n        # Ensure the information gathered by other STAC API links associated with the collection are added to the original path\n        # \"href\" is the identifier for each of the tiles stored in the STAC API\n        items_url = next[0][\"href\"]\n\n    # Return the information about the total number of granules found associated with the collection (Air-Sea CO2 Flux ECCO-Darwin model)\n    return count\n\n\n# Apply the function created above \"get_item_count\" to the Air-Sea CO2 Flux ECCO-Darwin collection\nnumber_of_items = get_item_count(collection_name)\n\n# Get the information about the number of granules found in the collection\nitems = requests.get(f\"{STAC_API_URL}/collections/{collection_name}/items?limit={number_of_items}\").json()[\"features\"]\n\n# Print the total number of items (granules) found\nprint(f\"Found {len(items)} items\")\n\n\n# Examine the first item in the collection\n# Keep in mind that a list starts from 0, 1, 2... therefore items[0] is referring to the first item in the list/collection\nitems[0]",
    "crumbs": [
      "Data Usage Notebooks",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "Air-Sea CO₂ Flux, ECCO-Darwin Model v5"
    ]
  },
  {
    "objectID": "user_data_notebooks/eccodarwin-co2flux-monthgrid-v5_User_Notebook.html#exploring-changes-in-co₂-levels-using-the-raster-api",
    "href": "user_data_notebooks/eccodarwin-co2flux-monthgrid-v5_User_Notebook.html#exploring-changes-in-co₂-levels-using-the-raster-api",
    "title": "Air-Sea CO₂ Flux, ECCO-Darwin Model v5",
    "section": "Exploring Changes in CO₂ Levels Using the Raster API",
    "text": "Exploring Changes in CO₂ Levels Using the Raster API\nIn this notebook, we will explore the global changes of CO₂ flux over time in urban regions. We will visualize the outputs on a map using folium.\n\n# Now we create a dictionary where the start datetime values for each granule is queried more explicitly by year and month (e.g., 2020-02)\nitems = {item[\"properties\"][\"start_datetime\"]: item for item in items}\n\n# Next, we need to specify the asset name for this collection.\n# The asset name is referring to the raster band containing the pixel values for the parameter of interest.\n# For the case of the Air-Sea CO2 Flux ECCO-Darwin collection, the parameter of interest is “co2”.\nasset_name = \"co2\"\n\nBelow, we are entering the minimum and maximum values to provide our upper and lower bounds in the rescale_values.\n\n# Fetch the minimum and maximum values for the CO2 value range\nrescale_values = {\"max\":0.0007, \"min\":-0.0007}\n\nNow, we will pass the item id, collection name, asset name, and the rescaling factor to the Raster API endpoint. This step is done twice so that we can visualize two arbitrary events independently.\n\n# Choose a color map for displaying the first observation (event)\n# Please refer to matplotlib library if you'd prefer choosing a different color ramp.\n# For more information on Colormaps in Matplotlib, please visit https://matplotlib.org/stable/users/explain/colors/colormaps.html\ncolor_map = \"magma\"\n\n# Make a GET request to retrieve information for the December 2022 tile which is the 1st item in the collection\n# To retrieve the first item in the collection we use \"0\" in the \"(items.keys())[0]\" statement \n# If you want to select another item (granule) in the list (collection), you can refer to the Data Browser in the U.S. Greenhouse Gas Center website  \n# URL to the Air-Sea CO2 Flux ECCO-Darwin collection in the US GHG Center: https://dljsq618eotzp.cloudfront.net/browseui/#eccodarwin-co2flux-monthgrid-v5/\n\n# A GET request is made for the December 2022 tile\ndecember_2022_tile = requests.get(\n\n    # Pass the collection name, the item number in the list, and its ID\n    f\"{RASTER_API_URL}/stac/tilejson.json?collection={items[list(items.keys())[0]]['collection']}&item={items[list(items.keys())[0]]['id']}\"\n\n    # Pass the asset name\n    f\"&assets={asset_name}\"\n\n    # Pass the color formula and colormap for custom visualization\n    f\"&color_formula=gamma+r+1.05&colormap_name={color_map}\"\n\n    # Pass the minimum and maximum values for rescaling \n    f\"&rescale={rescale_values['min']},{rescale_values['max']}\", \n\n# Return the response in JSON format\n).json()\n\n# Print the properties of the retrieved granule to the console\ndecember_2022_tile\n\n\n# Make a GET request to retrieve information for the April 2021 tile which is the 21th item in the collection\n# To retrieve the 21st item in the collection we use \"20\" in the \"(items.keys())[20]\" statement \n# Keep in mind that a list starts from 0, therefore \"items[20]\" is referring to the 21st item in the list/collection\n\n# A GET request is made for the April 2021 tile\napril_2021_tile = requests.get(\n\n    # Pass the collection name, the item number in the list, and its ID\n    f\"{RASTER_API_URL}/stac/tilejson.json?collection={items[list(items.keys())[20]]['collection']}&item={items[list(items.keys())[20]]['id']}\"\n\n    # Pass the asset name\n    f\"&assets={asset_name}\"\n\n    # Pass the color formula and colormap for custom visualization\n    f\"&color_formula=gamma+r+1.05&colormap_name={color_map}\"\n\n    # Pass the minimum and maximum values for rescaling\n    f\"&rescale={rescale_values['min']},{rescale_values['max']}\", \n\n# Return the response in JSON format\n).json()\n\n# Print the properties of the retrieved granule to the console\napril_2021_tile",
    "crumbs": [
      "Data Usage Notebooks",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "Air-Sea CO₂ Flux, ECCO-Darwin Model v5"
    ]
  },
  {
    "objectID": "user_data_notebooks/eccodarwin-co2flux-monthgrid-v5_User_Notebook.html#visualizing-co₂-flux-emissions",
    "href": "user_data_notebooks/eccodarwin-co2flux-monthgrid-v5_User_Notebook.html#visualizing-co₂-flux-emissions",
    "title": "Air-Sea CO₂ Flux, ECCO-Darwin Model v5",
    "section": "Visualizing CO₂ flux Emissions",
    "text": "Visualizing CO₂ flux Emissions\n\n# For this study we are going to compare the CO2 level in 2021 and 2022 along the coast of California\n# To change the location, you can simply insert the latitude and longitude of the area of your interest in the \"location=(LAT, LONG)\" statement\n\n# Set the initial zoom level and center of map for both tiles\n# 'folium.plugins' allows mapping side-by-side\nmap_ = folium.plugins.DualMap(location=(34, -118), zoom_start=6)\n\n\n# Define the first map layer with the CO2 Flux data for December 2022\nmap_layer_1 = TileLayer(\n    tiles=december_2022_tile[\"tiles\"][0], # Path to retrieve the tile\n    attr=\"GHG\", # Set the attribution \n    name='December 2022 CO2 Flux', # Title for the layer\n    overlay=True, # The layer can be overlaid on the map\n    opacity=0.8, # Adjust the transparency of the layer\n)\n# Add the first layer to the Dual Map \nmap_layer_1.add_to(map_.m1)\n\n\n# Define the second map layer with the CO2 Flux data for April 2021\nmap_layer_2 = TileLayer(\n    tiles=april_2021_tile[\"tiles\"][0], # Path to retrieve the tile\n    attr=\"GHG\", # Set the attribution \n    name='April 2021 CO2 Flux', # Title for the layer\n    overlay=True, # The layer can be overlaid on the map\n    opacity=0.8, # Adjust the transparency of the layer\n)\n# Add the second layer to the Dual Map \nmap_layer_2.add_to(map_.m2)\n\n\n# Display data markers (titles) on both maps\nfolium.Marker((40, 5.0), tooltip=\"both\").add_to(map_)\n\n# Add a layer control to switch between map layers\nfolium.LayerControl(collapsed=False).add_to(map_)\n\n# Add a legend to the dual map using the 'branca' library\n# Note: the inserted legend is representing the minimum and maximum values for both tiles\n# Minimum value = -0.0007, maximum value = 0.0007\ncolormap = branca.colormap.LinearColormap(colors=[\"#0000FF\", \"#3399FF\", \"#66CCFF\", \"#FFFFFF\", \"#FF66CC\", \"#FF3399\", \"#FF0000\"], vmin=-0.0007, vmax=0.0007) \n\n# Add the data unit as caption \ncolormap.caption = 'Millimoles per meter squared per second (mmol m²/s)'\n\n# Define custom tick values for the legend bar\ntick_val = [-0.0007, -0.00035, 0, 0.00035, 0.0007]\n\n# Create a HTML representation\nlegend_html = colormap._repr_html_()\n\n# Create a customized HTML structure for the legend\nlegend_html = f'''\n&lt;div style=\"position: fixed; bottom: 50px; left: 50px; z-index: 1000; width: 400px; height: auto; background-color: rgba(255, 255, 255, 0.8);\n             border-radius: 5px; border: 1px solid grey; padding: 10px; font-size: 14px; color: black;\"&gt;\n    &lt;b&gt;{colormap.caption}&lt;/b&gt;&lt;br&gt;\n    &lt;div style=\"display: flex; justify-content: space-between;\"&gt;\n        &lt;div&gt;{tick_val[0]}&lt;/div&gt; \n        &lt;div&gt;{tick_val[1]}&lt;/div&gt; \n        &lt;div&gt;{tick_val[2]}&lt;/div&gt; \n        &lt;div&gt;{tick_val[3]}&lt;/div&gt; \n        &lt;div&gt;{tick_val[4]}&lt;/div&gt; \n    &lt;/div&gt;\n    &lt;div style=\"background: linear-gradient(to right,\n                {'#0000FF'}, {'#3399FF'} {20}%,\n                {'#3399FF'} {20}%, {'#66CCFF'} {40}%,\n                {'#66CCFF'} {40}%, {'#FFFFFF'} {50}%,\n                {'#FFFFFF'} {50}%, {'#FF66CC'} {80}%,\n                {'#FF66CC'} {80}%, {'#FF3399'}); height: 10px;\"&gt;&lt;/div&gt;\n&lt;/div&gt;\n'''\n\n# Display the legend and caption on the map\nmap_.get_root().html.add_child(folium.Element(legend_html))\n\n# Visualize the Dual Map\nmap_",
    "crumbs": [
      "Data Usage Notebooks",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "Air-Sea CO₂ Flux, ECCO-Darwin Model v5"
    ]
  },
  {
    "objectID": "user_data_notebooks/eccodarwin-co2flux-monthgrid-v5_User_Notebook.html#visualizing-the-data-as-a-time-series",
    "href": "user_data_notebooks/eccodarwin-co2flux-monthgrid-v5_User_Notebook.html#visualizing-the-data-as-a-time-series",
    "title": "Air-Sea CO₂ Flux, ECCO-Darwin Model v5",
    "section": "Visualizing the Data as a Time Series",
    "text": "Visualizing the Data as a Time Series\nWe can now explore the fossil fuel emission time series (January 2020 -December 2022) available for the Coastal California area of the U.S. We can plot the data set using the code below:\n\n# Sort the DataFrame by the datetime column so the plot is displaying the values from left to right (2020 -&gt; 2022)\ndf_sorted = df.sort_values(by=\"datetime\")\n\n# Plot the timeseries analysis of the monthly air-sea CO₂ flux changes along the coast of California\n# Figure size: 20 representing the width, 10 representing the height\nfig = plt.figure(figsize=(20, 10))\n\nplt.plot(\n    df_sorted[\"datetime\"],    # X-axis: sorted datetime\n    df_sorted[\"max\"],         # Y-axis: maximum CO₂ value\n    color=\"purple\",           # Line color\n    linestyle=\"-\",            # Line style\n    linewidth=1,              # Line width\n    label=\"CO2 Emissions\",    # Legend label\n)\n\n# Display legend\nplt.legend()\n\n# Insert label for the X-axis\nplt.xlabel(\"Years\")\n\n# Insert label for the Y-axis\nplt.ylabel(\"CO2 Emissions mmol m²/s\")\n\n# Insert title for the plot\nplt.title(\"CO2 Emission Values for Coastal California (2020-2022)\")\n\n# Rotate x-axis labels to avoid cramping\nplt.xticks(rotation=90)\n\n# Add data citation\nplt.text(\n    df_sorted[\"datetime\"].iloc[0],           # X-coordinate of the text (first datetime value)\n    df_sorted[\"max\"].min(),                  # Y-coordinate of the text (minimum CO2 value)\n\n    # Text to be displayed\n    \"Source: NASA Air-Sea CO₂ Flux, ECCO-Darwin Model v5\",                   \n    fontsize=12,                             # Font size\n    horizontalalignment=\"left\",              # Horizontal alignment\n    verticalalignment=\"bottom\",              # Vertical alignment\n    color=\"blue\",                            # Text color\n)\n\n# Plot the time series\nplt.show()\n\nLooking at the plot above, we notice that CO₂ emission level increases particularly around 2022-09-01 for the defined area of interest. To take a closer look at monthly CO₂ flux variability across this region, we are going to retrieve and display data collected during the September 2022 observation.\n\n# The 2022-09-01 observation is the 4th item in the list. \n# Considering that a list starts with \"0\", we need to insert \"3\" in the \"items[3]\" statement\nprint(items[3][\"properties\"][\"start_datetime\"])\n\n\n# A GET request is made for the September 2022 tile\nSeptember2022_co2_flux = requests.get(\n\n    # Pass the collection name, the item number in the list, and its ID\n    f\"{RASTER_API_URL}/stac/tilejson.json?collection={items[3]['collection']}&item={items[3]['id']}\"\n\n    # Pass the asset name\n    f\"&assets={asset_name}\"\n\n    # Pass the color formula and colormap for custom visualization\n    f\"&color_formula=gamma+r+1.05&colormap_name={color_map}\"\n\n    # Pass the minimum and maximum values for rescaling\n    f\"&rescale={rescale_values['min']},{rescale_values['max']}\",\n\n# Return the response in JSON format\n).json()\n\n# Print the properties of the retrieved granule to the console  \nSeptember2022_co2_flux\n\n\n# Create a new map to display the September 2022 tile\naoi_map_bbox = Map(\n\n    # Base map is set to OpenStreetMap\n    tiles=\"OpenStreetMap\",\n\n    # Set the center of the map\n    location=[\n        34, -120\n    ],\n\n    # Set the zoom value\n    zoom_start=5.5,\n)\n\n# Define the map layer with the CO2 flux data for September 2022\nmap_layer = TileLayer(\n    tiles=September2022_co2_flux[\"tiles\"][0], # Path to retrieve the tile\n    attr=\"GHG\", # Set the attribution\n    opacity = 0.7, # Adjust the transparency of the layer\n)\n\n# Add the layer to the map\nmap_layer.add_to(aoi_map_bbox)\n\n# Add a legend to the map\n# Minimum value = -0.0007, maximum value = 0.0007\ncolormap = branca.colormap.LinearColormap(colors=[\"#0000FF\", \"#3399FF\", \"#66CCFF\", \"#FFFFFF\", \"#FF66CC\", \"#FF3399\", \"#FF0000\"], vmin=-0.0007, vmax=0.0007) \n\n# Add the data unit as caption \ncolormap.caption = 'Millimoles per meter squared per second (mmol m²/s)'\n\n# Define custom tick values for the legend bar\ntick_val = [-0.0007, -0.00035, 0, 0.00035, 0.0007]\n\n# Create a HTML representation\nlegend_html = colormap._repr_html_()\n\n# Create a customized HTML structure for the legend\nlegend_html = f'''\n&lt;div style=\"position: fixed; bottom: 50px; left: 50px; z-index: 1000; width: 400px; height: auto; background-color: rgba(255, 255, 255, 0.8);\n             border-radius: 5px; border: 1px solid grey; padding: 10px; font-size: 14px; color: black;\"&gt;\n    &lt;b&gt;{colormap.caption}&lt;/b&gt;&lt;br&gt;\n    &lt;div style=\"display: flex; justify-content: space-between;\"&gt;\n        &lt;div&gt;{tick_val[0]}&lt;/div&gt; \n        &lt;div&gt;{tick_val[1]}&lt;/div&gt; \n        &lt;div&gt;{tick_val[2]}&lt;/div&gt; \n        &lt;div&gt;{tick_val[3]}&lt;/div&gt; \n        &lt;div&gt;{tick_val[4]}&lt;/div&gt; \n    &lt;/div&gt;\n    &lt;div style=\"background: linear-gradient(to right,\n                {'#0000FF'}, {'#3399FF'} {20}%,\n                {'#3399FF'} {20}%, {'#66CCFF'} {40}%,\n                {'#66CCFF'} {40}%, {'#FFFFFF'} {50}%,\n                {'#FFFFFF'} {50}%, {'#FF66CC'} {80}%,\n                {'#FF66CC'} {80}%, {'#FF3399'}); height: 10px;\"&gt;&lt;/div&gt;\n&lt;/div&gt;\n'''\n\n# Display the legend and caption on the map\naoi_map_bbox.get_root().html.add_child(folium.Element(legend_html))\n\n# Add the title to the map\ntitle_html = '''\n&lt;div style=\"position: fixed; top: 10px; right: 10px; z-index: 1000; background-color: rgba(255, 255, 255, 0.8); border-radius: 5px; border: 1px solid grey; padding: 10px;\"&gt;\n    &lt;b&gt;Air-Sea CO₂ Flux, ECCO-Darwin&lt;/b&gt;&lt;br&gt;\n    September 2022\n&lt;/div&gt;\n'''\n# Display the title on the map\naoi_map_bbox.get_root().html.add_child(folium.Element(title_html))\n\n# Visualize the map\naoi_map_bbox",
    "crumbs": [
      "Data Usage Notebooks",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "Air-Sea CO₂ Flux, ECCO-Darwin Model v5"
    ]
  },
  {
    "objectID": "user_data_notebooks/eccodarwin-co2flux-monthgrid-v5_User_Notebook.html#summary",
    "href": "user_data_notebooks/eccodarwin-co2flux-monthgrid-v5_User_Notebook.html#summary",
    "title": "Air-Sea CO₂ Flux, ECCO-Darwin Model v5",
    "section": "Summary",
    "text": "Summary\nIn this notebook we have successfully completed the following steps for the STAC collection for the NASA Air-Sea CO₂ Flux ECCO Darwin dataset: 1. Install and import the necessary libraries 2. Fetch the collection from STAC collections using the appropriate endpoints 3. Count the number of existing granules within the collection 4. Map and compare the CO₂ Flux levels over the Coastal California area for two distinctive months/years 5. Create a table that displays the minimum, maximum, and sum of the CO₂ Flux values for a specified region 6. Generate a time-series graph of the CO₂ Flux values for a specified region\nIf you have any questions regarding this user notebook, please contact us using the feedback form.",
    "crumbs": [
      "Data Usage Notebooks",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "Air-Sea CO₂ Flux, ECCO-Darwin Model v5"
    ]
  },
  {
    "objectID": "user_data_notebooks/noaa-insitu_User_Notebook.html",
    "href": "user_data_notebooks/noaa-insitu_User_Notebook.html",
    "title": "Atmospheric Carbon Dioxide Concentrations from NOAA Global Monitoring Laboratory",
    "section": "",
    "text": "You can launch this notebook in the US GHG Center JupyterHub by clicking the link below.\nLaunch in the US GHG Center JupyterHub (requires access)",
    "crumbs": [
      "Data Usage Notebooks",
      "Greenhouse Gas Concentrations",
      "Atmospheric Carbon Dioxide Concentrations from NOAA Global Monitoring Laboratory"
    ]
  },
  {
    "objectID": "user_data_notebooks/noaa-insitu_User_Notebook.html#run-this-notebook",
    "href": "user_data_notebooks/noaa-insitu_User_Notebook.html#run-this-notebook",
    "title": "Atmospheric Carbon Dioxide Concentrations from NOAA Global Monitoring Laboratory",
    "section": "",
    "text": "You can launch this notebook in the US GHG Center JupyterHub by clicking the link below.\nLaunch in the US GHG Center JupyterHub (requires access)",
    "crumbs": [
      "Data Usage Notebooks",
      "Greenhouse Gas Concentrations",
      "Atmospheric Carbon Dioxide Concentrations from NOAA Global Monitoring Laboratory"
    ]
  },
  {
    "objectID": "user_data_notebooks/noaa-insitu_User_Notebook.html#approach",
    "href": "user_data_notebooks/noaa-insitu_User_Notebook.html#approach",
    "title": "Atmospheric Carbon Dioxide Concentrations from NOAA Global Monitoring Laboratory",
    "section": "Approach",
    "text": "Approach\n\nIdentify available dates and temporal frequency of observations for the given data. The collection processed in this notebook is the Atmospheric Carbon Dioxide Concentrations from NOAA Global Monitoring Laboratory.\nVisualize the time series data",
    "crumbs": [
      "Data Usage Notebooks",
      "Greenhouse Gas Concentrations",
      "Atmospheric Carbon Dioxide Concentrations from NOAA Global Monitoring Laboratory"
    ]
  },
  {
    "objectID": "user_data_notebooks/noaa-insitu_User_Notebook.html#about-the-data",
    "href": "user_data_notebooks/noaa-insitu_User_Notebook.html#about-the-data",
    "title": "Atmospheric Carbon Dioxide Concentrations from NOAA Global Monitoring Laboratory",
    "section": "About the Data",
    "text": "About the Data\nThe Global Greenhouse Gas Reference Network (GGGRN) for the Carbon Cycle and Greenhouse Gases (CCGG) Group is part of NOAA’S Global Monitoring Laboratory (GML) in Boulder, CO. The Reference Network measures the atmospheric distribution and trends of the three main long-term drivers of climate change, carbon dioxide (CO₂), methane (CH₄), and nitrous oxide (N2O), as well as carbon monoxide (CO) and many other trace gases which help interpretation of the main GHGs. The Reference Network measurement program includes continuous in-situ measurements at 4 baseline observatories (global background sites) and 8 tall towers, as well as flask-air samples collected by volunteers at over 50 additional regional background sites and from small aircraft conducting regular vertical profiles. The air samples are returned to GML for analysis where measurements of about 55 trace gases are done. NOAA’s GGGRN maintains the World Meteorological Organization international calibration scales for CO₂, CH₄, CO, N2O, and SF6 in air. The measurements from the GGGRN serve as a comparison with measurements made by many other international laboratories, and with regional studies. They are widely used in modeling studies that infer space-time patterns of emissions and removals of greenhouse gases that are optimally consistent with the atmospheric observations, given wind patterns. These data serve as an early warning for climate “surprises”. The measurements are also helpful for the ongoing evaluation of remote sensing technologies.\nFor more information regarding this dataset, please visit the Atmospheric Carbon Dioxide Concentrations from NOAA GML data overview page.",
    "crumbs": [
      "Data Usage Notebooks",
      "Greenhouse Gas Concentrations",
      "Atmospheric Carbon Dioxide Concentrations from NOAA Global Monitoring Laboratory"
    ]
  },
  {
    "objectID": "user_data_notebooks/noaa-insitu_User_Notebook.html#reading-the-noaa-data-from-github-repo",
    "href": "user_data_notebooks/noaa-insitu_User_Notebook.html#reading-the-noaa-data-from-github-repo",
    "title": "Atmospheric Carbon Dioxide Concentrations from NOAA Global Monitoring Laboratory",
    "section": "Reading the NOAA data from GitHub repo",
    "text": "Reading the NOAA data from GitHub repo\n\ngithub_repo_owner = \"NASA-IMPACT\"\ngithub_repo_name = \"noaa-viz\"\nfolder_path_ch4, folder_path_co2 = \"flask/ch4\", \"flask/c02\"\ncombined_df_co2, combined_df_ch4 = pd.DataFrame(), pd.DataFrame()\n\n\n# Function to fetch and append a file from GitHub\ndef append_github_file(file_url):\n    response = requests.get(file_url)\n    response.raise_for_status()\n    return response.text\n\n# Get the list of CH4 files in the specified directory using GitHub API\ngithub_api_url = f\"https://api.github.com/repos/{github_repo_owner}/{github_repo_name}/contents/{folder_path_ch4}\"\nresponse = requests.get(github_api_url)\nresponse.raise_for_status()\nfile_list_ch4 = response.json()\n\n# Get the list of CO2 files in the specified directory using GitHub API\ngithub_api_url = f\"https://api.github.com/repos/{github_repo_owner}/{github_repo_name}/contents/{folder_path_ch4}\"\nresponse = requests.get(github_api_url)\nresponse.raise_for_status()\nfile_list_co2 = response.json()",
    "crumbs": [
      "Data Usage Notebooks",
      "Greenhouse Gas Concentrations",
      "Atmospheric Carbon Dioxide Concentrations from NOAA Global Monitoring Laboratory"
    ]
  },
  {
    "objectID": "user_data_notebooks/noaa-insitu_User_Notebook.html#concatenating-the-ch4-data-into-a-single-dataframe",
    "href": "user_data_notebooks/noaa-insitu_User_Notebook.html#concatenating-the-ch4-data-into-a-single-dataframe",
    "title": "Atmospheric Carbon Dioxide Concentrations from NOAA Global Monitoring Laboratory",
    "section": "Concatenating the CH4 data into a single DataFrame",
    "text": "Concatenating the CH4 data into a single DataFrame\n\nfor file_info in file_list_ch4:\n    if file_info[\"name\"].endswith(\"txt\"):\n        file_content = append_github_file(file_info[\"download_url\"])\n        Lines = file_content.splitlines()\n        index = Lines.index(\"# VARIABLE ORDER\")+2\n        df = pd.read_csv(StringIO(\"\\n\".join(Lines[index:])), delim_whitespace=True)\n        combined_df_ch4 = pd.concat([combined_df_ch4, df], ignore_index=True)",
    "crumbs": [
      "Data Usage Notebooks",
      "Greenhouse Gas Concentrations",
      "Atmospheric Carbon Dioxide Concentrations from NOAA Global Monitoring Laboratory"
    ]
  },
  {
    "objectID": "user_data_notebooks/noaa-insitu_User_Notebook.html#concatenating-the-co2-data-into-a-single-dataframe",
    "href": "user_data_notebooks/noaa-insitu_User_Notebook.html#concatenating-the-co2-data-into-a-single-dataframe",
    "title": "Atmospheric Carbon Dioxide Concentrations from NOAA Global Monitoring Laboratory",
    "section": "Concatenating the CO2 data into a single DataFrame",
    "text": "Concatenating the CO2 data into a single DataFrame\n\nfor file_info in file_list_co2:\n    if file_info[\"name\"].endswith(\"txt\"):\n        file_content = append_github_file(file_info[\"download_url\"])\n        Lines = file_content.splitlines()\n        index = Lines.index(\"# VARIABLE ORDER\")+2\n        df = pd.read_csv(StringIO(\"\\n\".join(Lines[index:])), delim_whitespace=True)\n        combined_df_co2 = pd.concat([combined_df_co2, df], ignore_index=True)",
    "crumbs": [
      "Data Usage Notebooks",
      "Greenhouse Gas Concentrations",
      "Atmospheric Carbon Dioxide Concentrations from NOAA Global Monitoring Laboratory"
    ]
  },
  {
    "objectID": "user_data_notebooks/noaa-insitu_User_Notebook.html#visualizing-the-noaa-data-for-ch4-and-co2",
    "href": "user_data_notebooks/noaa-insitu_User_Notebook.html#visualizing-the-noaa-data-for-ch4-and-co2",
    "title": "Atmospheric Carbon Dioxide Concentrations from NOAA Global Monitoring Laboratory",
    "section": "Visualizing the NOAA data for CH4 and CO2",
    "text": "Visualizing the NOAA data for CH4 and CO2\n\nsite_to_filter = 'ABP'\nfiltered_df = combined_df_co2[combined_df_co2['site_code'] == site_to_filter]\n\nfiltered_df['datetime'] = pd.to_datetime(filtered_df['datetime'])\n\n# Set the \"Date\" column as the index\nfiltered_df.set_index('datetime', inplace=True)\n\n# Create a time series plot for 'Data' and 'Value'\nplt.figure(figsize=(12, 6))\nplt.plot(filtered_df.index, filtered_df['value'], label='Carbon Dioxide(CO2) Concentration (ppm)')\nplt.xlabel(\"Observed Date/Time\")\nplt.ylabel(\"Carbon Dioxide(CO2) Concentration (ppm)\")\nplt.title(f\"Observed Co2 Concentration {site_to_filter}\")\nplt.legend()\nplt.grid(True)\n# plt.show()\n\n/var/folders/7b/5rrvrjx51l54jchgs0tqps0c0000gn/T/ipykernel_70808/2606016741.py:4: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  filtered_df['datetime'] = pd.to_datetime(filtered_df['datetime'])\n\n\n\n\n\n\n\n\n\n\nsite_to_filter = 'ABP'\nfiltered_df = combined_df_ch4[combined_df_ch4['site_code'] == site_to_filter]\nfiltered_df['datetime'] = pd.to_datetime(filtered_df['datetime'])\n\n# Set the \"Date\" column as the index\nfiltered_df.set_index('datetime', inplace=True)\n\n# Create a time series plot for 'Data' and 'Value'\nplt.figure(figsize=(12, 6))\nplt.plot(filtered_df.index, filtered_df['value'], label='Methane Ch4 Concentration (ppb)')\nplt.xlabel(\"Observation Date/Time\")\nplt.ylabel(\"Methane Ch4 Concentration (ppb)\")\nplt.title(f\"Observed CH4 Concentration {site_to_filter}\")\nplt.legend()\nplt.grid(True)\nplt.show()\n\n/var/folders/7b/5rrvrjx51l54jchgs0tqps0c0000gn/T/ipykernel_70808/1635934907.py:3: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  filtered_df['datetime'] = pd.to_datetime(filtered_df['datetime'])",
    "crumbs": [
      "Data Usage Notebooks",
      "Greenhouse Gas Concentrations",
      "Atmospheric Carbon Dioxide Concentrations from NOAA Global Monitoring Laboratory"
    ]
  },
  {
    "objectID": "user_data_notebooks/noaa-insitu_User_Notebook.html#summary",
    "href": "user_data_notebooks/noaa-insitu_User_Notebook.html#summary",
    "title": "Atmospheric Carbon Dioxide Concentrations from NOAA Global Monitoring Laboratory",
    "section": "Summary",
    "text": "Summary\nIn this notebook we have successfully visualized the data for Atmospheric Carbon Dioxide Concentrations from NOAA Global Monitoring Laboratory.\n\nInstall and import the necessary libraries\nFetch the collection from GitHub API using the appropriate endpoints\nConcatenating the CO2 and CH4 data into a single DataFrame\nVisualizing the NOAA data for CO2 and CH4\n\nIf you have any questions regarding this user notebook, please contact us using the feedback form.",
    "crumbs": [
      "Data Usage Notebooks",
      "Greenhouse Gas Concentrations",
      "Atmospheric Carbon Dioxide Concentrations from NOAA Global Monitoring Laboratory"
    ]
  },
  {
    "objectID": "user_data_notebooks/oco2-mip-National-co2budget.html",
    "href": "user_data_notebooks/oco2-mip-National-co2budget.html",
    "title": "OCO-2 MIP National Top-Down CO2 Budgets",
    "section": "",
    "text": "You can launch this notebook in the US GHG Center JupyterHub by clicking the link below.\nLaunch in the US GHG Center JupyterHub (requires access)"
  },
  {
    "objectID": "user_data_notebooks/oco2-mip-National-co2budget.html#run-this-notebook",
    "href": "user_data_notebooks/oco2-mip-National-co2budget.html#run-this-notebook",
    "title": "OCO-2 MIP National Top-Down CO2 Budgets",
    "section": "",
    "text": "You can launch this notebook in the US GHG Center JupyterHub by clicking the link below.\nLaunch in the US GHG Center JupyterHub (requires access)"
  },
  {
    "objectID": "user_data_notebooks/oco2-mip-National-co2budget.html#approach",
    "href": "user_data_notebooks/oco2-mip-National-co2budget.html#approach",
    "title": "OCO-2 MIP National Top-Down CO2 Budgets",
    "section": "Approach",
    "text": "Approach\n\nRead in National CO2 Budgets using Pandas\nSub-select the data structure using Pandas\nVisualize the CO2 budgets for a country\nInvestigate uncertainties and metrics for understanding the dataset"
  },
  {
    "objectID": "user_data_notebooks/oco2-mip-National-co2budget.html#about-the-data",
    "href": "user_data_notebooks/oco2-mip-National-co2budget.html#about-the-data",
    "title": "OCO-2 MIP National Top-Down CO2 Budgets",
    "section": "About the Data",
    "text": "About the Data\nThis tutorial guides a user to further explore data from the Carbon Observatory (OCO-2) modeling intercomparison project (MIP). It is designed for those with more understanding of the science and is therefore labeled as intermediate level.\nThe code is used to estimate the annual net terrestrial carbon stock loss (ΔCloss) and net carbon exchange (NCE) for a given country using the “top-down” NCE outputs from the Carbon Observatory (OCO-2) modeling intercomparison project (MIP). Several standardized experiments are studied in this notebook based on the OCO-2 MIP dataset including flux estimates from in situ CO₂ measurements (IS), flux estimates from OCO-2 land CO₂ data (LNLG), combined in situ and OCO-2 land CO₂ data (LNLGIS), and combined in situ and OCO-2 land and ocean CO₂ data (LNLGOGIS). Estimates and uncertainties associated with fossil fuels, riverine fluxes, and wood and crop fluxes are also graphed along with the ΔCloss and NCE variables.\nFor more information about this data collection, please visit the OCO-2 MIP Top-Down CO2 Budgets data overview page.\nFor more information regarding this dataset, please visit the U.S. Greenhouse Gas Center."
  },
  {
    "objectID": "user_data_notebooks/oco2-mip-National-co2budget.html#import-required-modules",
    "href": "user_data_notebooks/oco2-mip-National-co2budget.html#import-required-modules",
    "title": "OCO-2 MIP National Top-Down CO2 Budgets",
    "section": "Import required modules",
    "text": "Import required modules\nFirst we will need to import the relevant python modules:\n\nimport pandas as pd # for manipulating csv dataset\nimport numpy as np\nimport matplotlib.pyplot as plt # make plots\nfrom scipy.stats import norm # We will use this for understanding significance"
  },
  {
    "objectID": "user_data_notebooks/oco2-mip-National-co2budget.html#read-the-co2-national-budget-dataset",
    "href": "user_data_notebooks/oco2-mip-National-co2budget.html#read-the-co2-national-budget-dataset",
    "title": "OCO-2 MIP National Top-Down CO2 Budgets",
    "section": "Read the CO2 National budget dataset",
    "text": "Read the CO2 National budget dataset\nNow we will read in the csv dataset from https://ceos.org/gst/carbon-dioxide.html\n\nurl ='https://ceos.org/gst/files/pilot_topdown_CO2_Budget_countries_v1.csv'\ndf_all = pd.read_csv(url, skiprows=52)"
  },
  {
    "objectID": "user_data_notebooks/oco2-mip-National-co2budget.html#sub-select-a-single-top-down-dataset-experiment",
    "href": "user_data_notebooks/oco2-mip-National-co2budget.html#sub-select-a-single-top-down-dataset-experiment",
    "title": "OCO-2 MIP National Top-Down CO2 Budgets",
    "section": "Sub-select a single top-down dataset (experiment)",
    "text": "Sub-select a single top-down dataset (experiment)\nTo simplify the analysis, let’s subselect the results for a single experiment. The experiments are: - IS: estimates fluxes from in situ CO2 measurements - LNLG: estimates fluxes from OCO-2 land CO2 data - LNLGIS: combines in situ and OCO-2 land CO2 data - LNLGOGIS: combines in situ and OCO-2 land and ocean CO2 data\nWe would like to use the experiment that uses the most high-quality CO2 data. There are some concerns about small residual biases in OCO-2 ocean data (Byrne et al., 2023), so let’s use the LNLGIS experiment.\n\n# Choose one experiment from the list ['IS', 'LNLG', 'LNLGIS', 'LNLGOGIS']\nexperiment = 'LNLGIS'\n\n# Subset of columns for a given experiment\nif experiment == 'IS':\n    df = df_all.drop(df_all.columns[[4,5,6,7,8,9,12,13,14,15,16,17,20,21,22,23,24,25,34,35,36]], axis=1)\nif experiment == 'LNLG':\n    df = df_all.drop(df_all.columns[[2,3,6,7,8,9,10,11,14,15,16,17,18,19,22,23,24,25,33,35,36]], axis=1)\nif experiment == 'LNLGIS':\n    df = df_all.drop(df_all.columns[[2,3,4,5,8,9,10,11,12,13,16,17,18,19,20,21,24,25,33,34,36]], axis=1)\nif experiment == 'LNLGOGIS':\n    df = df_all.drop(df_all.columns[[2,3,4,5,6,7,10,11,12,13,14,15,18,19,20,21,22,23,33,34,35]], axis=1)\n\n# We can now look at the colums of data\ndf.head()\n\n\n\n\n\n\n\n\nAlpha 3 Code\nYear\nLNLGIS dC_loss (TgCO2)\nLNLGIS dC_loss unc (TgCO2)\nLNLGIS NBE (TgCO2)\nLNLGIS NBE unc (TgCO2)\nLNLGIS NCE (TgCO2)\nLNLGIS NCE unc (TgCO2)\nRivers (TgCO2)\nRiver unc (TgCO2)\nWood+Crop (TgCO2)\nWood+Crop unc (TgCO2)\nFF (TgCO2)\nFF unc (TgCO2)\nZ-statistic\nFUR LNLGIS\n\n\n\n\n0\nAFG\n2015\n39.3407\n153.746\n40.9643\n153.746\n60.3537\n153.744\n-2.43286\n1.69832\n4.05648\n1.21694\n19.3894\n0.797698\n0.37\n0.19\n\n\n1\nAFG\n2016\n50.6167\n175.454\n52.5114\n175.454\n73.0333\n175.452\n-2.16185\n2.24033\n4.05648\n1.21694\n20.5220\n0.678080\n0.31\n0.19\n\n\n2\nAFG\n2017\n54.5096\n179.794\n56.4726\n179.794\n77.5355\n179.793\n-2.09349\n2.37705\n4.05648\n1.21694\n21.0629\n0.695856\n0.47\n0.19\n\n\n3\nAFG\n2018\n116.4260\n243.057\n118.4610\n243.057\n143.9580\n243.056\n-2.02199\n2.52005\n4.05648\n1.21694\n25.4974\n0.695856\n0.39\n0.19\n\n\n4\nAFG\n2019\n64.0162\n181.516\n66.0388\n181.516\n93.8974\n181.514\n-2.03383\n2.49637\n4.05648\n1.21694\n27.8585\n0.797698\n0.49\n0.19"
  },
  {
    "objectID": "user_data_notebooks/oco2-mip-National-co2budget.html#sub-select-a-single-country",
    "href": "user_data_notebooks/oco2-mip-National-co2budget.html#sub-select-a-single-country",
    "title": "OCO-2 MIP National Top-Down CO2 Budgets",
    "section": "Sub-select a single country",
    "text": "Sub-select a single country\nLet’s further filter the dataset to look at a specific country. Choose a country by entering the alpha code in the country_name variable below\n\n# Choose a country\ncountry_name = 'USA' \n\n# We can sub-select the data for the country\ncountry_data = df[df['Alpha 3 Code'] == country_name]\n\n# Now we can look at the data for a specific experiment and country\ncountry_data.head()\n\n\n\n\n\n\n\n\nAlpha 3 Code\nYear\nLNLGIS dC_loss (TgCO2)\nLNLGIS dC_loss unc (TgCO2)\nLNLGIS NBE (TgCO2)\nLNLGIS NBE unc (TgCO2)\nLNLGIS NCE (TgCO2)\nLNLGIS NCE unc (TgCO2)\nRivers (TgCO2)\nRiver unc (TgCO2)\nWood+Crop (TgCO2)\nWood+Crop unc (TgCO2)\nFF (TgCO2)\nFF unc (TgCO2)\nZ-statistic\nFUR LNLGIS\n\n\n\n\n1232\nUSA\n2015\n-1031.83\n721.213\n-1346.46\n721.213\n4017.31\n713.897\n-165.430\n71.7453\n-149.196\n-44.7589\n5363.77\n102.4670\n-0.81\n0.91\n\n\n1233\nUSA\n2016\n-1419.92\n399.738\n-1743.80\n399.738\n3529.45\n387.079\n-174.684\n53.2375\n-149.196\n-44.7589\n5273.24\n99.8012\n0.04\n0.91\n\n\n1234\nUSA\n2017\n-1375.12\n1034.010\n-1696.63\n1034.010\n3515.14\n1029.250\n-172.308\n57.9894\n-149.196\n-44.7589\n5211.76\n99.0981\n0.67\n0.91\n\n\n1235\nUSA\n2018\n-1018.89\n784.463\n-1333.83\n784.463\n4036.65\n778.179\n-165.747\n71.1117\n-149.196\n-44.7589\n5370.48\n99.0981\n-0.20\n0.91\n\n\n1236\nUSA\n2019\n-1161.41\n718.054\n-1504.61\n718.054\n3728.95\n710.705\n-194.005\n14.5948\n-149.196\n-44.7589\n5233.56\n102.4670\n-0.38\n0.91\n\n\n\n\n\n\n\n#This dataset contains fluxes over a five year period, 2015-2020.\nLet’s look at a plot of the annual net terrestrial carbon stock loss (ΔCloss) for each year.\n\n# Make plot\nfig, ax1 = plt.subplots(1, 1, figsize=(6, 4))\nax1.errorbar(country_data['Year'],country_data[experiment+' dC_loss (TgCO2)'],\n                    yerr=country_data[experiment+' dC_loss unc (TgCO2)'],label=experiment,capsize=10)\nax1.legend(loc='upper right')\nax1.set_ylabel('$\\Delta$C$_\\mathrm{loss}$ (TgCO$_2$ year$^{-1}$)')\nax1.set_xlabel('Year')\nax1.set_title('$\\Delta$C$_\\mathrm{loss}$ for '+country_name)\nymin, ymax = ax1.get_ylim()\nmax_abs_y = max(abs(ymin), abs(ymax))\nax1.set_ylim([-max_abs_y, max_abs_y])\nxmin, xmax = ax1.get_xlim()\nax1.plot([xmin,xmax],[0,0],'k',linewidth=0.5)\nax1.set_xlim([xmin, xmax])\n\n\n\n\n\n\n\n\nNext, we can look at the full carbon budget for a given year.\nThe code below creates a plot similar to Fig 13 of Byrne et al. (2023). Each of the bars on the left side of the dashed vertical line (Fossil fuel emissions, lateral C transport by rivers, lateral C transport in crop and wood products, and the net terrestrial carbon stock loss combined to give the net carbon exchange (net surface-atmosphere CO2 flux) shown on the right.\n\n# Pick a specifc year (or mean year)\nyear='mean'\n\n# Make plot\ncountry_data_mean = country_data[country_data['Year'] == year]\na=country_data_mean['Wood+Crop (TgCO2)']\nb=country_data_mean['Wood+Crop unc (TgCO2)']\nprint(b)\n#\nplt.bar(1, country_data_mean['FF (TgCO2)'], yerr=country_data_mean['FF unc (TgCO2)'], label='FF', alpha=0.5)\nplt.bar(2, country_data_mean['Rivers (TgCO2)'], yerr=country_data_mean['River unc (TgCO2)'], label='Rivers', alpha=0.5)\nplt.bar(3, country_data_mean['Wood+Crop (TgCO2)'], yerr=abs(country_data_mean['Wood+Crop unc (TgCO2)']), label='WoodCrop', alpha=0.5)\nplt.bar(4, country_data_mean[experiment+' dC_loss (TgCO2)'], yerr=country_data_mean['LNLGIS dC_loss unc (TgCO2)'], label='dC', alpha=0.5)\nplt.bar(6, country_data_mean[experiment+' NCE (TgCO2)'], yerr=country_data_mean['LNLGIS NCE unc (TgCO2)'], label='NCE', alpha=0.5)\nax = plt.gca()\nymin, ymax = ax.get_ylim()\nplt.plot([5,5],[ymin,ymax],'k:')\nxmin, xmax = ax.get_xlim()\nplt.plot([xmin,xmax],[0,0],'k',linewidth=0.5)\nplt.xlim([xmin,xmax])\nplt.ylim([ymin,ymax])\n#\nplt.xticks([1,2,3,4,6], ['Fossil\\nFuels','Rivers','Wood+\\nCrops','$\\mathrm{\\Delta C _{loss}}$','NCE'])\nplt.title(country_name+' '+year)\nplt.ylabel('CO$_2$ Flux (TgCO$_2$ year$^{-1}$)')\n\n1238   -44.7589\nName: Wood+Crop unc (TgCO2), dtype: float64\n\n\nText(0, 0.5, 'CO$_2$ Flux (TgCO$_2$ year$^{-1}$)')\n\n\n\n\n\n\n\n\n\nUncertainty is an important consideration when analyzing the flux estimates provided by Byrne et al. (2023).\nEach flux estimate is provided with an error estimate representing the standard deviation, and assuming the errors are well prepresented by a normal distribution. This probability dirtribution provided by this uncertainty can be visualized below. We can further quantify the\n\n\n# Select NCE, NBE or dC_loss\nquantity = 'dC_loss'\n\n# Value for comparison\ncomparison_value = 1000 # TgCO2/year\n\n\nMIP_mean = country_data_mean[experiment+' '+quantity+' (TgCO2)'].item()\nMIP_std = country_data_mean[experiment+' '+quantity+' unc (TgCO2)'].item()\n\n# Perform t-test\nt_value = abs(MIP_mean - comparison_value)/(MIP_std / np.sqrt(11))\ncrtical_value = 2.23 # use p=0.05 significance\nif t_value &gt; crtical_value:\n    ttest = 'statistically different'\nif t_value &lt; crtical_value:\n    ttest = 'not statistically\\ndifferent'\n\n# Make plot\nxbounds = abs(MIP_mean)+MIP_std*4\nif abs(crtical_value) &gt; xbounds:\n    xbounds = abs(crtical_value)\nx_axis = np.arange(-1.*xbounds, xbounds, 1) \nplt.plot(x_axis, norm.pdf(x_axis, MIP_mean, MIP_std)) \nax = plt.gca()\nymin, ymax = ax.get_ylim()\nxmin, xmax = ax.get_xlim()\nplt.plot([0,0],[ymin,ymax*1.2],'k:',linewidth=0.5)\nplt.plot([xmin,xmax],[0,0],'k:',linewidth=0.5)\nplt.plot([comparison_value,comparison_value],[ymin,ymax*1.2],'k')\nplt.text(comparison_value+(xmax-xmin)*0.01,ymax*0.96,'value = '+str(comparison_value),ha='left',va='top')\nplt.text(comparison_value+(xmax-xmin)*0.01,ymax*0.9,ttest,ha='left',va='top')\nplt.ylim([ymin,ymax*1.2])\nplt.xlim([xmin,xmax])\nplt.plot(MIP_mean,ymax*1.03,'ko')\nplt.plot([MIP_mean-MIP_std,\n         MIP_mean+MIP_std],\n         [ymax*1.03,ymax*1.03],'k')\nplt.plot([MIP_mean-MIP_std,\n         MIP_mean-MIP_std],\n         [ymax*1.005,ymax*1.055],'k')\nplt.plot([MIP_mean+MIP_std,\n         MIP_mean+MIP_std],\n         [ymax*1.005,ymax*1.055],'k')\nplt.text(MIP_mean,ymax*1.115,\n         str(round(MIP_mean))+' $\\pm$ '+\n         str(round(MIP_std))+' TgCO$_2$',ha='center')\nplt.title(country_name+' '+year+' '+quantity+'')\nplt.yticks([])\nplt.ylabel('Probability')\nplt.xlabel(quantity+' (TgCO$_2$ year$^{-1}$)')\n\nText(0.5, 0, 'dC_loss (TgCO$_2$ year$^{-1}$)')\n\n\n\n\n\n\n\n\n\nFinally, we will examine two metrics that are useful for understanding the confidence in the top-down results:\n\nZ-statistic: metric of agreement in NCE estimates across the experiments that assimilate different CO2 datasets. Experiments are considered significantly different if the magnitude exceeds 1.96\nFractional Uncertainty Reduction (FUR): metric of how strongly the assimilated CO2 data on reduce NCE uncertainties. Values range from 0 to 1, with 0 meaning zero error reduction and 1 meaning complete error reduction\n\nHere we will add a plot of the Z-statistic for each year, and add the FUR value for the country.\n\n# Make plot\nfig, ax1 = plt.subplots(1, 1, figsize=(6, 4))\nax1.plot(country_data['Year'],country_data['Z-statistic'],label=experiment)\nax1.legend(loc='upper right')\nax1.set_ylabel('Z-statistic')\nax1.set_xlabel('Year')\nax1.set_title(country_name)\nymin, ymax = ax1.get_ylim()\nmax_abs_y = max(abs(ymin), abs(ymax))\nax1.set_ylim([-3, 3])\nxmin, xmax = ax1.get_xlim()\nax1.plot([xmin,xmax],[0,0],'k',linewidth=0.5)\nax1.plot([xmin,xmax],[-1.96,-1.96],'k--',linewidth=0.5)\nax1.plot([xmin,xmax],[1.96,1.96],'k--',linewidth=0.5)\nax1.set_xlim([xmin, xmax])\nax1.text(xmin+0.12,2.6,'Fractional error reduction: '+str(country_data['FUR '+experiment].iloc[1]))\n\nText(-0.18000000000000005, 2.6, 'Fractional error reduction: 0.91')"
  },
  {
    "objectID": "services/jupyterhub.html",
    "href": "services/jupyterhub.html",
    "title": "JupyterHub",
    "section": "",
    "text": "The US GHG Center promotes the use of JupyterHub environments for interactive data science. JupyterHub enables you to analyze massive archives of Earth science data in the cloud in an interactive environment that alleviates the complexities of managing compute resources (virtual machines, roles and permissions, etc).\nUsers affiliated with the US GHG Center can get access to a dedicated JupyterHub service, provided in collaboration with 2i2c: hub.ghg.center. Please find instructions for requesting access below.\nIf you are a scientist affiliated with other NASA projects such as VEDA, EIS, and MAAP, you can also keep using the resources provided by these projects. Through the use of open-source technology, we make sure our services are interoperable and exchangeable.",
    "crumbs": [
      "User Services",
      "JupyterHub"
    ]
  },
  {
    "objectID": "services/jupyterhub.html#to-get-us-ghg-center-jupyterhub-access",
    "href": "services/jupyterhub.html#to-get-us-ghg-center-jupyterhub-access",
    "title": "JupyterHub",
    "section": "To Get US GHG Center JupyterHub access:",
    "text": "To Get US GHG Center JupyterHub access:\nThe US GHG Center notebook environment is available to authorized users on an as-need basis. If you are a user affiliated with the US GHG Center, you can gain access by using our Hub Access Request form.\n\nMake sure you have a GitHub Account. Take note of your GitHub username.\nFill out the request form and provide needed information.\nWatch your email for notification of authorization and the invite to join the US GHG Center Hub Access GitHub Team.\nOnce you accept the invitation, you can go to hub.ghg.center and login using your GitHub credentials.",
    "crumbs": [
      "User Services",
      "JupyterHub"
    ]
  },
  {
    "objectID": "services/jupyterhub.html#to-access-user-notebooks",
    "href": "services/jupyterhub.html#to-access-user-notebooks",
    "title": "JupyterHub",
    "section": "To access User Notebooks",
    "text": "To access User Notebooks\nThis site provides Jupyter notebooks showing how to load and analyze Earth data in the interactive cloud computing environment.\nFurther instructions are included in each notebook.\nIf you have any questions, please use the feedback form to contact the US GHG Center user support team.",
    "crumbs": [
      "User Services",
      "JupyterHub"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "U.S. Greenhouse Gas Center: Documentation",
    "section": "",
    "text": "The U.S. Greenhouse Gas (GHG) Center provides a cloud-based system for exploring and analyzing U.S. government and other curated greenhouse gas datasets.\nOn this site, you can find the technical documentation for the services the center provides, how to load the datasets, and how the datasets were transformed from their source formats (eg. netCDF, HDF, etc.) into cloud-optimized formats that enable efficient cloud data access and visualization.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#welcome",
    "href": "index.html#welcome",
    "title": "U.S. Greenhouse Gas Center: Documentation",
    "section": "",
    "text": "The U.S. Greenhouse Gas (GHG) Center provides a cloud-based system for exploring and analyzing U.S. government and other curated greenhouse gas datasets.\nOn this site, you can find the technical documentation for the services the center provides, how to load the datasets, and how the datasets were transformed from their source formats (eg. netCDF, HDF, etc.) into cloud-optimized formats that enable efficient cloud data access and visualization.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#contents",
    "href": "index.html#contents",
    "title": "U.S. Greenhouse Gas Center: Documentation",
    "section": "Contents",
    "text": "Contents\n\nServices provided for accessing and analyzing the US GHG Center datasets, such as the JupyterHub environment for interactive computing.\nDataset usage examples, e.g. for the Wetland Methane Emissions from the LPJ-EOSIM model dataset, that shows how to load the dataset in Python in JupyterHub.\nDataset transformation scripts, which document the code used to transform datasets for display in the US GHG Center. An example is the ODIAC Fossil Fuel CO₂ Emissions dataset transformation code.\nData processing and verification reports that openly present the process we used to check and verify that any transformation did not alter the original source data. An example is the GOSAT-based Top-down Total and Natural Methane Emissions dataset.\nData Flow Diagrams, which provide a high level summary of how each dataset was integrated into the US GHG Center. See the MiCASA Land Carbon Flux Flow Diagram as an example.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#contact",
    "href": "index.html#contact",
    "title": "U.S. Greenhouse Gas Center: Documentation",
    "section": "Contact",
    "text": "Contact\nFor technical help or general questions, please contact the support team using the feedback form.",
    "crumbs": [
      "Welcome"
    ]
  }
]