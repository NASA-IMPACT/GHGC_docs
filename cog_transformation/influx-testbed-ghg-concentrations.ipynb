{
 "cells": [
  {
   "cell_type": "raw",
   "id": "c5850703-19b9-43d9-9820-d441c17d6a9f",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: Carbon Dioxide and Methane Concentrations from the Indianapolis Flux Experiment (INFLUX)\n",
    "description: Documentation of data transformation \n",
    "author: Paridhi Parajuli \n",
    "date: September 19, 2024\n",
    "execute:\n",
    "  freeze: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba060420-788e-4a04-b1a6-9901f4573178",
   "metadata": {},
   "source": [
    "This script was used to transform the NIST INFLUX  dataset into meaningful csv files for ingestion to vector dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d38300e-0d5f-4cfb-ae21-cab513fe6448",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import zipfile\n",
    "import wget\n",
    "from collections import defaultdict\n",
    "from io import StringIO\n",
    "import re\n",
    "import warnings\n",
    "import warnings\n",
    "from datetime import datetime, timedelta\n",
    "# Ignore the FutureWarning\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56f3d7c-3b90-4669-a52b-bfab7130d94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "selected_level=\"level1\"\n",
    "base_dir = \"data/\"\n",
    "output_dir = \"output/\"\n",
    "dat_file_pattern = f\"{base_dir}/*/*.dat\"\n",
    "output_base_dataset_name = \"PSU_INFLUX_INSITU\" \n",
    "constant_variables = [\"datetime\",\"latitude\",\"longitude\",\"level\",\"elevation_m\",\"intake_height_m\",\"Instr\"]\n",
    "variables =[['CO2(ppm)'],['CH4(ppb)']] # exclude CO\n",
    "metadata_link= \"UrbanTestBed-Metadata - INFLUX.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33681ce3-9cc6-4abc-b04b-8faf06c45392",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions\n",
    "def filter_dict(site_dict, selected_level):\n",
    "    return {key: [x for x in value if selected_level in x] for key, value in site_dict.items()}\n",
    "\n",
    "def flag_desired_level(df, desired_level):\n",
    "    df['is_max_height_data'] = df['level']== desired_level\n",
    "    return df\n",
    "\n",
    "def add_location(link, site_number):\n",
    "    meta= pd.read_csv(link)\n",
    "    location =meta[meta['Station Code']==f\"Site {site_number[-2:]}\"][['City','State']]#(get the actual site number)\n",
    "    return location['City'].item()+\",\"+location['State'].item()\n",
    "\n",
    "def convert_to_datetime(row):\n",
    "    year = int(row['Year'])\n",
    "    doy = int(row['DOY'])\n",
    "    hour = int(row['Hour'])\n",
    "    \n",
    "    # Create a datetime object for the start of the year\n",
    "    date = datetime(year, 1, 1) + timedelta(days=doy - 1)\n",
    "    # Add the hours\n",
    "    datetime_obj = date + timedelta(hours=hour)\n",
    "    # Format as yyyy-mm-ddThh:mm:ssZ\n",
    "    return datetime_obj.strftime('%Y-%m-%dT%H:%M:%SZ')\n",
    "\n",
    "def download_and_extract_zip_files(base_dir, levels):\n",
    "    \"\"\"\n",
    "    Download, extract, and delete zip files for the specified levels.\n",
    "\n",
    "    Parameters:\n",
    "    base_dir (str): The base directory for storing the downloaded files.\n",
    "    levels (list): A list of levels to download and extract.\n",
    "    \"\"\"\n",
    "    # Ensure the base directory exists\n",
    "    os.makedirs(base_dir, exist_ok=True)\n",
    "\n",
    "    # Loop through the levels and handle the download and extraction\n",
    "    for level in levels:\n",
    "        download_link = f\"https://www.datacommons.psu.edu/download/meteorology/influx/influx-tower-data/wmo-x2019-scale/level{level}.zip\"\n",
    "        fname = download_link.split(\"/\")[-1]\n",
    "        target_path = os.path.join(base_dir, fname)\n",
    "        \n",
    "        # Download the zip file\n",
    "        wget.download(download_link, target_path)\n",
    "        print(f\"Downloaded {download_link} to {target_path}\")\n",
    "\n",
    "        # Extract the zip file\n",
    "        with zipfile.ZipFile(target_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(base_dir)\n",
    "            print(f\"Extracted {fname}\")\n",
    "\n",
    "        # Delete the zip file after extraction\n",
    "        os.remove(target_path)\n",
    "\n",
    "def create_site_dict(pattern):\n",
    "    \"\"\"\n",
    "    Creates a dictionary where keys are site numbers extracted from file paths,\n",
    "    and values are lists of file paths corresponding to each site number.\n",
    "    \n",
    "    Args:\n",
    "    - pattern (str): Glob pattern to match files.\n",
    "    \n",
    "    Returns:\n",
    "    - dict: Dictionary mapping site numbers to lists of file paths.\n",
    "    \"\"\"\n",
    "    all_files = glob.glob(pattern)\n",
    "    site_dict = defaultdict(list)\n",
    "    \n",
    "    for file_path in all_files:\n",
    "        site_number = file_path.split('_')[-4]\n",
    "        site_dict[site_number].append(file_path)\n",
    "    \n",
    "    return dict(site_dict)\n",
    "\n",
    "def process_site_files(site_number, file_list):\n",
    "    \"\"\"\n",
    "    Process files for a given site number and save the combined DataFrame to CSV.\n",
    "    \n",
    "    Args:\n",
    "    - site_number (str): Site number to process.\n",
    "    - file_list (list): List of file paths corresponding to the site number.\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    for file_path in file_list:\n",
    "        with open(file_path, 'r') as file:\n",
    "            data = file.read()\n",
    "            \n",
    "        contents = data.split(\"\\nSite\")\n",
    "        lat = float((re.search(r'LATITUDE:\\s*([0-9.]+)\\s*[NS]', contents[0])).group(1))\n",
    "        lat_hemisphere = (re.search(r'LATITUDE:\\s*([0-9.]+)\\s*[NS]', contents[0])).group(0)[-1]\n",
    "        \n",
    "        lon = float((re.search(r'LONGITUDE:\\s*([0-9.]+)\\s*[EW]', contents[0])).group(1))\n",
    "        lon_hemisphere = (re.search(r'LONGITUDE:\\s*([0-9.]+)\\s*[EW]', contents[0])).group(0)[-1]\n",
    "        \n",
    "        level= file_path.split(\"/\")[-2]\n",
    "        \n",
    "        elevation= re.search(r'ALTITUDE:\\s*([0-9.]+)\\s*m\\s*ASL', contents[0]).group(1)\n",
    "        intake_height= re.search(r'SAMPLING HEIGHT:\\s*([0-9.]+)\\s*m\\s*AGL', contents[0]).group(1)\n",
    "\n",
    "        \n",
    "        data_io = StringIO(contents[1])\n",
    "        tmp_data = pd.read_csv(data_io, delim_whitespace=True)\n",
    "        tmp_data = tmp_data.reset_index().rename(columns={'index': 'Site'})\n",
    "        tmp= tmp_data.query(\"Flag==1\").copy()# 1 means no known problem, 0 is not recommemded, 9 is instrument issue (unrealistic)\n",
    "        #tmp['SiteCode'] = int(re.search(r'\\d+', site_number).group()) \n",
    "        tmp['latitude'] = lat\n",
    "        tmp['longitude'] = lon\n",
    "        tmp['level'] = int(re.search(r'\\d+', level).group())\n",
    "        tmp['elevation_m'] = elevation\n",
    "        tmp['intake_height_m']= intake_height\n",
    "\n",
    "        if lat_hemisphere == 'S':\n",
    "            tmp['latitude'] = -1* tmp[\"latitude\"]\n",
    "        if lon_hemisphere == 'W':\n",
    "            tmp['longitude'] = -1* tmp[\"longitude\"]\n",
    "\n",
    "        df = pd.concat([df, tmp], ignore_index=True)\n",
    "\n",
    "    # Ensure the output directory exists\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    os.makedirs(output_dir+\"PSU_INFLUX_INSITU/\", exist_ok=True)\n",
    "    \n",
    "\n",
    "    df['datetime'] = df[[\"Year\",\"DOY\",\"Hour\"]].apply(convert_to_datetime, axis=1)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    for v in variables:\n",
    "        tmp_file=df[constant_variables + v].copy()\n",
    "        tmp_file['unit'] = v[0][-4:-1] #CO2(ppm) get  the unit only\n",
    "        \n",
    "        tmp_file.rename(columns={v[0]: 'value'}, inplace=True)\n",
    "        tmp_file['location']= add_location(metadata_link, site_number)\n",
    "        tmp_file = flag_desired_level(tmp_file, 1) # Flagging only level 1 data\n",
    "\n",
    "        # Remove nan\n",
    "        tmp_file.dropna(subset=[\"value\"], inplace=True)\n",
    "\n",
    "        #filter 0 values\n",
    "        tmp_file[tmp_file[\"value\"]!=0].to_csv(f\"{output_dir}/PSU_INFLUX_INSITU/NIST-FLUX-IN-{site_number}-{v[0][:-5]}-hourly-concentrations.csv\", index=False)\n",
    "        print(f\"CSV Created for Site {site_number}-{v[0][:-5]}!!!\")\n",
    "    return \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6b92fb-1b21-4fad-b0d1-c0a07ff388b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Download and extract zip files\n",
    "levels_to_download = range(1, 5)\n",
    "#download_and_extract_zip_files(base_dir=base_dir, levels=levels_to_download)\n",
    "\n",
    "# Create site dictionary\n",
    "site_dict = create_site_dict(dat_file_pattern)\n",
    "\n",
    "# Comment if you want data from all levels\n",
    "#site_dict = filter_dict(site_dict, selected_level)\n",
    "\n",
    "# Process each site's files\n",
    "for site_number, file_list in site_dict.items():\n",
    "    print(f\"Processing Site Number: {site_number}, Total Files: {len(file_list)}\")\n",
    "    process_site_files(site_number, file_list)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
