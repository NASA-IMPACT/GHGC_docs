{
 "cells": [
  {
   "cell_type": "raw",
   "id": "2f231b28-78b7-47fa-9d37-571b0537c798",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: Carbon Dioxide and Methane Concentrations from the Los Angeles Megacity Carbon Project\n",
    "description: Documentation of data transformation \n",
    "author: Paridhi Parajuli \n",
    "date: August 28, 2023\n",
    "execute:\n",
    "  freeze: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1fc9d19-8d59-4cc4-a858-ba9d65ab324e",
   "metadata": {},
   "source": [
    "This script was used to transform the NIST INFLUX  dataset into meaningful csv files for ingestion to vector dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6185ff3-141c-443b-acc0-969aa7cfee6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import warnings\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdcbec12-f1e8-4401-a070-242b24351db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download data from https://data.nist.gov/od/id/mds2-2388 into your desired_folder\n",
    "source_dir = \"CA\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308f7691-b6ce-4515-9e93-9f554c882051",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping the files for preparation\n",
    "config_ca = pd.read_csv(\"LAM_sites-2.csv\") #metadata from providers\n",
    "all_files= glob.glob(f\"{source_dir}/*.csv\")\n",
    "all_files = [i.split(\"/\")[-1].split('.')[0] for i in glob.glob(f\"{source_dir}/*.csv\") ]\n",
    "my_dict={}\n",
    "for site in list(config_ca.SiteCode):\n",
    "    # for each site and variable, append into the dict\n",
    "    if (config_ca[config_ca[\"SiteCode\"]==site][\"Tower\"].item()) ==1 :\n",
    "\n",
    "        co2_files = [f for f in all_files if site in f and \"upwind\" not in f and \"all\" not in f and \"co2\" in f]\n",
    "        my_dict[f\"{site}-co2\"] = co2_files\n",
    "        # Find the files that do not have \"upwind\" or \"all\" and have \"ch4\"\n",
    "        ch4_files = [f for f in all_files if site in f and \"upwind\" not in f and \"all\" not in f and \"ch4\" in f]\n",
    "        my_dict[f\"{site}-ch4\"] = ch4_files\n",
    "    else:\n",
    "        co2_upwind_files = [f for f in all_files if site in f and \"upwind\" in f and \"co2\" in f]\n",
    "        my_dict[f\"{site}-co2\"] = co2_upwind_files\n",
    "        \n",
    "        # Find the files that have \"upwind\" and \"ch4\"\n",
    "        ch4_upwind_files = [f for f in all_files if site in f and \"upwind\" in f and \"ch4\" in f]\n",
    "        my_dict[f\"{site}-ch4\"] = ch4_upwind_files\n",
    "\n",
    "        if site in [\"IRV\",\"RAN\"]:\n",
    "            co2_files = [f for f in all_files if site in f and \"all\" in f and \"co2\" in f]\n",
    "            my_dict[f\"{site}-co2\"] = co2_files\n",
    "            ch4_files = [f for f in all_files if site in f and \"all\" in f and \"ch4\" in f]\n",
    "            my_dict[f\"{site}-ch4\"] = ch4_files\n",
    "        \n",
    "del my_dict['USC2-co2']\n",
    "del my_dict['USC2-ch4']\n",
    "\n",
    "for key in my_dict:\n",
    "    my_dict[key] = sorted(my_dict[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdcab4b2-28fe-4fa5-8e15-aa3f9852fd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to generate transformed data for CA\n",
    "output_dir = \"output_LAM\"\n",
    "os.makedirs(output_dir,exist_ok=True)\n",
    "for key, value in my_dict.items():\n",
    "    df=pd.DataFrame()\n",
    "    variable = key.split(\"-\")[-1]\n",
    "    val = f\"{variable}_ppm\" if variable == 'co2' else f\"{variable}_ppb\"\n",
    "    columns = [\"latitude\",\"longitude\",\"intake_height_m\",\"elevation_m\",\"datetime\",val ]\n",
    "    for file in value:\n",
    "        tmp = pd.read_csv(f\"CA/{file}.csv\")\n",
    "        tmp.dropna(subset=[val], inplace=True)\n",
    "        tmp.rename(columns={'datetime_UTC': 'datetime'}, inplace=True)\n",
    "        tmp= tmp[columns]\n",
    "        tmp.rename(columns={val: 'value'}, inplace=True)\n",
    "        tmp['datetime'] = pd.to_datetime(tmp['datetime'])\n",
    "        tmp['datetime'] = tmp['datetime'].dt.strftime('%Y-%m-%dT%H:%M:%SZ')\n",
    "        tmp['location'] = config_ca[config_ca['SiteCode']==site][\"Location\"].item()\n",
    "        df = pd.concat([df, tmp], ignore_index=True)\n",
    "        \n",
    "    df['year']= df['datetime'].apply(lambda x: x[:4])\n",
    "    result = df.groupby(\"year\").agg(max_height= (\"intake_height_m\",\"max\"))\n",
    "    if result['max_height'].std() !=0:\n",
    "        print(f\"More than one max height for {file}\",result['max_height'].unique())\n",
    "    merged_df=pd.merge(df, result, on='year')\n",
    "    merged_df[\"is_max_height_data\"]= merged_df[\"max_height\"] == merged_df[\"intake_height_m\"]\n",
    "    merged_df=merged_df.drop(columns=['year','max_height'])\n",
    "    merged_df.reset_index(drop=True, inplace=True)\n",
    "    merged_df.to_csv(f\"{output_dir}/NIST-testbed-LAM-{key}-hourly-concentrations.csv\", index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4336997d-c52f-48a5-b44d-91f4480a1d5e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
