{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: Air-Sea CO₂ Flux, ECCO-Darwin Model v5\n",
    "description: Documentation of data transformation\n",
    "author: Vishal Gaur\n",
    "date: August 31, 2023\n",
    "execute:\n",
    "  freeze: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script was used to transform the Air-Sea CO₂ Flux, ECCO-Darwin Mode dataset from netCDF to Cloud Optimized GeoTIFF (COG) format for display in the Greenhouse Gas (GHG) Center.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import xarray\n",
    "import re\n",
    "import pandas as pd\n",
    "import json\n",
    "import tempfile\n",
    "import boto3\n",
    "import rasterio\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = boto3.session.Session()\n",
    "s3_client = session.client(\"s3\")\n",
    "\n",
    "bucket_name = (\n",
    "    \"ghgc-data-store-dev\"  # S3 bucket where the COGs are stored after transformation\n",
    ")\n",
    "FOLDER_NAME = \"ecco-darwin\"\n",
    "s3_fol_name = \"ecco_darwin\"\n",
    "\n",
    "# Reading the raw netCDF files from local machine\n",
    "files_processed = pd.DataFrame(\n",
    "    columns=[\"file_name\", \"COGs_created\"]\n",
    ")  # A dataframe to keep track of the files that we have transformed into COGs\n",
    "for name in os.listdir(FOLDER_NAME):\n",
    "    xds = xarray.open_dataset(\n",
    "        f\"{FOLDER_NAME}/{name}\",\n",
    "        engine=\"netcdf4\",\n",
    "    )\n",
    "    xds = xds.rename({\"y\": \"latitude\", \"x\": \"longitude\"})\n",
    "    xds = xds.assign_coords(longitude=((xds.longitude / 1440) * 360) - 180).sortby(\n",
    "        \"longitude\"\n",
    "    )\n",
    "    xds = xds.assign_coords(latitude=((xds.latitude / 721) * 180) - 90).sortby(\n",
    "        \"latitude\"\n",
    "    )\n",
    "\n",
    "    variable = [var for var in xds.data_vars]\n",
    "\n",
    "    for time_increment in xds.time.values:\n",
    "        for var in variable[2:]:\n",
    "            filename = name.split(\"/ \")[-1]\n",
    "            filename_elements = re.split(\"[_ .]\", filename)\n",
    "            data = xds[var]\n",
    "\n",
    "            data = data.reindex(latitude=list(reversed(data.latitude)))\n",
    "            data.rio.set_spatial_dims(\"longitude\", \"latitude\", inplace=True)\n",
    "            data.rio.write_crs(\"epsg:4326\", inplace=True)\n",
    "\n",
    "            # generate COG\n",
    "            COG_PROFILE = {\"driver\": \"COG\", \"compress\": \"DEFLATE\"}\n",
    "\n",
    "            filename_elements.pop()\n",
    "            filename_elements[-1] = filename_elements[-2] + filename_elements[-1]\n",
    "            filename_elements.pop(-2)\n",
    "            # # insert date of generated COG into filename\n",
    "            cog_filename = \"_\".join(filename_elements)\n",
    "            # # add extension\n",
    "            cog_filename = f\"{cog_filename}.tif\"\n",
    "\n",
    "            with tempfile.NamedTemporaryFile() as temp_file:\n",
    "                data.rio.to_raster(temp_file.name, **COG_PROFILE)\n",
    "                s3_client.upload_file(\n",
    "                    Filename=temp_file.name,\n",
    "                    Bucket=bucket_name,\n",
    "                    Key=f\"{s3_fol_name}/{cog_filename}\",\n",
    "                )\n",
    "\n",
    "            files_processed = files_processed._append(\n",
    "                {\"file_name\": name, \"COGs_created\": cog_filename},\n",
    "                ignore_index=True,\n",
    "            )\n",
    "            del data\n",
    "\n",
    "            print(f\"Generated and saved COG: {cog_filename}\")\n",
    "\n",
    "# Generate the json file with the metadata that is present in the netCDF files.\n",
    "with tempfile.NamedTemporaryFile(mode=\"w+\") as fp:\n",
    "    json.dump(xds.attrs, fp)\n",
    "    json.dump({\"data_dimensions\": dict(xds.dims)}, fp)\n",
    "    json.dump({\"data_variables\": list(xds.data_vars)}, fp)\n",
    "    fp.flush()\n",
    "\n",
    "    s3_client.upload_file(\n",
    "        Filename=fp.name,\n",
    "        Bucket=bucket_name,\n",
    "        Key=\"s3_fol_name/metadata.json\",\n",
    "    )\n",
    "\n",
    "# A csv file to store the names of all the files converted.\n",
    "files_processed.to_csv(\n",
    "    f\"s3://{bucket_name}/{s3_fol_name}/files_converted.csv\",\n",
    ")\n",
    "print(\"Done generating COGs\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
