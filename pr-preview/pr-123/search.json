[
  {
    "objectID": "processingreport.html",
    "href": "processingreport.html",
    "title": "U.S. Greenhouse Gas Center: Processing and Verification Reports",
    "section": "",
    "text": "Welcome to the U.S. Greenhouse Gas (GHG) Center processing and verification reports. These reports verify that the accuracy and integrity of each dataset in the US GHG Center is maintained once it is processed into the Center.\nThe reports are grouped topically and labeled by dataset name. Click on a dataset name to view the processing and verification report for that dataset.\nExamples of processing that may occur include transforming data from its source format into a could-optimized format, converting the units of the source data into a more common or standard unit, and flagging “nodata” values to ensure accurate data visualization. We strive to handle all data with extreme care, and share these reports to provide transparency and insight into any processing that is applied, while ensuring accuracy and reliability every step of the way.\nJoin us in our mission to make data-driven environmental solutions accessible. Explore, analyze, and make a difference with the US GHG Center.\nView the US GHG Center Data Catalog",
    "crumbs": [
      "Processing and Verification Reports"
    ]
  },
  {
    "objectID": "processingreport.html#gridded-anthropogenic-greenhouse-gas-emissions",
    "href": "processingreport.html#gridded-anthropogenic-greenhouse-gas-emissions",
    "title": "U.S. Greenhouse Gas Center: Processing and Verification Reports",
    "section": "Gridded Anthropogenic Greenhouse Gas Emissions",
    "text": "Gridded Anthropogenic Greenhouse Gas Emissions\n\nOCO-2 MIP Top-Down CO₂ Budgets Processing and Verification Report\nODIAC Fossil Fuel CO₂ Emissions Processing and Verification Report\nTM5-4DVar Isotopic CH₄ Inverse Fluxes Processing and Verification Report\nU.S. Gridded Anthropogenic Methane Emissions Inventory Processing and Verification Report\nVulcan Fossil Fuel CO₂ Emissions Processing and Verification Report",
    "crumbs": [
      "Processing and Verification Reports"
    ]
  },
  {
    "objectID": "processingreport.html#natural-greenhouse-gas-emissions-and-sinks",
    "href": "processingreport.html#natural-greenhouse-gas-emissions-and-sinks",
    "title": "U.S. Greenhouse Gas Center: Processing and Verification Reports",
    "section": "Natural Greenhouse Gas Emissions and Sinks",
    "text": "Natural Greenhouse Gas Emissions and Sinks\n\nAir-Sea CO₂ Flux, ECCO-Darwin Model v5 Processing and Verification Report\nMiCASA Land Carbon Flux Processing and Verification Report\nGOSAT-based Top-down Total and Natural Methane Emissions Processing and Verification Report\nOCO-2 MIP Top-Down CO₂ Budgets Processing and Verification Report\nTM5-4DVar Isotopic CH₄ Inverse Fluxes Processing and Verification Report\nWetland Methane Emissions, LPJ-EOSIM model Processing and Verification Report\nGRA²PES Greenhouse Gas and Air Quality Species Processing and Verification Report",
    "crumbs": [
      "Processing and Verification Reports"
    ]
  },
  {
    "objectID": "processingreport.html#large-emissions-events",
    "href": "processingreport.html#large-emissions-events",
    "title": "U.S. Greenhouse Gas Center: Processing and Verification Reports",
    "section": "Large Emissions Events",
    "text": "Large Emissions Events\n\nEMIT Methane Point Source Plume Complexes Processing and Verification Report",
    "crumbs": [
      "Processing and Verification Reports"
    ]
  },
  {
    "objectID": "processingreport.html#greenhouse-gas-concentrations",
    "href": "processingreport.html#greenhouse-gas-concentrations",
    "title": "U.S. Greenhouse Gas Center: Processing and Verification Reports",
    "section": "Greenhouse Gas Concentrations",
    "text": "Greenhouse Gas Concentrations\n\nAtmospheric Carbon Dioxide Concentrations from NOAA Global Monitoring Laboratory Processing and Verification Report\nAtmospheric Methane Concentrations from NOAA Global Monitoring Laboratory Processing and Verification Report\nOCO-2 GEOS Column CO₂ Concentrations Processing and Verification Report\nCarbon Dioxide and Methane Concentrations from the Indianapolis Flux Experiment (INFLUX)\nCarbon Dioxide and Methane Concentrations from the Los Angeles Megacity Carbon Project\nCarbon Dioxide and Methane Concentrations from the Northeast Corridor (NEC) Urban Test Bed",
    "crumbs": [
      "Processing and Verification Reports"
    ]
  },
  {
    "objectID": "processingreport.html#socioeconomic",
    "href": "processingreport.html#socioeconomic",
    "title": "U.S. Greenhouse Gas Center: Processing and Verification Reports",
    "section": "Socioeconomic",
    "text": "Socioeconomic\n\nSEDAC Gridded World Population Density Processing and Verification Report",
    "crumbs": [
      "Processing and Verification Reports"
    ]
  },
  {
    "objectID": "processingreport.html#contact",
    "href": "processingreport.html#contact",
    "title": "U.S. Greenhouse Gas Center: Processing and Verification Reports",
    "section": "Contact",
    "text": "Contact\nFor technical help or general questions, please contact the support team using the feedback form.",
    "crumbs": [
      "Processing and Verification Reports"
    ]
  },
  {
    "objectID": "services/jupyterhub.html",
    "href": "services/jupyterhub.html",
    "title": "JupyterHub",
    "section": "",
    "text": "The US GHG Center promotes the use of JupyterHub environments for interactive data science. JupyterHub enables you to analyze massive archives of Earth science data in the cloud in an interactive environment that alleviates the complexities of managing compute resources (virtual machines, roles and permissions, etc).\nUsers affiliated with the US GHG Center can get access to a dedicated JupyterHub service, provided in collaboration with 2i2c: hub.ghg.center. Please find instructions for requesting access below.\nIf you are a scientist affiliated with other NASA projects such as VEDA, EIS, and MAAP, you can also keep using the resources provided by these projects. Through the use of open-source technology, we make sure our services are interoperable and exchangeable.",
    "crumbs": [
      "User Services",
      "JupyterHub"
    ]
  },
  {
    "objectID": "services/jupyterhub.html#to-get-us-ghg-center-jupyterhub-access",
    "href": "services/jupyterhub.html#to-get-us-ghg-center-jupyterhub-access",
    "title": "JupyterHub",
    "section": "To Get US GHG Center JupyterHub access:",
    "text": "To Get US GHG Center JupyterHub access:\nThe US GHG Center notebook environment is available to authorized users on an as-need basis. If you are a user affiliated with the US GHG Center, you can gain access by using our Hub Access Request form.\n\nMake sure you have a GitHub Account. Take note of your GitHub username.\nFill out the request form and provide needed information.\nWatch your email for notification of authorization and the invite to join the US GHG Center Hub Access GitHub Team.\nOnce you accept the invitation, you can go to hub.ghg.center and login using your GitHub credentials.",
    "crumbs": [
      "User Services",
      "JupyterHub"
    ]
  },
  {
    "objectID": "services/jupyterhub.html#to-access-user-notebooks",
    "href": "services/jupyterhub.html#to-access-user-notebooks",
    "title": "JupyterHub",
    "section": "To access User Notebooks",
    "text": "To access User Notebooks\nThis site provides Jupyter notebooks showing how to load and analyze Earth data in the interactive cloud computing environment.\nFurther instructions are included in each notebook.\nIf you have any questions, please use the feedback form to contact the US GHG Center user support team.",
    "crumbs": [
      "User Services",
      "JupyterHub"
    ]
  },
  {
    "objectID": "processing_and_verification_reports/odiac-ffco2-monthgrid-v2023_Processing and Verification Report.html",
    "href": "processing_and_verification_reports/odiac-ffco2-monthgrid-v2023_Processing and Verification Report.html",
    "title": "ODIAC Fossil Fuel CO₂ Emissions",
    "section": "",
    "text": "This browser does not support PDFs. Please download the PDF to view it: Download PDF.\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Processing and Verification Reports",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "ODIAC Fossil Fuel CO₂ Emissions"
    ]
  },
  {
    "objectID": "processing_and_verification_reports/oco2geos-co2-daygrid-v10r_Processing and Verification Report.html",
    "href": "processing_and_verification_reports/oco2geos-co2-daygrid-v10r_Processing and Verification Report.html",
    "title": "OCO-2 GEOS Column CO₂ Concentrations",
    "section": "",
    "text": "This browser does not support PDFs. Please download the PDF to view it: Download PDF.\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Processing and Verification Reports",
      "Greenhouse Gas Concentrations",
      "OCO-2 GEOS Column CO₂ Concentrations"
    ]
  },
  {
    "objectID": "processing_and_verification_reports/eccodarwin-co2flux-monthgrid-v5_Processing and Verification Report.html",
    "href": "processing_and_verification_reports/eccodarwin-co2flux-monthgrid-v5_Processing and Verification Report.html",
    "title": "Air-Sea CO₂ Flux, ECCO-Darwin Model v5",
    "section": "",
    "text": "This browser does not support PDFs. Please download the PDF to view it: Download PDF.\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Processing and Verification Reports",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "Air-Sea CO₂ Flux, ECCO-Darwin Model v5"
    ]
  },
  {
    "objectID": "processing_and_verification_reports/lam-testbed-ghg-concentrations_Processing and Verification Report.html",
    "href": "processing_and_verification_reports/lam-testbed-ghg-concentrations_Processing and Verification Report.html",
    "title": "Carbon Dioxide and Methane Concentrations from the Los Angeles Megacity Carbon Project",
    "section": "",
    "text": "This browser does not support PDFs. Please download the PDF to view it: Download PDF.\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Processing and Verification Reports",
      "Greenhouse Gas Concentrations",
      "Carbon Dioxide and Methane Concentrations from the Los Angeles Megacity Carbon Project"
    ]
  },
  {
    "objectID": "processing_and_verification_reports/influx-testbed-ghg-concentrations_Processing and Verification Report.html",
    "href": "processing_and_verification_reports/influx-testbed-ghg-concentrations_Processing and Verification Report.html",
    "title": "Carbon Dioxide and Methane Concentrations from the Indianapolis Flux Experiment (INFLUX)",
    "section": "",
    "text": "This browser does not support PDFs. Please download the PDF to view it: Download PDF.\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Processing and Verification Reports",
      "Greenhouse Gas Concentrations",
      "Carbon Dioxide and Methane Concentrations from the Indianapolis Flux Experiment (INFLUX)"
    ]
  },
  {
    "objectID": "processing_and_verification_reports/epa-ch4emission-grid-v2express_Processing and Verification Report.html",
    "href": "processing_and_verification_reports/epa-ch4emission-grid-v2express_Processing and Verification Report.html",
    "title": "Gridded Anthropogenic Methane Emissions Inventory",
    "section": "",
    "text": "This browser does not support PDFs. Please download the PDF to view it: Download PDF.\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Processing and Verification Reports",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "Gridded Anthropogenic Methane Emissions Inventory"
    ]
  },
  {
    "objectID": "processing_and_verification_reports/nec-testbed-ghg-concentrations_Processing and Verification Report.html",
    "href": "processing_and_verification_reports/nec-testbed-ghg-concentrations_Processing and Verification Report.html",
    "title": "Carbon Dioxide and Methane Concentrations from the Northeast Corridor (NEC) Urban Test Bed",
    "section": "",
    "text": "This browser does not support PDFs. Please download the PDF to view it: Download PDF.\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Processing and Verification Reports",
      "Greenhouse Gas Concentrations",
      "Carbon Dioxide and Methane Concentrations from the Northeast Corridor (NEC) Urban Test Bed"
    ]
  },
  {
    "objectID": "processing_and_verification_reports/lpjeosim-wetlandch4-grid-v2_Processing and Verification Report.html",
    "href": "processing_and_verification_reports/lpjeosim-wetlandch4-grid-v2_Processing and Verification Report.html",
    "title": "Wetland Methane Emissions, LPJ-EOSIM Model",
    "section": "",
    "text": "This browser does not support PDFs. Please download the PDF to view it: Download PDF.\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Processing and Verification Reports",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "Wetland Methane Emissions, LPJ-EOSIM Model"
    ]
  },
  {
    "objectID": "processing_and_verification_reports/vulcan-ffco2-yeargrid-v4_Processing and Verification Report.html",
    "href": "processing_and_verification_reports/vulcan-ffco2-yeargrid-v4_Processing and Verification Report.html",
    "title": "Vulcan Fossil Fuel CO₂ Emissions",
    "section": "",
    "text": "This browser does not support PDFs. Please download the PDF to view it: Download PDF.\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Processing and Verification Reports",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "Vulcan Fossil Fuel CO₂ Emissions"
    ]
  },
  {
    "objectID": "user_data_notebooks/oco2-mip-co2budget-yeargrid-v1_User_Notebook.html",
    "href": "user_data_notebooks/oco2-mip-co2budget-yeargrid-v1_User_Notebook.html",
    "title": "OCO-2 MIP Top-Down CO₂ Budgets",
    "section": "",
    "text": "You can launch this notebook in the US GHG Center JupyterHub by clicking the link below.\nLaunch in the US GHG Center JupyterHub (requires access)",
    "crumbs": [
      "Data Usage Notebooks",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "OCO-2 MIP Top-Down CO₂ Budgets"
    ]
  },
  {
    "objectID": "user_data_notebooks/oco2-mip-co2budget-yeargrid-v1_User_Notebook.html#run-this-notebook",
    "href": "user_data_notebooks/oco2-mip-co2budget-yeargrid-v1_User_Notebook.html#run-this-notebook",
    "title": "OCO-2 MIP Top-Down CO₂ Budgets",
    "section": "",
    "text": "You can launch this notebook in the US GHG Center JupyterHub by clicking the link below.\nLaunch in the US GHG Center JupyterHub (requires access)",
    "crumbs": [
      "Data Usage Notebooks",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "OCO-2 MIP Top-Down CO₂ Budgets"
    ]
  },
  {
    "objectID": "user_data_notebooks/oco2-mip-co2budget-yeargrid-v1_User_Notebook.html#approach",
    "href": "user_data_notebooks/oco2-mip-co2budget-yeargrid-v1_User_Notebook.html#approach",
    "title": "OCO-2 MIP Top-Down CO₂ Budgets",
    "section": "Approach",
    "text": "Approach\n\nIdentify available dates and temporal frequency of observations for the given collection using the GHGC API /stac endpoint. The collection processed in this notebook is the OCO-2 MIP Top-Down CO₂ Budgets data product.\nPass the STAC item into the raster API /stac/tilejson.jsonendpoint.\nUsing folium.plugins.DualMap, we will visualize two tiles (side-by-side), allowing us to compare time points.\nAfter the visualization, we will perform zonal statistics for a given polygon.",
    "crumbs": [
      "Data Usage Notebooks",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "OCO-2 MIP Top-Down CO₂ Budgets"
    ]
  },
  {
    "objectID": "user_data_notebooks/oco2-mip-co2budget-yeargrid-v1_User_Notebook.html#about-the-data",
    "href": "user_data_notebooks/oco2-mip-co2budget-yeargrid-v1_User_Notebook.html#about-the-data",
    "title": "OCO-2 MIP Top-Down CO₂ Budgets",
    "section": "About the Data",
    "text": "About the Data\nThe Committee on Earth Observation Satellites (CEOS) Atmospheric Composition - Virtual Constellation (AC-VC) Greenhouse Gas (GHG) team has generated the CEOS CO₂ Budgets dataset, which provides annual top-down carbon dioxide (CO2) emissions and removals from 2015 - 2020 gridded globally at 1° resolution, and as national totals. Data is provided in units of grams of carbon dioxide per square meter per year (g CO2/m2/yr). Only a subset of the full dataset is displayed in the GHG Center explore view.\nFor more information regarding this dataset, please visit the OCO-2 MIP Top-Down CO₂ Budgets data overview page.",
    "crumbs": [
      "Data Usage Notebooks",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "OCO-2 MIP Top-Down CO₂ Budgets"
    ]
  },
  {
    "objectID": "user_data_notebooks/oco2-mip-co2budget-yeargrid-v1_User_Notebook.html#querying-the-stac-api",
    "href": "user_data_notebooks/oco2-mip-co2budget-yeargrid-v1_User_Notebook.html#querying-the-stac-api",
    "title": "OCO-2 MIP Top-Down CO₂ Budgets",
    "section": "Querying the STAC API",
    "text": "Querying the STAC API\nFirst, we are going to import the required libraries. Once imported, they allow better executing a query in the GHG Center Spatio Temporal Asset Catalog (STAC) Application Programming Interface (API) where the granules for this collection are stored.\n\nimport requests\nimport folium\nimport folium.plugins\nfrom folium import Map, TileLayer\nfrom pystac_client import Client\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n\n# Provide the STAC and RASTER API endpoints\n# The endpoint is referring to a location within the API that executes a request on a data collection nesting on the server.\n\n# The STAC API is a catalog of all the existing data collections that are stored in the GHG Center.\nSTAC_API_URL = \"https://earth.gov/ghgcenter/api/stac\"\n\n# The RASTER API is used to fetch collections for visualization\nRASTER_API_URL = \"https://earth.gov/ghgcenter/api/raster\"\n\n# The collection name is used to fetch the dataset from the STAC API. First, we define the collection name as a variable\n# Name of the collection for CEOS National Top-Down CO₂ Budgets dataset \ncollection_name = \"oco2-mip-co2budget-yeargrid-v1\"\n\n\n# Fetch the collection from the STAC API using the appropriate endpoint\n# The 'requests' library allows a HTTP request possible\ncollection = requests.get(f\"{STAC_API_URL}/collections/{collection_name}\").json()\n\n# Print the properties of the collection to the console\ncollection\n\nExamining the contents of our collection under the temporal variable, we see that the data is available from January 2015 to December 2020. By looking at the dashboard:time density, we observe that the periodic frequency of these observations is yearly.\n\n# Create a function that would search for a data collection in the US GHG Center STAC API\n\n# First, we need to define the function\n# The name of the function = \"get_item_count\"\n# The argument that will be passed through the defined function = \"collection_id\"\ndef get_item_count(collection_id):\n\n    # Set a counter for the number of items existing in the collection\n    count = 0\n\n    # Define the path to retrieve the granules (items) of the collection of interest in the STAC API\n    items_url = f\"{STAC_API_URL}/collections/{collection_id}/items\"\n\n    # Run a while loop to make HTTP requests until there are no more URLs associated with the collection in the STAC API\n    while True:\n\n        # Retrieve information about the granules by sending a \"get\" request to the STAC API using the defined collection path\n        response = requests.get(items_url)\n\n        # If the items do not exist, print an error message and quit the loop\n        if not response.ok:\n            print(\"error getting items\")\n            exit()\n\n        # Return the results of the HTTP response as JSON\n        stac = response.json()\n\n        # Increase the \"count\" by the number of items (granules) returned in the response\n        count += int(stac[\"context\"].get(\"returned\", 0))\n\n        # Retrieve information about the next URL associated with the collection in the STAC API (if applicable)\n        next = [link for link in stac[\"links\"] if link[\"rel\"] == \"next\"]\n\n        # Exit the loop if there are no other URLs\n        if not next:\n            break\n        \n        # Ensure the information gathered by other STAC API links associated with the collection are added to the original path\n        # \"href\" is the identifier for each of the tiles stored in the STAC API\n        items_url = next[0][\"href\"]\n\n    # Return the information about the total number of granules found associated with the collection\n    return count\n\n\n# Apply the function created above \"get_item_count\" to the data collection\nnumber_of_items = get_item_count(collection_name)\n\n# Get the information about the number of granules found in the collection\nitems = requests.get(f\"{STAC_API_URL}/collections/{collection_name}/items?limit={number_of_items}\").json()[\"features\"]\n\n# Print the total number of items (granules) found\nprint(f\"Found {len(items)} items\")\n\nFound 6 items\n\n\n\n# Examine the first item in the collection\n# Keep in mind that a list starts from 0, 1, 2... therefore items[0] is referring to the first item in the list/collection\nitems[0]",
    "crumbs": [
      "Data Usage Notebooks",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "OCO-2 MIP Top-Down CO₂ Budgets"
    ]
  },
  {
    "objectID": "user_data_notebooks/oco2-mip-co2budget-yeargrid-v1_User_Notebook.html#exploring-changes-in-co₂-levels-using-the-raster-api",
    "href": "user_data_notebooks/oco2-mip-co2budget-yeargrid-v1_User_Notebook.html#exploring-changes-in-co₂-levels-using-the-raster-api",
    "title": "OCO-2 MIP Top-Down CO₂ Budgets",
    "section": "Exploring Changes in CO₂ Levels Using the Raster API",
    "text": "Exploring Changes in CO₂ Levels Using the Raster API\nIn this notebook, we will explore the global changes of CO₂ budgets over time in urban regions. We will visualize the outputs on a map using folium.\n\n# Now we create a dictionary where the start datetime values for each granule is queried more explicitly by year and month (e.g., 2020-02)\nitems = {item[\"properties\"][\"start_datetime\"]: item for item in items} \n\n# Next, we need to specify the asset name for this collection\n# The asset name is referring to the raster band containing the pixel values for the parameter of interest\n# For the case of the OCO-2 MIP Top-Down CO₂ Budgets collection, the parameter of interest is “ff”\nasset_name = \"ff\" #fossil fuel\n\nBelow, we are entering the minimum and maximum values to provide our upper and lower bounds in the rescale_values.\n\n# Fetching the min and max values for a specific item\nrescale_values = {\"max\":items[list(items.keys())[0]][\"assets\"][asset_name][\"raster:bands\"][0][\"histogram\"][\"max\"], \"min\":items[list(items.keys())[0]][\"assets\"][asset_name][\"raster:bands\"][0][\"histogram\"][\"min\"]}\n\n# Hardcoding the min and max values to match the scale in the GHG Center dashboard\nrescale_values = {\"max\": 450, \"min\": 0}\n\nNow, we will pass the item id, collection name, asset name, and the rescaling factor to the Raster API endpoint. We will do this twice, once for 2020 and again for 2019, so that we can visualize each event independently.\n\n# Choose a color map for displaying the first observation (event)\n# Please refer to matplotlib library if you'd prefer choosing a different color ramp.\n# For more information on Colormaps in Matplotlib, please visit https://matplotlib.org/stable/users/explain/colors/colormaps.html\ncolor_map = \"purd\"\n\n# Make a GET request to retrieve information for the 2020 tile which is the 1st item in the collection\n# To retrieve the first item in the collection we use \"0\" in the \"(items.keys())[0]\" statement\n\n# 2020\nco2_flux_1 = requests.get(\n\n    # Pass the collection name, the item number in the list, and its ID\n    f\"{RASTER_API_URL}/stac/tilejson.json?collection={items[list(items.keys())[0]]['collection']}&item={items[list(items.keys())[0]]['id']}\"\n\n    # Pass the asset name\n    f\"&assets={asset_name}\"\n\n    # Pass the color formula and colormap for custom visualization\n    f\"&color_formula=gamma+r+1.05&colormap_name={color_map}\"\n\n    # Pass the minimum and maximum values for rescaling\n    f\"&rescale={rescale_values['min']},{rescale_values['max']}\", \n\n# Return the response in JSON format\n).json()\n\n# Print the properties of the retrieved granule to the console\nco2_flux_1\n\n{'tilejson': '2.2.0',\n 'version': '1.0.0',\n 'scheme': 'xyz',\n 'tiles': ['https://earth.gov/ghgcenter/api/raster/stac/tiles/WebMercatorQuad/{z}/{x}/{y}@1x?collection=oco2-mip-co2budget-yeargrid-v1&item=oco2-mip-co2budget-yeargrid-v1-2020&assets=ff&color_formula=gamma+r+1.05&colormap_name=purd&rescale=0%2C450'],\n 'minzoom': 0,\n 'maxzoom': 24,\n 'bounds': [-180.0, -90.0, 180.0, 90.0],\n 'center': [0.0, 0.0, 0]}\n\n\n\n# Make a GET request to retrieve information for the 2019 tile which is the 2st item in the collection\n# To retrieve the second item in the collection we use \"1\" in the \"(items.keys())[1]\" statement\n\n# 2019\nco2_flux_2 = requests.get(\n\n    # Pass the collection name, the item number in the list, and its ID\n    f\"{RASTER_API_URL}/stac/tilejson.json?collection={items[list(items.keys())[1]]['collection']}&item={items[list(items.keys())[1]]['id']}\"\n\n    # Pass the asset name\n    f\"&assets={asset_name}\"\n\n    # Pass the color formula and colormap for custom visualization\n    f\"&color_formula=gamma+r+1.05&colormap_name={color_map}\"\n\n    # Pass the minimum and maximum values for rescaling\n    f\"&rescale={rescale_values['min']},{rescale_values['max']}\", \n\n# Return the response in JSON format\n).json()\n\n# Print the properties of the retrieved granule to the console\nco2_flux_2\n\n{'tilejson': '2.2.0',\n 'version': '1.0.0',\n 'scheme': 'xyz',\n 'tiles': ['https://earth.gov/ghgcenter/api/raster/stac/tiles/WebMercatorQuad/{z}/{x}/{y}@1x?collection=oco2-mip-co2budget-yeargrid-v1&item=oco2-mip-co2budget-yeargrid-v1-2019&assets=ff&color_formula=gamma+r+1.05&colormap_name=purd&rescale=0%2C450'],\n 'minzoom': 0,\n 'maxzoom': 24,\n 'bounds': [-180.0, -90.0, 180.0, 90.0],\n 'center': [0.0, 0.0, 0]}",
    "crumbs": [
      "Data Usage Notebooks",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "OCO-2 MIP Top-Down CO₂ Budgets"
    ]
  },
  {
    "objectID": "user_data_notebooks/oco2-mip-co2budget-yeargrid-v1_User_Notebook.html#visualizing-co₂-emissions",
    "href": "user_data_notebooks/oco2-mip-co2budget-yeargrid-v1_User_Notebook.html#visualizing-co₂-emissions",
    "title": "OCO-2 MIP Top-Down CO₂ Budgets",
    "section": "Visualizing CO₂ Emissions",
    "text": "Visualizing CO₂ Emissions\n\n# For this study we are going to compare the CO2 budget in 2020 and 2019 along the coast of California\n# To change the location, you can simply insert the latitude and longitude of the area of your interest in the \"location=(LAT, LONG)\" statement\n\n# Set the initial zoom level and center of map for both tiles\n# 'folium.plugins' allows mapping side-by-side\nmap_ = folium.plugins.DualMap(location=(34, -118), zoom_start=6)\n\n# Define the first map layer (2020)\nmap_layer_2020 = TileLayer(\n    tiles=co2_flux_1[\"tiles\"][0], # Path to retrieve the tile\n    attr=\"GHG\", # Set the attribution\n    opacity=0.5, # Adjust the transparency of the layer\n)\n\n# Add the first layer to the Dual Map\nmap_layer_2020.add_to(map_.m1)\n\n# Define the second map layer (2019)\nmap_layer_2019 = TileLayer(\n    tiles=co2_flux_2[\"tiles\"][0], # Path to retrieve the tile\n    attr=\"GHG\", # Set the attribution\n    opacity=0.5, # Adjust the transparency of the layer\n)\n\n# Add the second layer to the Dual Map\nmap_layer_2019.add_to(map_.m2)\n\n# Visualize the Dual Map\nmap_\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook",
    "crumbs": [
      "Data Usage Notebooks",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "OCO-2 MIP Top-Down CO₂ Budgets"
    ]
  },
  {
    "objectID": "user_data_notebooks/oco2-mip-co2budget-yeargrid-v1_User_Notebook.html#visualizing-the-data-as-a-time-series",
    "href": "user_data_notebooks/oco2-mip-co2budget-yeargrid-v1_User_Notebook.html#visualizing-the-data-as-a-time-series",
    "title": "OCO-2 MIP Top-Down CO₂ Budgets",
    "section": "Visualizing the Data as a Time Series",
    "text": "Visualizing the Data as a Time Series\nWe can now explore the fossil fuel emission time series (January 2015 -December 2020) available for the Dallas, Texas area of the U.S. We can plot the data set using the code below:\n\n# Figure size: 20 representing the width, 10 representing the height\nfig = plt.figure(figsize=(20, 10))\n\nplt.plot(\n    df[\"datetime\"], # X-axis: sorted datetime\n    df[\"max\"], # Y-axis: maximum CO₂ emission\n    color=\"red\", # Line color\n    linestyle=\"-\", # Line style\n    linewidth=0.5, # Line width\n    label=\"CO2 emissions\", # Legend label\n)\n\n# Display legend\nplt.legend()\n\n# Insert label for the X-axis\nplt.xlabel(\"Years\")\n\n# Insert label for the Y-axis\nplt.ylabel(\"CO2 emissions gC/m2/year1\")\n\n# Insert title for the plot\nplt.title(\"CO2 emission Values for Texas, Dallas (2015-2020)\")\n\n# Add data citation\nplt.text(\n    df[\"datetime\"].iloc[0],           # X-coordinate of the text \n    df[\"max\"].min(),                  # Y-coordinate of the text \n\n\n    # Text to be displayed\n    \"Source: NASA/NOAA OCO-2 MIP Top-Down CO₂ Budgets\",                  \n    fontsize=12,                             # Font size\n    horizontalalignment=\"left\",              # Horizontal alignment\n    verticalalignment=\"top\",                 # Vertical alignment\n    color=\"blue\",                            # Text color\n)\n\n# Plot the time series\nplt.show()\n\nText(0.5, 1.0, 'CO2 emission Values for Texas, Dallas (2015-2020)')\n\n\n\n\n\n\n\n\n\n\n# The 2018-01-01 observation is the 3rd item in the list.\n# Considering that a list starts with \"0\", we need to insert \"2\" in the \"items[2]\" statement\nprint(items[2][\"properties\"][\"start_datetime\"])\n\n2018-01-01T00:00:00+00:00\n\n\n\n# A GET request is made for the 2018-01-01 tile\nco2_flux_3 = requests.get(\n\n    # Pass the collection name, the item number in the list, and its ID\n    f\"{RASTER_API_URL}/stac/tilejson.json?collection={items[2]['collection']}&item={items[2]['id']}\"\n\n    # Pass the asset name\n    f\"&assets={asset_name}\"\n\n    # Pass the color formula and colormap for custom visualization\n    f\"&color_formula=gamma+r+1.05&colormap_name={color_map}\"\n\n    # Pass the minimum and maximum values for rescaling\n    f\"&rescale={rescale_values['min']},{rescale_values['max']}\",\n\n# Return the response in JSON format\n).json()\n\n# Print the properties of the retrieved granule to the console\nco2_flux_3\n\n{'tilejson': '2.2.0',\n 'version': '1.0.0',\n 'scheme': 'xyz',\n 'tiles': ['https://earth.gov/ghgcenter/api/raster/stac/tiles/WebMercatorQuad/{z}/{x}/{y}@1x?collection=oco2-mip-co2budget-yeargrid-v1&item=oco2-mip-co2budget-yeargrid-v1-2018&assets=ff&color_formula=gamma+r+1.05&colormap_name=purd&rescale=0%2C450'],\n 'minzoom': 0,\n 'maxzoom': 24,\n 'bounds': [-180.0, -90.0, 180.0, 90.0],\n 'center': [0.0, 0.0, 0]}\n\n\n\n# Create a new map to display the 2018-01-01 tile\naoi_map_bbox = Map(\n\n    # Base map is set to OpenStreetMap\n    tiles=\"OpenStreetMap\",\n\n    # Set the center of the map\n    location=[\n        30,-100\n    ],\n\n    # Set the zoom value\n    zoom_start=6.8,\n)\n\n# Define the map layer\nmap_layer = TileLayer(\n\n    # Path to retrieve the tile\n    tiles=co2_flux_3[\"tiles\"][0],\n\n    # Set the attribution and adjust the transparency of the layer\n    attr=\"GHG\", opacity = 0.7\n)\n\n# Add the layer to the map\nmap_layer.add_to(aoi_map_bbox)\n\n# Visualize the map\naoi_map_bbox\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook",
    "crumbs": [
      "Data Usage Notebooks",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "OCO-2 MIP Top-Down CO₂ Budgets"
    ]
  },
  {
    "objectID": "user_data_notebooks/oco2-mip-co2budget-yeargrid-v1_User_Notebook.html#summary",
    "href": "user_data_notebooks/oco2-mip-co2budget-yeargrid-v1_User_Notebook.html#summary",
    "title": "OCO-2 MIP Top-Down CO₂ Budgets",
    "section": "Summary",
    "text": "Summary\nIn this notebook we have successfully explored, analyzed, and visualized the STAC collection for OCO-2 MIP Top-Down CO₂ Budgets.\n\nInstall and import the necessary libraries\nFetch the collection from STAC collections using the appropriate endpoints\nCount the number of existing granules within the collection\nVisualizing CO₂ Emissions for two distinctive months/years\nGenerate zonal statistics for a specified region\nGenerate a time-series graph\n\nIf you have any questions regarding this user notebook, please contact us using the feedback form.",
    "crumbs": [
      "Data Usage Notebooks",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "OCO-2 MIP Top-Down CO₂ Budgets"
    ]
  },
  {
    "objectID": "user_data_notebooks/oco2geos-co2-daygrid-v10r_User_Notebook.html",
    "href": "user_data_notebooks/oco2geos-co2-daygrid-v10r_User_Notebook.html",
    "title": "OCO-2 GEOS Column CO₂ Concentrations",
    "section": "",
    "text": "You can launch this notebook in the US GHG Center JupyterHub by clicking the link below.\nLaunch in the US GHG Center JupyterHub (requires access)",
    "crumbs": [
      "Data Usage Notebooks",
      "Greenhouse Gas Concentrations",
      "OCO-2 GEOS Column CO₂ Concentrations"
    ]
  },
  {
    "objectID": "user_data_notebooks/oco2geos-co2-daygrid-v10r_User_Notebook.html#run-this-notebook",
    "href": "user_data_notebooks/oco2geos-co2-daygrid-v10r_User_Notebook.html#run-this-notebook",
    "title": "OCO-2 GEOS Column CO₂ Concentrations",
    "section": "",
    "text": "You can launch this notebook in the US GHG Center JupyterHub by clicking the link below.\nLaunch in the US GHG Center JupyterHub (requires access)",
    "crumbs": [
      "Data Usage Notebooks",
      "Greenhouse Gas Concentrations",
      "OCO-2 GEOS Column CO₂ Concentrations"
    ]
  },
  {
    "objectID": "user_data_notebooks/oco2geos-co2-daygrid-v10r_User_Notebook.html#approach",
    "href": "user_data_notebooks/oco2geos-co2-daygrid-v10r_User_Notebook.html#approach",
    "title": "OCO-2 GEOS Column CO₂ Concentrations",
    "section": "Approach",
    "text": "Approach\n\nIdentify available dates and temporal frequency of observations for the given collection using the GHGC API /stac endpoint. The collection processed in this notebook is the OCO-2 GEOS Column CO₂ Concentrations data product.\nPass the STAC item into the raster API /stac/tilejson.json endpoint.\nUsing folium.plugins.DualMap, visualize two tiles (side-by-side), allowing time point comparison.\nAfter the visualization, perform zonal statistics for a given polygon.",
    "crumbs": [
      "Data Usage Notebooks",
      "Greenhouse Gas Concentrations",
      "OCO-2 GEOS Column CO₂ Concentrations"
    ]
  },
  {
    "objectID": "user_data_notebooks/oco2geos-co2-daygrid-v10r_User_Notebook.html#about-the-data",
    "href": "user_data_notebooks/oco2geos-co2-daygrid-v10r_User_Notebook.html#about-the-data",
    "title": "OCO-2 GEOS Column CO₂ Concentrations",
    "section": "About the Data",
    "text": "About the Data\nIn July 2014, NASA successfully launched the first dedicated Earth remote sensing satellite to study atmospheric carbon dioxide (CO₂) from space. The Orbiting Carbon Observatory-2 (OCO-2) is an exploratory science mission designed to collect space-based global measurements of atmospheric CO₂ with the precision, resolution, and coverage needed to characterize sources and sinks (fluxes) on regional scales (≥1000 km). This dataset provides global gridded, daily column-averaged carbon dioxide (XCO₂) concentrations from January 1, 2015 - February 28, 2022. The data are derived from OCO-2 observations that were input to the Goddard Earth Observing System (GEOS) Constituent Data Assimilation System (CoDAS), a modeling and data assimilation system maintained by NASA’s Global Modeling and Assimilation Office (GMAO). Concentrations are measured in moles of carbon dioxide per mole of dry air (mol CO₂/mol dry) at a spatial resolution of 0.5° x 0.625°. Data assimilation synthesizes simulations and observations, adjusting modeled atmospheric constituents like CO₂ to reflect observed values. With the support of NASA’s Carbon Monitoring System (CMS) Program and the OCO Science Team, this dataset was produced as part of the OCO-2 mission which provides the highest quality space-based XCO₂ retrievals to date.\nFor more information regarding this dataset, please visit the OCO-2 GEOS Column CO₂ Concentrations data overview page.",
    "crumbs": [
      "Data Usage Notebooks",
      "Greenhouse Gas Concentrations",
      "OCO-2 GEOS Column CO₂ Concentrations"
    ]
  },
  {
    "objectID": "user_data_notebooks/oco2geos-co2-daygrid-v10r_User_Notebook.html#install-the-required-libraries",
    "href": "user_data_notebooks/oco2geos-co2-daygrid-v10r_User_Notebook.html#install-the-required-libraries",
    "title": "OCO-2 GEOS Column CO₂ Concentrations",
    "section": "Install the Required Libraries",
    "text": "Install the Required Libraries\nRequired libraries are pre-installed on the GHG Center Hub. If you need to run this notebook elsewhere, please install them with this line in a code cell:\n%pip install requests folium rasterstats pystac_client pandas matplotlib –quiet\n\n# Import the following libraries\nimport requests\nimport folium\nimport folium.plugins\nfrom folium import Map, TileLayer\nfrom pystac_client import Client\nimport branca\nimport pandas as pd\nimport matplotlib.pyplot as plt",
    "crumbs": [
      "Data Usage Notebooks",
      "Greenhouse Gas Concentrations",
      "OCO-2 GEOS Column CO₂ Concentrations"
    ]
  },
  {
    "objectID": "user_data_notebooks/oco2geos-co2-daygrid-v10r_User_Notebook.html#querying-the-stac-api",
    "href": "user_data_notebooks/oco2geos-co2-daygrid-v10r_User_Notebook.html#querying-the-stac-api",
    "title": "OCO-2 GEOS Column CO₂ Concentrations",
    "section": "Querying the STAC API",
    "text": "Querying the STAC API\nFirst, we are going to import the required libraries. Once imported, they allow better executing a query in the GHG Center Spatio Temporal Asset Catalog (STAC) Application Programming Interface (API) where the granules for this collection are stored.\n\n# Provide STAC and RASTER API endpoints\nSTAC_API_URL = \"https://earth.gov/ghgcenter/api/stac\"\nRASTER_API_URL = \"https://earth.gov/ghgcenter/api/raster\"\n\n# Please use the collection name similar to the one used in STAC collection.\n# Name of the collection for OCO-2 GEOS Column CO₂ Concentrations. \ncollection_name = \"oco2geos-co2-daygrid-v10r\"\n\n\n# Fetching the collection from STAC collections using appropriate endpoint.\ncollection = requests.get(f\"{STAC_API_URL}/collections/{collection_name}\").json()\ncollection\n\nExamining the contents of our collection under the temporal variable, we see that the data is available from January 2015 to February 2022. By looking at the dashboard:time density, we can see that these observations are collected daily.\n\ndef get_item_count(collection_id):\n    count = 0\n    items_url = f\"{STAC_API_URL}/collections/{collection_id}/items\"\n\n    while True:\n        response = requests.get(items_url)\n\n        if not response.ok:\n            print(\"error getting items\")\n            exit()\n\n        stac = response.json()\n        count += int(stac[\"context\"].get(\"returned\", 0))\n        next = [link for link in stac[\"links\"] if link[\"rel\"] == \"next\"]\n\n        if not next:\n            break\n        items_url = next[0][\"href\"]\n\n    return count\n\n\n# Check total number of items available\nnumber_of_items = get_item_count(collection_name)\nitems = requests.get(f\"{STAC_API_URL}/collections/{collection_name}/items?limit={number_of_items}\").json()[\"features\"]\nprint(f\"Found {len(items)} items\")\n\n\n# Examining the first item in the collection\nitems[0]\n\nBelow, we enter minimum and maximum values to provide our upper and lower bounds in rescale_values.",
    "crumbs": [
      "Data Usage Notebooks",
      "Greenhouse Gas Concentrations",
      "OCO-2 GEOS Column CO₂ Concentrations"
    ]
  },
  {
    "objectID": "user_data_notebooks/oco2geos-co2-daygrid-v10r_User_Notebook.html#exploring-changes-in-column-averaged-xco₂-concentrations-levels-using-the-raster-api",
    "href": "user_data_notebooks/oco2geos-co2-daygrid-v10r_User_Notebook.html#exploring-changes-in-column-averaged-xco₂-concentrations-levels-using-the-raster-api",
    "title": "OCO-2 GEOS Column CO₂ Concentrations",
    "section": "Exploring Changes in Column-Averaged XCO₂ Concentrations Levels Using the Raster API",
    "text": "Exploring Changes in Column-Averaged XCO₂ Concentrations Levels Using the Raster API\nIn this notebook, we will explore the temporal impacts of CO₂ emissions. We will visualize the outputs on a map using folium.\n\n# To access the year value from each item more easily, this will let us query more explicitly by year and month (e.g., 2020-02)\nitems = {item[\"properties\"][\"datetime\"]: item for item in items} \nasset_name = \"xco2\" #fossil fuel\n\n\n# Fetching the min and max values for a specific item\nrescale_values = {\"max\":items[list(items.keys())[0]][\"assets\"][asset_name][\"raster:bands\"][0][\"histogram\"][\"max\"], \"min\":items[list(items.keys())[0]][\"assets\"][asset_name][\"raster:bands\"][0][\"histogram\"][\"min\"]}\n\nNow, we will pass the item id, collection name, and rescaling_factor to the Raster API endpoint. We will do this twice, once for 2022-02-08 and again for 2022-01-27, so that we can visualize each event independently.\n\ncolor_map = \"magma\"\noco2_1 = requests.get(\n    f\"{RASTER_API_URL}/stac/tilejson.json?collection={items[list(items.keys())[0]]['collection']}&item={items[list(items.keys())[0]]['id']}\"\n    f\"&assets={asset_name}\"\n    f\"&color_formula=gamma+r+1.05&colormap_name={color_map}\"\n    f\"&rescale={rescale_values['min']},{rescale_values['max']}\", \n).json()\noco2_1\n\n\noco2_2 = requests.get(\n    f\"{RASTER_API_URL}/stac/tilejson.json?collection={items[list(items.keys())[1]]['collection']}&item={items[list(items.keys())[1]]['id']}\"\n    f\"&assets={asset_name}\"\n    f\"&color_formula=gamma+r+1.05&colormap_name={color_map}\"\n    f\"&rescale={rescale_values['min']},{rescale_values['max']}\", \n).json()\noco2_2",
    "crumbs": [
      "Data Usage Notebooks",
      "Greenhouse Gas Concentrations",
      "OCO-2 GEOS Column CO₂ Concentrations"
    ]
  },
  {
    "objectID": "user_data_notebooks/oco2geos-co2-daygrid-v10r_User_Notebook.html#visualizing-daily-column-averaged-xco₂-concentrations",
    "href": "user_data_notebooks/oco2geos-co2-daygrid-v10r_User_Notebook.html#visualizing-daily-column-averaged-xco₂-concentrations",
    "title": "OCO-2 GEOS Column CO₂ Concentrations",
    "section": "Visualizing Daily Column-Averaged XCO₂ Concentrations",
    "text": "Visualizing Daily Column-Averaged XCO₂ Concentrations\n\n# Set initial zoom and center of map for XCO₂ Layer\n# Centre of map [latitude,longitude]\nmap_ = folium.plugins.DualMap(location=(34, -118), zoom_start=6)\n\n\nmap_layer_2020 = TileLayer(\n    tiles=oco2_1[\"tiles\"][0],\n    attr=\"GHG\",\n    opacity=0.5,\n)\nmap_layer_2020.add_to(map_.m1)\n\nmap_layer_2019 = TileLayer(\n    tiles=oco2_2[\"tiles\"][0],\n    attr=\"GHG\",\n    opacity=0.5,\n)\nmap_layer_2019.add_to(map_.m2)\n\n# visualising the map\nmap_",
    "crumbs": [
      "Data Usage Notebooks",
      "Greenhouse Gas Concentrations",
      "OCO-2 GEOS Column CO₂ Concentrations"
    ]
  },
  {
    "objectID": "user_data_notebooks/oco2geos-co2-daygrid-v10r_User_Notebook.html#visualizing-the-data-as-a-time-series",
    "href": "user_data_notebooks/oco2geos-co2-daygrid-v10r_User_Notebook.html#visualizing-the-data-as-a-time-series",
    "title": "OCO-2 GEOS Column CO₂ Concentrations",
    "section": "Visualizing the Data as a Time Series",
    "text": "Visualizing the Data as a Time Series\nWe can now explore the XCO₂ concentrations time series (January 1, 2015 - February 28, 2022) available for the Dallas, Texas area of the U.S. We can plot the data set using the code below:\n\nfig = plt.figure(figsize=(20, 10))\n\n\nplt.plot(\n    df[\"datetime\"],\n    df[\"max\"],\n    color=\"red\",\n    linestyle=\"-\",\n    linewidth=0.5,\n    label=\"CO₂ concentrations\",\n)\n\nplt.legend()\nplt.xlabel(\"Years\")\nplt.ylabel(\"CO2 concentrations ppm\")\nplt.title(\"CO₂ concentrations Values for Texas, Dallas (Jan 2015- Feb 2022)\")\n\n\nprint(items[2][\"properties\"][\"datetime\"])\n\n\noco2_3 = requests.get(\n    f\"{RASTER_API_URL}/stac/tilejson.json?collection={items[2]['collection']}&item={items[2]['id']}\"\n    f\"&assets={asset_name}\"\n    f\"&color_formula=gamma+r+1.05&colormap_name={color_map}\"\n    f\"&rescale={rescale_values['min']},{rescale_values['max']}\",\n).json()\noco2_3\n\n\n# Use bbox initial zoom and map\n# Set up a map located w/in event bounds\naoi_map_bbox = Map(\n    tiles=\"OpenStreetMap\",\n    location=[\n        30,-100\n    ],\n    zoom_start=6.8,\n)\n\nmap_layer = TileLayer(\n    tiles=oco2_3[\"tiles\"][0],\n    attr=\"GHG\", opacity = 0.7\n)\n\nmap_layer.add_to(aoi_map_bbox)\n\naoi_map_bbox",
    "crumbs": [
      "Data Usage Notebooks",
      "Greenhouse Gas Concentrations",
      "OCO-2 GEOS Column CO₂ Concentrations"
    ]
  },
  {
    "objectID": "user_data_notebooks/oco2geos-co2-daygrid-v10r_User_Notebook.html#summary",
    "href": "user_data_notebooks/oco2geos-co2-daygrid-v10r_User_Notebook.html#summary",
    "title": "OCO-2 GEOS Column CO₂ Concentrations",
    "section": "Summary",
    "text": "Summary\nIn this notebook, we have successfully explored, analyzed, and visualized the STAC collection for OCO-2 GEOS Column CO₂ Concentrations.\n\nInstall and import the necessary libraries\nFetch the collection from STAC collections using the appropriate endpoints\nCount the number of existing granules within the collection\nMap and compare the Column-Averaged XCO₂ Concentrations Levels for two distinctive months/years\nGenerate zonal statistics for the area of interest (AOI)\nVisualizing the Data as a Time Series\n\nIf you have any questions regarding this user notebook, please contact us using the feedback form.",
    "crumbs": [
      "Data Usage Notebooks",
      "Greenhouse Gas Concentrations",
      "OCO-2 GEOS Column CO₂ Concentrations"
    ]
  },
  {
    "objectID": "user_data_notebooks/influx-testbed-ghg-concentrations_User_Notebook.html",
    "href": "user_data_notebooks/influx-testbed-ghg-concentrations_User_Notebook.html",
    "title": "Carbon Dioxide and Methane Concentrations from the Indianapolis Flux Experiment (INFLUX)",
    "section": "",
    "text": "Identify available dates and temporal frequency of observations for the given data. The collection processed in this notebook is the Atmospheric concentrations of carbon dioxide (CO₂) and methane (CH₄) collected at NIST Urban Test Bed tower sites in the Northeastern U.S.\nVisualize the time series data",
    "crumbs": [
      "Data Usage Notebooks",
      "Greenhouse Gas Concentrations",
      "Carbon Dioxide and Methane Concentrations from the Indianapolis Flux Experiment (INFLUX)"
    ]
  },
  {
    "objectID": "user_data_notebooks/influx-testbed-ghg-concentrations_User_Notebook.html#approach",
    "href": "user_data_notebooks/influx-testbed-ghg-concentrations_User_Notebook.html#approach",
    "title": "Carbon Dioxide and Methane Concentrations from the Indianapolis Flux Experiment (INFLUX)",
    "section": "",
    "text": "Identify available dates and temporal frequency of observations for the given data. The collection processed in this notebook is the Atmospheric concentrations of carbon dioxide (CO₂) and methane (CH₄) collected at NIST Urban Test Bed tower sites in the Northeastern U.S.\nVisualize the time series data",
    "crumbs": [
      "Data Usage Notebooks",
      "Greenhouse Gas Concentrations",
      "Carbon Dioxide and Methane Concentrations from the Indianapolis Flux Experiment (INFLUX)"
    ]
  },
  {
    "objectID": "user_data_notebooks/influx-testbed-ghg-concentrations_User_Notebook.html#about-the-data",
    "href": "user_data_notebooks/influx-testbed-ghg-concentrations_User_Notebook.html#about-the-data",
    "title": "Carbon Dioxide and Methane Concentrations from the Indianapolis Flux Experiment (INFLUX)",
    "section": "About the Data",
    "text": "About the Data\nNIST is engaged in research to improve measurement of greenhouse gas emissions in areas containing multiple emission sources and sinks, such as site01ies. NIST’s objective is to develop measurement tools supporting independent means to increase the accuracy of greenhouse gas emissions data at urban and regional geospatial scales. NIST has established three test beds in U.S. site01ies to develop and evaluate the performance of advanced measurement capabilities for emissions independent of their origin. Located in Indianapolis, Indiana, the Los Angeles air basin of California, and the U.S. Northeast corridor (beginning with the Baltimore/Washington D.C. region), the test beds have been selected for their varying meteorology, terrain and emissions characteristics. These test beds will serve as a means to independently diagnose the accuracy of emissions data obtained directly from emission or uptake sources.\nFor more information regarding this dataset, please visit the Carbon Dioxide and Methane Concentrations from the Northeast Corridor (NEC) Urban Test Bed data overview page.",
    "crumbs": [
      "Data Usage Notebooks",
      "Greenhouse Gas Concentrations",
      "Carbon Dioxide and Methane Concentrations from the Indianapolis Flux Experiment (INFLUX)"
    ]
  },
  {
    "objectID": "user_data_notebooks/influx-testbed-ghg-concentrations_User_Notebook.html#querying-the-feature-vector-api",
    "href": "user_data_notebooks/influx-testbed-ghg-concentrations_User_Notebook.html#querying-the-feature-vector-api",
    "title": "Carbon Dioxide and Methane Concentrations from the Indianapolis Flux Experiment (INFLUX)",
    "section": "Querying the Feature Vector API",
    "text": "Querying the Feature Vector API\nFirst, we are going to import the required libraries. Once imported, they allow better executing a query in the GHG Center Feature Vector Application Programming Interface (API) where the items for this collection are stored.\n\nFEATURE_API_URL=\"https://earth.gov/ghgcenter/api/features\"\n\n\n# Function to fetch CSV data for a station with a limit parameter\ndef get_station_data_csv(station_code, gas_type, frequency, elevation_m, limit=100000):\n    # Use the ?f=csv and limit query to get more rows\n    url = f\"https://earth.gov/ghgcenter/api/features/collections/public.nist_flux_in_{station_code}_{gas_type}_{frequency}_concentrations/items?f=csv&elevation_m={elevation_m}&limit={limit}\"\n    print(url)\n    try:\n        response = requests.get(url)\n        \n        # Check if the response is successful\n        if response.status_code != 200:\n            print(f\"Failed to fetch data for {station_code}. Status code: {response.status_code}\")\n            return pd.DataFrame()\n\n        # Check if the content type is CSV\n        content_type = response.headers.get('Content-Type')\n        if 'text/csv' not in content_type:\n            print(f\"Unexpected content type for {station_code}: {content_type}\")\n            print(\"Response content:\", response.text)\n            return pd.DataFrame()\n\n        # Read the CSV content into a pandas DataFrame\n        csv_data = StringIO(response.text)\n        return pd.read_csv(csv_data)\n    \n    except requests.exceptions.RequestException as e:\n        print(f\"Request failed: {e}\")\n        return pd.DataFrame()",
    "crumbs": [
      "Data Usage Notebooks",
      "Greenhouse Gas Concentrations",
      "Carbon Dioxide and Methane Concentrations from the Indianapolis Flux Experiment (INFLUX)"
    ]
  },
  {
    "objectID": "user_data_notebooks/influx-testbed-ghg-concentrations_User_Notebook.html#visualizing-the-ch₄-data-for-two-nec-stations",
    "href": "user_data_notebooks/influx-testbed-ghg-concentrations_User_Notebook.html#visualizing-the-ch₄-data-for-two-nec-stations",
    "title": "Carbon Dioxide and Methane Concentrations from the Indianapolis Flux Experiment (INFLUX)",
    "section": "Visualizing the CH₄ data for two NEC stations",
    "text": "Visualizing the CH₄ data for two NEC stations\n\n# Get station name and elevation from metdata dataframe\n# Fetch data for site01 (elevation 256) and site09 (elevation 277), using limit=10000\n# ch4/co2 select the ghg \n\nsite01_data = get_station_data_csv('site01', 'ch4', 'hourly', 256,limit=10000)\nsite09_data = get_station_data_csv('site09', 'ch4', 'hourly', 277,limit=10000)\n\n# Check if data was successfully retrieved before proceeding\nif site01_data.empty or site09_data.empty:\n    print(\"No data available for one or both stations. Exiting.\")\nelse:\n    # Convert the 'datetime' column to datetime for plotting\n    site01_data['datetime'] = pd.to_datetime(site01_data['datetime'], format='%Y-%m-%dT%H:%M:%SZ')\n    site09_data['datetime'] = pd.to_datetime(site09_data['datetime'], format='%Y-%m-%dT%H:%M:%SZ')\n\n    # Plot the data\n    plt.figure(figsize=(10, 6))\n    plt.plot(site01_data['datetime'], site01_data['value'], label='site01 (256m)', color='blue', marker='o')\n    plt.plot(site09_data['datetime'], site09_data['value'], label='site09 (277m)', color='green', marker='o')\n\n    plt.title('Methane (CH₄) Hourly Concentrations Over Time for site01 and site09 Stations')\n    plt.xlabel('Time')\n    plt.ylabel('CH₄ Concentration (ppb)')\n    plt.legend()\n    plt.grid(True)\n\n    # Show plot\n    plt.show()\n\nhttps://earth.gov/ghgcenter/api/features/collections/public.nist_flux_in_site01_ch4_hourly_concentrations/items?f=csv&elevation_m=256&limit=10000\nhttps://earth.gov/ghgcenter/api/features/collections/public.nist_flux_in_site09_ch4_hourly_concentrations/items?f=csv&elevation_m=277&limit=10000",
    "crumbs": [
      "Data Usage Notebooks",
      "Greenhouse Gas Concentrations",
      "Carbon Dioxide and Methane Concentrations from the Indianapolis Flux Experiment (INFLUX)"
    ]
  },
  {
    "objectID": "user_data_notebooks/emit-ch4plume-v1_User_Notebook.html",
    "href": "user_data_notebooks/emit-ch4plume-v1_User_Notebook.html",
    "title": "Utilizing NASA’s EMIT Instrument to Monitor Methane Plumes from Point Source Emitters",
    "section": "",
    "text": "You can launch this notebook in the US GHG Center JupyterHub by clicking the link below. If you are a new user, you should first sign up for the hub by filling out this request form and providing the required information.\nAccess the EMIT Methane Point Source Plume Complexes notebook in the US GHG Center JupyterHub.",
    "crumbs": [
      "Data Usage Notebooks",
      "Large Emissions Events",
      "Utilizing NASA's EMIT Instrument to Monitor Methane Plumes from Point Source Emitters"
    ]
  },
  {
    "objectID": "user_data_notebooks/emit-ch4plume-v1_User_Notebook.html#access-this-notebook",
    "href": "user_data_notebooks/emit-ch4plume-v1_User_Notebook.html#access-this-notebook",
    "title": "Utilizing NASA’s EMIT Instrument to Monitor Methane Plumes from Point Source Emitters",
    "section": "",
    "text": "You can launch this notebook in the US GHG Center JupyterHub by clicking the link below. If you are a new user, you should first sign up for the hub by filling out this request form and providing the required information.\nAccess the EMIT Methane Point Source Plume Complexes notebook in the US GHG Center JupyterHub.",
    "crumbs": [
      "Data Usage Notebooks",
      "Large Emissions Events",
      "Utilizing NASA's EMIT Instrument to Monitor Methane Plumes from Point Source Emitters"
    ]
  },
  {
    "objectID": "user_data_notebooks/emit-ch4plume-v1_User_Notebook.html#table-of-contents",
    "href": "user_data_notebooks/emit-ch4plume-v1_User_Notebook.html#table-of-contents",
    "title": "Utilizing NASA’s EMIT Instrument to Monitor Methane Plumes from Point Source Emitters",
    "section": "Table of Contents",
    "text": "Table of Contents\n\nData Summary and Application\nApproach\nAbout the Data\nInstall the Required Libraries\nQuery the STAC API\nMap Out Selected Tiles\nCalculate Zonal Statistics\nSummary",
    "crumbs": [
      "Data Usage Notebooks",
      "Large Emissions Events",
      "Utilizing NASA's EMIT Instrument to Monitor Methane Plumes from Point Source Emitters"
    ]
  },
  {
    "objectID": "user_data_notebooks/emit-ch4plume-v1_User_Notebook.html#data-summary-and-application",
    "href": "user_data_notebooks/emit-ch4plume-v1_User_Notebook.html#data-summary-and-application",
    "title": "Utilizing NASA’s EMIT Instrument to Monitor Methane Plumes from Point Source Emitters",
    "section": "Data Summary and Application",
    "text": "Data Summary and Application\n\nSpatial coverage: 52°N to 52°S latitude within target mask\nSpatial resolution: 60 m\nTemporal extent: August 1, 2022 - Ongoing\nTemporal resolution: Variable\nUnit: Parts per million meter (ppm-m)\nUtility: Methane Emissions, Plume Detection, Climate Monitoring\n\nFor more, visit the EMIT Methane Point Source Plume Complexes data overview page.",
    "crumbs": [
      "Data Usage Notebooks",
      "Large Emissions Events",
      "Utilizing NASA's EMIT Instrument to Monitor Methane Plumes from Point Source Emitters"
    ]
  },
  {
    "objectID": "user_data_notebooks/emit-ch4plume-v1_User_Notebook.html#approach",
    "href": "user_data_notebooks/emit-ch4plume-v1_User_Notebook.html#approach",
    "title": "Utilizing NASA’s EMIT Instrument to Monitor Methane Plumes from Point Source Emitters",
    "section": "Approach",
    "text": "Approach\n\nIdentify available dates and temporal frequency of observations for the given collection using the GHGC API /stac endpoint. The collection processed in this notebook is the Earth Surface Mineral Dust Source Investigation (EMIT) methane emission plumes data product.\nPass the STAC item into the raster API /stac/tilejson.json endpoint.\nUsing folium.Map, visualize the plumes.\nAfter the visualization, perform zonal statistics for a given polygon.",
    "crumbs": [
      "Data Usage Notebooks",
      "Large Emissions Events",
      "Utilizing NASA's EMIT Instrument to Monitor Methane Plumes from Point Source Emitters"
    ]
  },
  {
    "objectID": "user_data_notebooks/emit-ch4plume-v1_User_Notebook.html#about-the-data",
    "href": "user_data_notebooks/emit-ch4plume-v1_User_Notebook.html#about-the-data",
    "title": "Utilizing NASA’s EMIT Instrument to Monitor Methane Plumes from Point Source Emitters",
    "section": "About the Data",
    "text": "About the Data\nThe Earth Surface Mineral Dust Source Investigation (EMIT) instrument builds upon NASA’s long history of developing advanced imaging spectrometers for new science and applications. EMIT launched to the International Space Station (ISS) on July 14, 2022. The data shows high-confidence research grade methane plumes from point source emitters - updated as they are identified - in keeping with Jet Propulsion Laboratory (JPL) Open Science and Open Data policy.\nLarge methane emissions, typically referred to as point source emissions, represent a significant proportion of total methane emissions from the production, transport, and processing of oil and natural gas, landfills, and other sources. By measuring the spectral fingerprint of methane, EMIT can map areas of high methane concentration over background levels in the atmosphere, identifying plume complexes, and estimating the methane enhancements.\nFor more information regarding this dataset, please visit the EMIT Methane Point Source Plume Complexes data overview page.",
    "crumbs": [
      "Data Usage Notebooks",
      "Large Emissions Events",
      "Utilizing NASA's EMIT Instrument to Monitor Methane Plumes from Point Source Emitters"
    ]
  },
  {
    "objectID": "user_data_notebooks/emit-ch4plume-v1_User_Notebook.html#query-the-stac-api",
    "href": "user_data_notebooks/emit-ch4plume-v1_User_Notebook.html#query-the-stac-api",
    "title": "Utilizing NASA’s EMIT Instrument to Monitor Methane Plumes from Point Source Emitters",
    "section": "Query the STAC API",
    "text": "Query the STAC API\nFirst, you need to import the required libraries. Once imported, they allow better execution of a query in the GHG Center Spatio Temporal Asset Catalog (STAC) Application Programming Interface (API) where the granules for this collection are stored. You will learn the functionality of each library throughout the notebook.\n\n# Provide the STAC and RASTER API endpoints\n# The endpoint is referring to a location within the API that executes a request on a data collection nesting on the server.\n\n# The STAC API is a catalog of all the existing data collections that are stored in the GHG Center.\nSTAC_API_URL = \"https://earth.gov/ghgcenter/api/stac/\"\n\n# The RASTER API is used to fetch collections for visualization\nRASTER_API_URL = \"https://earth.gov/ghgcenter/api/raster/\"\n\nSTAC API Collection Names\nNow, you must fetch the dataset from the STAC API by defining its associated STAC API collection ID as a variable. The collection ID, also known as the collection name, for the EMIT Methane Point Source Plume Complexes dataset is emit-ch4plume-v1\n\n# The collection name is used to fetch the dataset from the STAC API. First, we define the collection name as a variable\n# Name of the collection for methane emission plumes \ncollection_name = \"emit-ch4plume-v1\"\n\n\n# Fetch the collection from the STAC API using the appropriate endpoint\n# The 'requests' library allows a HTTP request possible\ncollection = requests.get(f\"{STAC_API_URL}/collections/{collection_name}\").json()\n\n# Print the properties of the collection in a table\n# Adjust display settings\npd.set_option('display.max_colwidth', None)  # Set maximum column width to \"None\" to prevent cutting off text\n\n# Extract the relevant information about the collection\ncollection_info = {\n    \"Title\": collection.get(\"title\", \"N/A\"), # Extract the title of the collection \n    \"Description\": collection.get(\"description\", \"N/A\"), # Extract the dataset description\n    \"Temporal Extent\": collection.get(\"extent\", {}).get(\"temporal\", {}).get(\"interval\", \"N/A\"), # Extract the temporal coverage of the collection\n    \"Spatial Extent\": collection.get(\"extent\", {}).get(\"spatial\", {}).get(\"bbox\", \"N/A\"), # Extract the spatial coverage of the collection\n}\n\n# Convert the derived information into a DataFrame format\nproperties_table = pd.DataFrame(list(collection_info.items()), columns=[\"Collection Summary\", \"\"])\n\n# Display the properties in a table\ncollection_summary = properties_table.style.set_properties(**{'text-align': 'left'}) \\\n                                           .set_table_styles([\n    {\n        'selector': 'th.col0, td.col0',    # Select the first column\n        'props': [('min-width', '200px'),  # Set a minimum width\n                  ('text-align', 'left')]  # Align text to the left\n    },\n    {\n        'selector': 'td.col1',             # Select the second column\n        'props': [('text-align', 'left')]  # Align text to the left\n    }\n])\n\n# Print the collection summary table\ncollection_summary\n\nNext, you will examine the contents of the collection under the temporal variable. You’ll see that the data is available since August 2022. Looking at the dashboard: time density, you can see that observations are conducted daily and non-periodically (i.e., there are plumes emissions for multiple places on the same dates).\n\n# Create a function that would search for data collection in the US GHG Center STAC API\n\n# First, we need to define the function\n# The name of the function is \"get_item_count\" \n# The argument that will be passed to the defined function is \"collection_id\"\ndef get_item_count(collection_id):\n\n    # Set a counter for the number of items existing in the collection \n    count = 0 \n\n    # Define the path to retrieve the granules (items) of the collection of interest in the STAC API\n    items_url = f\"{STAC_API_URL}/collections/{collection_id}/items\" \n\n    # Run a while loop to make HTTP requests until there are no more URLs associated with the collection in the STAC API\n    while True:\n\n        # Retrieve information about the granules by sending a \"get\" request to the STAC API using the defined collection path \n        response = requests.get(items_url) \n\n        # If the items do not exist, print an error message and quit the loop\n        if not response.ok:\n            print(\"error getting items\")\n            exit()\n\n        # Return the results of the HTTP response as JSON\n        stac = response.json()\n\n        # Increase the \"count\" by the number of items (granules) returned in the response\n        count += int(stac[\"context\"].get(\"returned\", 0))\n\n        # Retrieve information about the next URL associated with the collection in the STAC API (if applicable)\n        next = [link for link in stac[\"links\"] if link[\"rel\"] == \"next\"]\n\n        # Exit the loop if there are no other URLs\n        if not next:\n            break\n        \n        # Ensure the information gathered by other STAC API links associated with the collection are added to the original path\n        # \"href\" is the identifier for each of the tiles stored in the STAC API\n        items_url = next[0][\"href\"]\n\n    # Return the information about the total number of granules found associated with the collection\n    return count\n\n\n# Apply the function created above \"get_item_count\" to the collection\nnumber_of_items = get_item_count(collection_name)\n\n# Get the information about the number of granules found in the collection\nitems = requests.get(f\"{STAC_API_URL}/collections/{collection_name}/items?limit={number_of_items}\").json()[\"features\"]\n\n# Print the total number of items (granules) found\nprint(f\"Found {len(items)} observations\")\n\n# Sort the items based on their date-time attribute\nitems_sorted = sorted(items, key=lambda x: x[\"properties\"][\"datetime\"])\n\n# Create an empty list\ntable_data = []\n# Extract the ID and date-time information for each granule and add them to the list\n# By default, only the first 5 items in the collection are extracted to be displayed in the table. \n# To see the date-time of all existing granules in this collection, remove \"5\" from \"item_sorted[:5]\" in the line below. \nfor item in items_sorted[:5]:\n    table_data.append([item['id'], item['properties']['datetime']])\n\n# Define the table headers\nheaders = [\"Item ID\", \"Date-Time\"]\n\nprint(\"Below you see the first 5 items in the collection, along with their item IDs and corresponding Start Date-Time.\")\n\n# Print the table using tabulate\nprint(tabulate(table_data, headers=headers, tablefmt=\"fancy_grid\"))\n\n\n# Examine the first item in the collection\n# Keep in mind that a list starts from 0, 1, 2... therefore items[0] refers to the first item (granule) in the list/collection\nitems_sorted[0]",
    "crumbs": [
      "Data Usage Notebooks",
      "Large Emissions Events",
      "Utilizing NASA's EMIT Instrument to Monitor Methane Plumes from Point Source Emitters"
    ]
  },
  {
    "objectID": "user_data_notebooks/emit-ch4plume-v1_User_Notebook.html#map-out-selected-tiles",
    "href": "user_data_notebooks/emit-ch4plume-v1_User_Notebook.html#map-out-selected-tiles",
    "title": "Utilizing NASA’s EMIT Instrument to Monitor Methane Plumes from Point Source Emitters",
    "section": "Map Out Selected Tiles",
    "text": "Map Out Selected Tiles\nYou will now explore global methane emission plumes from point sources and visualize the results on a map using folium.\n\n# Once again, apply the function created above \"get_item_count\" to the Air-Sea CO2 Flux ECCO-Darwin collection\n# This step allows retrieving the number of granules “observations” in the collection.\nnumber_of_items = get_item_count(collection_name)\nitems = requests.get(f\"{STAC_API_URL}/collections/{collection_name}/items?limit={number_of_items}\").json()[\"features\"]\n\n\n# Next, you need to create a dictionary where the \"id\" field of each item in the collection are queried more explicitly\nplume_complexes = {items[\"id\"]: items for items in items} \n\n\n# Next, you need to specify the asset name for this collection.\n# The asset name refers to the raster band containing the pixel values for the parameter of interest.\n# For the case of the EMIT Methane Point Source collection, the parameter of interest is “ch4-plume-emissions”.\nasset_name = \"ch4-plume-emissions\"\n\nBelow, you will enter the minimum and maximum values to provide our upper and lower bounds in the rescale_values.\n\n# Fetching the min and max values for a specific item\nrescale_values = {\"max\":plume_complexes[list(plume_complexes.keys())[0]][\"assets\"][asset_name][\"raster:bands\"][0][\"histogram\"][\"max\"], \"min\":plume_complexes[list(plume_complexes.keys())[0]][\"assets\"][asset_name][\"raster:bands\"][0][\"histogram\"][\"min\"]}\n\nNow, you will pass the item id, collection name, asset name, and the rescaling factor to the Raster API endpoint.\n\n# Select the item ID which you want to visualize. Item ID is in the format yyyymmdd followed by the timestamp. This ID can be extracted from the COG name as well.\n# To browse and select other tiles in the collection, please visit https://search.earthdata.nasa.gov/search/granules?p=C2748088093-LPCLOUD&pg[0][v]=f&pg[0][gsk]=-start_date&q=emit%20plume&tl=1694622854.77!3!!\n\n# You need to copy the entire granule nomenclature \nitem_id = \"EMIT_L2B_CH4PLM_001_20230418T200118_000829\"\n\n# Choose a color map for displaying the first observation (event)\n# Please refer to matplotlib library if you'd prefer to choose a different color ramp.\n# For more information on Colormaps in Matplotlib, please visit https://matplotlib.org/stable/users/explain/colors/colormaps.html\ncolor_map = \"magma\"\n\n# Make a GET request to retrieve information for the selected tile defined in \"item_id\"\nmethane_plume_tile = requests.get(\n    \n    # Pass the collection name and item ID directly from the first item in the list\n    f\"{RASTER_API_URL}/collections/{plume_complexes[item_id]['collection']}/items/{plume_complexes[item_id]['id']}/tilejson.json?\"\n    \n    # Pass the asset name\n    f\"&assets={asset_name}\"\n    \n    # Pass the color formula and colormap for custom visualization\n    f\"&color_formula=gamma+r+1.05&colormap_name={color_map}\"\n    \n    # Pass the minimum and maximum values for rescaling \n    f\"&rescale={rescale_values['min']},{rescale_values['max']}\", \n    \n# Return the response in JSON format\n).json()\n\n# Print the properties of the retrieved granule to the console\nmethane_plume_tile\n\n\n# Set a colormap for the granule\n# Please refer to matplotlib library if you'd prefer choosing a different color ramp (https://matplotlib.org/stable/users/explain/colors/colormaps.html)\ncolormap = \"magma\" \n\n\n# Defining the breaks in the colormap \ncolor_map = cm.LinearColormap(colors = ['#310597', '#4C02A1', '#6600A7', '#7E03A8', '#9511A1', '#AA2395', '#BC3587', '#CC4778', '#DA5A6A', '#E66C5C', '#F0804E', '#F89540','#FDAC33', '#FDC527', '#F8DF25'], vmin = 0, vmax = 1500 )\n\n\n# Add an appropriate caption, in this case it would be Parts per million meter\ncolor_map.caption = 'ppm-m'\n\n# Set initial zoom and center of map for plume Layer\nmap_ = folium.Map(location=(methane_plume_tile[\"center\"][1], methane_plume_tile[\"center\"][0]), zoom_start=14, tiles=None, tooltip = 'test tool tip')\nfolium.TileLayer(tiles='https://server.arcgisonline.com/ArcGIS/rest/services/World_Imagery/MapServer/tile/{z}/{y}/{x}.png', name='ESRI World Imagery', attr='Tiles &copy; Esri &mdash; Source: Esri, i-cubed, USDA, USGS, AEX, GeoEye, Getmapping, Aerogrid, IGN, IGP, UPR-EGP, and the GIS User Community',overlay='True').add_to(map_)\n\n\n# Use the 'TileLayer' library to display the raster layer, add an appropriate caption, and adjust the transparency of the layer on the map\nmap_layer = TileLayer(\n    tiles=methane_plume_tile[\"tiles\"][0], # Path to retrieve the tile\n    name='Plume Complex Landfill',\n    overlay='True', # The layer can be overlaid on the map\n    attr=\"GHG\", # Set the attribution \n    opacity=1, # Adjust the transparency of the layer\n)\nmap_layer.add_to(map_)\n\n\n# Adjust map elements \nfolium.LayerControl(collapsed=False, position='bottomleft').add_to(map_)\nmap_.add_child(color_map)\nsvg_style = '&lt;style&gt;svg#legend {font-size: 14px; background-color: white;}&lt;/style&gt;'\nmap_.get_root().header.add_child(folium.Element(svg_style))\n\n\n# Visualizing the map\nmap_",
    "crumbs": [
      "Data Usage Notebooks",
      "Large Emissions Events",
      "Utilizing NASA's EMIT Instrument to Monitor Methane Plumes from Point Source Emitters"
    ]
  },
  {
    "objectID": "user_data_notebooks/emit-ch4plume-v1_User_Notebook.html#summary",
    "href": "user_data_notebooks/emit-ch4plume-v1_User_Notebook.html#summary",
    "title": "Utilizing NASA’s EMIT Instrument to Monitor Methane Plumes from Point Source Emitters",
    "section": "Summary",
    "text": "Summary\nIn this notebook we have successfully completed the following steps for the STAC collection for the EMIT Methane Point Source Plume Complexes dataset: 1. Install and import the necessary libraries 2. Fetch the collection from STAC collections using the appropriate endpoints 3. Count the number of existing granules within the collection 4. Map the methane emission plumes 5. Generate statistics for the area of interest (AOI)\nIf you have any questions regarding this user notebook, please contact us using the feedback form.",
    "crumbs": [
      "Data Usage Notebooks",
      "Large Emissions Events",
      "Utilizing NASA's EMIT Instrument to Monitor Methane Plumes from Point Source Emitters"
    ]
  },
  {
    "objectID": "user_data_notebooks/eccodarwin-co2flux-monthgrid-v5_User_Notebook.html",
    "href": "user_data_notebooks/eccodarwin-co2flux-monthgrid-v5_User_Notebook.html",
    "title": "Utilizing the Air-Sea CO₂ Flux ECCO-Darwin Model to Visualize CO₂ Exchange and Dynamics Between the Atmosphere and the Ocean",
    "section": "",
    "text": "You can launch this notebook in the US GHG Center JupyterHub by clicking the link below. If you are a new user, you should first sign up for the hub by filling out this request form and providing the required information.\nAccess the Air-Sea CO₂ Flux, ECCO-Darwin Model notebook in the US GHG Center JupyterHub.",
    "crumbs": [
      "Data Usage Notebooks",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "Utilizing the Air-Sea CO₂ Flux ECCO-Darwin Model to Visualize CO₂ Exchange and Dynamics Between the Atmosphere and the Ocean"
    ]
  },
  {
    "objectID": "user_data_notebooks/eccodarwin-co2flux-monthgrid-v5_User_Notebook.html#access-this-notebook",
    "href": "user_data_notebooks/eccodarwin-co2flux-monthgrid-v5_User_Notebook.html#access-this-notebook",
    "title": "Utilizing the Air-Sea CO₂ Flux ECCO-Darwin Model to Visualize CO₂ Exchange and Dynamics Between the Atmosphere and the Ocean",
    "section": "",
    "text": "You can launch this notebook in the US GHG Center JupyterHub by clicking the link below. If you are a new user, you should first sign up for the hub by filling out this request form and providing the required information.\nAccess the Air-Sea CO₂ Flux, ECCO-Darwin Model notebook in the US GHG Center JupyterHub.",
    "crumbs": [
      "Data Usage Notebooks",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "Utilizing the Air-Sea CO₂ Flux ECCO-Darwin Model to Visualize CO₂ Exchange and Dynamics Between the Atmosphere and the Ocean"
    ]
  },
  {
    "objectID": "user_data_notebooks/eccodarwin-co2flux-monthgrid-v5_User_Notebook.html#table-of-contents",
    "href": "user_data_notebooks/eccodarwin-co2flux-monthgrid-v5_User_Notebook.html#table-of-contents",
    "title": "Utilizing the Air-Sea CO₂ Flux ECCO-Darwin Model to Visualize CO₂ Exchange and Dynamics Between the Atmosphere and the Ocean",
    "section": "Table of Contents",
    "text": "Table of Contents\n\nData Summary and Application\nApproach\nAbout the Data\nInstall the Required Libraries\nQuery the STAC API\nVisual Comparison Across Time Periods\nMap Out Selected Tiles\nCalculate Zonal Statistics\nTime-Series Analysis\nSummary",
    "crumbs": [
      "Data Usage Notebooks",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "Utilizing the Air-Sea CO₂ Flux ECCO-Darwin Model to Visualize CO₂ Exchange and Dynamics Between the Atmosphere and the Ocean"
    ]
  },
  {
    "objectID": "user_data_notebooks/eccodarwin-co2flux-monthgrid-v5_User_Notebook.html#data-summary-and-application",
    "href": "user_data_notebooks/eccodarwin-co2flux-monthgrid-v5_User_Notebook.html#data-summary-and-application",
    "title": "Utilizing the Air-Sea CO₂ Flux ECCO-Darwin Model to Visualize CO₂ Exchange and Dynamics Between the Atmosphere and the Ocean",
    "section": "Data Summary and Application",
    "text": "Data Summary and Application\n\nSpatial coverage: Global\nSpatial resolution: 1/3°\nTemporal extent: January 2020 - December 2022\nTemporal resolution: Monthly\nUnit: Millimoles of CO₂ per meter squared per second\nUtility: Climate Research, Oceanography, Carbon Stock Monitoring\n\nFor more, visit the Air-Sea CO₂ Flux ECCO-Darwin Model data overview page.",
    "crumbs": [
      "Data Usage Notebooks",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "Utilizing the Air-Sea CO₂ Flux ECCO-Darwin Model to Visualize CO₂ Exchange and Dynamics Between the Atmosphere and the Ocean"
    ]
  },
  {
    "objectID": "user_data_notebooks/eccodarwin-co2flux-monthgrid-v5_User_Notebook.html#approach",
    "href": "user_data_notebooks/eccodarwin-co2flux-monthgrid-v5_User_Notebook.html#approach",
    "title": "Utilizing the Air-Sea CO₂ Flux ECCO-Darwin Model to Visualize CO₂ Exchange and Dynamics Between the Atmosphere and the Ocean",
    "section": "Approach",
    "text": "Approach\nDuring this notebook, you will:\n\nIdentify available dates and temporal frequency of observations for a given collection using the GHGC API /stac endpoint\nPass the STAC item into the raster API /stac/tilejson.json endpoint\nUsing folium.plugins.DualMap, you will visualize two tiles (side-by-side), allowing comparison of time points\nAfter the visualization, you will perform zonal statistics for a given location",
    "crumbs": [
      "Data Usage Notebooks",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "Utilizing the Air-Sea CO₂ Flux ECCO-Darwin Model to Visualize CO₂ Exchange and Dynamics Between the Atmosphere and the Ocean"
    ]
  },
  {
    "objectID": "user_data_notebooks/eccodarwin-co2flux-monthgrid-v5_User_Notebook.html#about-the-data",
    "href": "user_data_notebooks/eccodarwin-co2flux-monthgrid-v5_User_Notebook.html#about-the-data",
    "title": "Utilizing the Air-Sea CO₂ Flux ECCO-Darwin Model to Visualize CO₂ Exchange and Dynamics Between the Atmosphere and the Ocean",
    "section": "About the Data",
    "text": "About the Data\nThe ocean is a major sink for atmospheric carbon dioxide (CO₂), largely due to the presence of phytoplankton that use the CO₂ to grow. Studies have shown that global ocean CO₂ uptake has increased over recent decades, however, there is uncertainty in the various mechanisms that affect ocean CO₂ flux and storage and how the ocean carbon sink will respond to future climate change.\nBecause CO₂ fluxes can vary significantly across space and time, combined with deficiencies in ocean and atmosphere CO₂ observations, there is a need for models that can thoroughly represent these processes. Ocean biogeochemical models (OBMs) can resolve the physical and biogeochemical mechanisms contributing to spatial and temporal variations in air-sea CO₂ fluxes but previous OBMs do not integrate observations to improve model accuracy and have not been able to operate on the seasonal and multi-decadal timescales needed to adequately characterize these processes.\nThe ECCO-Darwin model is an OBM that assimilates Estimating the Circulation and Climate of the Ocean (ECCO) consortium ocean circulation estimates and biogeochemical processes from the Massachusetts Institute of Technology (MIT) Darwin Project. A pilot study using ECCO-Darwin was completed by Brix et al. (2015) however, an improved version of the model was developed by Carroll et al. (2020) in which issues present in the first model were addressed using data assimilation and adjustments were made to initial conditions and biogeochemical parameters. The updated ECCO-Darwin model was compared with interpolation-based products to estimate surface ocean partial pressure (pCO2) and air-sea CO₂ flux. This dataset contains the gridded global, monthly mean air-sea CO₂ fluxes from version 5 of the ECCO-Darwin model.\nThe data available in the US GHG Center hub are available at ~1/3° horizontal resolution at the equator (~18 km at high latitudes) from January 2020 through December 2022. For more information regarding this dataset, please visit the Air-Sea CO₂ Flux ECCO-Darwin Model data overview page.",
    "crumbs": [
      "Data Usage Notebooks",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "Utilizing the Air-Sea CO₂ Flux ECCO-Darwin Model to Visualize CO₂ Exchange and Dynamics Between the Atmosphere and the Ocean"
    ]
  },
  {
    "objectID": "user_data_notebooks/eccodarwin-co2flux-monthgrid-v5_User_Notebook.html#query-the-stac-api",
    "href": "user_data_notebooks/eccodarwin-co2flux-monthgrid-v5_User_Notebook.html#query-the-stac-api",
    "title": "Utilizing the Air-Sea CO₂ Flux ECCO-Darwin Model to Visualize CO₂ Exchange and Dynamics Between the Atmosphere and the Ocean",
    "section": "Query the STAC API",
    "text": "Query the STAC API\nFirst, you need to import the required libraries. Once imported, they allow better execution of a query in the GHG Center Spatio Temporal Asset Catalog (STAC) Application Programming Interface (API) where the granules for this collection are stored. You will learn the functionality of each library throughout the notebook.\n\n# Provide the STAC and RASTER API endpoints\n# The endpoint refers to a location within the API that executes a request on a data collection nesting on the server.\n\n# The STAC API is a catalog of all the existing data collections stored in the GHG Center.\nSTAC_API_URL = \"https://earth.gov/ghgcenter/api/stac/\"\n\n# The RASTER API is used to fetch collections for visualization.\nRASTER_API_URL = \"https://earth.gov/ghgcenter/api/raster/\"\n\nSTAC API Collection Names\nNow, you must fetch the dataset from the STAC API by defining its associated STAC API collection ID as a variable. The collection ID, also known as the collection name, for the Air-Sea CO₂ Flux ECCO-Darwin Model dataset is eccodarwin-co2flux-monthgrid-v5\n\n# The collection name is used to fetch the dataset from the STAC API. First, you must define the collection name as a variable. \n# The collection name in the STAC API\ncollection_name = \"eccodarwin-co2flux-monthgrid-v5\"\n\n# Fetch the collection from the STAC API using the appropriate endpoint\n# The 'requests' library allows a HTTP request possible\ncollection = requests.get(f\"{STAC_API_URL}/collections/{collection_name}\").json()\n\n# Print the properties of the collection in a table\n# Adjust display settings\npd.set_option('display.max_colwidth', None)  # Set maximum column width to \"None\" to prevent cutting off text\n\n# Extract the relevant information about the collection\ncollection_info = {\n    \"Title\": collection.get(\"title\", \"N/A\"), # Extract the title of the collection \n    \"Description\": collection.get(\"description\", \"N/A\"), # Extract the dataset description\n    \"Temporal Extent\": collection.get(\"extent\", {}).get(\"temporal\", {}).get(\"interval\", \"N/A\"), # Extract the temporal coverage of the collection\n    \"Spatial Extent\": collection.get(\"extent\", {}).get(\"spatial\", {}).get(\"bbox\", \"N/A\"), # Extract the spatial coverage of the collection\n}\n\n# Convert the derived information into a DataFrame format\nproperties_table = pd.DataFrame(list(collection_info.items()), columns=[\"Collection Summary\", \"\"])\n\n# Display the properties in a table\ncollection_summary = properties_table.style.set_properties(**{'text-align': 'left'}) \\\n                                           .set_table_styles([\n    {\n        'selector': 'th.col0, td.col0',    # Select the first column\n        'props': [('min-width', '200px'),  # Set a minimum width\n                  ('text-align', 'left')]  # Align text to the left\n    },\n    {\n        'selector': 'td.col1',             # Select the second column\n        'props': [('text-align', 'left')]  # Align text to the left\n    }\n])\n\n# Print the collection summary table\ncollection_summary\n\nNext, you will examine the contents of the collection under the temporal variable. You’ll see that the data is available from January 2020 to December 2022. Looking at the dashboard:time density, you can observe that the data is periodic with monthly time density.\n\n# Create a function that would search for data collection in the US GHG Center STAC API\n\n# First, we need to define the function\n# The name of the function is \"get_item_count\" \n# The argument that will be passed to the defined function is \"collection_id\"\ndef get_item_count(collection_id):\n\n    # Set a counter for the number of items existing in the collection \n    count = 0 \n\n    # Define the path to retrieve the granules (items) of the collection of interest in the STAC API\n    items_url = f\"{STAC_API_URL}/collections/{collection_id}/items\" \n\n    # Run a while loop to make HTTP requests until there are no more URLs associated with the collection in the STAC API\n    while True:\n\n        # Retrieve information about the granules by sending a \"get\" request to the STAC API using the defined collection path \n        response = requests.get(items_url) \n\n        # If the items do not exist, print an error message and quit the loop\n        if not response.ok:\n            print(\"error getting items\")\n            exit()\n\n        # Return the results of the HTTP response as JSON\n        stac = response.json()\n\n        # Increase the \"count\" by the number of items (granules) returned in the response\n        count += int(stac[\"context\"].get(\"returned\", 0))\n\n        # Retrieve information about the next URL associated with the collection in the STAC API (if applicable)\n        next = [link for link in stac[\"links\"] if link[\"rel\"] == \"next\"]\n\n        # Exit the loop if there are no other URLs\n        if not next:\n            break\n        \n        # Ensure the information gathered by other STAC API links associated with the collection are added to the original path\n        # \"href\" is the identifier for each of the tiles stored in the STAC API\n        items_url = next[0][\"href\"]\n\n    # Return the information about the total number of granules found associated with the collection\n    return count\n\n\n# Apply the function created above \"get_item_count\" to the Air-Sea CO2 Flux ECCO-Darwin collection\nnumber_of_items = get_item_count(collection_name)\n\n# Get the information about the number of granules found in the collection\nitems = requests.get(f\"{STAC_API_URL}/collections/{collection_name}/items?limit={number_of_items}\").json()[\"features\"]\n\n# Print the total number of items (granules) found\nprint(f\"Found {len(items)} observations\")\n\n# Sort the items based on their date-time attribute\nitems_sorted = sorted(items, key=lambda x: x[\"properties\"][\"start_datetime\"])\n\n# Create an empty list\ntable_data = []\n# Extract the ID and date-time information for each granule and add them to the list\n# By default, only the first 5 items in the collection are extracted to be displayed in the table. \n# To see the date-time of all existing granules in this collection, remove \"5\" from \"item_sorted[:5]\" in the line below. \nfor item in items_sorted[:5]:\n    table_data.append([item['id'], item['properties']['start_datetime']])\n\n# Define the table headers\nheaders = [\"Item ID\", \"Start Date-Time\"]\n\nprint(\"Below you see the first 5 items in the collection, along with their item IDs and corresponding Start Date-Time.\")\n\n# Print the table using tabulate\nprint(tabulate(table_data, headers=headers, tablefmt=\"fancy_grid\"))\n\n\n# Examine the first item in the collection\n# Keep in mind that a list starts from 0, 1, 2... therefore items[0] refers to the first item (granule) in the list/collection\nitems_sorted[0]",
    "crumbs": [
      "Data Usage Notebooks",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "Utilizing the Air-Sea CO₂ Flux ECCO-Darwin Model to Visualize CO₂ Exchange and Dynamics Between the Atmosphere and the Ocean"
    ]
  },
  {
    "objectID": "user_data_notebooks/eccodarwin-co2flux-monthgrid-v5_User_Notebook.html#visual-comparison-across-time-periods",
    "href": "user_data_notebooks/eccodarwin-co2flux-monthgrid-v5_User_Notebook.html#visual-comparison-across-time-periods",
    "title": "Utilizing the Air-Sea CO₂ Flux ECCO-Darwin Model to Visualize CO₂ Exchange and Dynamics Between the Atmosphere and the Ocean",
    "section": "Visual Comparison Across Time Periods",
    "text": "Visual Comparison Across Time Periods\nYou will now explore changes in CO₂ flux at a given location and time. You will visualize the outputs on a map using folium.\n\n# Once again, apply the function created above \"get_item_count\" to the collection\n# This step allows retrieving the number of granules “observations” in the collection.\nnumber_of_items = get_item_count(collection_name)\nitems = requests.get(f\"{STAC_API_URL}/collections/{collection_name}/items?limit={number_of_items}\").json()[\"features\"]\n\n# Now you need to create a dictionary where the start datetime values for each granule are queried more explicitly by year and month (e.g., 2020-02)\nitems = {item[\"properties\"][\"start_datetime\"]: item for item in items}\n\n# Next, you need to specify the asset name for this collection.\n# The asset name refers to the raster band containing the pixel values for the parameter of interest.\n# For the case of this collection, the parameter of interest is “co2”.\nasset_name = \"co2\"\n\nBelow, you will enter the minimum and maximum values to provide our upper and lower bounds in the rescale_values.\n\n# Fetch the minimum and maximum values for the CO2 value range\nrescale_values = {\"max\":0.0007, \"min\":-0.0007}\n\nNow, you will pass the item id, collection name, asset name, and the rescaling factor to the Raster API endpoint. This step is done twice so that you can visualize two arbitrary events independently.\n\n# Choose a color map for displaying the first observation (event)\n# Please refer to matplotlib library if you'd prefer to choose a different color ramp.\n# For more information on Colormaps in Matplotlib, please visit https://matplotlib.org/stable/users/explain/colors/colormaps.html\ncolor_map = \"magma\"\n\n\n# You can retrieve the first observation of interest by defining the last 6 digits of its Item ID. \n# The numbers indicate the year and month (YYYYMM) when the data was gathered.\n# For example, the observation collected in December 2022 has the following item ID: eccodarwin-co2flux-monthgrid-v5-202212 \n# To set the time, you will need to insert \"202212\" in the line below. \n\n\n# Set the time\n# If you want to select another time, you can refer to the Data Browser on the U.S. Greenhouse Gas Center website  \n# URL to the Air-Sea CO2 Flux ECCO-Darwin collection in the US GHG Center: https://dljsq618eotzp.cloudfront.net/browseui/#eccodarwin-co2flux-monthgrid-v5/\nobservation_date_1 = '202212'\n\n\n# Don't change anything here\nobservation_1 = f'eccodarwin-co2flux-monthgrid-v5-{observation_date_1}'\n\n\n# Make a GET request to retrieve information for the December 2022 tile\n# A GET request is made for the December 2022 tile.\ndecember_2022_tile = requests.get(\n    # Pass the collection name and item ID directly from the first item in the list\n    (f\"{RASTER_API_URL}/collections/{collection_name}/items/{observation_1}/WebMercatorQuad/tilejson.json?\"\n    \n    # Pass the asset name\n    f\"&assets={asset_name}\"\n    \n    # Pass the color formula and colormap for custom visualization\n    f\"&color_formula=gamma+r+1.05&colormap_name={color_map}\"\n    \n    # Pass the minimum and maximum values for rescaling \n    f\"&rescale={rescale_values['min']},{rescale_values['max']}\"),\n    \n# Return the response in JSON format\n).json()\n\n\n# Print the properties of the retrieved granule to the console\ndecember_2022_tile\n\n\n# You will repeat the same approach used in the previous step to retrieve the second observation of interest\nobservation_date_2 = '202104'\n\n\n# Don't change anything here\nobservation_2 = f'eccodarwin-co2flux-monthgrid-v5-{observation_date_2}'\n\n\n# Make a GET request to retrieve information for the December 2022 tile\n# A GET request is made for the April 2021 tile.\napril_2021_tile = requests.get(\n    # Pass the collection name and item ID directly from the first item in the list\n    (f\"{RASTER_API_URL}/collections/{collection_name}/items/{observation_2}/WebMercatorQuad/tilejson.json?\"\n    \n    # Pass the asset name\n    f\"&assets={asset_name}\"\n    \n    # Pass the color formula and colormap for custom visualization\n    f\"&color_formula=gamma+r+1.05&colormap_name={color_map}\"\n    \n    # Pass the minimum and maximum values for rescaling \n    f\"&rescale={rescale_values['min']},{rescale_values['max']}\"),\n    \n# Return the response in JSON format\n).json()\n\n\n# Print the properties of the retrieved granule to the console\napril_2021_tile",
    "crumbs": [
      "Data Usage Notebooks",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "Utilizing the Air-Sea CO₂ Flux ECCO-Darwin Model to Visualize CO₂ Exchange and Dynamics Between the Atmosphere and the Ocean"
    ]
  },
  {
    "objectID": "user_data_notebooks/eccodarwin-co2flux-monthgrid-v5_User_Notebook.html#map-out-selected-tiles",
    "href": "user_data_notebooks/eccodarwin-co2flux-monthgrid-v5_User_Notebook.html#map-out-selected-tiles",
    "title": "Utilizing the Air-Sea CO₂ Flux ECCO-Darwin Model to Visualize CO₂ Exchange and Dynamics Between the Atmosphere and the Ocean",
    "section": "Map Out Selected Tiles",
    "text": "Map Out Selected Tiles\nFor this study, you are going to compare the CO₂ levels along the coast of California.\n\n# To change the location, you can simply insert the latitude and longitude of the area of your interest in the \"location=(LAT, LONG)\" statement\n\n# Set the initial zoom level and center of map for both tiles\n# 'folium.plugins' allows mapping side-by-side\nmap_ = folium.plugins.DualMap(location=(34, -118), zoom_start=7)\n\n\n# Define the first map layer with the CO2 Flux data for December 2022\nmap_layer_1 = TileLayer(\n    tiles=december_2022_tile[\"tiles\"][0], # Path to retrieve the tile\n    attr=\"GHG\", # Set the attribution \n    name='December 2022 CO2 Flux', # Title for the layer\n    overlay=True, # The layer can be overlaid on the map\n    opacity=0.8, # Adjust the transparency of the layer\n)\n# Add the first layer to the Dual Map \nmap_layer_1.add_to(map_.m1)\n\n\n# Define the second map layer with the CO2 Flux data for April 2021\nmap_layer_2 = TileLayer(\n    tiles=april_2021_tile[\"tiles\"][0], # Path to retrieve the tile\n    attr=\"GHG\", # Set the attribution \n    name='April 2021 CO2 Flux', # Title for the layer\n    overlay=True, # The layer can be overlaid on the map\n    opacity=0.8, # Adjust the transparency of the layer\n)\n# Add the second layer to the Dual Map \nmap_layer_2.add_to(map_.m2)\n\n\n# Display data markers (titles) on both maps\nfolium.Marker((40, 5.0), tooltip=\"both\").add_to(map_)\n\n# Add a layer control to switch between map layers\nfolium.LayerControl(collapsed=False).add_to(map_)\n\n# Add a legend to the dual map using the 'branca' library\n# Note: the inserted legend is representing the minimum and maximum values for both tiles\n# Minimum value = -0.0007, maximum value = 0.0007\ncolormap = branca.colormap.LinearColormap(colors=[\"#0000FF\", \"#3399FF\", \"#66CCFF\", \"#FFFFFF\", \"#FF66CC\", \"#FF3399\", \"#FF0000\"], vmin=-0.0007, vmax=0.0007) \n\n# Add the data unit as caption \ncolormap.caption = 'Millimoles per meter squared per second (mmol m²/s)'\n\n# Define custom tick values for the legend bar\ntick_val = [-0.0007, -0.00035, 0, 0.00035, 0.0007]\n\n# Create a HTML representation\nlegend_html = colormap._repr_html_()\n\n# Create a customized HTML structure for the legend\nlegend_html = f'''\n&lt;div style=\"position: fixed; bottom: 50px; left: 50px; z-index: 1000; width: 400px; height: auto; background-color: rgba(255, 255, 255, 0.8);\n             border-radius: 5px; border: 1px solid grey; padding: 10px; font-size: 14px; color: black;\"&gt;\n    &lt;b&gt;{colormap.caption}&lt;/b&gt;&lt;br&gt;\n    &lt;div style=\"display: flex; justify-content: space-between;\"&gt;\n        &lt;div&gt;{tick_val[0]}&lt;/div&gt; \n        &lt;div&gt;{tick_val[1]}&lt;/div&gt; \n        &lt;div&gt;{tick_val[2]}&lt;/div&gt; \n        &lt;div&gt;{tick_val[3]}&lt;/div&gt; \n        &lt;div&gt;{tick_val[4]}&lt;/div&gt; \n    &lt;/div&gt;\n    &lt;div style=\"background: linear-gradient(to right,\n                {'#0000FF'}, {'#3399FF'} {20}%,\n                {'#3399FF'} {20}%, {'#66CCFF'} {40}%,\n                {'#66CCFF'} {40}%, {'#FFFFFF'} {50}%,\n                {'#FFFFFF'} {50}%, {'#FF66CC'} {80}%,\n                {'#FF66CC'} {80}%, {'#FF3399'}); height: 10px;\"&gt;&lt;/div&gt;\n&lt;/div&gt;\n'''\n\n# Display the legend and caption on the map\nmap_.get_root().html.add_child(folium.Element(legend_html))\n\n# Visualize the Dual Map\nmap_",
    "crumbs": [
      "Data Usage Notebooks",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "Utilizing the Air-Sea CO₂ Flux ECCO-Darwin Model to Visualize CO₂ Exchange and Dynamics Between the Atmosphere and the Ocean"
    ]
  },
  {
    "objectID": "user_data_notebooks/eccodarwin-co2flux-monthgrid-v5_User_Notebook.html#time-series-analysis",
    "href": "user_data_notebooks/eccodarwin-co2flux-monthgrid-v5_User_Notebook.html#time-series-analysis",
    "title": "Utilizing the Air-Sea CO₂ Flux ECCO-Darwin Model to Visualize CO₂ Exchange and Dynamics Between the Atmosphere and the Ocean",
    "section": "Time-Series Analysis",
    "text": "Time-Series Analysis\nYou can now explore the fossil fuel emission using this data collection (January 2020 -December 2022) for the Coastal California region. You can plot the data set using the code below:\n\n# Sort the DataFrame by the datetime column so the plot displays the values from left to right (2020 -&gt; 2022)\ndf_sorted = df.sort_values(by=\"datetime\")\n\n# Plot the timeseries analysis of the monthly air-sea CO₂ flux changes along the coast of California\n# Figure size: 20 representing the width, 10 representing the height\nfig = plt.figure(figsize=(20, 10))\nplt.plot(\n    df_sorted[\"datetime\"],    # X-axis: sorted datetime\n    df_sorted[\"max\"],         # Y-axis: maximum CO₂ value\n    color=\"purple\",           # Line color\n    linestyle=\"-\",            # Line style\n    linewidth=1,              # Line width\n    label=\"CO2 Emissions\",    # Legend label\n)\n\n# Display legend\nplt.legend()\n\n# Insert label for the X-axis\nplt.xlabel(\"Years\")\n\n# Insert label for the Y-axis\nplt.ylabel(\"CO2 Emissions mmol m²/s\")\n\n# Insert title for the plot\nplt.title(\"CO2 Emission Values for Coastal California (2020-2022)\")\n\n# Rotate x-axis labels to avoid cramping\nplt.xticks(rotation=90)\n\n# Add data citation\nplt.text(\n    df_sorted[\"datetime\"].iloc[0],           # X-coordinate of the text (first datetime value)\n    df_sorted[\"max\"].min(),                  # Y-coordinate of the text (minimum CO2 value)\n\n    # Text to be displayed\n    \"Source: NASA Air-Sea CO₂ Flux, ECCO-Darwin Model v5\",                   \n    fontsize=12,                             # Font size\n    horizontalalignment=\"left\",              # Horizontal alignment\n    verticalalignment=\"bottom\",              # Vertical alignment\n    color=\"blue\",                            # Text color\n)\n\n# Plot the time series\nplt.show()\n\nNext, you can take a closer look at the monthly CO₂ flux variability across this region for a particular day. For example, you can retrieve and display data collected during the September 2022 observation\n\n# Choose the date of interest\ndate_of_interest = '202209'\n\n\n# Don't change anything here\nobservation_of_interest = f'eccodarwin-co2flux-monthgrid-v5-{date_of_interest}'\n\n\n# Make a GET request to retrieve information for the september 2022 tile\n# A GET request is made for the April 2021 tile.\nseptember_2022_tile = requests.get(\n    # Pass the collection name and item ID directly from the first item in the list\n    (f\"{RASTER_API_URL}collections/{collection_name}/items/{observation_of_interest}/WebMercatorQuad/tilejson.json?\"\n    \n    # Pass the asset name\n    f\"&assets={asset_name}\"\n    \n    # Pass the color formula and colormap for custom visualization\n    f\"&color_formula=gamma+r+1.05&colormap_name={color_map}\"\n    \n    # Pass the minimum and maximum values for rescaling \n    f\"&rescale={rescale_values['min']},{rescale_values['max']}\"),\n    \n# Return the response in JSON format\n).json()\n\n\n# Print the properties of the retrieved granule to the console\nseptember_2022_tile\n\n\n# Create a new map to display the September 2022 tile\naoi_map_bbox = Map(\n\n    # Base map is set to OpenStreetMap\n    tiles=\"OpenStreetMap\",\n\n    # Set the center of the map\n    location=[\n        34, -120\n    ],\n\n    # Set the zoom value\n    zoom_start=5.5,\n)\n\n# Define the map layer with the CO2 flux data for September 2022\nmap_layer = TileLayer(\n    tiles=september_2022_tile[\"tiles\"][0], # Path to retrieve the tile\n    attr=\"GHG\", # Set the attribution\n    opacity = 0.7, # Adjust the transparency of the layer\n)\n\n# Add the layer to the map\nmap_layer.add_to(aoi_map_bbox)\n\n# Add a legend to the map\n# Minimum value = -0.0007, maximum value = 0.0007\ncolormap = branca.colormap.LinearColormap(colors=[\"#0000FF\", \"#3399FF\", \"#66CCFF\", \"#FFFFFF\", \"#FF66CC\", \"#FF3399\", \"#FF0000\"], vmin=-0.0007, vmax=0.0007) \n\n# Add the data unit as caption \ncolormap.caption = 'Millimoles per meter squared per second (mmol m²/s)'\n\n# Define custom tick values for the legend bar\ntick_val = [-0.0007, -0.00035, 0, 0.00035, 0.0007]\n\n# Create a HTML representation\nlegend_html = colormap._repr_html_()\n\n# Create a customized HTML structure for the legend\nlegend_html = f'''\n&lt;div style=\"position: fixed; bottom: 50px; left: 50px; z-index: 1000; width: 400px; height: auto; background-color: rgba(255, 255, 255, 0.8);\n             border-radius: 5px; border: 1px solid grey; padding: 10px; font-size: 14px; color: black;\"&gt;\n    &lt;b&gt;{colormap.caption}&lt;/b&gt;&lt;br&gt;\n    &lt;div style=\"display: flex; justify-content: space-between;\"&gt;\n        &lt;div&gt;{tick_val[0]}&lt;/div&gt; \n        &lt;div&gt;{tick_val[1]}&lt;/div&gt; \n        &lt;div&gt;{tick_val[2]}&lt;/div&gt; \n        &lt;div&gt;{tick_val[3]}&lt;/div&gt; \n        &lt;div&gt;{tick_val[4]}&lt;/div&gt; \n    &lt;/div&gt;\n    &lt;div style=\"background: linear-gradient(to right,\n                {'#0000FF'}, {'#3399FF'} {20}%,\n                {'#3399FF'} {20}%, {'#66CCFF'} {40}%,\n                {'#66CCFF'} {40}%, {'#FFFFFF'} {50}%,\n                {'#FFFFFF'} {50}%, {'#FF66CC'} {80}%,\n                {'#FF66CC'} {80}%, {'#FF3399'}); height: 10px;\"&gt;&lt;/div&gt;\n&lt;/div&gt;\n'''\n\n# Display the legend and caption on the map\naoi_map_bbox.get_root().html.add_child(folium.Element(legend_html))\n\n# Add the title to the map\ntitle_html = '''\n&lt;div style=\"position: fixed; top: 10px; right: 10px; z-index: 1000; background-color: rgba(255, 255, 255, 0.8); border-radius: 5px; border: 1px solid grey; padding: 10px;\"&gt;\n    &lt;b&gt;Air-Sea CO₂ Flux, ECCO-Darwin&lt;/b&gt;&lt;br&gt;\n    September 2022\n&lt;/div&gt;\n'''\n# Display the title on the map\naoi_map_bbox.get_root().html.add_child(folium.Element(title_html))\n\n# Visualize the map\naoi_map_bbox",
    "crumbs": [
      "Data Usage Notebooks",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "Utilizing the Air-Sea CO₂ Flux ECCO-Darwin Model to Visualize CO₂ Exchange and Dynamics Between the Atmosphere and the Ocean"
    ]
  },
  {
    "objectID": "user_data_notebooks/eccodarwin-co2flux-monthgrid-v5_User_Notebook.html#summary",
    "href": "user_data_notebooks/eccodarwin-co2flux-monthgrid-v5_User_Notebook.html#summary",
    "title": "Utilizing the Air-Sea CO₂ Flux ECCO-Darwin Model to Visualize CO₂ Exchange and Dynamics Between the Atmosphere and the Ocean",
    "section": "Summary",
    "text": "Summary\nIn this notebook we have successfully completed the following steps for the STAC collection for the NASA Air-Sea CO₂ Flux ECCO Darwin dataset: 1. Install and import the necessary libraries 2. Fetch the collection from STAC collections using the appropriate endpoints 3. Count the number of existing granules within the collection 4. Map and compare the CO₂ Flux levels over the Coastal California area for two distinctive months/years 5. Create a table that displays the minimum, maximum, and sum of the CO₂ Flux values for a specified region 6. Generate a time-series graph of the CO₂ Flux values for a specified region\nIf you have any questions regarding this user notebook, please contact us using the feedback form.",
    "crumbs": [
      "Data Usage Notebooks",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "Utilizing the Air-Sea CO₂ Flux ECCO-Darwin Model to Visualize CO₂ Exchange and Dynamics Between the Atmosphere and the Ocean"
    ]
  },
  {
    "objectID": "user_data_notebooks/lam-testbed-ghg-concentrations_User_Notebook.html",
    "href": "user_data_notebooks/lam-testbed-ghg-concentrations_User_Notebook.html",
    "title": "Carbon Dioxide and Methane Concentrations from the Los Angeles Megacity Carbon Project",
    "section": "",
    "text": "Identify available dates and temporal frequency of observations for the given data. The collection processed in this notebook is the Atmospheric concentrations of carbon dioxide (CO₂) and methane (CH₄) collected at NIST Urban Test Bed tower sites in the Northeastern U.S.\nVisualize the time series data",
    "crumbs": [
      "Data Usage Notebooks",
      "Greenhouse Gas Concentrations",
      "Carbon Dioxide and Methane Concentrations from the Los Angeles Megacity Carbon Project"
    ]
  },
  {
    "objectID": "user_data_notebooks/lam-testbed-ghg-concentrations_User_Notebook.html#approach",
    "href": "user_data_notebooks/lam-testbed-ghg-concentrations_User_Notebook.html#approach",
    "title": "Carbon Dioxide and Methane Concentrations from the Los Angeles Megacity Carbon Project",
    "section": "",
    "text": "Identify available dates and temporal frequency of observations for the given data. The collection processed in this notebook is the Atmospheric concentrations of carbon dioxide (CO₂) and methane (CH₄) collected at NIST Urban Test Bed tower sites in the Northeastern U.S.\nVisualize the time series data",
    "crumbs": [
      "Data Usage Notebooks",
      "Greenhouse Gas Concentrations",
      "Carbon Dioxide and Methane Concentrations from the Los Angeles Megacity Carbon Project"
    ]
  },
  {
    "objectID": "user_data_notebooks/lam-testbed-ghg-concentrations_User_Notebook.html#about-the-data",
    "href": "user_data_notebooks/lam-testbed-ghg-concentrations_User_Notebook.html#about-the-data",
    "title": "Carbon Dioxide and Methane Concentrations from the Los Angeles Megacity Carbon Project",
    "section": "About the Data",
    "text": "About the Data\nNIST is engaged in research to improve measurement of greenhouse gas emissions in areas containing multiple emission sources and sinks, such as sciies. NIST’s objective is to develop measurement tools supporting independent means to increase the accuracy of greenhouse gas emissions data at urban and regional geospatial scales. NIST has established three test beds in U.S. sciies to develop and evaluate the performance of advanced measurement capabilities for emissions independent of their origin. Located in Indianapolis, Indiana, the Los Angeles air basin of California, and the U.S. Northeast corridor (beginning with the Baltimore/Washington D.C. region), the test beds have been selected for their varying meteorology, terrain and emissions characteristics. These test beds will serve as a means to independently diagnose the accuracy of emissions data obtained directly from emission or uptake sources.\nFor more information regarding this dataset, please visit the Carbon Dioxide and Methane Concentrations from the Northeast Corridor (NEC) Urban Test Bed data overview page.",
    "crumbs": [
      "Data Usage Notebooks",
      "Greenhouse Gas Concentrations",
      "Carbon Dioxide and Methane Concentrations from the Los Angeles Megacity Carbon Project"
    ]
  },
  {
    "objectID": "user_data_notebooks/lam-testbed-ghg-concentrations_User_Notebook.html#querying-the-feature-vector-api",
    "href": "user_data_notebooks/lam-testbed-ghg-concentrations_User_Notebook.html#querying-the-feature-vector-api",
    "title": "Carbon Dioxide and Methane Concentrations from the Los Angeles Megacity Carbon Project",
    "section": "Querying the Feature Vector API",
    "text": "Querying the Feature Vector API\nFirst, we are going to import the required libraries. Once imported, they allow better executing a query in the GHG Center Feature Vector Application Programming Interface (API) where the items for this collection are stored.\n\nFEATURE_API_URL=\"https://earth.gov/ghgcenter/api/features\"\n\n\n# Function to fetch CSV data for a station with a limit parameter\ndef get_station_data_csv(station_code, gas_type, frequency, elevation_m, limit=100000):\n    # Use the ?f=csv and limit query to get more rows\n    url = f\"https://earth.gov/ghgcenter/api/features/collections/public.nist_testbed_lam_{station_code}_{gas_type}_{frequency}_concentrations/items?f=csv&elevation_m={elevation_m}&limit={limit}\"\n    print(url)\n    try:\n        response = requests.get(url)\n        \n        # Check if the response is successful\n        if response.status_code != 200:\n            print(f\"Failed to fetch data for {station_code}. Status code: {response.status_code}\")\n            return pd.DataFrame()\n\n        # Check if the content type is CSV\n        content_type = response.headers.get('Content-Type')\n        if 'text/csv' not in content_type:\n            print(f\"Unexpected content type for {station_code}: {content_type}\")\n            print(\"Response content:\", response.text)\n            return pd.DataFrame()\n\n        # Read the CSV content into a pandas DataFrame\n        csv_data = StringIO(response.text)\n        return pd.read_csv(csv_data)\n    \n    except requests.exceptions.RequestException as e:\n        print(f\"Request failed: {e}\")\n        return pd.DataFrame()",
    "crumbs": [
      "Data Usage Notebooks",
      "Greenhouse Gas Concentrations",
      "Carbon Dioxide and Methane Concentrations from the Los Angeles Megacity Carbon Project"
    ]
  },
  {
    "objectID": "user_data_notebooks/lam-testbed-ghg-concentrations_User_Notebook.html#visualizing-the-co₂-data-for-two-nec-stations",
    "href": "user_data_notebooks/lam-testbed-ghg-concentrations_User_Notebook.html#visualizing-the-co₂-data-for-two-nec-stations",
    "title": "Carbon Dioxide and Methane Concentrations from the Los Angeles Megacity Carbon Project",
    "section": "Visualizing the CO₂ data for two NEC stations",
    "text": "Visualizing the CO₂ data for two NEC stations\n\n# Get station name and elevation from metdata dataframe\n# Fetch data for SCI (elevation 489) and COM (elevation 9), using limit=10000\n# ch4/co2 select the ghg \nsci_data = get_station_data_csv('sci', 'co2', 'hourly', 489, limit=10000)\ncom_data = get_station_data_csv('com', 'co2', 'hourly', 9, limit=10000)\n\n# Check if data was successfully retrieved before proceeding\nif sci_data.empty or com_data.empty:\n    print(\"No data available for one or both stations. Exiting.\")\nelse:\n    # Convert the 'datetime' column to datetime for plotting\n    sci_data['datetime'] = pd.to_datetime(sci_data['datetime'], format='%Y-%m-%dT%H:%M:%SZ')\n    com_data['datetime'] = pd.to_datetime(com_data['datetime'], format='%Y-%m-%dT%H:%M:%SZ')\n\n    # Plot the data\n    plt.figure(figsize=(10, 6))\n    plt.plot(sci_data['datetime'], sci_data['value'], label='SCI (489m)', color='blue', marker='o')\n    plt.plot(com_data['datetime'], com_data['value'], label='COM (9m)', color='green', marker='o')\n\n    plt.title('Carbon Dioxide (CO₂) Hourly Concentrations Over Time for SCI and COM Stations')\n    plt.xlabel('Time')\n    plt.ylabel('CO₂ Concentration (ppm)')\n    plt.legend()\n    plt.grid(True)\n\n    # Show plot\n    plt.show()\n\nhttps://earth.gov/ghgcenter/api/features/collections/public.nist_testbed_lam_sci_co2_hourly_concentrations/items?f=csv&elevation_m=489&limit=10000\nhttps://earth.gov/ghgcenter/api/features/collections/public.nist_testbed_lam_com_co2_hourly_concentrations/items?f=csv&elevation_m=9&limit=10000",
    "crumbs": [
      "Data Usage Notebooks",
      "Greenhouse Gas Concentrations",
      "Carbon Dioxide and Methane Concentrations from the Los Angeles Megacity Carbon Project"
    ]
  },
  {
    "objectID": "user_data_notebooks/gosat-based-ch4budget-yeargrid-v1_User_Notebook.html",
    "href": "user_data_notebooks/gosat-based-ch4budget-yeargrid-v1_User_Notebook.html",
    "title": "GOSAT-based Top-down Total and Natural Methane Emissions",
    "section": "",
    "text": "You can launch this notebook in the US GHG Center JupyterHub by clicking the link below.\nLaunch in the US GHG Center JupyterHub (requires access)",
    "crumbs": [
      "Data Usage Notebooks",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "GOSAT-based Top-down Total and Natural Methane Emissions"
    ]
  },
  {
    "objectID": "user_data_notebooks/gosat-based-ch4budget-yeargrid-v1_User_Notebook.html#run-this-notebook",
    "href": "user_data_notebooks/gosat-based-ch4budget-yeargrid-v1_User_Notebook.html#run-this-notebook",
    "title": "GOSAT-based Top-down Total and Natural Methane Emissions",
    "section": "",
    "text": "You can launch this notebook in the US GHG Center JupyterHub by clicking the link below.\nLaunch in the US GHG Center JupyterHub (requires access)",
    "crumbs": [
      "Data Usage Notebooks",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "GOSAT-based Top-down Total and Natural Methane Emissions"
    ]
  },
  {
    "objectID": "user_data_notebooks/gosat-based-ch4budget-yeargrid-v1_User_Notebook.html#approach",
    "href": "user_data_notebooks/gosat-based-ch4budget-yeargrid-v1_User_Notebook.html#approach",
    "title": "GOSAT-based Top-down Total and Natural Methane Emissions",
    "section": "Approach",
    "text": "Approach\n\nIdentify available dates and temporal frequency of observations for the given collection using the GHGC API /stac endpoint. The collection processed in this notebook is the gridded methane emissions data product.\nPass the STAC item into the raster API /stac/tilejson.jsonendpoint.\nUsing folium.plugins.DualMap, we will visualize two tiles (side-by-side), allowing us to compare time points.\nAfter the visualization, we will perform zonal statistics for a given polygon.",
    "crumbs": [
      "Data Usage Notebooks",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "GOSAT-based Top-down Total and Natural Methane Emissions"
    ]
  },
  {
    "objectID": "user_data_notebooks/gosat-based-ch4budget-yeargrid-v1_User_Notebook.html#about-the-data",
    "href": "user_data_notebooks/gosat-based-ch4budget-yeargrid-v1_User_Notebook.html#about-the-data",
    "title": "GOSAT-based Top-down Total and Natural Methane Emissions",
    "section": "About the Data",
    "text": "About the Data\nThe NASA Carbon Monitoring System Flux (CMS-Flux) team analyzed remote sensing observations from Japan’s Greenhouse gases Observing SATellite (GOSAT) to produce the global Committee on Earth Observation Satellites (CEOS) CH₄ Emissions data product. They used an analytic Bayesian inversion approach and the GEOS-Chem global chemistry transport model to quantify annual methane (CH₄) emissions and their uncertainties at a spatial resolution of 1° by 1° and then projected these to each country for 2019.\nFor more information regarding this dataset, please visit the GOSAT-based Top-down Total and Natural Methane Emissions data overview page.",
    "crumbs": [
      "Data Usage Notebooks",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "GOSAT-based Top-down Total and Natural Methane Emissions"
    ]
  },
  {
    "objectID": "user_data_notebooks/gosat-based-ch4budget-yeargrid-v1_User_Notebook.html#querying-the-stac-api",
    "href": "user_data_notebooks/gosat-based-ch4budget-yeargrid-v1_User_Notebook.html#querying-the-stac-api",
    "title": "GOSAT-based Top-down Total and Natural Methane Emissions",
    "section": "Querying the STAC API",
    "text": "Querying the STAC API\nFirst, we are going to import the required libraries. Once imported, they allow better executing a query in the GHG Center Spatio Temporal Asset Catalog (STAC) Application Programming Interface (API) where the granules for this collection are stored.\n\n# Import the following libraries\nimport requests\nimport folium\nimport folium.plugins\nfrom folium import Map, TileLayer\nfrom pystac_client import Client\nimport branca\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n\n# Provide STAC and RASTER API endpoints\nSTAC_API_URL = \"https://earth.gov/ghgcenter/api/stac\"\nRASTER_API_URL = \"https://earth.gov/ghgcenter/api/raster\"\n\n# Please use the collection name similar to the one used in STAC collection.\n\n# Name of the collection for gosat budget methane. \ncollection_name = \"gosat-based-ch4budget-yeargrid-v1\"\n\n\n# Fetching the collection from STAC collections using appropriate endpoint.\ncollection = requests.get(f\"{STAC_API_URL}/collections/{collection_name}\").json()\ncollection\n\nExamining the contents of our collection under the temporal variable, we see that the data is available from January 2012 to December 2018. By looking at the dashboard:time density, we observe that the data is available for only one year, i.e. 2019.\n\ndef get_item_count(collection_id):\n    count = 0\n    items_url = f\"{STAC_API_URL}/collections/{collection_id}/items\"\n\n    while True:\n        response = requests.get(items_url)\n\n        if not response.ok:\n            print(\"error getting items\")\n            exit()\n\n        stac = response.json()\n        count += int(stac[\"context\"].get(\"returned\", 0))\n        next = [link for link in stac[\"links\"] if link[\"rel\"] == \"next\"]\n\n        if not next:\n            break\n        items_url = next[0][\"href\"]\n\n    return count\n\n\n# Check total number of items available\nnumber_of_items = get_item_count(collection_name)\nitems = requests.get(f\"{STAC_API_URL}/collections/{collection_name}/items?limit={number_of_items}\").json()[\"features\"]\nprint(f\"Found {len(items)} items\")\n\n\n# Examining the first item in the collection\nitems[0]\n\nBelow, we enter minimum and maximum values to provide our upper and lower bounds in rescale_values.",
    "crumbs": [
      "Data Usage Notebooks",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "GOSAT-based Top-down Total and Natural Methane Emissions"
    ]
  },
  {
    "objectID": "user_data_notebooks/gosat-based-ch4budget-yeargrid-v1_User_Notebook.html#exploring-changes-in-gosat-methane-budgets-ch4-levels-using-the-raster-api",
    "href": "user_data_notebooks/gosat-based-ch4budget-yeargrid-v1_User_Notebook.html#exploring-changes-in-gosat-methane-budgets-ch4-levels-using-the-raster-api",
    "title": "GOSAT-based Top-down Total and Natural Methane Emissions",
    "section": "Exploring Changes in GOSAT Methane budgets (CH4) Levels Using the Raster API",
    "text": "Exploring Changes in GOSAT Methane budgets (CH4) Levels Using the Raster API\nIn this notebook, we will explore the impacts of methane emissions and by examining changes over time in urban regions. We will visualize the outputs on a map using folium.\n\n# To access the year value from each item more easily, this will let us query more explicity by year and month (e.g., 2020-02)\nitems = {item[\"properties\"][\"start_datetime\"][:10]: item for item in items} \nasset_name = \"prior-total\"\n\n\n# Fetching the min and max values for a specific item\nrescale_values = {\"max\":items[list(items.keys())[0]][\"assets\"][asset_name][\"raster:bands\"][0][\"histogram\"][\"max\"], \"min\":items[list(items.keys())[0]][\"assets\"][asset_name][\"raster:bands\"][0][\"histogram\"][\"min\"]}\n\n\nitems.keys()\n\nNow, we will pass the item id, collection name, and rescaling_factor to the Raster API endpoint. We will do this for first January 2019.\n\ncolor_map = \"rainbow\" # please select the color ramp from matplotlib library.\njanuary_2019_tile = requests.get(\n    f\"{RASTER_API_URL}/stac/tilejson.json?collection={items['2019-01-01']['collection']}&item={items['2019-01-01']['id']}\"\n    f\"&assets={asset_name}\"\n    f\"&color_formula=gamma+r+1.05&colormap_name={color_map}\"\n    f\"&rescale={rescale_values['min']},{rescale_values['max']}\", \n).json()\njanuary_2019_tile",
    "crumbs": [
      "Data Usage Notebooks",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "GOSAT-based Top-down Total and Natural Methane Emissions"
    ]
  },
  {
    "objectID": "user_data_notebooks/gosat-based-ch4budget-yeargrid-v1_User_Notebook.html#visualizing-ch₄-emissions",
    "href": "user_data_notebooks/gosat-based-ch4budget-yeargrid-v1_User_Notebook.html#visualizing-ch₄-emissions",
    "title": "GOSAT-based Top-down Total and Natural Methane Emissions",
    "section": "Visualizing CH₄ Emissions",
    "text": "Visualizing CH₄ Emissions\n\n# Set initial zoom and center of map for CH₄ Layer\n# Centre of map [latitude,longitude]\nmap_ = folium.Map(location=(34, -118), zoom_start=6)\n\n# January 2019\nmap_layer_2019 = TileLayer(\n    tiles=january_2019_tile[\"tiles\"][0],\n    attr=\"GHG\",\n    opacity=0.7,\n)\nmap_layer_2019.add_to(map_)\n\n# # January 2012\n# map_layer_2012 = TileLayer(\n#     tiles=january_2012_tile[\"tiles\"][0],\n#     attr=\"GHG\",\n#     opacity=0.7,\n# )\n# map_layer_2012.add_to(map_.m2)\n\n# visualising the map\nmap_",
    "crumbs": [
      "Data Usage Notebooks",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "GOSAT-based Top-down Total and Natural Methane Emissions"
    ]
  },
  {
    "objectID": "user_data_notebooks/gosat-based-ch4budget-yeargrid-v1_User_Notebook.html#summary",
    "href": "user_data_notebooks/gosat-based-ch4budget-yeargrid-v1_User_Notebook.html#summary",
    "title": "GOSAT-based Top-down Total and Natural Methane Emissions",
    "section": "Summary",
    "text": "Summary\nIn this notebook we have successfully completed the following steps for the STAC collection for the GOSAT-based Top-down Total and Natural Methane Emissions dataset.\n\nInstall and import the necessary libraries\nFetch the collection from STAC collections using the appropriate endpoints\nCount the number of existing granules within the collection\nMap the methane emission levels\nGenerate zonal statistics for the area of interest (AOI)\n\nIf you have any questions regarding this user notebook, please contact us using the feedback form.",
    "crumbs": [
      "Data Usage Notebooks",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "GOSAT-based Top-down Total and Natural Methane Emissions"
    ]
  },
  {
    "objectID": "user_data_notebooks/sedac-popdensity-yeargrid5yr-v4.11_User_Notebook.html",
    "href": "user_data_notebooks/sedac-popdensity-yeargrid5yr-v4.11_User_Notebook.html",
    "title": "SEDAC Gridded World Population Density",
    "section": "",
    "text": "You can launch this notebook in the US GHG Center JupyterHub by clicking the link below.\nLaunch in the US GHG Center JupyterHub (requires access)",
    "crumbs": [
      "Data Usage Notebooks",
      "Socioeconomic",
      "SEDAC Gridded World Population Density"
    ]
  },
  {
    "objectID": "user_data_notebooks/sedac-popdensity-yeargrid5yr-v4.11_User_Notebook.html#run-this-notebook",
    "href": "user_data_notebooks/sedac-popdensity-yeargrid5yr-v4.11_User_Notebook.html#run-this-notebook",
    "title": "SEDAC Gridded World Population Density",
    "section": "",
    "text": "You can launch this notebook in the US GHG Center JupyterHub by clicking the link below.\nLaunch in the US GHG Center JupyterHub (requires access)",
    "crumbs": [
      "Data Usage Notebooks",
      "Socioeconomic",
      "SEDAC Gridded World Population Density"
    ]
  },
  {
    "objectID": "user_data_notebooks/sedac-popdensity-yeargrid5yr-v4.11_User_Notebook.html#approach",
    "href": "user_data_notebooks/sedac-popdensity-yeargrid5yr-v4.11_User_Notebook.html#approach",
    "title": "SEDAC Gridded World Population Density",
    "section": "Approach",
    "text": "Approach\n\nIdentify available dates and temporal frequency of observations for the given collection using the GHGC API /stac endpoint. Collection processed in this notebook is SEDAC gridded population density.\nPass the STAC item into raster API /stac/tilejson.json endpoint\nWe’ll visualize two tiles (side-by-side) allowing for comparison of each of the time points using folium.plugins.DualMap\nAfter the visualization, we’ll perform zonal statistics for a given polygon.",
    "crumbs": [
      "Data Usage Notebooks",
      "Socioeconomic",
      "SEDAC Gridded World Population Density"
    ]
  },
  {
    "objectID": "user_data_notebooks/sedac-popdensity-yeargrid5yr-v4.11_User_Notebook.html#about-the-data",
    "href": "user_data_notebooks/sedac-popdensity-yeargrid5yr-v4.11_User_Notebook.html#about-the-data",
    "title": "SEDAC Gridded World Population Density",
    "section": "About the Data",
    "text": "About the Data\nThe SEDAC Gridded Population of the World: Population Density, v4.11 dataset provides annual estimates of population density for the years 2000, 2005, 2010, 2015, and 2020 on a 30 arc-second (~1 km) grid. These data can be used for assessing disaster impacts, risk mapping, and any other applications that include a human dimension. This population density dataset is provided by NASA’s Socioeconomic Data and Applications Center (SEDAC) hosted by the Center for International Earth Science Information Network (CIESIN) at Columbia University. The population estimates are provided as a continuous raster for the entire globe.\nFor more information regarding this dataset, please visit the SEDAC Gridded World Population Density data overview page.",
    "crumbs": [
      "Data Usage Notebooks",
      "Socioeconomic",
      "SEDAC Gridded World Population Density"
    ]
  },
  {
    "objectID": "user_data_notebooks/sedac-popdensity-yeargrid5yr-v4.11_User_Notebook.html#querying-the-stac-api",
    "href": "user_data_notebooks/sedac-popdensity-yeargrid5yr-v4.11_User_Notebook.html#querying-the-stac-api",
    "title": "SEDAC Gridded World Population Density",
    "section": "Querying the STAC API",
    "text": "Querying the STAC API\nFirst, we are going to import the required libraries. Once imported, they allow better executing a query in the GHG Center Spatio Temporal Asset Catalog (STAC) Application Programming Interface (API) where the granules for this collection are stored.\n\n# Provide the STAC and RASTER API endpoints\n# The endpoint is referring to a location within the API that executes a request on a data collection nesting on the server.\n\n# The STAC API is a catalog of all the existing data collections that are stored in the GHG Center.\nSTAC_API_URL = \"https://earth.gov/ghgcenter/api/stac\"\n\n# The RASTER API is used to fetch collections for visualization\nRASTER_API_URL = \"https://earth.gov/ghgcenter/api/raster\"\n\n# The collection name is used to fetch the dataset from the STAC API. First, we define the collection name as a variable\n# Name of the collection for SEDAC population density dataset \ncollection_name = \"sedac-popdensity-yeargrid5yr-v4.11\"\n\n\n# Fetch the collection from the STAC API using the appropriate endpoint\n# The 'requests' library allows a HTTP request possible\ncollection = requests.get(f\"{STAC_API_URL}/collections/{collection_name}\").json()\n\n# Print the properties of the collection to the console\ncollection\n\nExamining the contents of our collection under summaries we see that the data is available from January 2000 to December 2020. By looking at the dashboard:time density we observe that the data is available for the years 2000, 2005, 2010, 2015, 2020.\n\n# Create a function that would search for a data collection in the US GHG Center STAC API\n\n# First, we need to define the function\n# The name of the function = \"get_item_count\"\n# The argument that will be passed through the defined function = \"collection_id\"\ndef get_item_count(collection_id):\n\n    # Set a counter for the number of items existing in the collection\n    count = 0\n\n    # Define the path to retrieve the granules (items) of the collection of interest in the STAC API\n    items_url = f\"{STAC_API_URL}/collections/{collection_id}/items\"\n\n    # Run a while loop to make HTTP requests until there are no more URLs associated with the collection in the STAC API\n    while True:\n\n        # Retrieve information about the granules by sending a \"get\" request to the STAC API using the defined collection path\n        response = requests.get(items_url)\n\n        # If the items do not exist, print an error message and quit the loop\n        if not response.ok:\n            print(\"error getting items\")\n            exit()\n\n        # Return the results of the HTTP response as JSON\n        stac = response.json()\n\n        # Increase the \"count\" by the number of items (granules) returned in the response\n        count += int(stac[\"context\"].get(\"returned\", 0))\n\n        # Retrieve information about the next URL associated with the collection in the STAC API (if applicable)\n        next = [link for link in stac[\"links\"] if link[\"rel\"] == \"next\"]\n\n        # Exit the loop if there are no other URLs\n        if not next:\n            break\n        \n        # Ensure the information gathered by other STAC API links associated with the collection are added to the original path\n        # \"href\" is the identifier for each of the tiles stored in the STAC API\n        items_url = next[0][\"href\"]\n\n    # Return the information about the total number of granules found associated with the collection\n    return count\n\n\n# Apply the function created above \"get_item_count\" to the data collection\nnumber_of_items = get_item_count(collection_name)\n\n# Get the information about the number of granules found in the collection\nitems = requests.get(f\"{STAC_API_URL}/collections/{collection_name}/items?limit={number_of_items}\").json()[\"features\"]\n\n# Print the total number of items (granules) found\nprint(f\"Found {len(items)} items\")\n\n\n# Examine the first item in the collection\n# Keep in mind that a list starts from 0, 1, 2... therefore items[0] is referring to the first item in the list/collection\nitems[0]",
    "crumbs": [
      "Data Usage Notebooks",
      "Socioeconomic",
      "SEDAC Gridded World Population Density"
    ]
  },
  {
    "objectID": "user_data_notebooks/sedac-popdensity-yeargrid5yr-v4.11_User_Notebook.html#exploring-changes-in-the-world-population-density-using-the-raster-api",
    "href": "user_data_notebooks/sedac-popdensity-yeargrid5yr-v4.11_User_Notebook.html#exploring-changes-in-the-world-population-density-using-the-raster-api",
    "title": "SEDAC Gridded World Population Density",
    "section": "Exploring Changes in the World Population Density using the Raster API",
    "text": "Exploring Changes in the World Population Density using the Raster API\nWe will explore changes in population density in urban regions. In this notebook, we’ll explore the changes in population density over time. We’ll then visualize the outputs on a map using folium.\n\n# Now we create a dictionary where the start datetime values for each granule is queried more explicitly by year and month (e.g., 2020-02)\nitems = {item[\"properties\"][\"start_datetime\"][:7]: item for item in items} \n\n# Next, we need to specify the asset name for this collection\n# The asset name is referring to the raster band containing the pixel values for the parameter of interest\n# For the case of the SEDAC Gridded World Population Density collection, the parameter of interest is “population-density”\nasset_name = \"population-density\"\n\nBelow, we are entering the minimum and maximum values to provide our upper and lower bounds in the rescale_values.\n\n# Fetching the min and max values\nrescale_values = {\"max\":items[list(items.keys())[0]][\"assets\"][asset_name][\"raster:bands\"][0][\"histogram\"][\"max\"], \"min\":items[list(items.keys())[0]][\"assets\"][asset_name][\"raster:bands\"][0][\"histogram\"][\"min\"]}\n\nNow, we will pass the item id, collection name, asset name, and the rescaling factor to the Raster API endpoint. We will do this twice, once for January 2000 and again for January 2020, so that we can visualize each event independently.\n\n# Choose a color map for displaying the first observation (event)\n# Please refer to matplotlib library if you'd prefer choosing a different color ramp.\n# For more information on Colormaps in Matplotlib, please visit https://matplotlib.org/stable/users/explain/colors/colormaps.html\ncolor_map = \"rainbow\" \n\n# Make a GET request to retrieve information for the 2020 tile\njanuary_2020_tile = requests.get(\n\n    # Pass the collection name, the item number in the list, and its ID\n    f\"{RASTER_API_URL}/stac/tilejson.json?collection={items['2020-01']['collection']}&item={items['2020-01']['id']}\"\n\n    # Pass the asset name\n    f\"&assets={asset_name}\"\n\n    # Pass the color formula and colormap for custom visualization\n    f\"&color_formula=gamma+r+1.05&colormap_name={color_map}\"\n\n    # Pass the minimum and maximum values for rescaling\n    f\"&rescale={rescale_values['min']},{rescale_values['max']}\",\n\n# Return the response in JSON format \n).json()\n\n# Print the properties of the retrieved granule to the console\njanuary_2020_tile\n\n\n# Make a GET request to retrieve information for the 2000 tile\njanuary_2000_tile = requests.get(\n\n    # Pass the collection name, the item number in the list, and its ID\n    f\"{RASTER_API_URL}/stac/tilejson.json?collection={items['2000-01']['collection']}&item={items['2000-01']['id']}\"\n\n    # Pass the asset name\n    f\"&assets={asset_name}\"\n\n    # Pass the color formula and colormap for custom visualization\n    f\"&color_formula=gamma+r+1.05&colormap_name={color_map}\"\n\n    # Pass the minimum and maximum values for rescaling\n    f\"&rescale={rescale_values['min']},{rescale_values['max']}\",\n\n# Return the response in JSON format  \n).json()\n\n# Print the properties of the retrieved granule to the console\njanuary_2000_tile",
    "crumbs": [
      "Data Usage Notebooks",
      "Socioeconomic",
      "SEDAC Gridded World Population Density"
    ]
  },
  {
    "objectID": "user_data_notebooks/sedac-popdensity-yeargrid5yr-v4.11_User_Notebook.html#visualizing-population-density.",
    "href": "user_data_notebooks/sedac-popdensity-yeargrid5yr-v4.11_User_Notebook.html#visualizing-population-density.",
    "title": "SEDAC Gridded World Population Density",
    "section": "Visualizing Population Density.",
    "text": "Visualizing Population Density.\n\n# Set initial zoom and center of map for population density Layer\n# 'folium.plugins' allows mapping side-by-side\nmap_ = folium.plugins.DualMap(location=(34, -118), zoom_start=6)\n\n# Define the first map layer (January 2020)\nmap_layer_2020 = TileLayer(\n    tiles=january_2020_tile[\"tiles\"][0], # Path to retrieve the tile\n    attr=\"GHG\", # Set the attribution\n    opacity=1, # Adjust the transparency of the layer\n)\n\n# Add the first layer to the Dual Map\nmap_layer_2020.add_to(map_.m1)\n\n# Define the second map layer (January 2000)\nmap_layer_2000 = TileLayer(\n    tiles=january_2000_tile[\"tiles\"][0], # Path to retrieve the tile\n    attr=\"GHG\", # Set the attribution\n    opacity=1, # Adjust the transparency of the layer\n)\n\n# Add the second layer to the Dual Map\nmap_layer_2000.add_to(map_.m2)\n\n# Visualize the Dual Map\nmap_",
    "crumbs": [
      "Data Usage Notebooks",
      "Socioeconomic",
      "SEDAC Gridded World Population Density"
    ]
  },
  {
    "objectID": "user_data_notebooks/sedac-popdensity-yeargrid5yr-v4.11_User_Notebook.html#visualizing-the-data-as-a-time-series",
    "href": "user_data_notebooks/sedac-popdensity-yeargrid5yr-v4.11_User_Notebook.html#visualizing-the-data-as-a-time-series",
    "title": "SEDAC Gridded World Population Density",
    "section": "Visualizing the Data as a Time Series",
    "text": "Visualizing the Data as a Time Series\nWe can now explore the SEDAC population density dataset time series available for the Texas, Dallas area of USA. We can plot the dataset using the code below:\n\n# Figure size: 20 representing the width, 10 representing the height\nfig = plt.figure(figsize=(20, 10))\n\nplt.plot(\n    df[\"date\"], # X-axis: sorted datetime\n    df[\"max\"], # Y-axis: maximum pop density\n    color=\"red\", # Line color\n    linestyle=\"-\", # Line style\n    linewidth=0.5, # Line width\n    label=\"Population density over the years\", # Legend label\n)\n\n# Display legend\nplt.legend()\n\n# Insert label for the X-axis\nplt.xlabel(\"Years\")\n\n# Insert label for the Y-axis\nplt.ylabel(\"Population density\")\n\n# Insert title for the plot\nplt.title(\"Population density over Texas, Dallas (2000-2020)\")\n\n###\n# Add data citation\nplt.text(\n    df[\"date\"].iloc[0],           # X-coordinate of the text\n    df[\"max\"].min(),              # Y-coordinate of the text\n\n\n\n\n    # Text to be displayed\n    \"Source: NASA SEDAC Gridded World Population Density\",                  \n    fontsize=12,                             # Font size\n    horizontalalignment=\"right\",             # Horizontal alignment\n    verticalalignment=\"bottom\",              # Vertical alignment\n    color=\"blue\",                            # Text color\n)\n\n\n# Plot the time series\nplt.show()\n\n\n# Print the properties for the 3rd item in the collection\nprint(items[2][\"properties\"][\"start_datetime\"])\n\n\n# A GET request is made for the 2010 tile\njanuary2010_tile = requests.get(\n\n    # Pass the collection name, the item number in the list, and its ID\n    f\"{RASTER_API_URL}/stac/tilejson.json?collection={items[2]['collection']}&item={items[2]['id']}\"\n\n    # Pass the asset name\n    f\"&assets={asset_name}\"\n\n    # Pass the color formula and colormap for custom visualization\n    f\"&color_formula=gamma+r+1.05&colormap_name={color_map}\"\n\n    # Pass the minimum and maximum values for rescaling\n    f\"&rescale={rescale_values['min']},{rescale_values['max']}\",\n\n# Return the response in JSON format\n).json()\n\n# Print the properties of the retrieved granule to the console\njanuary2010_tile\n\n\n# Create a new map to display the 2010 tile\naoi_map_bbox = Map(\n\n    # Base map is set to OpenStreetMap\n    tiles=\"OpenStreetMap\",\n\n    # Set the center of the map\n    location=[\n        30,-100\n    ],\n\n    # Set the zoom value\n    zoom_start=8,\n)\n\n# Define the map layer\nmap_layer = TileLayer(\n\n    # Path to retrieve the tile\n    tiles=january2010_tile[\"tiles\"][0],\n\n    # Set the attribution and adjust the transparency of the layer\n    attr=\"GHG\", opacity = 0.5\n)\n\n# Add the layer to the map\nmap_layer.add_to(aoi_map_bbox)\n\n# Visualize the map\naoi_map_bbox",
    "crumbs": [
      "Data Usage Notebooks",
      "Socioeconomic",
      "SEDAC Gridded World Population Density"
    ]
  },
  {
    "objectID": "user_data_notebooks/sedac-popdensity-yeargrid5yr-v4.11_User_Notebook.html#summary",
    "href": "user_data_notebooks/sedac-popdensity-yeargrid5yr-v4.11_User_Notebook.html#summary",
    "title": "SEDAC Gridded World Population Density",
    "section": "Summary",
    "text": "Summary\nIn this notebook we have successfully explored, analyzed and visualized the STAC collection for the SEDAC Gridded World Population Density dataset.\n\nInstall and import the necessary libraries\nFetch the collection from STAC collections using the appropriate endpoints\nCount the number of existing granules within the collection\nMap and compare population density for two distinctive months/years\nGenerate zonal statistics for the area of interest (AOI)\nVisualizing the Data as a Time Series\n\nIf you have any questions regarding this user notebook, please contact us using the feedback form.",
    "crumbs": [
      "Data Usage Notebooks",
      "Socioeconomic",
      "SEDAC Gridded World Population Density"
    ]
  },
  {
    "objectID": "user_data_notebooks/oco2-mip-National-co2budget.html",
    "href": "user_data_notebooks/oco2-mip-National-co2budget.html",
    "title": "OCO-2 MIP National Top-Down CO2 Budgets",
    "section": "",
    "text": "You can launch this notebook in the US GHG Center JupyterHub by clicking the link below.\nLaunch in the US GHG Center JupyterHub (requires access)"
  },
  {
    "objectID": "user_data_notebooks/oco2-mip-National-co2budget.html#run-this-notebook",
    "href": "user_data_notebooks/oco2-mip-National-co2budget.html#run-this-notebook",
    "title": "OCO-2 MIP National Top-Down CO2 Budgets",
    "section": "",
    "text": "You can launch this notebook in the US GHG Center JupyterHub by clicking the link below.\nLaunch in the US GHG Center JupyterHub (requires access)"
  },
  {
    "objectID": "user_data_notebooks/oco2-mip-National-co2budget.html#approach",
    "href": "user_data_notebooks/oco2-mip-National-co2budget.html#approach",
    "title": "OCO-2 MIP National Top-Down CO2 Budgets",
    "section": "Approach",
    "text": "Approach\n\nRead in National CO2 Budgets using Pandas\nSub-select the data structure using Pandas\nVisualize the CO2 budgets for a country\nInvestigate uncertainties and metrics for understanding the dataset"
  },
  {
    "objectID": "user_data_notebooks/oco2-mip-National-co2budget.html#about-the-data",
    "href": "user_data_notebooks/oco2-mip-National-co2budget.html#about-the-data",
    "title": "OCO-2 MIP National Top-Down CO2 Budgets",
    "section": "About the Data",
    "text": "About the Data\nThis tutorial guides a user to further explore data from the Carbon Observatory (OCO-2) modeling intercomparison project (MIP). It is designed for those with more understanding of the science and is therefore labeled as intermediate level.\nThe code is used to estimate the annual net terrestrial carbon stock loss (ΔCloss) and net carbon exchange (NCE) for a given country using the “top-down” NCE outputs from the Carbon Observatory (OCO-2) modeling intercomparison project (MIP). Several standardized experiments are studied in this notebook based on the OCO-2 MIP dataset including flux estimates from in situ CO₂ measurements (IS), flux estimates from OCO-2 land CO₂ data (LNLG), combined in situ and OCO-2 land CO₂ data (LNLGIS), and combined in situ and OCO-2 land and ocean CO₂ data (LNLGOGIS). Estimates and uncertainties associated with fossil fuels, riverine fluxes, and wood and crop fluxes are also graphed along with the ΔCloss and NCE variables.\nFor more information about this data collection, please visit the OCO-2 MIP Top-Down CO2 Budgets data overview page.\nFor more information regarding this dataset, please visit the U.S. Greenhouse Gas Center."
  },
  {
    "objectID": "user_data_notebooks/oco2-mip-National-co2budget.html#import-required-modules",
    "href": "user_data_notebooks/oco2-mip-National-co2budget.html#import-required-modules",
    "title": "OCO-2 MIP National Top-Down CO2 Budgets",
    "section": "Import required modules",
    "text": "Import required modules\nFirst we will need to import the relevant python modules:\n\nimport pandas as pd # for manipulating csv dataset\nimport numpy as np\nimport matplotlib.pyplot as plt # make plots\nfrom scipy.stats import norm # We will use this for understanding significance"
  },
  {
    "objectID": "user_data_notebooks/oco2-mip-National-co2budget.html#read-the-co2-national-budget-dataset",
    "href": "user_data_notebooks/oco2-mip-National-co2budget.html#read-the-co2-national-budget-dataset",
    "title": "OCO-2 MIP National Top-Down CO2 Budgets",
    "section": "Read the CO2 National budget dataset",
    "text": "Read the CO2 National budget dataset\nNow we will read in the csv dataset from https://ceos.org/gst/carbon-dioxide.html\n\nurl ='https://ceos.org/gst/files/pilot_topdown_CO2_Budget_countries_v1.csv'\ndf_all = pd.read_csv(url, skiprows=52)"
  },
  {
    "objectID": "user_data_notebooks/oco2-mip-National-co2budget.html#sub-select-a-single-top-down-dataset-experiment",
    "href": "user_data_notebooks/oco2-mip-National-co2budget.html#sub-select-a-single-top-down-dataset-experiment",
    "title": "OCO-2 MIP National Top-Down CO2 Budgets",
    "section": "Sub-select a single top-down dataset (experiment)",
    "text": "Sub-select a single top-down dataset (experiment)\nTo simplify the analysis, let’s subselect the results for a single experiment. The experiments are: - IS: estimates fluxes from in situ CO2 measurements - LNLG: estimates fluxes from OCO-2 land CO2 data - LNLGIS: combines in situ and OCO-2 land CO2 data - LNLGOGIS: combines in situ and OCO-2 land and ocean CO2 data\nWe would like to use the experiment that uses the most high-quality CO2 data. There are some concerns about small residual biases in OCO-2 ocean data (Byrne et al., 2023), so let’s use the LNLGIS experiment.\n\n# Choose one experiment from the list ['IS', 'LNLG', 'LNLGIS', 'LNLGOGIS']\nexperiment = 'LNLGIS'\n\n# Subset of columns for a given experiment\nif experiment == 'IS':\n    df = df_all.drop(df_all.columns[[4,5,6,7,8,9,12,13,14,15,16,17,20,21,22,23,24,25,34,35,36]], axis=1)\nif experiment == 'LNLG':\n    df = df_all.drop(df_all.columns[[2,3,6,7,8,9,10,11,14,15,16,17,18,19,22,23,24,25,33,35,36]], axis=1)\nif experiment == 'LNLGIS':\n    df = df_all.drop(df_all.columns[[2,3,4,5,8,9,10,11,12,13,16,17,18,19,20,21,24,25,33,34,36]], axis=1)\nif experiment == 'LNLGOGIS':\n    df = df_all.drop(df_all.columns[[2,3,4,5,6,7,10,11,12,13,14,15,18,19,20,21,22,23,33,34,35]], axis=1)\n\n# We can now look at the colums of data\ndf.head()\n\n\n\n\n\n\n\n\nAlpha 3 Code\nYear\nLNLGIS dC_loss (TgCO2)\nLNLGIS dC_loss unc (TgCO2)\nLNLGIS NBE (TgCO2)\nLNLGIS NBE unc (TgCO2)\nLNLGIS NCE (TgCO2)\nLNLGIS NCE unc (TgCO2)\nRivers (TgCO2)\nRiver unc (TgCO2)\nWood+Crop (TgCO2)\nWood+Crop unc (TgCO2)\nFF (TgCO2)\nFF unc (TgCO2)\nZ-statistic\nFUR LNLGIS\n\n\n\n\n0\nAFG\n2015\n39.3407\n153.746\n40.9643\n153.746\n60.3537\n153.744\n-2.43286\n1.69832\n4.05648\n1.21694\n19.3894\n0.797698\n0.37\n0.19\n\n\n1\nAFG\n2016\n50.6167\n175.454\n52.5114\n175.454\n73.0333\n175.452\n-2.16185\n2.24033\n4.05648\n1.21694\n20.5220\n0.678080\n0.31\n0.19\n\n\n2\nAFG\n2017\n54.5096\n179.794\n56.4726\n179.794\n77.5355\n179.793\n-2.09349\n2.37705\n4.05648\n1.21694\n21.0629\n0.695856\n0.47\n0.19\n\n\n3\nAFG\n2018\n116.4260\n243.057\n118.4610\n243.057\n143.9580\n243.056\n-2.02199\n2.52005\n4.05648\n1.21694\n25.4974\n0.695856\n0.39\n0.19\n\n\n4\nAFG\n2019\n64.0162\n181.516\n66.0388\n181.516\n93.8974\n181.514\n-2.03383\n2.49637\n4.05648\n1.21694\n27.8585\n0.797698\n0.49\n0.19"
  },
  {
    "objectID": "user_data_notebooks/oco2-mip-National-co2budget.html#sub-select-a-single-country",
    "href": "user_data_notebooks/oco2-mip-National-co2budget.html#sub-select-a-single-country",
    "title": "OCO-2 MIP National Top-Down CO2 Budgets",
    "section": "Sub-select a single country",
    "text": "Sub-select a single country\nLet’s further filter the dataset to look at a specific country. Choose a country by entering the alpha code in the country_name variable below\n\n# Choose a country\ncountry_name = 'USA' \n\n# We can sub-select the data for the country\ncountry_data = df[df['Alpha 3 Code'] == country_name]\n\n# Now we can look at the data for a specific experiment and country\ncountry_data.head()\n\n\n\n\n\n\n\n\nAlpha 3 Code\nYear\nLNLGIS dC_loss (TgCO2)\nLNLGIS dC_loss unc (TgCO2)\nLNLGIS NBE (TgCO2)\nLNLGIS NBE unc (TgCO2)\nLNLGIS NCE (TgCO2)\nLNLGIS NCE unc (TgCO2)\nRivers (TgCO2)\nRiver unc (TgCO2)\nWood+Crop (TgCO2)\nWood+Crop unc (TgCO2)\nFF (TgCO2)\nFF unc (TgCO2)\nZ-statistic\nFUR LNLGIS\n\n\n\n\n1232\nUSA\n2015\n-1031.83\n721.213\n-1346.46\n721.213\n4017.31\n713.897\n-165.430\n71.7453\n-149.196\n-44.7589\n5363.77\n102.4670\n-0.81\n0.91\n\n\n1233\nUSA\n2016\n-1419.92\n399.738\n-1743.80\n399.738\n3529.45\n387.079\n-174.684\n53.2375\n-149.196\n-44.7589\n5273.24\n99.8012\n0.04\n0.91\n\n\n1234\nUSA\n2017\n-1375.12\n1034.010\n-1696.63\n1034.010\n3515.14\n1029.250\n-172.308\n57.9894\n-149.196\n-44.7589\n5211.76\n99.0981\n0.67\n0.91\n\n\n1235\nUSA\n2018\n-1018.89\n784.463\n-1333.83\n784.463\n4036.65\n778.179\n-165.747\n71.1117\n-149.196\n-44.7589\n5370.48\n99.0981\n-0.20\n0.91\n\n\n1236\nUSA\n2019\n-1161.41\n718.054\n-1504.61\n718.054\n3728.95\n710.705\n-194.005\n14.5948\n-149.196\n-44.7589\n5233.56\n102.4670\n-0.38\n0.91\n\n\n\n\n\n\n\n#This dataset contains fluxes over a five year period, 2015-2020.\nLet’s look at a plot of the annual net terrestrial carbon stock loss (ΔCloss) for each year.\n\n# Make plot\nfig, ax1 = plt.subplots(1, 1, figsize=(6, 4))\nax1.errorbar(country_data['Year'],country_data[experiment+' dC_loss (TgCO2)'],\n                    yerr=country_data[experiment+' dC_loss unc (TgCO2)'],label=experiment,capsize=10)\nax1.legend(loc='upper right')\nax1.set_ylabel('$\\Delta$C$_\\mathrm{loss}$ (TgCO$_2$ year$^{-1}$)')\nax1.set_xlabel('Year')\nax1.set_title('$\\Delta$C$_\\mathrm{loss}$ for '+country_name)\nymin, ymax = ax1.get_ylim()\nmax_abs_y = max(abs(ymin), abs(ymax))\nax1.set_ylim([-max_abs_y, max_abs_y])\nxmin, xmax = ax1.get_xlim()\nax1.plot([xmin,xmax],[0,0],'k',linewidth=0.5)\nax1.set_xlim([xmin, xmax])\n\n\n\n\n\n\n\n\nNext, we can look at the full carbon budget for a given year.\nThe code below creates a plot similar to Fig 13 of Byrne et al. (2023). Each of the bars on the left side of the dashed vertical line (Fossil fuel emissions, lateral C transport by rivers, lateral C transport in crop and wood products, and the net terrestrial carbon stock loss combined to give the net carbon exchange (net surface-atmosphere CO2 flux) shown on the right.\n\n# Pick a specifc year (or mean year)\nyear='mean'\n\n# Make plot\ncountry_data_mean = country_data[country_data['Year'] == year]\na=country_data_mean['Wood+Crop (TgCO2)']\nb=country_data_mean['Wood+Crop unc (TgCO2)']\nprint(b)\n#\nplt.bar(1, country_data_mean['FF (TgCO2)'], yerr=country_data_mean['FF unc (TgCO2)'], label='FF', alpha=0.5)\nplt.bar(2, country_data_mean['Rivers (TgCO2)'], yerr=country_data_mean['River unc (TgCO2)'], label='Rivers', alpha=0.5)\nplt.bar(3, country_data_mean['Wood+Crop (TgCO2)'], yerr=abs(country_data_mean['Wood+Crop unc (TgCO2)']), label='WoodCrop', alpha=0.5)\nplt.bar(4, country_data_mean[experiment+' dC_loss (TgCO2)'], yerr=country_data_mean['LNLGIS dC_loss unc (TgCO2)'], label='dC', alpha=0.5)\nplt.bar(6, country_data_mean[experiment+' NCE (TgCO2)'], yerr=country_data_mean['LNLGIS NCE unc (TgCO2)'], label='NCE', alpha=0.5)\nax = plt.gca()\nymin, ymax = ax.get_ylim()\nplt.plot([5,5],[ymin,ymax],'k:')\nxmin, xmax = ax.get_xlim()\nplt.plot([xmin,xmax],[0,0],'k',linewidth=0.5)\nplt.xlim([xmin,xmax])\nplt.ylim([ymin,ymax])\n#\nplt.xticks([1,2,3,4,6], ['Fossil\\nFuels','Rivers','Wood+\\nCrops','$\\mathrm{\\Delta C _{loss}}$','NCE'])\nplt.title(country_name+' '+year)\nplt.ylabel('CO$_2$ Flux (TgCO$_2$ year$^{-1}$)')\n\n1238   -44.7589\nName: Wood+Crop unc (TgCO2), dtype: float64\n\n\nText(0, 0.5, 'CO$_2$ Flux (TgCO$_2$ year$^{-1}$)')\n\n\n\n\n\n\n\n\n\nUncertainty is an important consideration when analyzing the flux estimates provided by Byrne et al. (2023).\nEach flux estimate is provided with an error estimate representing the standard deviation, and assuming the errors are well prepresented by a normal distribution. This probability dirtribution provided by this uncertainty can be visualized below. We can further quantify the\n\n\n# Select NCE, NBE or dC_loss\nquantity = 'dC_loss'\n\n# Value for comparison\ncomparison_value = 1000 # TgCO2/year\n\n\nMIP_mean = country_data_mean[experiment+' '+quantity+' (TgCO2)'].item()\nMIP_std = country_data_mean[experiment+' '+quantity+' unc (TgCO2)'].item()\n\n# Perform t-test\nt_value = abs(MIP_mean - comparison_value)/(MIP_std / np.sqrt(11))\ncrtical_value = 2.23 # use p=0.05 significance\nif t_value &gt; crtical_value:\n    ttest = 'statistically different'\nif t_value &lt; crtical_value:\n    ttest = 'not statistically\\ndifferent'\n\n# Make plot\nxbounds = abs(MIP_mean)+MIP_std*4\nif abs(crtical_value) &gt; xbounds:\n    xbounds = abs(crtical_value)\nx_axis = np.arange(-1.*xbounds, xbounds, 1) \nplt.plot(x_axis, norm.pdf(x_axis, MIP_mean, MIP_std)) \nax = plt.gca()\nymin, ymax = ax.get_ylim()\nxmin, xmax = ax.get_xlim()\nplt.plot([0,0],[ymin,ymax*1.2],'k:',linewidth=0.5)\nplt.plot([xmin,xmax],[0,0],'k:',linewidth=0.5)\nplt.plot([comparison_value,comparison_value],[ymin,ymax*1.2],'k')\nplt.text(comparison_value+(xmax-xmin)*0.01,ymax*0.96,'value = '+str(comparison_value),ha='left',va='top')\nplt.text(comparison_value+(xmax-xmin)*0.01,ymax*0.9,ttest,ha='left',va='top')\nplt.ylim([ymin,ymax*1.2])\nplt.xlim([xmin,xmax])\nplt.plot(MIP_mean,ymax*1.03,'ko')\nplt.plot([MIP_mean-MIP_std,\n         MIP_mean+MIP_std],\n         [ymax*1.03,ymax*1.03],'k')\nplt.plot([MIP_mean-MIP_std,\n         MIP_mean-MIP_std],\n         [ymax*1.005,ymax*1.055],'k')\nplt.plot([MIP_mean+MIP_std,\n         MIP_mean+MIP_std],\n         [ymax*1.005,ymax*1.055],'k')\nplt.text(MIP_mean,ymax*1.115,\n         str(round(MIP_mean))+' $\\pm$ '+\n         str(round(MIP_std))+' TgCO$_2$',ha='center')\nplt.title(country_name+' '+year+' '+quantity+'')\nplt.yticks([])\nplt.ylabel('Probability')\nplt.xlabel(quantity+' (TgCO$_2$ year$^{-1}$)')\n\nText(0.5, 0, 'dC_loss (TgCO$_2$ year$^{-1}$)')\n\n\n\n\n\n\n\n\n\nFinally, we will examine two metrics that are useful for understanding the confidence in the top-down results:\n\nZ-statistic: metric of agreement in NCE estimates across the experiments that assimilate different CO2 datasets. Experiments are considered significantly different if the magnitude exceeds 1.96\nFractional Uncertainty Reduction (FUR): metric of how strongly the assimilated CO2 data on reduce NCE uncertainties. Values range from 0 to 1, with 0 meaning zero error reduction and 1 meaning complete error reduction\n\nHere we will add a plot of the Z-statistic for each year, and add the FUR value for the country.\n\n# Make plot\nfig, ax1 = plt.subplots(1, 1, figsize=(6, 4))\nax1.plot(country_data['Year'],country_data['Z-statistic'],label=experiment)\nax1.legend(loc='upper right')\nax1.set_ylabel('Z-statistic')\nax1.set_xlabel('Year')\nax1.set_title(country_name)\nymin, ymax = ax1.get_ylim()\nmax_abs_y = max(abs(ymin), abs(ymax))\nax1.set_ylim([-3, 3])\nxmin, xmax = ax1.get_xlim()\nax1.plot([xmin,xmax],[0,0],'k',linewidth=0.5)\nax1.plot([xmin,xmax],[-1.96,-1.96],'k--',linewidth=0.5)\nax1.plot([xmin,xmax],[1.96,1.96],'k--',linewidth=0.5)\nax1.set_xlim([xmin, xmax])\nax1.text(xmin+0.12,2.6,'Fractional error reduction: '+str(country_data['FUR '+experiment].iloc[1]))\n\nText(-0.18000000000000005, 2.6, 'Fractional error reduction: 0.91')"
  },
  {
    "objectID": "user_data_notebooks/vulcan-ffco2-yeargrid-v4_User_Notebook.html",
    "href": "user_data_notebooks/vulcan-ffco2-yeargrid-v4_User_Notebook.html",
    "title": "Vulcan Fossil Fuel CO₂ Emissions",
    "section": "",
    "text": "You can launch this notebook in the US GHG Center JupyterHub by clicking the link below.\nLaunch in the US GHG Center JupyterHub (requires access)",
    "crumbs": [
      "Data Usage Notebooks",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "Vulcan Fossil Fuel CO₂ Emissions"
    ]
  },
  {
    "objectID": "user_data_notebooks/vulcan-ffco2-yeargrid-v4_User_Notebook.html#run-this-notebook",
    "href": "user_data_notebooks/vulcan-ffco2-yeargrid-v4_User_Notebook.html#run-this-notebook",
    "title": "Vulcan Fossil Fuel CO₂ Emissions",
    "section": "",
    "text": "You can launch this notebook in the US GHG Center JupyterHub by clicking the link below.\nLaunch in the US GHG Center JupyterHub (requires access)",
    "crumbs": [
      "Data Usage Notebooks",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "Vulcan Fossil Fuel CO₂ Emissions"
    ]
  },
  {
    "objectID": "user_data_notebooks/vulcan-ffco2-yeargrid-v4_User_Notebook.html#approach",
    "href": "user_data_notebooks/vulcan-ffco2-yeargrid-v4_User_Notebook.html#approach",
    "title": "Vulcan Fossil Fuel CO₂ Emissions",
    "section": "Approach",
    "text": "Approach\n\nIdentify available dates and temporal frequency of observations for the given collection using the GHGC API /stac endpoint. The collection processed in this notebook is the Vulcan Fossil Fuel CO₂ Emissions Data product.\nPass the STAC item into the raster API /stac/tilejson.jsonendpoint.\nUsing folium.plugins.DualMap, we will visualize two tiles (side-by-side), allowing us to compare time points.\nAfter the visualization, we will perform zonal statistics for a given polygon.",
    "crumbs": [
      "Data Usage Notebooks",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "Vulcan Fossil Fuel CO₂ Emissions"
    ]
  },
  {
    "objectID": "user_data_notebooks/vulcan-ffco2-yeargrid-v4_User_Notebook.html#about-the-data",
    "href": "user_data_notebooks/vulcan-ffco2-yeargrid-v4_User_Notebook.html#about-the-data",
    "title": "Vulcan Fossil Fuel CO₂ Emissions",
    "section": "About the Data",
    "text": "About the Data\nThe Vulcan version 4.0 data product represents total carbon dioxide (CO2) emissions resulting from the combustion of fossil fuel (ff) for the contiguous United States and District of Columbia. Referred to as ffCO2, the emissions from Vulcan are also categorized into 10 source sectors including; airports, cement production, commercial marine vessels, commercial, power plants, industrial, non-road, on-road, residential and railroads. Data are gridded annually on a 1-km grid for the years 2010 to 2021. These data are annual sums of hourly estimates. Shown is the estimated total annual ffCO2 for the United States, as well as the estimated total annual ffCO2 per sector.\nFor more information regarding this dataset, please visit the Vulcan Fossil Fuel CO₂ Emissions, Version 4 data overview page.",
    "crumbs": [
      "Data Usage Notebooks",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "Vulcan Fossil Fuel CO₂ Emissions"
    ]
  },
  {
    "objectID": "user_data_notebooks/vulcan-ffco2-yeargrid-v4_User_Notebook.html#querying-the-stac-api",
    "href": "user_data_notebooks/vulcan-ffco2-yeargrid-v4_User_Notebook.html#querying-the-stac-api",
    "title": "Vulcan Fossil Fuel CO₂ Emissions",
    "section": "Querying the STAC API",
    "text": "Querying the STAC API\nFirst, we are going to import the required libraries. Once imported, they allow better executing a query in the GHG Center Spatio Temporal Asset Catalog (STAC) Application Programming Interface (API) where the granules for this collection are stored.\n\n# Provide STAC and RASTER API endpoints\nSTAC_API_URL = \"https://earth.gov/ghgcenter/api/stac\"\nRASTER_API_URL = \"https://earth.gov/ghgcenter/api/raster\"\n\n# Please use the collection name similar to the one used in the STAC collection.\n# Name of the collection for Vulcan Fossil Fuel CO₂ Emissions, Version 4. \ncollection_name = \"vulcan-ffco2-yeargrid-v4\"\n\n\n# Fetch the collection from STAC collections using the appropriate endpoint\n# the 'requests' library allows a HTTP request possible\ncollection_vulcan = requests.get(f\"{STAC_API_URL}/collections/{collection_name}\").json()\n\nExamining the contents of our collection under the temporal variable, we see that the data is available from January 2010 to December 2021. By looking at the dashboard:time density, we observe that the data is periodic with year time density.\n\ncollection_vulcan\n\n{'id': 'vulcan-ffco2-yeargrid-v4-updated',\n 'type': 'Collection',\n 'links': [{'rel': 'items',\n   'type': 'application/geo+json',\n   'href': 'https://dev.ghg.center/api/stac/collections/vulcan-ffco2-yeargrid-v4-updated/items'},\n  {'rel': 'parent',\n   'type': 'application/json',\n   'href': 'https://dev.ghg.center/api/stac/'},\n  {'rel': 'root',\n   'type': 'application/json',\n   'href': 'https://dev.ghg.center/api/stac/'},\n  {'rel': 'self',\n   'type': 'application/json',\n   'href': 'https://dev.ghg.center/api/stac/collections/vulcan-ffco2-yeargrid-v4-updated'}],\n 'title': 'Vulcan Fossil Fuel CO₂ Emissions v4.0',\n 'extent': {'spatial': {'bbox': [[-128.22654896758996,\n     22.857766529124284,\n     -65.30917199495289,\n     51.44087947724907]]},\n  'temporal': {'interval': [['2010-01-01 00:00:00+00',\n     '2021-12-31 00:00:00+00']]}},\n 'license': 'CC-BY-NC-4.0',\n 'renders': {'air-co2': {'assets': ['air-co2'],\n   'rescale': [[0, 150]],\n   'colormap_name': 'rdylbu_r'},\n  'cmt-co2': {'assets': ['cmt-co2'],\n   'rescale': [[0, 150]],\n   'colormap_name': 'rdylbu_r'},\n  'cmv-co2': {'assets': ['cmv-co2'],\n   'rescale': [[0, 150]],\n   'colormap_name': 'rdylbu_r'},\n  'com-co2': {'assets': ['com-co2'],\n   'rescale': [[0, 150]],\n   'colormap_name': 'rdylbu_r'},\n  'elc-co2': {'assets': ['elc-co2'],\n   'rescale': [[0, 150]],\n   'colormap_name': 'rdylbu_r'},\n  'ind-co2': {'assets': ['ind-co2'],\n   'rescale': [[0, 150]],\n   'colormap_name': 'rdylbu_r'},\n  'nrd-co2': {'assets': ['nrd-co2'],\n   'rescale': [[0, 150]],\n   'colormap_name': 'rdylbu_r'},\n  'onr-co2': {'assets': ['onr-co2'],\n   'rescale': [[0, 150]],\n   'colormap_name': 'rdylbu_r'},\n  'res-co2': {'assets': ['res-co2'],\n   'rescale': [[0, 150]],\n   'colormap_name': 'rdylbu_r'},\n  'rrd-co2': {'assets': ['rrd-co2'],\n   'rescale': [[0, 150]],\n   'colormap_name': 'rdylbu_r'},\n  'dashboard': {'assets': ['total-co2'],\n   'rescale': [[0, 150]],\n   'colormap_name': 'rdylbu_r'},\n  'total-co2': {'assets': ['total-co2'],\n   'rescale': [[0, 150]],\n   'colormap_name': 'rdylbu_r'}},\n 'providers': [{'url': 'https://vulcan.rc.nau.edu/',\n   'name': 'North American Carbon Program',\n   'roles': ['producer', 'licensor']}],\n 'summaries': {'datetime': ['2010-01-01T00:00:00Z', '2021-12-31T00:00:00Z']},\n 'description': 'The Vulcan version 4.0 data product represents total carbon dioxide (CO₂) emissions resulting from the combustion of fossil fuel (FF) for the contiguous United States and District of Columbia. Referred to as FFCO₂, the emissions from Vulcan are also categorized into 10 source sectors including; airports, cement production, commercial marine vessels, commercial, power plants, industrial, non-road, on-road, residential and railroads. Data are gridded annually on a 1-km grid for the years 2010 to 2021. These data are annual sums of hourly estimates. Included is the estimated total annual FFCO2 for the United States, as well as the estimated total annual FFCO₂ per sector in units of metric tonnes of carbon per 1 km x 1 km grid cell per year (tonne C/km2/year). The Vulcan Project is a multiagency (NASA, DOE, NOAA, NIST) funded effort under the North American Carbon Program (NACP) to quantify North American fossil fuel carbon dioxide (FFCO₂) emissions at space and time scales much finer than has been achieved in the past. The purpose is to aid in quantification of the North American carbon budget, to support inverse estimation of carbon sources and sinks, and to support the demands posed by higher resolution FFCO₂ observations (in situ and remotely sensed). The detail and scope of the Vulcan FFCO₂ inventory has also made it a valuable tool for policymakers, demographers, social scientists and the public at large. Learn more at the Vulcan website: https://vulcan.rc.nau.edu/index.html',\n 'item_assets': {'air-co2': {'type': 'image/tiff; application=geotiff; profile=cloud-optimized',\n   'roles': ['data', 'layer'],\n   'title': 'Airport Fossil Fuel CO₂ Emissions',\n   'description': 'Estimated total annual FFCO₂ emissions from airports.'},\n  'cmt-co2': {'type': 'image/tiff; application=geotiff; profile=cloud-optimized',\n   'roles': ['data', 'layer'],\n   'title': 'Cement Fossil Fuel CO₂ Emissions',\n   'description': 'Estimated total annual FFCO₂ emissions from cement production.'},\n  'cmv-co2': {'type': 'image/tiff; application=geotiff; profile=cloud-optimized',\n   'roles': ['data', 'layer'],\n   'title': 'Commercial Marine Vessel Fossil Fuel CO₂ Emissions',\n   'description': 'Estimated total annual FFCO₂ emissions from commercial marine vessels.'},\n  'com-co2': {'type': 'image/tiff; application=geotiff; profile=cloud-optimized',\n   'roles': ['data', 'layer'],\n   'title': 'Commercial Fossil Fuel CO₂ Emissions',\n   'description': 'Estimated total annual FFCO₂ emissions from commercial sources.'},\n  'elc-co2': {'type': 'image/tiff; application=geotiff; profile=cloud-optimized',\n   'roles': ['data', 'layer'],\n   'title': 'Power Plant Fossil Fuel CO₂ Emissions',\n   'description': 'Estimated total annual FFCO₂ emissions from power plants.'},\n  'ind-co2': {'type': 'image/tiff; application=geotiff; profile=cloud-optimized',\n   'roles': ['data', 'layer'],\n   'title': 'Industrial Fossil Fuel CO₂ Emissions',\n   'description': 'Estimated total annual FFCO₂ emissions from industrial fossil fuel combustion.'},\n  'nrd-co2': {'type': 'image/tiff; application=geotiff; profile=cloud-optimized',\n   'roles': ['data', 'layer'],\n   'title': 'Non-Road Fossil Fuel CO₂ Emissions',\n   'description': 'Estimated total annual FFCO₂ emissions from non-road engines, equipment and vehicles.'},\n  'onr-co2': {'type': 'image/tiff; application=geotiff; profile=cloud-optimized',\n   'roles': ['data', 'layer'],\n   'title': 'On-Road Fossil Fuel CO₂ Emissions',\n   'description': 'Estimated total annual FFCO₂ emissions from on-road engines, equipment and vehicles.'},\n  'res-co2': {'type': 'image/tiff; application=geotiff; profile=cloud-optimized',\n   'roles': ['data', 'layer'],\n   'title': 'Residential Fossil Fuel CO₂ Emissions',\n   'description': 'Estimated total annual FFCO₂ emissions from residential sources.'},\n  'rrd-co2': {'type': 'image/tiff; application=geotiff; profile=cloud-optimized',\n   'roles': ['data', 'layer'],\n   'title': 'Railroad Fossil Fuel CO₂ Emissions',\n   'description': 'Estimated total annual FFCO₂ emissions coming from railroads.'},\n  'total-co2': {'type': 'image/tiff; application=geotiff; profile=cloud-optimized',\n   'roles': ['data', 'layer'],\n   'title': 'Total Fossil Fuel CO₂ Emissions',\n   'description': 'Estimated total annual CO₂ emissions from fossil fuel combustion (FFCO₂) across all sectors.'}},\n 'stac_version': '1.0.0',\n 'stac_extensions': ['https://stac-extensions.github.io/render/v1.0.0/schema.json',\n  'https://stac-extensions.github.io/item-assets/v1.0.0/schema.json'],\n 'dashboard:is_periodic': True,\n 'dashboard:time_density': 'year'}\n\n\n\n# Create a function that would search for the above data collection in the STAC API\ndef get_item_count(collection_id):\n    count = 0\n    items_url = f\"{STAC_API_URL}/collections/{collection_id}/items\"\n\n    while True:\n        response = requests.get(items_url)\n\n        if not response.ok:\n            print(\"error getting items\")\n            exit()\n\n        stac = response.json()\n        count += int(stac[\"context\"].get(\"returned\", 0))\n        next = [link for link in stac[\"links\"] if link[\"rel\"] == \"next\"]\n\n        if not next:\n            break\n        items_url = next[0][\"href\"]\n\n    return count\n\n\n# Apply the above function and check the total number of items available within the collection\nnumber_of_items = get_item_count(collection_name)\nitems_vulcan = requests.get(f\"{STAC_API_URL}/collections/{collection_name}/items?limit={number_of_items}\").json()[\"features\"]\nprint(f\"Found {len(items_vulcan)} items\")\n\nFound 12 items\n\n\n\n# Examine the first item in the collection\n# Keep in mind that a list starts from 0, 1, 2... therefore items[0] is referring to the first item in the list/collection\nitems_vulcan[0]\n\n\n# To access the year value from each item more easily, this will let us query more explicitly by year and month (e.g., 2020-02)\nitems = {item[\"properties\"][\"start_datetime\"][:4]: item for item in items_vulcan} \n# rh = Heterotrophic Respiration\nasset_name = \"total-co2\"\n\n\nrescale_values = {\"max\":items[list(items.keys())[0]][\"assets\"][asset_name][\"raster:bands\"][0][\"histogram\"][\"max\"], \"min\":items[list(items.keys())[0]][\"assets\"][asset_name][\"raster:bands\"][0][\"histogram\"][\"min\"]}\n\nNow, we will pass the item id, collection name, asset name, and the rescaling factor to the Raster API endpoint. We will do this twice, once for 2021 and again for 2010, so that we can visualize each event independently.\n\ncolor_map = \"spectral_r\" # please refer to matplotlib library if you'd prefer choosing a different color ramp.\n# For more information on Colormaps in Matplotlib, please visit https://matplotlib.org/stable/users/explain/colors/colormaps.html\n\n# To change the year and month of the observed parameter, you can modify the \"items['YYYY-MM']\" statement\n# For example, you can change the current statement \"items['2003-12']\" to \"items['2016-10']\" \n_2021_tile = requests.get(\n    f\"{RASTER_API_URL}/collections/{items['2021']['collection']}/items/{items['2021']['id']}/tilejson.json?collection={items['2021']['collection']}&item={items['2021']['id']}\"\n\n    f\"&assets={asset_name}\"\n    f\"&color_formula=gamma+r+1.05&colormap_name={color_map}\"\n    f\"&rescale=0,150\", \n).json()\n_2021_tile\n\n{'tilejson': '2.2.0',\n 'version': '1.0.0',\n 'scheme': 'xyz',\n 'tiles': ['https://dev.ghg.center/api/raster/collections/vulcan-ffco2-yeargrid-v4-updated/items/vulcan-ffco2-yeargrid-v4-updated-2021/tiles/WebMercatorQuad/{z}/{x}/{y}@1x?collection=vulcan-ffco2-yeargrid-v4-updated&item=vulcan-ffco2-yeargrid-v4-updated-2021&assets=total-co2&color_formula=gamma+r+1.05&colormap_name=spectral_r&rescale=0%2C150'],\n 'minzoom': 0,\n 'maxzoom': 24,\n 'bounds': [-128.22654896758996,\n  22.857766529124284,\n  -65.30917199495289,\n  51.44087947724907],\n 'center': [-96.76786048127143, 37.14932300318668, 0]}\n\n\n\n_2010_tile = requests.get(\n    f\"{RASTER_API_URL}/collections/{items['2010']['collection']}/items/{items['2010']['id']}/tilejson.json?collection={items['2010']['collection']}&item={items['2010']['id']}\"\n\n    f\"&assets={asset_name}\"\n    f\"&color_formula=gamma+r+1.05&colormap_name={color_map}\"\n    f\"&rescale=0,150\", \n).json()\n_2010_tile\n\n{'tilejson': '2.2.0',\n 'version': '1.0.0',\n 'scheme': 'xyz',\n 'tiles': ['https://dev.ghg.center/api/raster/collections/vulcan-ffco2-yeargrid-v4-updated/items/vulcan-ffco2-yeargrid-v4-updated-2010/tiles/WebMercatorQuad/{z}/{x}/{y}@1x?collection=vulcan-ffco2-yeargrid-v4-updated&item=vulcan-ffco2-yeargrid-v4-updated-2010&assets=total-co2&color_formula=gamma+r+1.05&colormap_name=spectral_r&rescale=0%2C150'],\n 'minzoom': 0,\n 'maxzoom': 24,\n 'bounds': [-128.22654896758996,\n  22.857766529124284,\n  -65.30917199495289,\n  51.44087947724907],\n 'center': [-96.76786048127143, 37.14932300318668, 0]}",
    "crumbs": [
      "Data Usage Notebooks",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "Vulcan Fossil Fuel CO₂ Emissions"
    ]
  },
  {
    "objectID": "user_data_notebooks/vulcan-ffco2-yeargrid-v4_User_Notebook.html#visualizing-total-fossil-fuel-co₂-emissions",
    "href": "user_data_notebooks/vulcan-ffco2-yeargrid-v4_User_Notebook.html#visualizing-total-fossil-fuel-co₂-emissions",
    "title": "Vulcan Fossil Fuel CO₂ Emissions",
    "section": "Visualizing Total Fossil Fuel CO₂ Emissions",
    "text": "Visualizing Total Fossil Fuel CO₂ Emissions\n\nmap_ = folium.plugins.DualMap(location=(34, -118), zoom_start=6)\n\n\n# Define the first map layer with the CO2 Flux data for December 2022\nmap_layer_2021 = TileLayer(\n    tiles=_2021_tile[\"tiles\"][0], # Path to retrieve the tile\n    attr=\"GHG\", # Set the attribution \n    name='2021 Total CO2 Fossil Fuel Emissions', # Title for the layer\n    overlay=True, # The layer can be overlaid on the map\n    opacity=0.8, # Adjust the transparency of the layer\n)\n# Add the first layer to the Dual Map \nmap_layer_2021.add_to(map_.m1)\n\nmap_layer_2010 = TileLayer(\n    tiles=_2010_tile[\"tiles\"][0], # Path to retrieve the tile\n    attr=\"GHG\", # Set the attribution \n    name='2010 Total CO2 Fossil Fuel Emissions', # Title for the layer\n    overlay=True, # The layer can be overlaid on the map\n    opacity=0.8, # Adjust the transparency of the layer\n)\n# Add the first layer to the Dual Map \nmap_layer_2010.add_to(map_.m2)\n\nmap_\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook",
    "crumbs": [
      "Data Usage Notebooks",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "Vulcan Fossil Fuel CO₂ Emissions"
    ]
  },
  {
    "objectID": "user_data_notebooks/vulcan-ffco2-yeargrid-v4_User_Notebook.html#visualizing-the-data-as-a-time-series",
    "href": "user_data_notebooks/vulcan-ffco2-yeargrid-v4_User_Notebook.html#visualizing-the-data-as-a-time-series",
    "title": "Vulcan Fossil Fuel CO₂ Emissions",
    "section": "Visualizing the Data as a Time Series",
    "text": "Visualizing the Data as a Time Series\nWe can now explore the total fossil fuel emission time series (2010 -2021) available for the Dallas, Texas area of the U.S. We can plot the data set using the code below:\n\n# Figure size: 20 representing the width, 10 representing the height\nfig = plt.figure(figsize=(20, 10))\n\nplt.plot(\n    df[\"datetime\"], # X-axis: sorted datetime\n    df[\"max\"], # Y-axis: maximum CO₂\n    color=\"red\", # Line color\n    linestyle=\"-\", # Line style\n    linewidth=0.5, # Line width\n    label=\"CO₂ emissions\", # Legend label\n)\n\n# Display legend\nplt.legend()\n\n# Insert label for the X-axis\nplt.xlabel(\"Years\")\n\n# Insert label for the Y-axis\nplt.ylabel(\"tC/km²/year\")\nplt.xticks(rotation = 90)\n\n# Insert title for the plot\nplt.title(\"Total Fossil Fuel CO₂ Emissions for Texas, Dallas (2010-2021)\")\n\n# Add data citation\nplt.text(\n    df[\"datetime\"].iloc[0],           # X-coordinate of the text\n    df[\"max\"].min(),                  # Y-coordinate of the text\n\n\n\n\n    # Text to be displayed\n    \"Source: https://doi.org/10.3334/ORNLDAAC/1741\",                  \n    fontsize=12,                             # Font size\n    horizontalalignment=\"left\",              # Horizontal alignment\n    verticalalignment=\"top\",                 # Vertical alignment\n    color=\"blue\",                            # Text color\n)\n\n\n# Plot the time series\nplt.show()",
    "crumbs": [
      "Data Usage Notebooks",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "Vulcan Fossil Fuel CO₂ Emissions"
    ]
  },
  {
    "objectID": "user_data_notebooks/vulcan-ffco2-yeargrid-v4_User_Notebook.html#summary",
    "href": "user_data_notebooks/vulcan-ffco2-yeargrid-v4_User_Notebook.html#summary",
    "title": "Vulcan Fossil Fuel CO₂ Emissions",
    "section": "Summary",
    "text": "Summary\nIn this notebook we have successfully explored, analyzed, and visualized the STAC collection for Vulcan Fossil Fuel CO₂ Emissions, Version 4 dataset.\n\nInstall and import the necessary libraries\nFetch the collection from STAC collections using the appropriate endpoints\nCount the number of existing granules within the collection\nMap and compare the total fossil fuel CO₂ emissions for two distinctive months/years\nGenerate zonal statistics for the area of interest (AOI)\nVisualizing the Data as a Time Series\n\nIf you have any questions regarding this user notebook, please contact us using the feedback form.",
    "crumbs": [
      "Data Usage Notebooks",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "Vulcan Fossil Fuel CO₂ Emissions"
    ]
  },
  {
    "objectID": "workflow.html",
    "href": "workflow.html",
    "title": "U.S. Greenhouse Gas Center: Data Flow Diagrams",
    "section": "",
    "text": "Welcome to the homepage for U.S. Greenhouse Gas (GHG) Center data workflow diagrams. Use these diagrams to discover the journey of each dataset from acquisition to integration in the US GHG Center.\nData flow diagrams are grouped topically and labeled by dataset name. Click on a dataset name to view the data flow diagram for that dataset, which summarizes the process followed to bring the dataset into the US GHG Center.\nJoin us in our mission to make data-driven environmental solutions accessible. Explore, analyze, and make a difference with the US GHG Center.\nView the US GHG Center Data Catalog",
    "crumbs": [
      "Data Flow Diagrams"
    ]
  },
  {
    "objectID": "workflow.html#gridded-anthropogenic-greenhouse-gas-emissions",
    "href": "workflow.html#gridded-anthropogenic-greenhouse-gas-emissions",
    "title": "U.S. Greenhouse Gas Center: Data Flow Diagrams",
    "section": "Gridded Anthropogenic Greenhouse Gas Emissions",
    "text": "Gridded Anthropogenic Greenhouse Gas Emissions\n\nOCO-2 MIP Top-Down CO₂ Budgets Data Flow Diagram\nODIAC Fossil Fuel CO₂ Emissions Data Flow Diagram\nTM5-4DVar Isotopic CH₄ Inverse Fluxes Data Flow Diagram\nU.S. Gridded Anthropogenic Methane Emissions Inventory Data Flow Diagram\nVulcan Fossil Fuel CO₂ Emissions Data Flow Diagram",
    "crumbs": [
      "Data Flow Diagrams"
    ]
  },
  {
    "objectID": "workflow.html#natural-greenhouse-gas-sources-emissions-and-sinks",
    "href": "workflow.html#natural-greenhouse-gas-sources-emissions-and-sinks",
    "title": "U.S. Greenhouse Gas Center: Data Flow Diagrams",
    "section": "Natural Greenhouse Gas Sources Emissions and Sinks",
    "text": "Natural Greenhouse Gas Sources Emissions and Sinks\n\nAir-Sea CO₂ Flux, ECCO-Darwin Model v5 Data Flow Diagram\nMiCASA Land Carbon Flux Data Flow Diagram\nGOSAT-based Top-down Total and Natural Methane Emissions Data Flow Diagram\nOCO-2 MIP Top-Down CO₂ Budgets Data Flow Diagram\nTM5-4DVar Isotopic CH₄ Inverse Fluxes Data Flow Diagram\nWetland Methane Emissions, LPJ-EOSIM Model Data Flow Diagram\nGRA²PES Greenhouse Gas and Air Quality Species Data Flow Diagram",
    "crumbs": [
      "Data Flow Diagrams"
    ]
  },
  {
    "objectID": "workflow.html#large-emissions-events",
    "href": "workflow.html#large-emissions-events",
    "title": "U.S. Greenhouse Gas Center: Data Flow Diagrams",
    "section": "Large Emissions Events",
    "text": "Large Emissions Events\n\nEMIT Methane Point Source Plume Complexes Data Flow Diagram",
    "crumbs": [
      "Data Flow Diagrams"
    ]
  },
  {
    "objectID": "workflow.html#greenhouse-gas-concentrations",
    "href": "workflow.html#greenhouse-gas-concentrations",
    "title": "U.S. Greenhouse Gas Center: Data Flow Diagrams",
    "section": "Greenhouse Gas Concentrations",
    "text": "Greenhouse Gas Concentrations\n\nAtmospheric Carbon Dioxide Concentrations from NOAA Global Monitoring Laboratory Data Flow Diagram\nAtmospheric Methane Concentrations from NOAA Global Monitoring Laboratory Data Flow Diagram\nOCO-2 GEOS Column CO₂ Concentrations Data Flow Diagram\nCarbon Dioxide and Methane Concentrations from the Indianapolis Flux Experiment (INFLUX)\nCarbon Dioxide and Methane Concentrations from the Los Angeles Megacity Carbon Project\nCarbon Dioxide and Methane Concentrations from the Northeast Corridor (NEC) Urban Test Bed",
    "crumbs": [
      "Data Flow Diagrams"
    ]
  },
  {
    "objectID": "workflow.html#socioeconomic",
    "href": "workflow.html#socioeconomic",
    "title": "U.S. Greenhouse Gas Center: Data Flow Diagrams",
    "section": "Socioeconomic",
    "text": "Socioeconomic\n\nSEDAC Gridded World Population Density Data Flow Diagram",
    "crumbs": [
      "Data Flow Diagrams"
    ]
  },
  {
    "objectID": "workflow.html#contact",
    "href": "workflow.html#contact",
    "title": "U.S. Greenhouse Gas Center: Data Flow Diagrams",
    "section": "Contact",
    "text": "Contact\nFor technical help or general questions, please contact the support team using the feedback form.",
    "crumbs": [
      "Data Flow Diagrams"
    ]
  },
  {
    "objectID": "cog_transformation/gra2pes-ghg-monthgrid-v1.html",
    "href": "cog_transformation/gra2pes-ghg-monthgrid-v1.html",
    "title": "GRA²PES Greenhouse Gas and Air Quality Species",
    "section": "",
    "text": "This script was used to transform the GRA2PES dataset to Cloud Optimized GeoTIFF (COG) format for display in the Greenhouse Gas (GHG) Center.\n\nimport xarray as xr\nimport os\nimport glob\nfrom  datetime import datetime\nimport boto3\nimport s3fs\nimport tempfile\nimport numpy as np\n\nimport rasterio\nfrom rasterio.enums import Resampling\nfrom rio_cogeo.cogeo import cog_translate\nfrom rio_cogeo.profiles import cog_profiles\n\n\nconfig = {\n    \"data_acquisition_method\": \"s3\",\n    \"raw_data_bucket\" : \"gsfc-ghg-store\",\n    \"raw_data_prefix\": \"GRA2PES/monthly_subset_regrid/2021\", \n    \"cog_data_bucket\": \"ghgc-data-store-develop\",\n    \"cog_data_prefix\": \"transformed_cogs/GRAAPES\",\n    \"date_fmt\" :\"%Y%m\",\n    \"transformation\": {}\n}\n\n\nsession = boto3.session.Session()\ns3_client = session.client(\"s3\")\n\nraw_data_bucket = config[\"raw_data_bucket\"]\nraw_data_prefix= config[\"raw_data_prefix\"]\n\ncog_data_bucket = config['cog_data_bucket']\ncog_data_prefix= config[\"cog_data_prefix\"]\n\ndate_fmt=config['date_fmt']\n\nfs = s3fs.S3FileSystem()\n\n\ndef get_all_s3_keys(bucket, model_name, ext):\n    \"\"\"Get a list of all keys in an S3 bucket.\"\"\"\n    keys = []\n\n    kwargs = {\"Bucket\": bucket, \"Prefix\": f\"{model_name}/\"}\n    while True:\n        resp = s3_client.list_objects_v2(**kwargs)\n        for obj in resp[\"Contents\"]:\n            if obj[\"Key\"].endswith(ext) and \"historical\" not in obj[\"Key\"]:\n                keys.append(obj[\"Key\"])\n\n        try:\n            kwargs[\"ContinuationToken\"] = resp[\"NextContinuationToken\"]\n        except KeyError:\n            break\n\n    return keys\n\nkeys = get_all_s3_keys(raw_data_bucket, raw_data_prefix, \".nc4\")\n\ndef download_s3_objects(bucket, keys, download_dir):\n    \"\"\"Download all S3 objects listed in keys to the specified local directory.\"\"\"\n    if not os.path.exists(download_dir):\n        os.makedirs(download_dir)\n\n    for key in keys:\n        local_filename = os.path.join(download_dir, os.path.basename(key))\n        try:\n            s3_client.download_file(bucket, key, local_filename)\n            print(f\"Downloaded {key} to {local_filename}\")\n        except (NoCredentialsError, PartialCredentialsError) as e:\n            print(f\"Credentials error: {e}\")\n        except Exception as e:\n            print(f\"Failed to download {key}: {e}\")\n\ndownload_s3_objects(raw_data_bucket, keys, \"data\")\n\n\n\ndef extract_date_from_key(key):\n    # Split the key to isolate the part that contains the date\n    parts = key.split('_')\n    for part in parts:\n        # Check if the part is numeric and has the length of 6 (YYYYMM format)\n        if part.isdigit() and len(part) == 6:\n            return part\n    return None\n\n\nCOG_PROFILE = {\"driver\": \"COG\", \"compress\": \"DEFLATE\"}\nOVERVIEW_LEVELS = 4 \nOVERVIEW_RESAMPLING = 'average'\n\nfor key in glob.glob(\"data/*.nc4\"):\n    xds= xr.open_dataset(key)\n    xds = xds.assign_coords(lon=(((xds.lon + 180) % 360) - 180)).sortby(\"lon\")\n    \n    for var in [\"PM25-PRI\",\"CO2\",\"CO\",\"NOX\",\"SOX\"]:\n        yearmonth = extract_date_from_key(key)\n        filename = f\"output/GRA2PESv1.0_total_{(\"-\").join(var.split('_'))}_{yearmonth}.tif\"\n        data = getattr(xds,var)\n        data.rio.set_spatial_dims(\"lon\", \"lat\", inplace=True)\n        data.rio.write_crs(\"epsg:4326\", inplace=True)\n        \n        # Create a temporary file to hold the COG\n        with tempfile.NamedTemporaryFile(suffix='.tif', delete=False) as temp_file:\n            data.rio.to_raster(f\"temp_{yearmonth}_{var}.tif\", **COG_PROFILE, nodata=-9999)\n            # Create COG with overviews and nodata value\n            cog_translate(\n                f\"temp_{yearmonth}_{var}.tif\",\n                temp_file.name,\n                cog_profiles.get(\"deflate\"),\n                overview_level=OVERVIEW_LEVELS,\n                overview_resampling=OVERVIEW_RESAMPLING,\n                nodata=-9999\n            )\n            \n            # Move the temporary file to the desired local path\n            os.rename(temp_file.name, filename)\n    \n        if os.path.exists(f\"temp_{yearmonth}_{var}.tif\"):\n            os.remove(f\"temp_{yearmonth}_{var}.tif\")\n        del data\n        print(f\"Done for: {filename}\")\n    \n\n\n\n\n Back to top",
    "crumbs": [
      "Data Transformation Notebooks",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "GRA²PES Greenhouse Gas and Air Quality Species"
    ]
  },
  {
    "objectID": "cog_transformation/oco2geos-co2-daygrid-v10r.html",
    "href": "cog_transformation/oco2geos-co2-daygrid-v10r.html",
    "title": "OCO-2 GEOS Column CO₂ Concentrations",
    "section": "",
    "text": "This script was used to transform the OCO-2 GEOS Column CO₂ Concentrations dataset from netCDF to Cloud Optimized GeoTIFF (COG) format for display in the Greenhouse Gas (GHG) Center.\n\nimport xarray\nimport re\nimport pandas as pd\nimport json\nimport tempfile\nimport boto3\nimport os\n\n\nsession = boto3.Session()\ns3_client = session.client(\"s3\")\nbucket_name = (\n    \"ghgc-data-store-dev\"  # S3 bucket where the COGs are stored after transformation\n)\nFOLDER_NAME = \"earth_data/geos_oco2\"\ns3_folder_name = \"geos-oco2\"\n\nerror_files = []\ncount = 0\nfiles_processed = pd.DataFrame(\n    columns=[\"file_name\", \"COGs_created\"]\n)  # A dataframe to keep track of the files that we have transformed into COGs\n\n# Reading the raw netCDF files from local machine\nfor name in os.listdir(FOLDER_NAME):\n    try:\n        xds = xarray.open_dataset(f\"{FOLDER_NAME}/{name}\", engine=\"netcdf4\")\n        xds = xds.assign_coords(lon=(((xds.lon + 180) % 360) - 180)).sortby(\"lon\")\n        variable = [var for var in xds.data_vars]\n        filename = name.split(\"/ \")[-1]\n        filename_elements = re.split(\"[_ .]\", filename)\n\n        for time_increment in range(0, len(xds.time)):\n            for var in variable:\n                filename = name.split(\"/ \")[-1]\n                filename_elements = re.split(\"[_ .]\", filename)\n                data = getattr(xds.isel(time=time_increment), var)\n                data = data.isel(lat=slice(None, None, -1))\n                data.rio.set_spatial_dims(\"lon\", \"lat\", inplace=True)\n                data.rio.write_crs(\"epsg:4326\", inplace=True)\n\n                # # insert date of generated COG into filename\n                filename_elements[-1] = filename_elements[-3]\n                filename_elements.insert(2, var)\n                filename_elements.pop(-3)\n                cog_filename = \"_\".join(filename_elements)\n                # # add extension\n                cog_filename = f\"{cog_filename}.tif\"\n\n                with tempfile.NamedTemporaryFile() as temp_file:\n                    data.rio.to_raster(\n                        temp_file.name,\n                        driver=\"COG\",\n                    )\n                    s3_client.upload_file(\n                        Filename=temp_file.name,\n                        Bucket=bucket_name,\n                        Key=f\"{s3_folder_name}/{cog_filename}\",\n                    )\n\n                files_processed = files_processed._append(\n                    {\"file_name\": name, \"COGs_created\": cog_filename},\n                    ignore_index=True,\n                )\n        count += 1\n        print(f\"Generated and saved COG: {cog_filename}\")\n    except OSError:\n        error_files.append(name)\n        pass\n\n# Generate the json file with the metadata that is present in the netCDF files.\nwith tempfile.NamedTemporaryFile(mode=\"w+\") as fp:\n    json.dump(xds.attrs, fp)\n    json.dump({\"data_dimensions\": dict(xds.dims)}, fp)\n    json.dump({\"data_variables\": list(xds.data_vars)}, fp)\n    fp.flush()\n\n    s3_client.upload_file(\n        Filename=fp.name,\n        Bucket=bucket_name,\n        Key=f\"{s3_folder_name}/metadata.json\",\n    )\n\n# creating the csv file with the names of files transformed.\nfiles_processed.to_csv(\n    f\"s3://{bucket_name}/{s3_folder_name}/files_converted.csv\",\n)\nprint(\"Done generating COGs\")\n\n\n\n\n Back to top",
    "crumbs": [
      "Data Transformation Notebooks",
      "Greenhouse Gas Concentrations",
      "OCO-2 GEOS Column CO₂ Concentrations"
    ]
  },
  {
    "objectID": "cog_transformation/casagfed-carbonflux-monthgrid-v3.html",
    "href": "cog_transformation/casagfed-carbonflux-monthgrid-v3.html",
    "title": "CASA-GFED3 Land Carbon Flux",
    "section": "",
    "text": "Code used to transform CASA-GFED3 Land Carbon Flux data from netcdf to Cloud Optimized Geotiff.\n\nimport os\nimport xarray\nimport re\nimport pandas as pd\nimport json\nimport tempfile\nimport boto3\n\n\nsession = boto3.session.Session()\ns3_client = session.client(\"s3\")\nbucket_name = \"ghgc-data-store-dev\"\ndate_fmt = \"%Y%m\"\n\nfiles_processed = pd.DataFrame(columns=[\"file_name\", \"COGs_created\"])\nfor name in os.listdir(\"geoscarb\"):\n    xds = xarray.open_dataset(\n        f\"geoscarb/{name}\",\n        engine=\"netcdf4\",\n    )\n    xds = xds.assign_coords(\n        longitude=(((xds.longitude + 180) % 360) - 180)\n    ).sortby(\"longitude\")\n    variable = [var for var in xds.data_vars]\n\n    for time_increment in range(0, len(xds.time)):\n        for var in variable[:-1]:\n            filename = name.split(\"/ \")[-1]\n            filename_elements = re.split(\"[_ .]\", filename)\n            data = getattr(xds.isel(time=time_increment), var)\n            data = data.isel(latitude=slice(None, None, -1))\n            data.rio.set_spatial_dims(\"longitude\", \"latitude\", inplace=True)\n            data.rio.write_crs(\"epsg:4326\", inplace=True)\n\n            date = data.time.dt.strftime(date_fmt).item(0)\n            # # insert date of generated COG into filename\n            filename_elements.pop()\n            filename_elements[-1] = date\n            filename_elements.insert(2, var)\n            cog_filename = \"_\".join(filename_elements)\n            # # add extension\n            cog_filename = f\"{cog_filename}.tif\"\n\n            with tempfile.NamedTemporaryFile() as temp_file:\n                data.rio.to_raster(\n                    temp_file.name,\n                    driver=\"COG\",\n                )\n                s3_client.upload_file(\n                    Filename=temp_file.name,\n                    Bucket=bucket_name,\n                    Key=f\"GEOS-Carbs/{cog_filename}\",\n                )\n\n            files_processed = files_processed._append(\n                {\"file_name\": name, \"COGs_created\": cog_filename},\n                ignore_index=True,\n            )\n\n            print(f\"Generated and saved COG: {cog_filename}\")\n\nwith tempfile.NamedTemporaryFile(mode=\"w+\") as fp:\n    json.dump(xds.attrs, fp)\n    json.dump({\"data_dimensions\": dict(xds.dims)}, fp)\n    json.dump({\"data_variables\": list(xds.data_vars)}, fp)\n    fp.flush()\n\n    s3_client.upload_file(\n        Filename=fp.name,\n        Bucket=bucket_name,\n        Key=\"GEOS-Carbs/metadata.json\",\n    )\nfiles_processed.to_csv(\n    f\"s3://{bucket_name}/GEOS-Carbs/files_converted.csv\",\n)\nprint(\"Done generating COGs\")\n\n\n\n\n Back to top"
  },
  {
    "objectID": "cog_transformation/lpjwsl-wetlandch4-daygrid-v1.html",
    "href": "cog_transformation/lpjwsl-wetlandch4-daygrid-v1.html",
    "title": "Wetland Methane Emissions, LPJ-wsl Model",
    "section": "",
    "text": "This script was used to transform the Wetland Methane Emissions, LPJ-wsl Model dataset from netCDF to Cloud Optimized GeoTIFF (COG) format for display in the Greenhouse Gas (GHG) Center.\n\nimport os\nimport xarray\nimport re\nimport pandas as pd\nimport json\nimport tempfile\nimport boto3\nfrom datetime import datetime, timedelta\n\n\nsession = boto3.session.Session()\ns3_client = session.client(\"s3\")\nbucket_name = (\n    \"ghgc-data-store-dev\"  # S3 bucket where the COGs are stored after transformation\n)\nFOLDER_NAME = \"NASA_GSFC_ch4_wetlands_daily\"\ndirectory = \"ch4_wetlands_daily\"\n\nfiles_processed = pd.DataFrame(\n    columns=[\"file_name\", \"COGs_created\"]\n)  # A dataframe to keep track of the files that we have transformed into COGs\n\n# Reading the raw netCDF files from local machine\nfor name in os.listdir(directory):\n    xds = xarray.open_dataset(\n        f\"{directory}/{name}\", engine=\"netcdf4\", decode_times=False\n    )\n    xds = xds.assign_coords(longitude=(((xds.longitude + 180) % 360) - 180)).sortby(\n        \"longitude\"\n    )\n    variable = [var for var in xds.data_vars]\n    filename = name.split(\"/ \")[-1]\n    filename_elements = re.split(\"[_ .]\", filename)\n    start_time = datetime(int(filename_elements[-2]), 1, 1)\n\n    for time_increment in range(0, len(xds.time)):\n        for var in variable:\n            filename = name.split(\"/ \")[-1]\n            filename_elements = re.split(\"[_ .]\", filename)\n            data = getattr(xds.isel(time=time_increment), var)\n            data = data.isel(latitude=slice(None, None, -1))\n            data = data * 1000\n            data.rio.set_spatial_dims(\"longitude\", \"latitude\", inplace=True)\n            data.rio.write_crs(\"epsg:4326\", inplace=True)\n            date = start_time + timedelta(hours=data.time.item(0))\n\n            # # insert date of generated COG into filename\n            filename_elements.pop()\n            filename_elements[-1] = date.strftime(\"%Y%m%d\")\n            filename_elements.insert(2, var)\n            cog_filename = \"_\".join(filename_elements)\n            # # add extension\n            cog_filename = f\"{cog_filename}.tif\"\n\n            with tempfile.NamedTemporaryFile() as temp_file:\n                data.rio.to_raster(\n                    temp_file.name,\n                    driver=\"COG\",\n                )\n                s3_client.upload_file(\n                    Filename=temp_file.name,\n                    Bucket=bucket_name,\n                    Key=f\"{FOLDER_NAME}/{cog_filename}\",\n                )\n\n            files_processed = files_processed._append(\n                {\"file_name\": name, \"COGs_created\": cog_filename},\n                ignore_index=True,\n            )\n\n            print(f\"Generated and saved COG: {cog_filename}\")\n\n# Generate the json file with the metadata that is present in the netCDF files.\nwith tempfile.NamedTemporaryFile(mode=\"w+\") as fp:\n    json.dump(xds.attrs, fp)\n    json.dump({\"data_dimensions\": dict(xds.dims)}, fp)\n    json.dump({\"data_variables\": list(xds.data_vars)}, fp)\n    fp.flush()\n\n    s3_client.upload_file(\n        Filename=fp.name,\n        Bucket=bucket_name,\n        Key=f\"{FOLDER_NAME}/metadata.json\",\n    )\n\n# creating the csv file with the names of files transformed.\nfiles_processed.to_csv(\n    f\"s3://{bucket_name}/{FOLDER_NAME}/files_converted.csv\",\n)\nprint(\"Done generating COGs\")\n\n\n\n\n Back to top"
  },
  {
    "objectID": "cog_transformation/eccodarwin-co2flux-monthgrid-v5.html",
    "href": "cog_transformation/eccodarwin-co2flux-monthgrid-v5.html",
    "title": "Air-Sea CO₂ Flux, ECCO-Darwin Model v5",
    "section": "",
    "text": "This script was used to transform the Air-Sea CO₂ Flux, ECCO-Darwin Mode dataset from netCDF to Cloud Optimized GeoTIFF (COG) format for display in the Greenhouse Gas (GHG) Center.\n\nimport os\nimport xarray\nimport re\nimport pandas as pd\nimport json\nimport tempfile\nimport boto3\nimport rasterio\nfrom datetime import datetime\nfrom dateutil.relativedelta import relativedelta\n\n\nsession = boto3.session.Session()\ns3_client = session.client(\"s3\")\n\nbucket_name = (\n    \"ghgc-data-store-dev\"  # S3 bucket where the COGs are stored after transformation\n)\nFOLDER_NAME = \"ecco-darwin\"\ns3_fol_name = \"ecco_darwin\"\n\n# Reading the raw netCDF files from local machine\nfiles_processed = pd.DataFrame(\n    columns=[\"file_name\", \"COGs_created\"]\n)  # A dataframe to keep track of the files that we have transformed into COGs\nfor name in os.listdir(FOLDER_NAME):\n    xds = xarray.open_dataset(\n        f\"{FOLDER_NAME}/{name}\",\n        engine=\"netcdf4\",\n    )\n    xds = xds.rename({\"y\": \"latitude\", \"x\": \"longitude\"})\n    xds = xds.assign_coords(longitude=((xds.longitude / 1440) * 360) - 180).sortby(\n        \"longitude\"\n    )\n    xds = xds.assign_coords(latitude=((xds.latitude / 721) * 180) - 90).sortby(\n        \"latitude\"\n    )\n\n    variable = [var for var in xds.data_vars]\n\n    for time_increment in xds.time.values:\n        for var in variable[2:]:\n            filename = name.split(\"/ \")[-1]\n            filename_elements = re.split(\"[_ .]\", filename)\n            data = xds[var]\n\n            data = data.reindex(latitude=list(reversed(data.latitude)))\n            data.rio.set_spatial_dims(\"longitude\", \"latitude\", inplace=True)\n            data.rio.write_crs(\"epsg:4326\", inplace=True)\n\n            # generate COG\n            COG_PROFILE = {\"driver\": \"COG\", \"compress\": \"DEFLATE\"}\n\n            filename_elements.pop()\n            filename_elements[-1] = filename_elements[-2] + filename_elements[-1]\n            filename_elements.pop(-2)\n            # # insert date of generated COG into filename\n            cog_filename = \"_\".join(filename_elements)\n            # # add extension\n            cog_filename = f\"{cog_filename}.tif\"\n\n            with tempfile.NamedTemporaryFile() as temp_file:\n                data.rio.to_raster(temp_file.name, **COG_PROFILE)\n                s3_client.upload_file(\n                    Filename=temp_file.name,\n                    Bucket=bucket_name,\n                    Key=f\"{s3_fol_name}/{cog_filename}\",\n                )\n\n            files_processed = files_processed._append(\n                {\"file_name\": name, \"COGs_created\": cog_filename},\n                ignore_index=True,\n            )\n            del data\n\n            print(f\"Generated and saved COG: {cog_filename}\")\n\n# Generate the json file with the metadata that is present in the netCDF files.\nwith tempfile.NamedTemporaryFile(mode=\"w+\") as fp:\n    json.dump(xds.attrs, fp)\n    json.dump({\"data_dimensions\": dict(xds.dims)}, fp)\n    json.dump({\"data_variables\": list(xds.data_vars)}, fp)\n    fp.flush()\n\n    s3_client.upload_file(\n        Filename=fp.name,\n        Bucket=bucket_name,\n        Key=\"s3_fol_name/metadata.json\",\n    )\n\n# A csv file to store the names of all the files converted.\nfiles_processed.to_csv(\n    f\"s3://{bucket_name}/{s3_fol_name}/files_converted.csv\",\n)\nprint(\"Done generating COGs\")\n\n\n\n\n Back to top",
    "crumbs": [
      "Data Transformation Notebooks",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "Air-Sea CO₂ Flux, ECCO-Darwin Model v5"
    ]
  },
  {
    "objectID": "cog_transformation/sedac-popdensity-yeargrid5yr-v4.11.html",
    "href": "cog_transformation/sedac-popdensity-yeargrid5yr-v4.11.html",
    "title": "SEDAC Gridded World Population Data",
    "section": "",
    "text": "This script was used to transform SEDAC Gridded World Population Data from netCDF to Cloud Optimized GeoTIFF (COG) format for display in the Greenhouse Gas (GHG) Center.\n\nimport os\nimport xarray\nimport re\nimport pandas as pd\n\nimport tempfile\nimport boto3\n\n\nsession = boto3.session.Session()\ns3_client = session.client(\"s3\")\nbucket_name = (\n    \"ghgc-data-store-dev\"  # S3 bucket where the COGs are stored after transformation\n)\n\nfold_names = os.listdir(\"gpw\")\n\nfiles_processed = pd.DataFrame(\n    columns=[\"file_name\", \"COGs_created\"]\n)  # A dataframe to keep track of the files that we have transformed into COGs\n\n# Reading the raw netCDF files from local machine\nfor fol_ in fold_names:\n    for name in os.listdir(f\"gpw/{fol_}\"):\n        if name.endswith(\".tif\"):\n            xds = xarray.open_dataarray(f\"gpw/{fol_}/{name}\")\n\n            filename = name.split(\"/ \")[-1]\n            filename_elements = re.split(\"[_ .]\", filename)\n            # # insert date of generated COG into filename\n            filename_elements.pop()\n            filename_elements.append(filename_elements[-3])\n\n            xds.rio.set_spatial_dims(\"x\", \"y\", inplace=True)\n            xds.rio.write_crs(\"epsg:4326\", inplace=True)\n\n            cog_filename = \"_\".join(filename_elements)\n            # # add extension\n            cog_filename = f\"{cog_filename}.tif\"\n\n            with tempfile.NamedTemporaryFile() as temp_file:\n                xds.rio.to_raster(temp_file.name, driver=\"COG\")\n                s3_client.upload_file(\n                    Filename=temp_file.name,\n                    Bucket=bucket_name,\n                    Key=f\"gridded_population_cog/{cog_filename}\",\n                )\n\n            files_processed = files_processed._append(\n                {\"file_name\": name, \"COGs_created\": cog_filename},\n                ignore_index=True,\n            )\n\n            print(f\"Generated and saved COG: {cog_filename}\")\n\n\n# creating the csv file with the names of files transformed.\nfiles_processed.to_csv(\n    f\"s3://{bucket_name}/gridded_population_cog/files_converted.csv\",\n)\nprint(\"Done generating COGs\")\n\n\n\n\n Back to top",
    "crumbs": [
      "Data Transformation Notebooks",
      "Socioeconomic",
      "SEDAC Gridded World Population Data"
    ]
  },
  {
    "objectID": "cog_transformation/lpjwsl-wetlandch4-monthgrid-v1.html",
    "href": "cog_transformation/lpjwsl-wetlandch4-monthgrid-v1.html",
    "title": "Wetland Methane Emissions, LPJ-wsl Model",
    "section": "",
    "text": "This script was used to transform the Wetland Methane Emissions, LPJ-wsl Model dataset from netCDF to Cloud Optimized GeoTIFF (COG) format for display in the Greenhouse Gas (GHG) Center.\n\nimport os\nimport xarray\nimport re\nimport pandas as pd\nimport json\nimport tempfile\nimport boto3\n\n\nsession = boto3.session.Session()\ns3_client = session.client(\"s3\")\nbucket_name = (\n    \"ghgc-data-store-dev\"  # S3 bucket where the COGs are stored after transformation\n)\nFOLDER_NAME = \"NASA_GSFC_ch4_wetlands_monthly\"\ndirectory = \"ch4_wetlands_monthly\"\n\nfiles_processed = pd.DataFrame(\n    columns=[\"file_name\", \"COGs_created\"]\n)  # A dataframe to keep track of the files that we have transformed into COGs\n\n# Reading the raw netCDF files from local machine\nfor name in os.listdir(directory):\n    xds = xarray.open_dataset(\n        f\"{directory}/{name}\", engine=\"netcdf4\", decode_times=False\n    )\n    xds = xds.assign_coords(longitude=(((xds.longitude + 180) % 360) - 180)).sortby(\n        \"longitude\"\n    )\n    variable = [var for var in xds.data_vars]\n    filename = name.split(\"/ \")[-1]\n    filename_elements = re.split(\"[_ .]\", filename)\n\n    for time_increment in range(0, len(xds.time)):\n        for var in variable:\n            filename = name.split(\"/ \")[-1]\n            filename_elements = re.split(\"[_ .]\", filename)\n            data = getattr(xds.isel(time=time_increment), var)\n            data = data.isel(latitude=slice(None, None, -1))\n            data = data * 1000\n            data.rio.set_spatial_dims(\"longitude\", \"latitude\", inplace=True)\n            data.rio.write_crs(\"epsg:4326\", inplace=True)\n\n            date = (\n                f\"0{int((data.time.item(0)/732)+1)}\"\n                if len(str(int((data.time.item(0) / 732) + 1))) == 1\n                else f\"{int((data.time.item(0)/732)+1)}\"\n            )\n            # # insert date of generated COG into filename\n            filename_elements.pop()\n            filename_elements[-1] = filename_elements[-1] + date\n            filename_elements.insert(2, var)\n            cog_filename = \"_\".join(filename_elements)\n            # # add extension\n            cog_filename = f\"{cog_filename}.tif\"\n\n            with tempfile.NamedTemporaryFile() as temp_file:\n                data.rio.to_raster(\n                    temp_file.name,\n                    driver=\"COG\",\n                )\n                s3_client.upload_file(\n                    Filename=temp_file.name,\n                    Bucket=bucket_name,\n                    Key=f\"{FOLDER_NAME}/{cog_filename}\",\n                )\n\n            files_processed = files_processed._append(\n                {\"file_name\": name, \"COGs_created\": cog_filename},\n                ignore_index=True,\n            )\n\n            print(f\"Generated and saved COG: {cog_filename}\")\n\n# Generate the json file with the metadata that is present in the netCDF files.\nwith tempfile.NamedTemporaryFile(mode=\"w+\") as fp:\n    json.dump(xds.attrs, fp)\n    json.dump({\"data_dimensions\": dict(xds.dims)}, fp)\n    json.dump({\"data_variables\": list(xds.data_vars)}, fp)\n    fp.flush()\n\n    s3_client.upload_file(\n        Filename=fp.name,\n        Bucket=bucket_name,\n        Key=f\"{FOLDER_NAME}/metadata.json\",\n    )\n\n# creating the csv file with the names of files transformed.\nfiles_processed.to_csv(\n    f\"s3://{bucket_name}/{FOLDER_NAME}/files_converted.csv\",\n)\nprint(\"Done generating COGs\")\n\n\n\n\n Back to top"
  },
  {
    "objectID": "cog_transformation/tm54dvar-ch4flux-monthgrid-v1.html",
    "href": "cog_transformation/tm54dvar-ch4flux-monthgrid-v1.html",
    "title": "TM5-4DVar Isotopic CH₄ Inverse Fluxes",
    "section": "",
    "text": "This script was used to transform the TM5-4DVar Isotopic CH₄ Inverse Fluxes dataset from netCDF to Cloud Optimized GeoTIFF (COG) format for display in the Greenhouse Gas (GHG) Center.\n\nimport os\nimport xarray\nimport re\nimport pandas as pd\nimport json\nimport tempfile\nimport boto3\nfrom datetime import datetime\n\n\nsession = boto3.session.Session()\ns3_client = session.client(\"s3\")\nbucket_name = (\n    \"ghgc-data-store-dev\"  # S3 bucket where the COGs are stored after transformation\n)\nFOLDER_NAME = \"tm5-ch4-inverse-flux\"\n\nfiles_processed = pd.DataFrame(\n    columns=[\"file_name\", \"COGs_created\"]\n)  # A dataframe to keep track of the files that we have transformed into COGs\n\n# Reading the raw netCDF files from local machine\nfor name in os.listdir(FOLDER_NAME):\n    xds = xarray.open_dataset(f\"{FOLDER_NAME}/{name}\", engine=\"netcdf4\")\n    xds = xds.rename({\"latitude\": \"lat\", \"longitude\": \"lon\"})\n    xds = xds.assign_coords(lon=(((xds.lon + 180) % 360) - 180)).sortby(\"lon\")\n    variable = [var for var in xds.data_vars if \"global\" not in var]\n\n    for time_increment in range(0, len(xds.months)):\n        filename = name.split(\"/ \")[-1]\n        filename_elements = re.split(\"[_ .]\", filename)\n        start_time = datetime(int(filename_elements[-2]), time_increment + 1, 1)\n        for var in variable:\n            data = getattr(xds.isel(months=time_increment), var)\n            data = data.isel(lat=slice(None, None, -1))\n            data.rio.set_spatial_dims(\"lon\", \"lat\", inplace=True)\n            data.rio.write_crs(\"epsg:4326\", inplace=True)\n\n            # # insert date of generated COG into filename\n            filename_elements.pop()\n            filename_elements[-1] = start_time.strftime(\"%Y%m\")\n            filename_elements.insert(2, var)\n            cog_filename = \"_\".join(filename_elements)\n            # # add extension\n            cog_filename = f\"{cog_filename}.tif\"\n\n            with tempfile.NamedTemporaryFile() as temp_file:\n                data.rio.to_raster(\n                    temp_file.name,\n                    driver=\"COG\",\n                )\n                s3_client.upload_file(\n                    Filename=temp_file.name,\n                    Bucket=bucket_name,\n                    Key=f\"{FOLDER_NAME}/{cog_filename}\",\n                )\n\n            files_processed = files_processed._append(\n                {\"file_name\": name, \"COGs_created\": cog_filename},\n                ignore_index=True,\n            )\n\n            print(f\"Generated and saved COG: {cog_filename}\")\n\n# Generate the json file with the metadata that is present in the netCDF files.\nwith tempfile.NamedTemporaryFile(mode=\"w+\") as fp:\n    json.dump(xds.attrs, fp)\n    json.dump({\"data_dimensions\": dict(xds.dims)}, fp)\n    json.dump({\"data_variables\": list(xds.data_vars)}, fp)\n    fp.flush()\n\n    s3_client.upload_file(\n        Filename=fp.name,\n        Bucket=bucket_name,\n        Key=f\"{FOLDER_NAME}/metadata.json\",\n    )\n\n# creating the csv file with the names of files transformed.\nfiles_processed.to_csv(\n    f\"s3://{bucket_name}/{FOLDER_NAME}/files_converted.csv\",\n)\nprint(\"Done generating COGs\")\n\n\n\n\n Back to top",
    "crumbs": [
      "Data Transformation Notebooks",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "TM5-4DVar Isotopic CH₄ Inverse Fluxes"
    ]
  },
  {
    "objectID": "cog_transformation/epa-ch4emission-grid-v2express_layers_update.html",
    "href": "cog_transformation/epa-ch4emission-grid-v2express_layers_update.html",
    "title": "Gridded Anthropogenic Methane Emissions Inventory",
    "section": "",
    "text": "This script was used to add concatenated layers and transform Gridded Anthropogenic Methane Emissions Inventory dataset from netCDF to Cloud Optimized GeoTIFF (COG) format for display in the Greenhouse Gas (GHG) Center.\n\nimport os\nimport xarray\nimport re\nimport pandas as pd\nimport json\nimport tempfile\nimport boto3\nfrom datetime import datetime\nimport numpy as np\n\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\nTrue\n\n\n\n# session = boto3.session.Session()\nsession = boto3.Session(\n    aws_access_key_id=os.environ.get(\"AWS_ACCESS_KEY_ID\"),\n    aws_secret_access_key=os.environ.get(\"AWS_SECRET_ACCESS_KEY\"),\n    aws_session_token=os.environ.get(\"AWS_SESSION_TOKEN\"),\n)\ns3_client = session.client(\"s3\")\nbucket_name = (\n    \"ghgc-data-store-dev\"  # S3 bucket where the COGs are stored after transformation\n)\nFOLDER_NAME = \"../data/epa_emissions_express_extension\"\ns3_folder_name = \"epa_express_extension_Mg_km2_yr\"\n\nfiles_processed = pd.DataFrame(\n    columns=[\"file_name\", \"COGs_created\"]\n)  # A dataframe to keep track of the files that we have transformed into COGs\n\n# Reading the raw netCDF files from local machine\nfor name in os.listdir(FOLDER_NAME):\n    xds = xarray.open_dataset(f\"{FOLDER_NAME}/{name}\", engine=\"netcdf4\")\n    xds = xds.assign_coords(lon=(((xds.lon + 180) % 360) - 180)).sortby(\"lon\")\n    variable = [var for var in xds.data_vars]\n    new_variables = {\n        \"all-variables\": variable[:-1],\n        \"agriculture\": variable[17:21],\n        \"natural-gas-systems\": variable[10:15] + [variable[26]],\n        \"petroleum-systems\": variable[5:9],\n        \"waste\": variable[21:26],\n        \"coal-mines\": variable[2:5],\n        \"other\": variable[:2] + [variable[9]] + variable[15:17],\n    }\n    filename = name.split(\"/ \")[-1]\n    filename_elements = re.split(\"[_ .]\", filename)\n    start_time = datetime(int(filename_elements[-2]), 1, 1)\n\n    for time_increment in range(0, len(xds.time)):\n        for key, value in new_variables.items():\n            data = np.zeros(dtype=np.float32, shape=(len(xds.lat), len(xds.lon)))\n            filename = name.split(\"/ \")[-1]\n            filename_elements = re.split(\"[_ .]\", filename)\n            for var in value:\n                data = data + getattr(xds.isel(time=time_increment), var)\n            # data = np.round(data / pow(10, 9), 2)\n            data.values[data.values==0] = np.nan\n            data = data*((1/(6.022*pow(10,23)))*(16.04*pow(10,-6))*366*pow(10,10)*86400)\n            data = data.fillna(-9999)\n            data = data.isel(lat=slice(None, None, -1))\n            data.rio.set_spatial_dims(\"lon\", \"lat\", inplace=True)\n            data.rio.write_crs(\"epsg:4326\", inplace=True)\n\n            # # insert date of generated COG into filename\n            filename_elements.pop()\n            filename_elements[-1] = start_time.strftime(\"%Y\")\n            filename_elements.insert(2, key)\n            cog_filename = \"_\".join(filename_elements)\n            # # add extension\n            cog_filename = f\"{cog_filename}.tif\"\n\n            with tempfile.NamedTemporaryFile() as temp_file:\n                data.rio.to_raster(\n                    temp_file.name,\n                    driver=\"COG\",\n                )\n                s3_client.upload_file(\n                    Filename=temp_file.name,\n                    Bucket=bucket_name,\n                    Key=f\"{s3_folder_name}/{cog_filename}\",\n                )\n\n                files_processed = files_processed._append(\n                    {\"file_name\": name, \"COGs_created\": cog_filename},\n                    ignore_index=True,\n                )\n\n                print(f\"Generated and saved COG: {cog_filename}\")\nprint(\"Done generating COGs\")\n\nTraceback (most recent call last):\n  File \"_pydevd_bundle/pydevd_cython.pyx\", line 1078, in _pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\n  File \"_pydevd_bundle/pydevd_cython.pyx\", line 297, in _pydevd_bundle.pydevd_cython.PyDBFrame.do_wait_suspend\n  File \"/Users/vgaur/miniconda3/envs/cmip6/lib/python3.9/site-packages/debugpy/_vendored/pydevd/pydevd.py\", line 1976, in do_wait_suspend\n    keep_suspended = self._do_wait_suspend(thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\n  File \"/Users/vgaur/miniconda3/envs/cmip6/lib/python3.9/site-packages/debugpy/_vendored/pydevd/pydevd.py\", line 2011, in _do_wait_suspend\n    time.sleep(0.01)\nKeyboardInterrupt\n\n\n\n---------------------------------------------------------------------------\nKeyboardInterrupt                         Traceback (most recent call last)\n/Users/vgaur/ghgc-docs/cog_transformation/epa-ch4emission-grid-v2express_layers_update.ipynb Cell 4 line 4\n     &lt;a href='vscode-notebook-cell:/Users/vgaur/ghgc-docs/cog_transformation/epa-ch4emission-grid-v2express_layers_update.ipynb#W3sZmlsZQ%3D%3D?line=45'&gt;46&lt;/a&gt; # data = data*(9.74*pow(10,-11))\n     &lt;a href='vscode-notebook-cell:/Users/vgaur/ghgc-docs/cog_transformation/epa-ch4emission-grid-v2express_layers_update.ipynb#W3sZmlsZQ%3D%3D?line=46'&gt;47&lt;/a&gt; # data.values[data.values&lt;=np.nanpercentile(data.values, 50)] = np.nan\n     &lt;a href='vscode-notebook-cell:/Users/vgaur/ghgc-docs/cog_transformation/epa-ch4emission-grid-v2express_layers_update.ipynb#W3sZmlsZQ%3D%3D?line=47'&gt;48&lt;/a&gt; data = data.fillna(-9999)\n---&gt; &lt;a href='vscode-notebook-cell:/Users/vgaur/ghgc-docs/cog_transformation/epa-ch4emission-grid-v2express_layers_update.ipynb#W3sZmlsZQ%3D%3D?line=48'&gt;49&lt;/a&gt; data = data.isel(lat=slice(None, None, -1))\n     &lt;a href='vscode-notebook-cell:/Users/vgaur/ghgc-docs/cog_transformation/epa-ch4emission-grid-v2express_layers_update.ipynb#W3sZmlsZQ%3D%3D?line=49'&gt;50&lt;/a&gt; data.rio.set_spatial_dims(\"lon\", \"lat\", inplace=True)\n     &lt;a href='vscode-notebook-cell:/Users/vgaur/ghgc-docs/cog_transformation/epa-ch4emission-grid-v2express_layers_update.ipynb#W3sZmlsZQ%3D%3D?line=50'&gt;51&lt;/a&gt; data.rio.write_crs(\"epsg:4326\", inplace=True)\n\n/Users/vgaur/ghgc-docs/cog_transformation/epa-ch4emission-grid-v2express_layers_update.ipynb Cell 4 line 4\n     &lt;a href='vscode-notebook-cell:/Users/vgaur/ghgc-docs/cog_transformation/epa-ch4emission-grid-v2express_layers_update.ipynb#W3sZmlsZQ%3D%3D?line=45'&gt;46&lt;/a&gt; # data = data*(9.74*pow(10,-11))\n     &lt;a href='vscode-notebook-cell:/Users/vgaur/ghgc-docs/cog_transformation/epa-ch4emission-grid-v2express_layers_update.ipynb#W3sZmlsZQ%3D%3D?line=46'&gt;47&lt;/a&gt; # data.values[data.values&lt;=np.nanpercentile(data.values, 50)] = np.nan\n     &lt;a href='vscode-notebook-cell:/Users/vgaur/ghgc-docs/cog_transformation/epa-ch4emission-grid-v2express_layers_update.ipynb#W3sZmlsZQ%3D%3D?line=47'&gt;48&lt;/a&gt; data = data.fillna(-9999)\n---&gt; &lt;a href='vscode-notebook-cell:/Users/vgaur/ghgc-docs/cog_transformation/epa-ch4emission-grid-v2express_layers_update.ipynb#W3sZmlsZQ%3D%3D?line=48'&gt;49&lt;/a&gt; data = data.isel(lat=slice(None, None, -1))\n     &lt;a href='vscode-notebook-cell:/Users/vgaur/ghgc-docs/cog_transformation/epa-ch4emission-grid-v2express_layers_update.ipynb#W3sZmlsZQ%3D%3D?line=49'&gt;50&lt;/a&gt; data.rio.set_spatial_dims(\"lon\", \"lat\", inplace=True)\n     &lt;a href='vscode-notebook-cell:/Users/vgaur/ghgc-docs/cog_transformation/epa-ch4emission-grid-v2express_layers_update.ipynb#W3sZmlsZQ%3D%3D?line=50'&gt;51&lt;/a&gt; data.rio.write_crs(\"epsg:4326\", inplace=True)\n\nFile _pydevd_bundle/pydevd_cython.pyx:1363, in _pydevd_bundle.pydevd_cython.SafeCallWrapper.__call__()\n\nFile _pydevd_bundle/pydevd_cython.pyx:662, in _pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch()\n\nFile _pydevd_bundle/pydevd_cython.pyx:1087, in _pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch()\n\nFile _pydevd_bundle/pydevd_cython.pyx:1078, in _pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch()\n\nFile _pydevd_bundle/pydevd_cython.pyx:297, in _pydevd_bundle.pydevd_cython.PyDBFrame.do_wait_suspend()\n\nFile ~/miniconda3/envs/cmip6/lib/python3.9/site-packages/debugpy/_vendored/pydevd/pydevd.py:1976, in PyDB.do_wait_suspend(self, thread, frame, event, arg, exception_type)\n   1973             from_this_thread.append(frame_custom_thread_id)\n   1975     with self._threads_suspended_single_notification.notify_thread_suspended(thread_id, stop_reason):\n-&gt; 1976         keep_suspended = self._do_wait_suspend(thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\n   1978 frames_list = None\n   1980 if keep_suspended:\n   1981     # This means that we should pause again after a set next statement.\n\nFile ~/miniconda3/envs/cmip6/lib/python3.9/site-packages/debugpy/_vendored/pydevd/pydevd.py:2011, in PyDB._do_wait_suspend(self, thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\n   2008         self._call_mpl_hook()\n   2010     self.process_internal_commands()\n-&gt; 2011     time.sleep(0.01)\n   2013 self.cancel_async_evaluation(get_current_thread_id(thread), str(id(frame)))\n   2015 # process any stepping instructions\n\nKeyboardInterrupt: \n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "cog_transformation/vulcan-ffco2-yeargrid-v4.html",
    "href": "cog_transformation/vulcan-ffco2-yeargrid-v4.html",
    "title": "Vulcan Fossil Fuel CO₂ Emissions",
    "section": "",
    "text": "This script was used to transform the VULCAN dataset provided in GeoTIFF format for display in the Greenhouse Gas (GHG) Center with the calaulation of validation statistics.\n\nimport xarray\nimport pandas as pd\nimport boto3\nimport glob\nimport s3fs\nimport tempfile\nfrom datetime import datetime\nimport os\nimport boto3\nfrom pyproj import CRS\nimport numpy as np\n\nimport rasterio\nfrom rasterio.warp import calculate_default_transform, reproject, Resampling\nfrom rasterio.enums import Resampling\nfrom rio_cogeo.cogeo import cog_translate\nfrom rio_cogeo.profiles import cog_profiles\n\n\nconfig = {\n    \"data_acquisition_method\": \"s3\",\n    \"raw_data_bucket\" : \"gsfc-ghg-store\",\n    \"raw_data_prefix\": \"Vulcan/v4.0/grid.1km.mn\",\n    \"cog_data_bucket\": \"ghgc-data-store-develop\",\n    \"cog_data_prefix\": \"transformed_cogs/VULCAN_v4\"\n}\n\n\nsession = boto3.session.Session()\ns3_client = session.client(\"s3\")\n\nraw_data_bucket = config[\"raw_data_bucket\"]\nraw_data_prefix= config[\"raw_data_prefix\"]\n\ncog_data_bucket = config['cog_data_bucket']\ncog_data_prefix= config[\"cog_data_prefix\"]\n\ndate_fmt=config['date_fmt']\n\nfs = s3fs.S3FileSystem()\n\n\ndef get_all_s3_keys(bucket, model_name, ext):\n    \"\"\"Get a list of all keys in an S3 bucket.\"\"\"\n    keys = []\n\n    kwargs = {\"Bucket\": bucket, \"Prefix\": f\"{model_name}/\"}\n    while True:\n        resp = s3_client.list_objects_v2(**kwargs)\n        for obj in resp[\"Contents\"]:\n            if obj[\"Key\"].endswith(ext) and \"historical\" not in obj[\"Key\"]:\n                keys.append(obj[\"Key\"])\n\n        try:\n            kwargs[\"ContinuationToken\"] = resp[\"NextContinuationToken\"]\n        except KeyError:\n            break\n\n    return keys\n\nkeys = get_all_s3_keys(raw_data_bucket, raw_data_prefix, \".tif\")\n\n\nlen(keys)\n\n\n# To calculate the validation stats\noverall= pd.DataFrame(columns=[\"data\",\"min\",\"max\",\"mean\",\"std\"])\n\n\n# Step 1: Reproject the data \n# Define the source and target CRS\n# Also calculate raw - monthly validation stats\nos.makedirs(\"reproj\", exist_ok=True)\nsrc_crs = CRS.from_wkt('PROJCS[\"unknown\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Lambert_Conformal_Conic_2SP\"],PARAMETER[\"latitude_of_origin\",40],PARAMETER[\"central_meridian\",-97],PARAMETER[\"standard_parallel_1\",33],PARAMETER[\"standard_parallel_2\",45],PARAMETER[\"false_easting\",0],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]')\ndst_crs = CRS.from_epsg(4326)  # WGS 84\ndf = pd.DataFrame(columns=['filename', 'min(raw)', 'max(raw)', 'mean(raw)', 'std(raw)'])\noverall_raw= []\nfor key in keys:\n    url = f\"s3://{raw_data_bucket}/{key}\"\n    with rasterio.open(url) as src:\n        filename_elements = key.split(\"/\")[-1].split(\".\")[:-1]\n        output_tif = \"_\".join(filename_elements) + \".tif\"\n        data = src.read(1)  # Read the first band\n        overall_raw.append(data)\n        \n        # Calculate statistics while ignoring NaN values\n        min_val = np.nanmin(data)\n        max_val = np.nanmax(data)\n        mean_val = np.nanmean(data)\n        std_val = np.nanstd(data)  \n        stats = [output_tif, min_val, max_val, mean_val, std_val]\n        df.loc[len(df)] = stats\n        \n        transform, width, height = calculate_default_transform(\n        src.crs, dst_crs, src.width, src.height, *src.bounds)\n        kwargs = src.meta.copy()\n        kwargs.update({\n        'crs': dst_crs,\n        'transform': transform,\n        'width': width,\n        'height': height,\n        'nodata': -9999\n        })\n\n        with rasterio.open(f\"reproj/{output_tif}\", 'w', **kwargs) as dst:\n            for i in range(1, src.count + 1):\n                reproject(\n                source=rasterio.band(src, i),\n                destination=rasterio.band(dst, i),\n                src_transform=src.transform,\n                src_crs=src.crs,\n                dst_transform=transform,\n                dst_crs=dst_crs,\n                resampling=Resampling.nearest)\n        print(f\"Done for {output_tif}\")\n\n\n\n\n# overall validation of raw data\noverall_raw= np.array(overall_raw)\nnan_min = np.nanmin(overall_raw)\nnan_max = np.nanmax(overall_raw)\nnan_mean = np.nanmean(overall_raw)\nnan_std = np.nanstd(overall_raw)\noverall.loc[len(overall)] = [\"raw\",nan_min,nan_max,nan_mean,nan_std]\n\n\n# validation for reprojected data - yearly calculation\noverall_reproj = []\nfiles = glob.glob(\"reproj/*.tif\")\ndf1 = pd.DataFrame(columns=['filename', 'min(reprojected)', 'max(reprojected)', 'mean(reprojected)', 'std(reprojected)'])\nfor file in files:\n    with rasterio.open(file) as src:\n        filename_elements = file.split(\"/\")[-1].split(\".\")[:-1]\n        output_tif = \"_\".join(filename_elements) + \".tif\"\n        data = src.read(1)  \n        data = np.ma.masked_equal(data, -9999)\n        overall_reproj.append(data)\n        \n        # Calculate statistics while ignoring NaN values\n        min_val = np.nanmin(data)\n        max_val = np.nanmax(data)\n        mean_val = np.nanmean(data)\n        std_val = np.nanstd(data)  \n        stats = [output_tif, min_val, max_val, mean_val, std_val]\n        df1.loc[len(df1)] = stats\n\n\n# overall validation of reprojected  data\noverall_reproj= np.array(overall_reproj)\noverall_reproj = np.ma.masked_equal(overall_reproj, -9999)\nnan_min = np.nanmin(overall_reproj)\nnan_max = np.nanmax(overall_reproj)\nnan_mean = np.nanmean(overall_reproj)\nnan_std = np.nanstd(overall_reproj)\noverall.loc[len(overall)] = [\"reprojected\",nan_min,nan_max,nan_mean,nan_std]\n\n\n# Step 2: Replace nan and 0 values with -9999\nos.makedirs(\"reproj2\", exist_ok=True)\nfiles = glob.glob(\"reproj/*.tif\")\nfor file in files:\n    filename = file.split('/')[-1]\n    xda = xarray.open_dataarray(file).sel(band=1)\n\n    # Multiply data\n    data = data *( 44/12)\n    \n    data = xda.where(xda != 0, -9999)  # Replace 0 with -9999\n    #data = data.where(data != -3.4e+38, -9999)  # Replace -3.4e+38 with -9999\n    data = data.fillna(-9999)  # Ensure all NaNs are replaced with -9999\n    data_array = data.values\n    \n\n    # Open the source raster to get metadata\n    with rasterio.open(file) as src:\n        meta = src.meta\n        meta.update({\n            'nodata': -9999,\n            'dtype': 'float32',\n            'driver': 'COG'\n        })\n        with rasterio.open(f\"reproj2/{filename}\", 'w', **meta) as dst:\n            dst.write(data_array, 1)\n\n\n# validation for reprojected data (non zero) - monthly calculation\noverall_reproj2=[]\nfiles = glob.glob(\"reproj/*.tif\")\ndf11 = pd.DataFrame(columns=['filename', 'min(reproj_nonzero)', 'max(reproj_nonzero)', 'mean(reproj_nonzero)', 'std(reproj_nonzero)'])\nfor file in files:\n    with rasterio.open(file) as src:\n        filename_elements = file.split(\"/\")[-1].split(\".\")[:-1]\n        output_tif = \"_\".join(filename_elements) + \".tif\"\n        data = src.read(1)  \n        data = np.ma.masked_where((data == -9999) | (data == 0), data)\n        overall_reproj2.append(data)\n\n        \n        # Calculate statistics while ignoring NaN values\n        min_val = np.nanmin(data)\n        max_val = np.nanmax(data)\n        mean_val = np.nanmean(data)\n        std_val = np.nanstd(data)  \n        stats = [output_tif, min_val, max_val, mean_val, std_val]\n        df11.loc[len(df11)] = stats\n\n\n# validation for reprojected data (non zero) - overall calculation\noverall_reproj2= np.array(overall_reproj2)\noverall_reproj2 = np.ma.masked_where((overall_reproj2 == -9999) | (overall_reproj2 == 0), overall_reproj2)\nnan_min = np.nanmin(overall_reproj2)\nnan_max = np.nanmax(overall_reproj2)\nnan_mean = np.nanmean(overall_reproj2)\nnan_std = np.nanstd(overall_reproj2)\noverall.loc[len(overall)] = [\"reprojected_non_zero\",nan_min,nan_max,nan_mean,nan_std]\n\n\n# Step 3: To put overviews\nCOG_PROFILE = {\"driver\": \"COG\", \"compress\": \"DEFLATE\"}\nOVERVIEW_LEVELS = 9\nOVERVIEW_RESAMPLING = 'average'\n\nfor file in glob.glob(\"reproj2/*.tif\"):\n    output_path = f\"output/{file.split(\"/\")[-1]}\"\n    \n    # Create a temporary file to hold the COG\n    with tempfile.NamedTemporaryFile(suffix='.tif', delete=False) as temp_file:       \n        # Create COG with overviews and nodata value\n        cog_translate(\n            file,\n            temp_file.name,\n            cog_profiles.get(\"deflate\"),\n            overview_level=OVERVIEW_LEVELS,\n            overview_resampling=OVERVIEW_RESAMPLING,\n            nodata=-9999\n        )\n        # Move the temporary file to the desired local path\n        os.rename(temp_file.name, output_path)\n\n\n# validation for final data with overviews - overall calculation\noverall_final=[]\nfiles = glob.glob(\"output/*.tif\")\ndf2 = pd.DataFrame(columns=['filename', 'min(transformed)', 'max(transformed)', 'mean(transformed)', 'std(transformed)'])\nfor file in files:\n    with rasterio.open(file) as src:\n        filename_elements = file.split(\"/\")[-1].split(\".\")[:-1]\n        output_tif = \"_\".join(filename_elements) + \".tif\"\n        data = src.read(1)  # Read the first band\n        \n        # Mask -9999 values and NaNs for statistics calculation\n        data = np.ma.masked_where((data == -9999) | np.isnan(data), data)\n        # Multiply data - undo the multiplication done during transformation\n        data = data *( 12/44)\n        overall_final.append(data)\n        \n        # Calculate statistics while ignoring NaN values\n        min_val = np.nanmin(data)\n        max_val = np.nanmax(data)\n        mean_val = np.nanmean(data)\n        total = np.nansum(data) \n        stats = [output_tif, min_val, max_val, mean_val, std_val]\n        df2.loc[len(df2)] = stats\n\n\n# validation for final data (with overviews) - overall calculation\noverall_final= np.array(overall_final)\noverall_final = np.ma.masked_where((overall_final == -9999) | np.isnan(overall_final), overall_final)\nnan_min = np.nanmin(overall_final)\nnan_max = np.nanmax(overall_final)\nnan_mean = np.nanmean(overall_final)\nnan_std = np.nanstd(overall_final)\noverall.loc[len(overall)] = [\"Transformed\",nan_min,nan_max,nan_mean,nan_std]\n\n\npd.merge(pd.merge(df,df1, on='filename', how='inner'), pd.merge(df11,df2, on='filename', how='inner'), how='inner',on='filename' )\n\n\noverall\n\n\n# Save to json\noverall.to_json(\"overall_stats.json\")\npd.merge(pd.merge(df,df1, on='filename', how='inner'), pd.merge(df11,df2, on='filename', how='inner'), how='inner',on='filename' ).to_json(\"yearly_stats.json\")\n\n\n\n\n Back to top",
    "crumbs": [
      "Data Transformation Notebooks",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "Vulcan Fossil Fuel CO₂ Emissions"
    ]
  },
  {
    "objectID": "cog_transformation/oco2-mip-co2budget-yeargrid-v1.html",
    "href": "cog_transformation/oco2-mip-co2budget-yeargrid-v1.html",
    "title": "OCO-2 MIP Top-Down CO₂ Budgets",
    "section": "",
    "text": "This script was used to transform the OCO-2 MIP Top-Down CO₂ Budgets dataset from netCDF to Cloud Optimized GeoTIFF (COG) format for display in the Greenhouse Gas (GHG) Center.\n\nimport os\nimport xarray\nimport re\nimport pandas as pd\nimport json\nimport tempfile\nimport boto3\nimport rasterio\nfrom datetime import datetime\nfrom dateutil.relativedelta import relativedelta\n\n\nsession = boto3.session.Session()\ns3_client = session.client(\"s3\")\nbucket_name = \"ghgc-data-store-dev\" # S3 bucket where the COGs are to be stored\nyear_ = datetime(2015, 1, 1)    # Initialize the starting date time of the dataset.\n\nCOG_PROFILE = {\"driver\": \"COG\", \"compress\": \"DEFLATE\"}\n\n# Reading the raw netCDF files from local machine\nfiles_processed = pd.DataFrame(columns=[\"file_name\", \"COGs_created\"])   # A dataframe to keep track of the files that are converted into COGs\nfor name in os.listdir(\"new_data\"):\n    ds = xarray.open_dataset(\n        f\"new_data/{name}\",\n        engine=\"netcdf4\",\n    )\n    ds = ds.rename({\"latitude\": \"lat\", \"longitude\": \"lon\"})\n    # assign coords from dimensions\n    ds = ds.assign_coords(lon=(((ds.lon + 180) % 360) - 180)).sortby(\"lon\")\n    ds = ds.assign_coords(lat=list(ds.lat))\n\n    variable = [var for var in ds.data_vars]\n\n    for time_increment in range(0, len(ds.year)):\n        for var in variable[2:]:\n            filename = name.split(\"/ \")[-1]\n            filename_elements = re.split(\"[_ .]\", filename)\n            try:\n                data = ds[var].sel(year=time_increment)\n                date = year_ + relativedelta(years=+time_increment)\n                filename_elements[-1] = date.strftime(\"%Y\")\n                # # insert date of generated COG into filename\n                filename_elements.insert(2, var)\n                cog_filename = \"_\".join(filename_elements)\n                # # add extension\n                cog_filename = f\"{cog_filename}.tif\"\n            except KeyError:\n                data = ds[var]\n                date = year_ + relativedelta(years=+(len(ds.year) - 1))\n                filename_elements.pop()\n                filename_elements.append(year_.strftime(\"%Y\"))\n                filename_elements.append(date.strftime(\"%Y\"))\n                filename_elements.insert(2, var)\n                cog_filename = \"_\".join(filename_elements)\n                # # add extension\n                cog_filename = f\"{cog_filename}.tif\"\n\n            data = data.reindex(lat=list(reversed(data.lat)))\n\n            data.rio.set_spatial_dims(\"lon\", \"lat\")\n            data.rio.write_crs(\"epsg:4326\", inplace=True)\n\n            # generate COG\n            COG_PROFILE = {\"driver\": \"COG\", \"compress\": \"DEFLATE\"}\n            with tempfile.NamedTemporaryFile() as temp_file:\n                data.rio.to_raster(temp_file.name, **COG_PROFILE)\n                s3_client.upload_file(\n                    Filename=temp_file.name,\n                    Bucket=bucket_name,\n                    Key=f\"ceos_co2_flux/{cog_filename}\",\n                )\n\n            files_processed = files_processed._append(\n                {\"file_name\": name, \"COGs_created\": cog_filename},\n                ignore_index=True,\n            )\n\n            print(f\"Generated and saved COG: {cog_filename}\")\n\n# creating the csv file with the names of files transformed.\nfiles_processed.to_csv(\n    f\"s3://{bucket_name}/ceos_co2_flux/files_converted.csv\",\n)\nprint(\"Done generating COGs\")\n\n\n\n\n Back to top",
    "crumbs": [
      "Data Transformation Notebooks",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "OCO-2 MIP Top-Down CO₂ Budgets"
    ]
  },
  {
    "objectID": "generating_statistics_for_validation/gra2pes-ghg-monthgrid-v1/gra2pes-ghg-monthgrid-v1-generate-statistics.html",
    "href": "generating_statistics_for_validation/gra2pes-ghg-monthgrid-v1/gra2pes-ghg-monthgrid-v1-generate-statistics.html",
    "title": "U.S. Greenhouse Gas Center Documentation",
    "section": "",
    "text": "import xarray as xr\nimport os\nimport glob\nfrom  datetime import datetime\nimport boto3\nimport s3fs\nimport tempfile\nimport numpy as np\nimport pandas as pd\nimport re\nimport json\n\n\nraw_files = glob.glob(\"data/*.nc4\")\noutput_files= glob.glob(\"output_final2/*.tif\")\n\n\ndef extract_date_from_key(key):\n    # Split the key to isolate the part that contains the date\n    parts = key.split('_')\n    for part in parts:\n        # Check if the part is numeric and has the length of 6 (YYYYMM format)\n        if part.isdigit() and len(part) == 6:\n            return part\n    return None\n\n\noverall_raw= []\nraw= pd.DataFrame(columns=['filename','min_raw','max_raw','mean_raw','std_raw'])\nfor file in raw_files:\n    xds= xr.open_dataset(file)\n    year_month = extract_date_from_key(file)\n    for var in [\"PM25-PRI\",\"CO2\",\"CO\",\"NOX\",\"SOX\"]:\n        data = getattr(xds,var)\n        overall_raw.append(data)\n        data = np.ma.masked_where((data == -9999), data)\n        min_val = np.nanmin(data)\n        max_val = np.nanmax(data)\n        mean_val = np.nanmean(data)\n        std_val = np.nanstd(data)\n        stats = [f\"{var}_{year_month}\", min_val, max_val, mean_val, std_val]\n        raw.loc[len(raw)] = stats\n\n\noverall_cog=[]\ncog= pd.DataFrame(columns=['filename','min_cog','max_cog','mean_cog','std_cog'])\nfor file in output_files:\n    data= xr.open_dataarray(file)\n    \n    year_month = file[:-4][-6:]\n    var = file.split(\"_\")[-2]\n    overall_cog.append(data)\n    data = np.ma.masked_where((data == -9999), data)\n    \n    \n    min_val = np.nanmin(data)\n    max_val = np.nanmax(data)\n    mean_val = np.nanmean(data)\n    std_val = np.nanstd(data)\n    stats = [f\"{var}_{year_month}\", min_val, max_val, mean_val, std_val]\n    cog.loc[len(cog)] = stats\n\n\n# validation for reprojected data (non zero) - overall calculation\noverall_raw= np.array(overall_raw)\noverall_raw= np.ma.masked_where((overall_raw == -9999) , overall_raw)\nnan_min = np.nanmin(overall_raw)\nnan_max = np.nanmax(overall_raw)\nnan_mean = np.nanmean(overall_raw)\nnan_std = np.nanstd(overall_raw)\n[\"overall_raw\",nan_min,nan_max,nan_mean,nan_std]\n\n['overall_raw', 0.0, 110011.766, 5.1753755, 172.26357]\n\n\n\noverall_cog= np.array(overall_cog)\nnan_min = np.nanmin(overall_cog)\nnan_max = np.nanmax(overall_cog)\nnan_mean = np.nanmean(overall_cog)\nnan_std = np.nanstd(overall_cog)\n[\"overall_cog\",nan_min,nan_max,nan_mean,nan_std]\n\n['overall_cog', 0.0, 110011.766, 5.1753297, 172.27177]\n\n\n\npd.merge(cog, raw, on='filename', how='inner').to_json(\"monthly_stats.json\")\n\n\n\nkeys = [\"data\", \"nan_min\", \"nan_max\", \"nan_mean\", \"nan_std\"]\nvalues_set1 = [\"overall_raw\", 0.0, 110011.766, 5.1753297, 172.27177]\nvalues_set2 = [\"overall_cog\", 0.0, 110011.766, 5.1753297, 172.27177]\n\ndata_dict = {key: [val1, val2] for key, val1, val2 in zip(keys, values_set1, values_set2)}\n\n# Save the dictionary as a JSON file\nwith open(\"overall_stats.json\", \"w\") as json_file:\n    json.dump(data_dict, json_file, indent=4)\n\n\n\n\n Back to top"
  },
  {
    "objectID": "datausage.html",
    "href": "datausage.html",
    "title": "U.S. Greenhouse Gas Center: Data Usage Notebooks",
    "section": "",
    "text": "Welcome to the U.S. Greenhouse Gas (GHG) Center data usage notebooks, your gateway to exploring and analyzing curated datasets on greenhouse gas emissions. Our cloud-based system offers seamless access to GHG curated datasets. Dive into the data with our data usage Jupyter notebooks, which demonstrate how to explore, access, visualize, and conduct basic data analysis for each GHG Center dataset in a code notebook environment. The data usage notebooks are grouped topically. Click on a notebook to learn more about the dataset and to view the data usage code.\nJoin us in our mission to make data-driven environmental solutions. Explore, analyze, and make a difference with the US GHG Center.\nView the US GHG Center Data Catalog",
    "crumbs": [
      "Data Usage Notebooks"
    ]
  },
  {
    "objectID": "datausage.html#gridded-anthropogenic-greenhouse-gas-emissions",
    "href": "datausage.html#gridded-anthropogenic-greenhouse-gas-emissions",
    "title": "U.S. Greenhouse Gas Center: Data Usage Notebooks",
    "section": "Gridded Anthropogenic Greenhouse Gas Emissions",
    "text": "Gridded Anthropogenic Greenhouse Gas Emissions\n\nOCO-2 MIP Top-Down CO₂ Budgets\n\nBeginner level notebook to access, visualize, explore statistics, and create a time series of the OCO-2 MIP Top-Down CO₂ Budgets dataset.\nIntermediate level notebook to read and visualize National CO₂ Budgets using OCO-2 MIP Top-Down CO₂ Budget country total data. This notebook utilizes the country totals available at https://ceos.org/gst/carbon-dioxide.html, which compliment the global 1° x 1° gridded CO₂ Budget data featured in the US GHG Center.\n\nODIAC Fossil Fuel CO₂ Emissions\n\nBeginner level notebook to access, visualize, explore statistics, and create a time series of the ODIAC Fossil Fuel CO₂ Emissions dataset.\n\nTM5-4DVar Isotopic CH₄ Inverse Fluxes\n\nBeginner level notebook to access, visualize, explore statistics, and create a time series of the TM5-4DVar Isotopic CH₄ Inverse Fluxes dataset.\n\nU.S. Gridded Anthropogenic Methane Emissions Inventory\n\nBeginner level notebook to access, visualize, explore statistics, and create a time series of the U.S. Gridded Anthropogenic Methane Emissions Inventory dataset.\n\nVulcan Fossil Fuel CO₂ Emissions\n\nBeginner level notebook to access, visualize, explore statistics, and create a time series of the Vulcan Fossil Fuel CO₂ Emissions, Version 4 dataset.",
    "crumbs": [
      "Data Usage Notebooks"
    ]
  },
  {
    "objectID": "datausage.html#natural-greenhouse-gas-emissions-and-sinks",
    "href": "datausage.html#natural-greenhouse-gas-emissions-and-sinks",
    "title": "U.S. Greenhouse Gas Center: Data Usage Notebooks",
    "section": "Natural Greenhouse Gas Emissions and Sinks",
    "text": "Natural Greenhouse Gas Emissions and Sinks\n\nAir-Sea CO₂ Flux, ECCO-Darwin Model v5\n\nBeginner level notebook to access, visualize, explore statistics, and create a time series of the Air-Sea CO₂ Flux, ECCO-Darwin Model v5 dataset.\n\nMiCASA Land Carbon Flux\n\nBeginner level notebook to access, visualize, explore statistics, and create a time series of the MiCASA Land Carbon Flux dataset.\n\nGOSAT-based Top-down Total and Natural Methane Emissions\n\nBeginner level notebook to access, visualize, explore statistics, and create a time series of the GOSAT-based Top-down Total and Natural Methane Emissions dataset.\n\nOCO-2 MIP Top-Down CO₂ Budgets\n\nBeginner level notebook to access, visualize, explore statistics, and create a time series of the OCO-2 MIP Top-Down CO₂ Budgets dataset.\nIntermediate level notebook to read and visualizeNational CO₂ Budgets using OCO-2 MIP Top-Down CO₂ Budget country total data. This notebook utilizes the country totals available at ceos.org/gst/carbon-dioxide, which compliment the global 1° x 1° gridded CO₂ Budget data featured in the US GHG Center.\n\nTM5-4DVar Isotopic CH₄ Inverse Fluxes\n\nBeginner level notebook to access, visualize, explore statistics, and create a time series of the TM5-4DVar Isotopic CH₄ Inverse Fluxes dataset.\n\nWetland Methane Emissions, LPJ-EOSIM model\n\nBeginner level notebook to access, visualize, explore statistics, and create a time series of the Wetland Methane Emissions, LPJ-EOSIM model dataset.\n\nGRA²PES Greenhouse Gas and Air Quality Species\n\nBeginner level notebook to access, visualize, explore statistics, and create a time series of the GRA2PES, Version 1 dataset.",
    "crumbs": [
      "Data Usage Notebooks"
    ]
  },
  {
    "objectID": "datausage.html#large-emissions-events",
    "href": "datausage.html#large-emissions-events",
    "title": "U.S. Greenhouse Gas Center: Data Usage Notebooks",
    "section": "Large Emissions Events",
    "text": "Large Emissions Events\n\nEMIT Methane Point Source Plume Complexes\n\nBeginner level notebook to access, visualize, explore statistics, and create a time series of the EMIT Methane Point Source Plume Complexes dataset.",
    "crumbs": [
      "Data Usage Notebooks"
    ]
  },
  {
    "objectID": "datausage.html#greenhouse-gas-concentrations",
    "href": "datausage.html#greenhouse-gas-concentrations",
    "title": "U.S. Greenhouse Gas Center: Data Usage Notebooks",
    "section": "Greenhouse Gas Concentrations",
    "text": "Greenhouse Gas Concentrations\n\nAtmospheric Carbon Dioxide Concentrations from NOAA Global Monitoring Laboratory\n\nBeginner level notebook to access, visualize, explore statistics, and create a time series of the Atmospheric Carbon Dioxide Concentrations from NOAA Global Monitoring Laboratory dataset.\n\nOCO-2 GEOS Column CO₂ Concentrations\n\nBeginner level notebook to access, visualize, explore statistics, and create a time series of the OCO-2 GEOS Column CO₂ Concentrations dataset.\n\nCarbon Dioxide and Methane Concentrations from the Indianapolis Flux Experiment (INFLUX)\nCarbon Dioxide and Methane Concentrations from the Los Angeles Megacity Carbon Project\nCarbon Dioxide and Methane Concentrations from the Northeast Corridor (NEC) Urban Test Bed",
    "crumbs": [
      "Data Usage Notebooks"
    ]
  },
  {
    "objectID": "datausage.html#socioeconomic",
    "href": "datausage.html#socioeconomic",
    "title": "U.S. Greenhouse Gas Center: Data Usage Notebooks",
    "section": "Socioeconomic",
    "text": "Socioeconomic\n\nSEDAC Gridded World Population Density\n\nBeginner level notebook to access, visualize, explore statistics, and create a time series of the SEDAC Gridded World Population Density dataset.",
    "crumbs": [
      "Data Usage Notebooks"
    ]
  },
  {
    "objectID": "datausage.html#contact",
    "href": "datausage.html#contact",
    "title": "U.S. Greenhouse Gas Center: Data Usage Notebooks",
    "section": "Contact",
    "text": "Contact\nFor technical help or general questions, please contact the support team using the feedback form.",
    "crumbs": [
      "Data Usage Notebooks"
    ]
  },
  {
    "objectID": "data_workflow/eccodarwin-co2flux-monthgrid-v5_Data_Flow.html",
    "href": "data_workflow/eccodarwin-co2flux-monthgrid-v5_Data_Flow.html",
    "title": "Air-Sea CO₂ Flux, ECCO-Darwin Model v5",
    "section": "",
    "text": "Air-Sea CO₂ Flux, ECCO-Darwin Model v5\n\n\n\nData Flow Diagram Extending From Acquisition/Creation to User Delivery\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Data Flow Diagrams",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "Air-Sea CO₂ Flux, ECCO-Darwin Model v5"
    ]
  },
  {
    "objectID": "data_workflow/noaa-gggrn-co2-concentrations_Data_Flow.html",
    "href": "data_workflow/noaa-gggrn-co2-concentrations_Data_Flow.html",
    "title": "Atmospheric Carbon Dioxide Concentrations from the NOAA Global Monitoring Laboratory",
    "section": "",
    "text": "Atmospheric Carbon Dioxide Concentrations from the NOAA Global Monitoring Laboratory\n\n\n\nData Flow Diagram Extending From Acquisition/Creation to User Delivery\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Data Flow Diagrams",
      "Greenhouse Gas Concentrations",
      "Atmospheric Carbon Dioxide Concentrations from the NOAA Global Monitoring Laboratory"
    ]
  },
  {
    "objectID": "data_workflow/sedac-popdensity-yeargrid5yr-v4.11_Data_Flow.html",
    "href": "data_workflow/sedac-popdensity-yeargrid5yr-v4.11_Data_Flow.html",
    "title": "SEDAC Gridded World Population Data",
    "section": "",
    "text": "SEDAC Gridded World Population Data\n\n\n\nData Flow Diagram Extending From Acquisition/Creation to User Delivery\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Data Flow Diagrams",
      "Socioeconomic",
      "SEDAC Gridded World Population Data"
    ]
  },
  {
    "objectID": "data_workflow/lam-testbed-ghg-concentrations_Data_Flow.html",
    "href": "data_workflow/lam-testbed-ghg-concentrations_Data_Flow.html",
    "title": "Carbon Dioxide and Methane Concentrations from the Los Angeles Megacity Carbon Project",
    "section": "",
    "text": "Carbon Dioxide and Methane Concentrations from the Los Angeles Megacity Carbon Project\n\n\n\nData Flow Diagram Extending From Acquisition/Creation to User Delivery\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Data Flow Diagrams",
      "Greenhouse Gas Concentrations",
      "Carbon Dioxide and Methane Concentrations from the Los Angeles Megacity Carbon Project"
    ]
  },
  {
    "objectID": "data_workflow/noaa-gggrn-ch4-concentrations_Data_Flow.html",
    "href": "data_workflow/noaa-gggrn-ch4-concentrations_Data_Flow.html",
    "title": "Atmospheric Methane Concentrations from the NOAA Global Monitoring Laboratory",
    "section": "",
    "text": "Atmospheric Methane Concentrations from the NOAA Global Monitoring Laboratory\n\n\n\nData Flow Diagram Extending From Acquisition/Creation to User Delivery\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Data Flow Diagrams",
      "Greenhouse Gas Concentrations",
      "Atmospheric Methane Concentrations from the NOAA Global Monitoring Laboratory"
    ]
  },
  {
    "objectID": "data_workflow/epa-ch4emission-grid-v2express_Data_Flow.html",
    "href": "data_workflow/epa-ch4emission-grid-v2express_Data_Flow.html",
    "title": "Gridded Anthropogenic Methane Emissions Inventory",
    "section": "",
    "text": "Gridded Anthropogenic Methane Emissions Inventory\n\n\n\nData Flow Diagram Extending From Acquisition/Creation to User Delivery\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Data Flow Diagrams",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "Gridded Anthropogenic Methane Emissions Inventory"
    ]
  },
  {
    "objectID": "data_workflow/vulcan-ffco2-yeargrid-v4_Data_Flow.html",
    "href": "data_workflow/vulcan-ffco2-yeargrid-v4_Data_Flow.html",
    "title": "Vulcan Fossil Fuel CO₂ Emissions",
    "section": "",
    "text": "Vulcan Fossil Fuel CO₂ Emissions\n\n\n\nData Flow Diagram Extending From Acquisition/Creation to User Delivery\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Data Flow Diagrams",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "Vulcan Fossil Fuel CO₂ Emissions"
    ]
  },
  {
    "objectID": "data_workflow/tm54dvar-ch4flux-monthgrid-v1_Data_Flow.html",
    "href": "data_workflow/tm54dvar-ch4flux-monthgrid-v1_Data_Flow.html",
    "title": "TM5-4DVar Isotopic CH₄ Inverse Fluxes",
    "section": "",
    "text": "TM5-4DVar Isotopic CH₄ Inverse Fluxes\n\n\n\nData Flow Diagram Extending From Acquisition/Creation to User Delivery\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Data Flow Diagrams",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "TM5-4DVar Isotopic CH₄ Inverse Fluxes"
    ]
  },
  {
    "objectID": "data_workflow/odiac-ffco2-monthgrid-v2023_Data_Flow.html",
    "href": "data_workflow/odiac-ffco2-monthgrid-v2023_Data_Flow.html",
    "title": "ODIAC Fossil Fuel CO₂ Emissions",
    "section": "",
    "text": "ODIAC Fossil Fuel CO₂ Emissions\n\n\n\nData Flow Diagram Extending From Acquisition/Creation to User Delivery\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Data Flow Diagrams",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "ODIAC Fossil Fuel CO₂ Emissions"
    ]
  },
  {
    "objectID": "advanceduser.html",
    "href": "advanceduser.html",
    "title": "U.S. Greenhouse Gas Center: Advanced User Notebook",
    "section": "",
    "text": "Welcome to the U.S. Greenhouse Gas (GHG) Center: Advanced User Notebook, your gateway to exploring and analyzing curated datasets on greenhouse gas emissions. Our cloud-based system offers seamless access to Greenhouse Gas curated datasets. Dive into the data with our data usage Jupyter notebooks, designed for efficient exploration, visualization, and analysis. Whether you are focused on specific focus areas or product types, our dataset usage notebooks provide invaluable insights to drive informed decision-making.\nJoin us in our mission to make data-driven environmental solutions. Explore, analyze, and make a difference with the US GHG Center.\nView the US GHG Center Data Catalogg"
  },
  {
    "objectID": "advanceduser.html#gridded-anthropogenic-greenhouse-gas-emissions",
    "href": "advanceduser.html#gridded-anthropogenic-greenhouse-gas-emissions",
    "title": "U.S. Greenhouse Gas Center: Advanced User Notebook",
    "section": "Gridded Anthropogenic Greenhouse Gas Emissions",
    "text": "Gridded Anthropogenic Greenhouse Gas Emissions\n\nOCO-2 MIP Top-Down National CO₂ Budgets - Model Output"
  },
  {
    "objectID": "advanceduser.html#natural-greenhouse-gas-emissions-and-sinks",
    "href": "advanceduser.html#natural-greenhouse-gas-emissions-and-sinks",
    "title": "U.S. Greenhouse Gas Center: Advanced User Notebook",
    "section": "Natural Greenhouse Gas Emissions and Sinks",
    "text": "Natural Greenhouse Gas Emissions and Sinks"
  },
  {
    "objectID": "advanceduser.html#large-emissions-events",
    "href": "advanceduser.html#large-emissions-events",
    "title": "U.S. Greenhouse Gas Center: Advanced User Notebook",
    "section": "Large Emissions Events",
    "text": "Large Emissions Events"
  },
  {
    "objectID": "advanceduser.html#greenhouse-gas-concentrations",
    "href": "advanceduser.html#greenhouse-gas-concentrations",
    "title": "U.S. Greenhouse Gas Center: Advanced User Notebook",
    "section": "Greenhouse Gas Concentrations",
    "text": "Greenhouse Gas Concentrations"
  },
  {
    "objectID": "advanceduser.html#socioeconomic",
    "href": "advanceduser.html#socioeconomic",
    "title": "U.S. Greenhouse Gas Center: Advanced User Notebook",
    "section": "Socioeconomic",
    "text": "Socioeconomic"
  },
  {
    "objectID": "advanceduser.html#contact",
    "href": "advanceduser.html#contact",
    "title": "U.S. Greenhouse Gas Center: Advanced User Notebook",
    "section": "Contact",
    "text": "Contact\nFor technical help or general questions, please contact the support team using the feedback form."
  },
  {
    "objectID": "data_usage.html",
    "href": "data_usage.html",
    "title": "Introduction to U.S. Greenhouse Gas Center: Data Usage Notebooks",
    "section": "",
    "text": "The U.S. Greenhouse Gas (GHG) Center provides a cloud-based system for exploring and analyzing U.S. government and other curated greenhouse gas datasets.\nOn this site, you can find the technical documentation for the services the center provides, how to load the datasets, and how the datasets were transformed from their source formats (eg. netCDF, HDF, etc.) into cloud-optimized formats that enable efficient cloud data access and visualization."
  },
  {
    "objectID": "data_usage.html#welcome",
    "href": "data_usage.html#welcome",
    "title": "Introduction to U.S. Greenhouse Gas Center: Data Usage Notebooks",
    "section": "",
    "text": "The U.S. Greenhouse Gas (GHG) Center provides a cloud-based system for exploring and analyzing U.S. government and other curated greenhouse gas datasets.\nOn this site, you can find the technical documentation for the services the center provides, how to load the datasets, and how the datasets were transformed from their source formats (eg. netCDF, HDF, etc.) into cloud-optimized formats that enable efficient cloud data access and visualization."
  },
  {
    "objectID": "data_usage.html#contents",
    "href": "data_usage.html#contents",
    "title": "Introduction to U.S. Greenhouse Gas Center: Data Usage Notebooks",
    "section": "Contents",
    "text": "Contents\n\nDataset usage examples listed below."
  },
  {
    "objectID": "data_usage.html#explore-data-usage-notebook",
    "href": "data_usage.html#explore-data-usage-notebook",
    "title": "Introduction to U.S. Greenhouse Gas Center: Data Usage Notebooks",
    "section": "Explore Data Usage Notebook",
    "text": "Explore Data Usage Notebook\n\nCASA-GFED3 Land Carbon Flux\nAir-Sea CO₂ Flux, ECCO-Darwin Model v5\nEMIT Methane Point Source Plume Complexes\nU.S. Gridded Anthropogenic Methane Emissions Inventory\nGOSAT-based Top-down Total and Natural Methane Emissions\nWetland Methane Emissions, LPJ-wsl Model\n\nOCO-2 MIP Top-Down CO₂ Budgets\nOCO-2 GEOS Column CO₂ Concentrations\nODIAC Fossil Fuel CO₂ Emissions\nSEDAC Gridded World Population Density\nTM5-4DVar Isotopic CH₄ Inverse Fluxes\nAtmospheric Carbon Dioxide Concentrations from NOAA Global Monitoring Laboratory\nVulcan Fossil Fuel CO₂ Emissions\nGreenhouse Gas And Air Pollutants Emissions System"
  },
  {
    "objectID": "data_workflow/gra2pes-ghg-monthgrid-v1_Data_Flow.html",
    "href": "data_workflow/gra2pes-ghg-monthgrid-v1_Data_Flow.html",
    "title": "GRA²PES Greenhouse Gas and Air Quality Species",
    "section": "",
    "text": "GRA²PES Greenhouse Gas and Air Quality Species\n\n\n\nData Flow Diagram Extending From Acquisition/Creation to User Delivery\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Data Flow Diagrams",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "GRA²PES Greenhouse Gas and Air Quality Species"
    ]
  },
  {
    "objectID": "data_workflow/gosat-based-ch4budget-yeargrid-v1_Data_Flow.html",
    "href": "data_workflow/gosat-based-ch4budget-yeargrid-v1_Data_Flow.html",
    "title": "GOSAT-based Top-down Total and Natural Methane Emissions",
    "section": "",
    "text": "GOSAT-based Top-down Total and Natural Methane Emissions\n\n\n\nData Flow Diagram Extending From Acquisition/Creation to User Delivery\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Data Flow Diagrams",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "GOSAT-based Top-down Total and Natural Methane Emissions"
    ]
  },
  {
    "objectID": "data_workflow/oco2-mip-co2budget-yeargrid-v1_Data_Flow.html",
    "href": "data_workflow/oco2-mip-co2budget-yeargrid-v1_Data_Flow.html",
    "title": "OCO-2 MIP Top-Down CO₂ Budgets",
    "section": "",
    "text": "OCO-2 MIP Top-Down CO₂ Budgets\n\n\n\nData Flow Diagram Extending From Acquisition/Creation to User Delivery\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Data Flow Diagrams",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "OCO-2 MIP Top-Down CO₂ Budgets"
    ]
  },
  {
    "objectID": "data_workflow/influx-testbed-ghg-concentrations_Data_Flow.html",
    "href": "data_workflow/influx-testbed-ghg-concentrations_Data_Flow.html",
    "title": "Carbon Dioxide and Methane Concentrations from the Indianapolis Flux Experiment (INFLUX)",
    "section": "",
    "text": "Carbon Dioxide and Methane Concentrations from the Indianapolis Flux Experiment (INFLUX)\n\n\n\nData Flow Diagram Extending From Acquisition/Creation to User Delivery\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Data Flow Diagrams",
      "Greenhouse Gas Concentrations",
      "Carbon Dioxide and Methane Concentrations from the Indianapolis Flux Experiment (INFLUX)"
    ]
  },
  {
    "objectID": "data_workflow/emit-ch4plume-v1_Data_Flow.html",
    "href": "data_workflow/emit-ch4plume-v1_Data_Flow.html",
    "title": "EMIT Methane Point Source Plume Complexes",
    "section": "",
    "text": "EMIT Methane Point Source Plume Complexes\n\n\n\nData Flow Diagram Extending From Acquisition/Creation to User Delivery\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Data Flow Diagrams",
      "Large Emissions Events",
      "EMIT Methane Point Source Plume Complexes"
    ]
  },
  {
    "objectID": "data_workflow/oco2geos-co2-daygrid-v10r_Data_Flow.html",
    "href": "data_workflow/oco2geos-co2-daygrid-v10r_Data_Flow.html",
    "title": "OCO-2 GEOS Column CO₂ Concentrations",
    "section": "",
    "text": "OCO-2 GEOS Column CO₂ Concentrations\n\n\n\nData Flow Diagram Extending From Acquisition/Creation to User Delivery\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Data Flow Diagrams",
      "Greenhouse Gas Concentrations",
      "OCO-2 GEOS Column CO₂ Concentrations"
    ]
  },
  {
    "objectID": "data_workflow/nec-testbed-ghg-concentrations_Data_Flow.html",
    "href": "data_workflow/nec-testbed-ghg-concentrations_Data_Flow.html",
    "title": "Carbon Dioxide and Methane Concentrations from the Northeast Corridor (NEC) Urban Test Bed",
    "section": "",
    "text": "Carbon Dioxide and Methane Concentrations from the Northeast Corridor (NEC) Urban Test Bed\n\n\n\nData Flow Diagram Extending From Acquisition/Creation to User Delivery\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Data Flow Diagrams",
      "Greenhouse Gas Concentrations",
      "Carbon Dioxide and Methane Concentrations from the Northeast Corridor (NEC) Urban Test Bed"
    ]
  },
  {
    "objectID": "data_workflow/lpjeosim-wetlandch4-grid-v2_Data_Flow.html",
    "href": "data_workflow/lpjeosim-wetlandch4-grid-v2_Data_Flow.html",
    "title": "Wetland Methane Emissions, LPJ-EOSIM Model",
    "section": "",
    "text": "Wetland Methane Emissions, LPJ-EOSIM Model\n\n\n\nData Flow Diagram Extending From Acquisition/Creation to User Delivery\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Data Flow Diagrams",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "Wetland Methane Emissions, LPJ-EOSIM Model"
    ]
  },
  {
    "objectID": "data_workflow/micasa-carbonflux-daygrid-v1_Data_Flow.html",
    "href": "data_workflow/micasa-carbonflux-daygrid-v1_Data_Flow.html",
    "title": "MiCASA Land Carbon Flux - Data Workflow",
    "section": "",
    "text": "MiCASA Land Carbon Flux - Data Workflow\n\n\n\nData Flow Diagram Extending From Acquisition/Creation to User Delivery\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Data Flow Diagrams",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "MiCASA Land Carbon Flux - Data Workflow"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "U.S. Greenhouse Gas Center: Documentation",
    "section": "",
    "text": "The U.S. Greenhouse Gas (GHG) Center provides a cloud-based system for exploring and analyzing U.S. government and other curated greenhouse gas datasets.\nOn this site, you can find the technical documentation for the services the center provides, how to load the datasets, and how the datasets were transformed from their source formats (eg. netCDF, HDF, etc.) into cloud-optimized formats that enable efficient cloud data access and visualization.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#welcome",
    "href": "index.html#welcome",
    "title": "U.S. Greenhouse Gas Center: Documentation",
    "section": "",
    "text": "The U.S. Greenhouse Gas (GHG) Center provides a cloud-based system for exploring and analyzing U.S. government and other curated greenhouse gas datasets.\nOn this site, you can find the technical documentation for the services the center provides, how to load the datasets, and how the datasets were transformed from their source formats (eg. netCDF, HDF, etc.) into cloud-optimized formats that enable efficient cloud data access and visualization.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#contents",
    "href": "index.html#contents",
    "title": "U.S. Greenhouse Gas Center: Documentation",
    "section": "Contents",
    "text": "Contents\n\nServices provided for accessing and analyzing the US GHG Center datasets, such as the JupyterHub environment for interactive computing.\nDataset usage examples, e.g. for the Wetland Methane Emissions from the LPJ-EOSIM model dataset, that shows how to load the dataset in Python in JupyterHub.\nDataset transformation scripts, which document the code used to transform datasets for display in the US GHG Center. An example is the ODIAC Fossil Fuel CO₂ Emissions dataset transformation code.\nData processing and verification reports that openly present the process we used to check and verify that any transformation did not alter the original source data. An example is the GOSAT-based Top-down Total and Natural Methane Emissions dataset.\nData Flow Diagrams, which provide a high level summary of how each dataset was integrated into the US GHG Center. See the MiCASA Land Carbon Flux Flow Diagram as an example.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#contact",
    "href": "index.html#contact",
    "title": "U.S. Greenhouse Gas Center: Documentation",
    "section": "Contact",
    "text": "Contact\nFor technical help or general questions, please contact the support team using the feedback form.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "generating_statistics_for_validation/odiac-stats-2023/generate_odiac_stats.html",
    "href": "generating_statistics_for_validation/odiac-stats-2023/generate_odiac_stats.html",
    "title": "U.S. Greenhouse Gas Center Documentation",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\nimport rasterio\nfrom glob import glob\nimport pathlib\nimport boto3\nimport pandas as pd\nimport calendar\nimport seaborn as sns\nimport json\nimport re\n\n\n# Enter the year you want to run validation on\nvyear=2022 # summary json files will be later generated for the year you provide here\ndata_dir=\"data/\" # make sure you have the data for vyear in your data directory\n\n\nsession = boto3.session.Session()\ns3_client = session.client(\"s3\")\n\ndataset_name= \"odiac-ffco2-monthgrid-v2023\"\ncog_data_bucket=\"ghgc-data-store-develop\"\ncog_data_prefix = f\"transformed_cogs/{dataset_name}\"\n\n\ndef get_all_s3_keys(bucket, model_name, ext):\n    \"\"\"Get a list of all keys in an S3 bucket.\"\"\"\n    keys = []\n\n    kwargs = {\"Bucket\": bucket, \"Prefix\": f\"{model_name}/\"}\n    while True:\n        resp = s3_client.list_objects_v2(**kwargs)\n        for obj in resp[\"Contents\"]:\n            if obj[\"Key\"].endswith(ext) and \"historical\" not in obj[\"Key\"]:\n                keys.append(obj[\"Key\"])\n\n        try:\n            kwargs[\"ContinuationToken\"] = resp[\"NextContinuationToken\"]\n        except KeyError:\n            break\n\n    return keys\n\nkeys = get_all_s3_keys(cog_data_bucket, cog_data_prefix, \".tif\")\n\n# Extract only the COGs for selected year\npattern = re.compile(rf'{vyear}(0[1-9]|1[0-2])')\nkeys = [path for path in keys if pattern.search(path)]\n\n\n# Initialize the summary variables\nsummary_dict_netcdf, summary_dict_cog = {}, {}\noverall_stats_netcdf, overall_stats_cog = {}, {}\nfull_data_df_netcdf, full_data_df_cog = pd.DataFrame(), pd.DataFrame()\n\n\n# Process the COGs to get the statistics\nfor key in keys:\n    url=f\"s3://{cog_data_bucket}/{key}\"\n    with rasterio.open(url) as src:\n        filename_elements = re.split(\"[_ ? . ]\", url)\n        for band in src.indexes:\n            print(\"_\".join(filename_elements[1:6]))\n            idx = pd.MultiIndex.from_product(\n                    [\n                        [\"_\".join(filename_elements[1:6])],\n                        [filename_elements[5]],\n                        [x for x in np.arange(1, src.height + 1)],\n                    ]\n                )\n            raster_data = src.read(band)\n            raster_data[raster_data == -9999] = 0 # because we did that in the transformation script\n            temp = pd.DataFrame(index=idx, data=raster_data)\n            full_data_df_cog = full_data_df_cog._append(temp, ignore_index=False)\n\n            # Calculate summary statistics\n            min_value = np.float64(temp.values.min())\n            max_value = np.float64(temp.values.max())\n            mean_value = np.float64(temp.values.mean())\n            std_value = np.float64(temp.values.std())\n\n            summary_dict_cog[\n                    f'{\"_\".join(filename_elements[1:5])}_{filename_elements[5][:4]}_{calendar.month_name[int(filename_elements[5][4:])]}'\n                ] = {\n                    \"min_value\": min_value,\n                    \"max_value\": max_value,\n                    \"mean_value\": mean_value,\n                    \"std_value\": std_value,\n                }\n\n\n# Process the raw files for selected year to get the statistics \ntif_files = glob(f\"{data_dir}{vyear}/*.tif\", recursive=True)\nfor tif_file in tif_files:\n    file_name = pathlib.Path(tif_file).name[:-4]\n    print(file_name)\n    with rasterio.open(tif_file) as src:\n        for band in src.indexes:\n            idx = pd.MultiIndex.from_product(\n                [\n                    [pathlib.Path(tif_file).name[:-9]],\n                    [pathlib.Path(tif_file).name[-8:-4]],\n                    [x for x in np.arange(1, src.height + 1)],\n                ]\n            )\n            # Read the raster data\n            raster_data = src.read(band)\n            #raster_data[raster_data == -9999] = np.nan\n            temp = pd.DataFrame(index=idx, data=raster_data)\n            full_data_df_netcdf = full_data_df_netcdf._append(temp, ignore_index=False)\n\n            # Calculate summary statistics\n            min_value = np.float64(temp.values.min())\n            max_value = np.float64(temp.values.max())\n            mean_value = np.float64(temp.values.mean())\n            std_value = np.float64(temp.values.std())\n\n            summary_dict_netcdf[\n                f'{tif_file.split(\"/\")[-1][:-9]}_{calendar.month_name[int(tif_file.split(\"/\")[-1][-6:-4])]}'\n            ] = {\n                \"min_value\": min_value,\n                \"max_value\": max_value,\n                \"mean_value\": mean_value,\n                \"std_value\": std_value,\n            }\n            \n\n\n# Merge monthly stats for COGs and raw files in a csv file \ncog_df = pd.DataFrame(summary_dict_cog).T.reset_index()\nraw_df = pd.DataFrame(summary_dict_netcdf).T.reset_index()\ncog_df['date']= cog_df[\"index\"].apply(lambda x: (x.split(\"_\")[-1]+x.split(\"_\")[-2]) )\nraw_df['date']= raw_df[\"index\"].apply(lambda x: (x.split(\"_\")[-1]+str(vyear)) )\ncheck_df=pd.merge(cog_df, raw_df[[\"min_value\",\"max_value\",\"mean_value\",\"std_value\",\"date\"]], how='inner', on='date',suffixes=('', '_raw'))\ncheck_df.to_csv(f\"monthly_stats_{vyear}.csv\")\n\n\n# Calculate the overall data stat for that year\noverall_stats_netcdf[\"min_value\"] = np.float64(full_data_df_netcdf.values.min())\noverall_stats_netcdf[\"max_value\"] = np.float64(full_data_df_netcdf.values.max())\noverall_stats_netcdf[\"mean_value\"] = np.float64(full_data_df_netcdf.values.mean())\noverall_stats_netcdf[\"std_value\"] = np.float64(full_data_df_netcdf.values.std())\n\noverall_stats_cog[\"min_value\"] = np.float64(full_data_df_cog.values.min())\noverall_stats_cog[\"max_value\"] = np.float64(full_data_df_cog.values.max())\noverall_stats_cog[\"mean_value\"] = np.float64(full_data_df_cog.values.mean())\noverall_stats_cog[\"std_value\"] = np.float64(full_data_df_cog.values.std())\n\n\n\ndata = {\n    \"Stats for raw netCDF files.\": summary_dict_netcdf,\n    \"Stats for transformed COG files.\": summary_dict_cog\n}\n\n# Writing to JSON file\nwith open(f\"monthly_stats_{vyear}.json\", \"w\") as fp:\n    json.dump(data, fp, indent=4) \n\ndata = {\n    \"Stats for raw netCDF files.\": overall_stats_netcdf,\n    \"Stats for transformed COG files.\": overall_stats_cog\n}\n\n# Writing to JSON file\nwith open(f\"overall_stats_{vyear}.json\", \"w\") as fp:\n    json.dump(data, fp, indent=4) \n\n\n\n\n Back to top"
  },
  {
    "objectID": "cog_transformation/emit-ch4plume-v1.html",
    "href": "cog_transformation/emit-ch4plume-v1.html",
    "title": "EMIT Methane Point Source Plume Complexes",
    "section": "",
    "text": "This script was used to read the EMIT Methane Point Source Plume Complexes dataset provided in Cloud Optimized GeoTIFF (COG) format for display in the Greenhouse Gas (GHG) Center.\n\nimport re\nimport pandas as pd\nimport json\nimport tempfile\nimport boto3\n\n\nsession_ghgc = boto3.session.Session(profile_name=\"ghg_user\")\ns3_client_ghgc = session_ghgc.client(\"s3\")\nsession_veda_smce = boto3.session.Session()\ns3_client_veda_smce = session_veda_smce.client(\"s3\")\n\n# Since the plume emissions were already COGs, we just had to transform their naming convention to be stored in the STAC collection.\nSOURCE_BUCKET_NAME = \"ghgc-data-staging-uah\"\nTARGET_BUCKET_NAME = \"ghgc-data-store-dev\"\n\n\nkeys = []\nresp = s3_client_ghgc.list_objects_v2(Bucket=SOURCE_BUCKET_NAME)\nfor obj in resp[\"Contents\"]:\n    if \"l3\" in obj[\"Key\"]:\n        keys.append(obj[\"Key\"])\n\nfor key in keys:\n    s3_obj = s3_client_ghgc.get_object(Bucket=SOURCE_BUCKET_NAME, Key=key)[\n        \"Body\"\n    ]\n    filename = key.split(\"/\")[-1]\n    filename_elements = re.split(\"[_ .]\", filename)\n\n    date = re.search(\"t\\d\\d\\d\\d\\d\\d\\d\\dt\", key).group(0)\n    filename_elements.insert(-1, date[1:-1])\n    filename_elements.pop()\n\n    cog_filename = \"_\".join(filename_elements)\n    # # add extension\n    cog_filename = f\"{cog_filename}.tif\"\n    s3_client_veda_smce.upload_fileobj(\n        Fileobj=s3_obj,\n        Bucket=TARGET_BUCKET_NAME,\n        Key=f\"plum_data/{cog_filename}\",\n    )\n\n\n\n\n Back to top",
    "crumbs": [
      "Data Transformation Notebooks",
      "Large Emissions Events",
      "EMIT Methane Point Source Plume Complexes"
    ]
  },
  {
    "objectID": "cog_transformation/lam-testbed-ghg-concentrations.html",
    "href": "cog_transformation/lam-testbed-ghg-concentrations.html",
    "title": "Carbon Dioxide and Methane Concentrations from the Los Angeles Megacity Carbon Project",
    "section": "",
    "text": "This script was used to transform the NIST INFLUX dataset into meaningful csv files for ingestion to vector dataset.\n\nimport pandas as pd\nimport glob\nimport os\nimport warnings\nimport warnings \nwarnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n\n\n# download data from https://data.nist.gov/od/id/mds2-2388 into your desired_folder\nsource_dir = \"CA\"\n\n\n# Grouping the files for preparation\nconfig_ca = pd.read_csv(\"LAM_sites-2.csv\") #metadata from providers\nall_files= glob.glob(f\"{source_dir}/*.csv\")\nall_files = [i.split(\"/\")[-1].split('.')[0] for i in glob.glob(f\"{source_dir}/*.csv\") ]\nmy_dict={}\nfor site in list(config_ca.SiteCode):\n    # for each site and variable, append into the dict\n    if (config_ca[config_ca[\"SiteCode\"]==site][\"Tower\"].item()) ==1 :\n\n        co2_files = [f for f in all_files if site in f and \"upwind\" not in f and \"all\" not in f and \"co2\" in f]\n        my_dict[f\"{site}-co2\"] = co2_files\n        # Find the files that do not have \"upwind\" or \"all\" and have \"ch4\"\n        ch4_files = [f for f in all_files if site in f and \"upwind\" not in f and \"all\" not in f and \"ch4\" in f]\n        my_dict[f\"{site}-ch4\"] = ch4_files\n    else:\n        co2_upwind_files = [f for f in all_files if site in f and \"upwind\" in f and \"co2\" in f]\n        my_dict[f\"{site}-co2\"] = co2_upwind_files\n        \n        # Find the files that have \"upwind\" and \"ch4\"\n        ch4_upwind_files = [f for f in all_files if site in f and \"upwind\" in f and \"ch4\" in f]\n        my_dict[f\"{site}-ch4\"] = ch4_upwind_files\n\n        if site in [\"IRV\",\"RAN\"]:\n            co2_files = [f for f in all_files if site in f and \"all\" in f and \"co2\" in f]\n            my_dict[f\"{site}-co2\"] = co2_files\n            ch4_files = [f for f in all_files if site in f and \"all\" in f and \"ch4\" in f]\n            my_dict[f\"{site}-ch4\"] = ch4_files\n        \ndel my_dict['USC2-co2']\ndel my_dict['USC2-ch4']\n\nfor key in my_dict:\n    my_dict[key] = sorted(my_dict[key])\n\n\n# code to generate transformed data for CA\noutput_dir = \"output_LAM\"\nos.makedirs(output_dir,exist_ok=True)\nfor key, value in my_dict.items():\n    df=pd.DataFrame()\n    variable = key.split(\"-\")[-1]\n    val = f\"{variable}_ppm\" if variable == 'co2' else f\"{variable}_ppb\"\n    columns = [\"latitude\",\"longitude\",\"intake_height_m\",\"elevation_m\",\"datetime\",val ]\n    for file in value:\n        tmp = pd.read_csv(f\"CA/{file}.csv\")\n        tmp.dropna(subset=[val], inplace=True)\n        tmp.rename(columns={'datetime_UTC': 'datetime'}, inplace=True)\n        tmp= tmp[columns]\n        tmp.rename(columns={val: 'value'}, inplace=True)\n        tmp['datetime'] = pd.to_datetime(tmp['datetime'])\n        tmp['datetime'] = tmp['datetime'].dt.strftime('%Y-%m-%dT%H:%M:%SZ')\n        tmp['location'] = config_ca[config_ca['SiteCode']==site][\"Location\"].item()\n        df = pd.concat([df, tmp], ignore_index=True)\n        \n    df['year']= df['datetime'].apply(lambda x: x[:4])\n    result = df.groupby(\"year\").agg(max_height= (\"intake_height_m\",\"max\"))\n    if result['max_height'].std() !=0:\n        print(f\"More than one max height for {file}\",result['max_height'].unique())\n    merged_df=pd.merge(df, result, on='year')\n    merged_df[\"is_max_height_data\"]= merged_df[\"max_height\"] == merged_df[\"intake_height_m\"]\n    merged_df=merged_df.drop(columns=['year','max_height'])\n    merged_df.reset_index(drop=True, inplace=True)\n    merged_df.to_csv(f\"{output_dir}/NIST-testbed-LAM-{key}-hourly-concentrations.csv\", index=False)\n    \n\n\n\n\n Back to top",
    "crumbs": [
      "Data Transformation Notebooks",
      "Greenhouse Gas Concentrations",
      "Carbon Dioxide and Methane Concentrations from the Los Angeles Megacity Carbon Project"
    ]
  },
  {
    "objectID": "cog_transformation/epa-ch4emission-monthgrid-v2.html",
    "href": "cog_transformation/epa-ch4emission-monthgrid-v2.html",
    "title": "Gridded Anthropogenic Methane Emissions Inventory",
    "section": "",
    "text": "This script was used to transform the Gridded Anthropogenic Methane Emissions Inventory dataset from netCDF to Cloud Optimized GeoTIFF (COG) format for display in the Greenhouse Gas (GHG) Center.\n\nimport os\nimport xarray\nimport re\nimport pandas as pd\nimport json\nimport tempfile\nimport boto3\nfrom datetime import datetime\nfrom dateutil.relativedelta import relativedelta\n\n\nsession = boto3.session.Session()\ns3_client = session.client(\"s3\")\nbucket_name = (\n    \"ghgc-data-store-dev\"  # S3 bucket where the COGs are stored after transformation\n)\nFOLDER_NAME = \"epa_emissions/monthly_scale\"\ns3_folder_name = \"epa-emissions-monthly-scale-factors\"\n\nfiles_processed = pd.DataFrame(\n    columns=[\"file_name\", \"COGs_created\"]\n)  # A dataframe to keep track of the files that we have transformed into COGs\n\n# Reading the raw netCDF files from local machine\nfor name in os.listdir(FOLDER_NAME):\n    xds = xarray.open_dataset(f\"{FOLDER_NAME}/{name}\", engine=\"netcdf4\")\n    xds = xds.assign_coords(lon=(((xds.lon + 180) % 360) - 180)).sortby(\"lon\")\n    variable = [var for var in xds.data_vars]\n    filename = name.split(\"/ \")[-1]\n    filename_elements = re.split(\"[_ .]\", filename)\n    start_time = datetime(int(filename_elements[-2]), 1, 1)\n\n    for time_increment in range(0, len(xds.time)):\n        for var in variable:\n            filename = name.split(\"/ \")[-1]\n            filename_elements = re.split(\"[_ .]\", filename)\n            data = getattr(xds.isel(time=time_increment), var)\n            data = data.isel(lat=slice(None, None, -1))\n            data.rio.set_spatial_dims(\"lon\", \"lat\", inplace=True)\n            data.rio.write_crs(\"epsg:4326\", inplace=True)\n            date = start_time + relativedelta(months=+time_increment)\n\n            # # insert date of generated COG into filename\n            filename_elements.pop()\n            filename_elements[-1] = date.strftime(\"%Y%m\")\n            filename_elements.insert(2, var)\n            cog_filename = \"_\".join(filename_elements)\n            # # add extension\n            cog_filename = f\"{cog_filename}.tif\"\n\n            with tempfile.NamedTemporaryFile() as temp_file:\n                data.rio.to_raster(\n                    temp_file.name,\n                    driver=\"COG\",\n                )\n                s3_client.upload_file(\n                    Filename=temp_file.name,\n                    Bucket=bucket_name,\n                    Key=f\"{s3_folder_name}/{cog_filename}\",\n                )\n\n            files_processed = files_processed._append(\n                {\"file_name\": name, \"COGs_created\": cog_filename},\n                ignore_index=True,\n            )\n\n            print(f\"Generated and saved COG: {cog_filename}\")\n\n# Generate the json file with the metadata that is present in the netCDF files.\nwith tempfile.NamedTemporaryFile(mode=\"w+\") as fp:\n    json.dump(xds.attrs, fp)\n    json.dump({\"data_dimensions\": dict(xds.dims)}, fp)\n    json.dump({\"data_variables\": list(xds.data_vars)}, fp)\n    fp.flush()\n\n    s3_client.upload_file(\n        Filename=fp.name,\n        Bucket=bucket_name,\n        Key=f\"{s3_folder_name}/metadata.json\",\n    )\n\n# creating the csv file with the names of files transformed.\nfiles_processed.to_csv(\n    f\"s3://{bucket_name}/{s3_folder_name}/files_converted.csv\",\n)\nprint(\"Done generating COGs\")\n\n\n\n\n Back to top"
  },
  {
    "objectID": "cog_transformation/epa-ch4emission-grid-v2express.html",
    "href": "cog_transformation/epa-ch4emission-grid-v2express.html",
    "title": "U.S. Gridded Anthropogenic Methane Emissions Inventory",
    "section": "",
    "text": "This script was used to transform the Gridded Anthropogenic Methane Emissions Inventory monthly dataset from netCDF to Cloud Optimized GeoTIFF (COG) format for display in the Greenhouse Gas (GHG) Center.\n\nimport os\nimport xarray\nimport re\nimport pandas as pd\nimport json\nimport tempfile\nimport boto3\nfrom datetime import datetime\nimport numpy as np\n\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\nTrue\n\n\n\n# session = boto3.session.Session()\nsession = boto3.Session(\n    aws_access_key_id=os.environ.get(\"AWS_ACCESS_KEY_ID\"),\n    aws_secret_access_key=os.environ.get(\"AWS_SECRET_ACCESS_KEY\"),\n    aws_session_token=os.environ.get(\"AWS_SESSION_TOKEN\"),\n)\ns3_client = session.client(\"s3\")\nbucket_name = \"ghgc-data-store-dev\" # S3 bucket where the COGs are stored after transformation\nFOLDER_NAME = \"../data/epa_emissions_express_extension\"\ns3_folder_name = \"epa_express_extension_Mg_km2_yr\"\n# raw gridded data [molec/cm2/s] * 1/6.022x10^23 [molec/mol] * 16.04x10^-6 [ Mg/mol] * 366 [days/yr] * 1x10^10 [cm2/km2]\n\nfiles_processed = pd.DataFrame(columns=[\"file_name\", \"COGs_created\"])   # A dataframe to keep track of the files that we have transformed into COGs\n\n# Reading the raw netCDF files from local machine\nfor name in os.listdir(FOLDER_NAME):\n    xds = xarray.open_dataset(f\"{FOLDER_NAME}/{name}\", engine=\"netcdf4\")\n    xds = xds.assign_coords(lon=(((xds.lon + 180) % 360) - 180)).sortby(\"lon\")\n    variable = [var for var in xds.data_vars]\n    filename = name.split(\"/ \")[-1]\n    filename_elements = re.split(\"[_ .]\", filename)\n    start_time = datetime(int(filename_elements[-2]), 1, 1)\n\n    for time_increment in range(0, len(xds.time)):\n        for var in variable:\n            filename = name.split(\"/ \")[-1]\n            filename_elements = re.split(\"[_ .]\", filename)\n            data = getattr(xds.isel(time=time_increment), var)\n            data.values[data.values==0] = np.nan\n            data = data*((1/(6.022*pow(10,23)))*(16.04*pow(10,-6))*366*pow(10,10)*86400)\n            data = data.fillna(-9999)\n            data = data.isel(lat=slice(None, None, -1))\n            data.rio.set_spatial_dims(\"lon\", \"lat\", inplace=True)\n            data.rio.write_crs(\"epsg:4326\", inplace=True)\n\n            # # insert date of generated COG into filename\n            filename_elements.pop()\n            filename_elements[-1] = start_time.strftime(\"%Y\")\n            filename_elements.insert(2, var)\n            cog_filename = \"_\".join(filename_elements)\n            # # add extension\n            cog_filename = f\"{cog_filename}.tif\"\n\n            with tempfile.NamedTemporaryFile() as temp_file:\n                data.rio.to_raster(\n                    temp_file.name,\n                    driver=\"COG\",\n                )\n                s3_client.upload_file(\n                    Filename=temp_file.name,\n                    Bucket=bucket_name,\n                    Key=f\"{s3_folder_name}/{cog_filename}\",\n                )\n\n            files_processed = files_processed._append(\n                {\"file_name\": name, \"COGs_created\": cog_filename},\n                ignore_index=True,\n            )\n\n            print(f\"Generated and saved COG: {cog_filename}\")\n\n# Generate the json file with the metadata that is present in the netCDF files.\nwith tempfile.NamedTemporaryFile(mode=\"w+\") as fp:\n    json.dump(xds.attrs, fp)\n    json.dump({\"data_dimensions\": dict(xds.dims)}, fp)\n    json.dump({\"data_variables\": list(xds.data_vars)}, fp)\n    fp.flush()\n\n    s3_client.upload_file(\n        Filename=fp.name,\n        Bucket=bucket_name,\n        Key=f\"{s3_folder_name}/metadata.json\",\n    )\n\n# creating the csv file with the names of files transformed.\nfiles_processed.to_csv(\n    f\"s3://{bucket_name}/{s3_folder_name}/files_converted.csv\",\n)\nprint(\"Done generating COGs\")\n\nGenerated and saved COG: Express_Extension_emi_ch4_1A_Combustion_Mobile_Gridded_GHGI_Methane_v2_2015.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1A_Combustion_Stationary_Gridded_GHGI_Methane_v2_2015.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B1a_Abandoned_Coal_Gridded_GHGI_Methane_v2_2015.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B1a_Surface_Coal_Gridded_GHGI_Methane_v2_2015.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B1a_Underground_Coal_Gridded_GHGI_Methane_v2_2015.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2a_Petroleum_Systems_Exploration_Gridded_GHGI_Methane_v2_2015.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2a_Petroleum_Systems_Production_Gridded_GHGI_Methane_v2_2015.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2a_Petroleum_Systems_Refining_Gridded_GHGI_Methane_v2_2015.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2a_Petroleum_Systems_Transport_Gridded_GHGI_Methane_v2_2015.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2ab_Abandoned_Oil_Gas_Gridded_GHGI_Methane_v2_2015.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_Distribution_Gridded_GHGI_Methane_v2_2015.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_Exploration_Gridded_GHGI_Methane_v2_2015.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_Processing_Gridded_GHGI_Methane_v2_2015.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_Production_Gridded_GHGI_Methane_v2_2015.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_TransmissionStorage_Gridded_GHGI_Methane_v2_2015.tif\nGenerated and saved COG: Express_Extension_emi_ch4_2B8_Industry_Petrochemical_Gridded_GHGI_Methane_v2_2015.tif\nGenerated and saved COG: Express_Extension_emi_ch4_2C2_Industry_Ferroalloy_Gridded_GHGI_Methane_v2_2015.tif\nGenerated and saved COG: Express_Extension_emi_ch4_3A_Enteric_Fermentation_Gridded_GHGI_Methane_v2_2015.tif\nGenerated and saved COG: Express_Extension_emi_ch4_3B_Manure_Management_Gridded_GHGI_Methane_v2_2015.tif\nGenerated and saved COG: Express_Extension_emi_ch4_3C_Rice_Cultivation_Gridded_GHGI_Methane_v2_2015.tif\nGenerated and saved COG: Express_Extension_emi_ch4_3F_Field_Burning_Gridded_GHGI_Methane_v2_2015.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5A1_Landfills_Industrial_Gridded_GHGI_Methane_v2_2015.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5A1_Landfills_MSW_Gridded_GHGI_Methane_v2_2015.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5B1_Composting_Gridded_GHGI_Methane_v2_2015.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5D_Wastewater_Treatment_Domestic_Gridded_GHGI_Methane_v2_2015.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5D_Wastewater_Treatment_Industrial_Gridded_GHGI_Methane_v2_2015.tif\nGenerated and saved COG: Express_Extension_emi_ch4_Supp_1B2b_PostMeter_Gridded_GHGI_Methane_v2_2015.tif\nGenerated and saved COG: Express_Extension_grid_cell_area_Gridded_GHGI_Methane_v2_2015.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1A_Combustion_Mobile_Gridded_GHGI_Methane_v2_2020.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1A_Combustion_Stationary_Gridded_GHGI_Methane_v2_2020.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B1a_Abandoned_Coal_Gridded_GHGI_Methane_v2_2020.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B1a_Surface_Coal_Gridded_GHGI_Methane_v2_2020.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B1a_Underground_Coal_Gridded_GHGI_Methane_v2_2020.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2a_Petroleum_Systems_Exploration_Gridded_GHGI_Methane_v2_2020.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2a_Petroleum_Systems_Production_Gridded_GHGI_Methane_v2_2020.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2a_Petroleum_Systems_Refining_Gridded_GHGI_Methane_v2_2020.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2a_Petroleum_Systems_Transport_Gridded_GHGI_Methane_v2_2020.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2ab_Abandoned_Oil_Gas_Gridded_GHGI_Methane_v2_2020.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_Distribution_Gridded_GHGI_Methane_v2_2020.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_Exploration_Gridded_GHGI_Methane_v2_2020.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_Processing_Gridded_GHGI_Methane_v2_2020.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_Production_Gridded_GHGI_Methane_v2_2020.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_TransmissionStorage_Gridded_GHGI_Methane_v2_2020.tif\nGenerated and saved COG: Express_Extension_emi_ch4_2B8_Industry_Petrochemical_Gridded_GHGI_Methane_v2_2020.tif\nGenerated and saved COG: Express_Extension_emi_ch4_2C2_Industry_Ferroalloy_Gridded_GHGI_Methane_v2_2020.tif\nGenerated and saved COG: Express_Extension_emi_ch4_3A_Enteric_Fermentation_Gridded_GHGI_Methane_v2_2020.tif\nGenerated and saved COG: Express_Extension_emi_ch4_3B_Manure_Management_Gridded_GHGI_Methane_v2_2020.tif\nGenerated and saved COG: Express_Extension_emi_ch4_3C_Rice_Cultivation_Gridded_GHGI_Methane_v2_2020.tif\nGenerated and saved COG: Express_Extension_emi_ch4_3F_Field_Burning_Gridded_GHGI_Methane_v2_2020.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5A1_Landfills_Industrial_Gridded_GHGI_Methane_v2_2020.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5A1_Landfills_MSW_Gridded_GHGI_Methane_v2_2020.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5B1_Composting_Gridded_GHGI_Methane_v2_2020.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5D_Wastewater_Treatment_Domestic_Gridded_GHGI_Methane_v2_2020.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5D_Wastewater_Treatment_Industrial_Gridded_GHGI_Methane_v2_2020.tif\nGenerated and saved COG: Express_Extension_emi_ch4_Supp_1B2b_PostMeter_Gridded_GHGI_Methane_v2_2020.tif\nGenerated and saved COG: Express_Extension_grid_cell_area_Gridded_GHGI_Methane_v2_2020.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1A_Combustion_Mobile_Gridded_GHGI_Methane_v2_2014.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1A_Combustion_Stationary_Gridded_GHGI_Methane_v2_2014.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B1a_Abandoned_Coal_Gridded_GHGI_Methane_v2_2014.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B1a_Surface_Coal_Gridded_GHGI_Methane_v2_2014.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B1a_Underground_Coal_Gridded_GHGI_Methane_v2_2014.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2a_Petroleum_Systems_Exploration_Gridded_GHGI_Methane_v2_2014.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2a_Petroleum_Systems_Production_Gridded_GHGI_Methane_v2_2014.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2a_Petroleum_Systems_Refining_Gridded_GHGI_Methane_v2_2014.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2a_Petroleum_Systems_Transport_Gridded_GHGI_Methane_v2_2014.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2ab_Abandoned_Oil_Gas_Gridded_GHGI_Methane_v2_2014.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_Distribution_Gridded_GHGI_Methane_v2_2014.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_Exploration_Gridded_GHGI_Methane_v2_2014.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_Processing_Gridded_GHGI_Methane_v2_2014.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_Production_Gridded_GHGI_Methane_v2_2014.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_TransmissionStorage_Gridded_GHGI_Methane_v2_2014.tif\nGenerated and saved COG: Express_Extension_emi_ch4_2B8_Industry_Petrochemical_Gridded_GHGI_Methane_v2_2014.tif\nGenerated and saved COG: Express_Extension_emi_ch4_2C2_Industry_Ferroalloy_Gridded_GHGI_Methane_v2_2014.tif\nGenerated and saved COG: Express_Extension_emi_ch4_3A_Enteric_Fermentation_Gridded_GHGI_Methane_v2_2014.tif\nGenerated and saved COG: Express_Extension_emi_ch4_3B_Manure_Management_Gridded_GHGI_Methane_v2_2014.tif\nGenerated and saved COG: Express_Extension_emi_ch4_3C_Rice_Cultivation_Gridded_GHGI_Methane_v2_2014.tif\nGenerated and saved COG: Express_Extension_emi_ch4_3F_Field_Burning_Gridded_GHGI_Methane_v2_2014.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5A1_Landfills_Industrial_Gridded_GHGI_Methane_v2_2014.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5A1_Landfills_MSW_Gridded_GHGI_Methane_v2_2014.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5B1_Composting_Gridded_GHGI_Methane_v2_2014.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5D_Wastewater_Treatment_Domestic_Gridded_GHGI_Methane_v2_2014.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5D_Wastewater_Treatment_Industrial_Gridded_GHGI_Methane_v2_2014.tif\nGenerated and saved COG: Express_Extension_emi_ch4_Supp_1B2b_PostMeter_Gridded_GHGI_Methane_v2_2014.tif\nGenerated and saved COG: Express_Extension_grid_cell_area_Gridded_GHGI_Methane_v2_2014.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1A_Combustion_Mobile_Gridded_GHGI_Methane_v2_2013.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1A_Combustion_Stationary_Gridded_GHGI_Methane_v2_2013.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B1a_Abandoned_Coal_Gridded_GHGI_Methane_v2_2013.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B1a_Surface_Coal_Gridded_GHGI_Methane_v2_2013.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B1a_Underground_Coal_Gridded_GHGI_Methane_v2_2013.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2a_Petroleum_Systems_Exploration_Gridded_GHGI_Methane_v2_2013.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2a_Petroleum_Systems_Production_Gridded_GHGI_Methane_v2_2013.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2a_Petroleum_Systems_Refining_Gridded_GHGI_Methane_v2_2013.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2a_Petroleum_Systems_Transport_Gridded_GHGI_Methane_v2_2013.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2ab_Abandoned_Oil_Gas_Gridded_GHGI_Methane_v2_2013.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_Distribution_Gridded_GHGI_Methane_v2_2013.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_Exploration_Gridded_GHGI_Methane_v2_2013.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_Processing_Gridded_GHGI_Methane_v2_2013.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_Production_Gridded_GHGI_Methane_v2_2013.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_TransmissionStorage_Gridded_GHGI_Methane_v2_2013.tif\nGenerated and saved COG: Express_Extension_emi_ch4_2B8_Industry_Petrochemical_Gridded_GHGI_Methane_v2_2013.tif\nGenerated and saved COG: Express_Extension_emi_ch4_2C2_Industry_Ferroalloy_Gridded_GHGI_Methane_v2_2013.tif\nGenerated and saved COG: Express_Extension_emi_ch4_3A_Enteric_Fermentation_Gridded_GHGI_Methane_v2_2013.tif\nGenerated and saved COG: Express_Extension_emi_ch4_3B_Manure_Management_Gridded_GHGI_Methane_v2_2013.tif\nGenerated and saved COG: Express_Extension_emi_ch4_3C_Rice_Cultivation_Gridded_GHGI_Methane_v2_2013.tif\nGenerated and saved COG: Express_Extension_emi_ch4_3F_Field_Burning_Gridded_GHGI_Methane_v2_2013.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5A1_Landfills_Industrial_Gridded_GHGI_Methane_v2_2013.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5A1_Landfills_MSW_Gridded_GHGI_Methane_v2_2013.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5B1_Composting_Gridded_GHGI_Methane_v2_2013.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5D_Wastewater_Treatment_Domestic_Gridded_GHGI_Methane_v2_2013.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5D_Wastewater_Treatment_Industrial_Gridded_GHGI_Methane_v2_2013.tif\nGenerated and saved COG: Express_Extension_emi_ch4_Supp_1B2b_PostMeter_Gridded_GHGI_Methane_v2_2013.tif\nGenerated and saved COG: Express_Extension_grid_cell_area_Gridded_GHGI_Methane_v2_2013.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1A_Combustion_Mobile_Gridded_GHGI_Methane_v2_2017.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1A_Combustion_Stationary_Gridded_GHGI_Methane_v2_2017.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B1a_Abandoned_Coal_Gridded_GHGI_Methane_v2_2017.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B1a_Surface_Coal_Gridded_GHGI_Methane_v2_2017.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B1a_Underground_Coal_Gridded_GHGI_Methane_v2_2017.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2a_Petroleum_Systems_Exploration_Gridded_GHGI_Methane_v2_2017.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2a_Petroleum_Systems_Production_Gridded_GHGI_Methane_v2_2017.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2a_Petroleum_Systems_Refining_Gridded_GHGI_Methane_v2_2017.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2a_Petroleum_Systems_Transport_Gridded_GHGI_Methane_v2_2017.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2ab_Abandoned_Oil_Gas_Gridded_GHGI_Methane_v2_2017.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_Distribution_Gridded_GHGI_Methane_v2_2017.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_Exploration_Gridded_GHGI_Methane_v2_2017.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_Processing_Gridded_GHGI_Methane_v2_2017.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_Production_Gridded_GHGI_Methane_v2_2017.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_TransmissionStorage_Gridded_GHGI_Methane_v2_2017.tif\nGenerated and saved COG: Express_Extension_emi_ch4_2B8_Industry_Petrochemical_Gridded_GHGI_Methane_v2_2017.tif\nGenerated and saved COG: Express_Extension_emi_ch4_2C2_Industry_Ferroalloy_Gridded_GHGI_Methane_v2_2017.tif\nGenerated and saved COG: Express_Extension_emi_ch4_3A_Enteric_Fermentation_Gridded_GHGI_Methane_v2_2017.tif\nGenerated and saved COG: Express_Extension_emi_ch4_3B_Manure_Management_Gridded_GHGI_Methane_v2_2017.tif\nGenerated and saved COG: Express_Extension_emi_ch4_3C_Rice_Cultivation_Gridded_GHGI_Methane_v2_2017.tif\nGenerated and saved COG: Express_Extension_emi_ch4_3F_Field_Burning_Gridded_GHGI_Methane_v2_2017.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5A1_Landfills_Industrial_Gridded_GHGI_Methane_v2_2017.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5A1_Landfills_MSW_Gridded_GHGI_Methane_v2_2017.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5B1_Composting_Gridded_GHGI_Methane_v2_2017.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5D_Wastewater_Treatment_Domestic_Gridded_GHGI_Methane_v2_2017.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5D_Wastewater_Treatment_Industrial_Gridded_GHGI_Methane_v2_2017.tif\nGenerated and saved COG: Express_Extension_emi_ch4_Supp_1B2b_PostMeter_Gridded_GHGI_Methane_v2_2017.tif\nGenerated and saved COG: Express_Extension_grid_cell_area_Gridded_GHGI_Methane_v2_2017.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1A_Combustion_Mobile_Gridded_GHGI_Methane_v2_2016.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1A_Combustion_Stationary_Gridded_GHGI_Methane_v2_2016.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B1a_Abandoned_Coal_Gridded_GHGI_Methane_v2_2016.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B1a_Surface_Coal_Gridded_GHGI_Methane_v2_2016.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B1a_Underground_Coal_Gridded_GHGI_Methane_v2_2016.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2a_Petroleum_Systems_Exploration_Gridded_GHGI_Methane_v2_2016.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2a_Petroleum_Systems_Production_Gridded_GHGI_Methane_v2_2016.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2a_Petroleum_Systems_Refining_Gridded_GHGI_Methane_v2_2016.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2a_Petroleum_Systems_Transport_Gridded_GHGI_Methane_v2_2016.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2ab_Abandoned_Oil_Gas_Gridded_GHGI_Methane_v2_2016.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_Distribution_Gridded_GHGI_Methane_v2_2016.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_Exploration_Gridded_GHGI_Methane_v2_2016.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_Processing_Gridded_GHGI_Methane_v2_2016.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_Production_Gridded_GHGI_Methane_v2_2016.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_TransmissionStorage_Gridded_GHGI_Methane_v2_2016.tif\nGenerated and saved COG: Express_Extension_emi_ch4_2B8_Industry_Petrochemical_Gridded_GHGI_Methane_v2_2016.tif\nGenerated and saved COG: Express_Extension_emi_ch4_2C2_Industry_Ferroalloy_Gridded_GHGI_Methane_v2_2016.tif\nGenerated and saved COG: Express_Extension_emi_ch4_3A_Enteric_Fermentation_Gridded_GHGI_Methane_v2_2016.tif\nGenerated and saved COG: Express_Extension_emi_ch4_3B_Manure_Management_Gridded_GHGI_Methane_v2_2016.tif\nGenerated and saved COG: Express_Extension_emi_ch4_3C_Rice_Cultivation_Gridded_GHGI_Methane_v2_2016.tif\nGenerated and saved COG: Express_Extension_emi_ch4_3F_Field_Burning_Gridded_GHGI_Methane_v2_2016.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5A1_Landfills_Industrial_Gridded_GHGI_Methane_v2_2016.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5A1_Landfills_MSW_Gridded_GHGI_Methane_v2_2016.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5B1_Composting_Gridded_GHGI_Methane_v2_2016.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5D_Wastewater_Treatment_Domestic_Gridded_GHGI_Methane_v2_2016.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5D_Wastewater_Treatment_Industrial_Gridded_GHGI_Methane_v2_2016.tif\nGenerated and saved COG: Express_Extension_emi_ch4_Supp_1B2b_PostMeter_Gridded_GHGI_Methane_v2_2016.tif\nGenerated and saved COG: Express_Extension_grid_cell_area_Gridded_GHGI_Methane_v2_2016.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1A_Combustion_Mobile_Gridded_GHGI_Methane_v2_2012.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1A_Combustion_Stationary_Gridded_GHGI_Methane_v2_2012.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B1a_Abandoned_Coal_Gridded_GHGI_Methane_v2_2012.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B1a_Surface_Coal_Gridded_GHGI_Methane_v2_2012.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B1a_Underground_Coal_Gridded_GHGI_Methane_v2_2012.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2a_Petroleum_Systems_Exploration_Gridded_GHGI_Methane_v2_2012.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2a_Petroleum_Systems_Production_Gridded_GHGI_Methane_v2_2012.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2a_Petroleum_Systems_Refining_Gridded_GHGI_Methane_v2_2012.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2a_Petroleum_Systems_Transport_Gridded_GHGI_Methane_v2_2012.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2ab_Abandoned_Oil_Gas_Gridded_GHGI_Methane_v2_2012.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_Distribution_Gridded_GHGI_Methane_v2_2012.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_Exploration_Gridded_GHGI_Methane_v2_2012.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_Processing_Gridded_GHGI_Methane_v2_2012.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_Production_Gridded_GHGI_Methane_v2_2012.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_TransmissionStorage_Gridded_GHGI_Methane_v2_2012.tif\nGenerated and saved COG: Express_Extension_emi_ch4_2B8_Industry_Petrochemical_Gridded_GHGI_Methane_v2_2012.tif\nGenerated and saved COG: Express_Extension_emi_ch4_2C2_Industry_Ferroalloy_Gridded_GHGI_Methane_v2_2012.tif\nGenerated and saved COG: Express_Extension_emi_ch4_3A_Enteric_Fermentation_Gridded_GHGI_Methane_v2_2012.tif\nGenerated and saved COG: Express_Extension_emi_ch4_3B_Manure_Management_Gridded_GHGI_Methane_v2_2012.tif\nGenerated and saved COG: Express_Extension_emi_ch4_3C_Rice_Cultivation_Gridded_GHGI_Methane_v2_2012.tif\nGenerated and saved COG: Express_Extension_emi_ch4_3F_Field_Burning_Gridded_GHGI_Methane_v2_2012.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5A1_Landfills_Industrial_Gridded_GHGI_Methane_v2_2012.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5A1_Landfills_MSW_Gridded_GHGI_Methane_v2_2012.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5B1_Composting_Gridded_GHGI_Methane_v2_2012.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5D_Wastewater_Treatment_Domestic_Gridded_GHGI_Methane_v2_2012.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5D_Wastewater_Treatment_Industrial_Gridded_GHGI_Methane_v2_2012.tif\nGenerated and saved COG: Express_Extension_emi_ch4_Supp_1B2b_PostMeter_Gridded_GHGI_Methane_v2_2012.tif\nGenerated and saved COG: Express_Extension_grid_cell_area_Gridded_GHGI_Methane_v2_2012.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1A_Combustion_Mobile_Gridded_GHGI_Methane_v2_2019.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1A_Combustion_Stationary_Gridded_GHGI_Methane_v2_2019.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B1a_Abandoned_Coal_Gridded_GHGI_Methane_v2_2019.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B1a_Surface_Coal_Gridded_GHGI_Methane_v2_2019.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B1a_Underground_Coal_Gridded_GHGI_Methane_v2_2019.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2a_Petroleum_Systems_Exploration_Gridded_GHGI_Methane_v2_2019.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2a_Petroleum_Systems_Production_Gridded_GHGI_Methane_v2_2019.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2a_Petroleum_Systems_Refining_Gridded_GHGI_Methane_v2_2019.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2a_Petroleum_Systems_Transport_Gridded_GHGI_Methane_v2_2019.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2ab_Abandoned_Oil_Gas_Gridded_GHGI_Methane_v2_2019.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_Distribution_Gridded_GHGI_Methane_v2_2019.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_Exploration_Gridded_GHGI_Methane_v2_2019.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_Processing_Gridded_GHGI_Methane_v2_2019.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_Production_Gridded_GHGI_Methane_v2_2019.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_TransmissionStorage_Gridded_GHGI_Methane_v2_2019.tif\nGenerated and saved COG: Express_Extension_emi_ch4_2B8_Industry_Petrochemical_Gridded_GHGI_Methane_v2_2019.tif\nGenerated and saved COG: Express_Extension_emi_ch4_2C2_Industry_Ferroalloy_Gridded_GHGI_Methane_v2_2019.tif\nGenerated and saved COG: Express_Extension_emi_ch4_3A_Enteric_Fermentation_Gridded_GHGI_Methane_v2_2019.tif\nGenerated and saved COG: Express_Extension_emi_ch4_3B_Manure_Management_Gridded_GHGI_Methane_v2_2019.tif\nGenerated and saved COG: Express_Extension_emi_ch4_3C_Rice_Cultivation_Gridded_GHGI_Methane_v2_2019.tif\nGenerated and saved COG: Express_Extension_emi_ch4_3F_Field_Burning_Gridded_GHGI_Methane_v2_2019.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5A1_Landfills_Industrial_Gridded_GHGI_Methane_v2_2019.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5A1_Landfills_MSW_Gridded_GHGI_Methane_v2_2019.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5B1_Composting_Gridded_GHGI_Methane_v2_2019.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5D_Wastewater_Treatment_Domestic_Gridded_GHGI_Methane_v2_2019.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5D_Wastewater_Treatment_Industrial_Gridded_GHGI_Methane_v2_2019.tif\nGenerated and saved COG: Express_Extension_emi_ch4_Supp_1B2b_PostMeter_Gridded_GHGI_Methane_v2_2019.tif\nGenerated and saved COG: Express_Extension_grid_cell_area_Gridded_GHGI_Methane_v2_2019.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1A_Combustion_Mobile_Gridded_GHGI_Methane_v2_2018.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1A_Combustion_Stationary_Gridded_GHGI_Methane_v2_2018.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B1a_Abandoned_Coal_Gridded_GHGI_Methane_v2_2018.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B1a_Surface_Coal_Gridded_GHGI_Methane_v2_2018.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B1a_Underground_Coal_Gridded_GHGI_Methane_v2_2018.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2a_Petroleum_Systems_Exploration_Gridded_GHGI_Methane_v2_2018.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2a_Petroleum_Systems_Production_Gridded_GHGI_Methane_v2_2018.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2a_Petroleum_Systems_Refining_Gridded_GHGI_Methane_v2_2018.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2a_Petroleum_Systems_Transport_Gridded_GHGI_Methane_v2_2018.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2ab_Abandoned_Oil_Gas_Gridded_GHGI_Methane_v2_2018.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_Distribution_Gridded_GHGI_Methane_v2_2018.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_Exploration_Gridded_GHGI_Methane_v2_2018.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_Processing_Gridded_GHGI_Methane_v2_2018.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_Production_Gridded_GHGI_Methane_v2_2018.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_TransmissionStorage_Gridded_GHGI_Methane_v2_2018.tif\nGenerated and saved COG: Express_Extension_emi_ch4_2B8_Industry_Petrochemical_Gridded_GHGI_Methane_v2_2018.tif\nGenerated and saved COG: Express_Extension_emi_ch4_2C2_Industry_Ferroalloy_Gridded_GHGI_Methane_v2_2018.tif\nGenerated and saved COG: Express_Extension_emi_ch4_3A_Enteric_Fermentation_Gridded_GHGI_Methane_v2_2018.tif\nGenerated and saved COG: Express_Extension_emi_ch4_3B_Manure_Management_Gridded_GHGI_Methane_v2_2018.tif\nGenerated and saved COG: Express_Extension_emi_ch4_3C_Rice_Cultivation_Gridded_GHGI_Methane_v2_2018.tif\nGenerated and saved COG: Express_Extension_emi_ch4_3F_Field_Burning_Gridded_GHGI_Methane_v2_2018.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5A1_Landfills_Industrial_Gridded_GHGI_Methane_v2_2018.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5A1_Landfills_MSW_Gridded_GHGI_Methane_v2_2018.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5B1_Composting_Gridded_GHGI_Methane_v2_2018.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5D_Wastewater_Treatment_Domestic_Gridded_GHGI_Methane_v2_2018.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5D_Wastewater_Treatment_Industrial_Gridded_GHGI_Methane_v2_2018.tif\nGenerated and saved COG: Express_Extension_emi_ch4_Supp_1B2b_PostMeter_Gridded_GHGI_Methane_v2_2018.tif\nGenerated and saved COG: Express_Extension_grid_cell_area_Gridded_GHGI_Methane_v2_2018.tif\nDone generating COGs\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Data Transformation Notebooks",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "U.S. Gridded Anthropogenic Methane Emissions Inventory"
    ]
  },
  {
    "objectID": "cog_transformation/nec-testbed-ghg-concentrations.html",
    "href": "cog_transformation/nec-testbed-ghg-concentrations.html",
    "title": "Carbon Dioxide and Methane Concentrations from the Northeast Corridor (NEC) Urban Test Bed",
    "section": "",
    "text": "This script was used to transform the NIST INFLUX dataset into meaningful csv files for ingestion to vector dataset.\n\nimport pandas as pd\nimport glob\nimport os\nimport warnings\nimport subprocess\nimport tarfile\nimport warnings \nimport requests\nwarnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n\n\nconfig = pd.read_csv(\"NEC_sites.csv\")  #https://data.nist.gov/od/id/mds2-3012\n\n\n# Code to download the files into csv folder \nsites = list(config.SiteCode)\nfor SiteCode in config.SiteCode[:2]:\n    print(SiteCode)\n    download_link = f\"https://data.nist.gov/od/ds/ark:/88434/mds2-3012/{SiteCode}.tgz\"\n    \n    # Check if the file exists on the server\n    response = requests.head(download_link)\n    if response.status_code != 404:\n        # File exists, proceed with download\n        result = subprocess.run([\"wget\", download_link, \"-O\", f\"{SiteCode}.tgz\"], \n                                stdout=subprocess.DEVNULL,\n                                stderr=subprocess.DEVNULL)\n\n        # Check if wget succeeded\n        if result.returncode == 0:\n            # Ensure the file is not empty\n            if os.path.getsize(f\"{SiteCode}.tgz\") &gt; 0:\n                # Extract the files\n                with tarfile.open(f\"{SiteCode}.tgz\", \"r:gz\") as tar:\n                    tar.extractall()\n\n                # Delete the .tgz file\n                os.remove(f\"{SiteCode}.tgz\")\n            else:\n                print(f\"File {SiteCode}.tgz is empty.\")\n                sites.remove(SiteCode)\n                os.remove(f\"{SiteCode}.tgz\")  # Remove the empty file\n        else:\n            print(f\"Failed to download {SiteCode}.tgz.\")\n            sites.remove(SiteCode)\n    else:\n        print(f\"File {SiteCode}.tgz does not exist on the server.\")\n        sites.remove(SiteCode)\n\n\nsites = list(config.SiteCode)\n# These are not available\nsites.remove('AWS')\nsites.remove('BVA')\nsites.remove('DNC')\n\n\nvariables = ['ch4','co2']\noutput_dir =\"output_NEC\"\nos.makedirs(output_dir,exist_ok=True)\n\n\nfor site in sites:\n    for variable in variables:\n        df = pd.DataFrame()\n        files = glob.glob(f\"csv/{site}-*-{variable}-*.csv\")\n        val = f\"{variable}_ppm\" if variable == 'co2' else f\"{variable}_ppb\"\n        for file in files:\n            tmp = pd.read_csv(file)\n            tmp.dropna(subset=[val], inplace=True)\n            tmp.rename(columns={'datetime_UTC': 'datetime'}, inplace=True)\n            columns = [\"latitude\",\"longitude\",\"intake_height_m\",\"elevation_m\",\"datetime\",val ]\n            tmp= tmp[columns]\n            tmp.rename(columns={val: 'value'}, inplace=True)\n            tmp['datetime'] = pd.to_datetime(tmp['datetime'])\n            tmp['datetime'] = tmp['datetime'].dt.strftime('%Y-%m-%dT%H:%M:%SZ')\n            tmp['location'] = config[config['SiteCode']==site][\"Location\"].item()\n            df = pd.concat([df, tmp], ignore_index=True)\n            \n        df['year']= df['datetime'].apply(lambda x: x[:4])\n        result = df.groupby(\"year\").agg(max_height= (\"intake_height_m\",\"max\"))\n        if result['max_height'].std() !=0:\n            print(f\"More than one max height for {file}\",result['max_height'].unique())\n        merged_df=pd.merge(df, result, on='year')\n        merged_df[\"is_max_height_data\"]= merged_df[\"max_height\"] == merged_df[\"intake_height_m\"]\n        merged_df=merged_df.drop(columns=['year','max_height'])\n        merged_df.reset_index(drop=True, inplace=True)\n        merged_df.to_csv(f\"{output_dir}/NIST-testbed-NEC-{site}-{variable}-hourly-concentrations.csv\", index=False)\n\n\n\n\n Back to top",
    "crumbs": [
      "Data Transformation Notebooks",
      "Greenhouse Gas Concentrations",
      "Carbon Dioxide and Methane Concentrations from the Northeast Corridor (NEC) Urban Test Bed"
    ]
  },
  {
    "objectID": "cog_transformation/odiac-ffco2-monthgrid-v2022.html",
    "href": "cog_transformation/odiac-ffco2-monthgrid-v2022.html",
    "title": "ODIAC Fossil Fuel CO₂ Emissions",
    "section": "",
    "text": "This script was used to transform the ODIAC Fossil Fuel CO₂ Emissions dataset from GeoTIFF to Cloud Optimized GeoTIFF (COG) format for display in the Greenhouse Gas (GHG) Center.\n\nimport os\nimport xarray\nimport re\nimport pandas as pd\n\nimport tempfile\nimport boto3\n\n\nsession = boto3.session.Session()\ns3_client = session.client(\"s3\")\nbucket_name = \"ghgc-data-store-dev\" # S3 bucket where the COGs are stored after transformation\n\nfold_names = os.listdir(\"ODIAC\")\n\nfiles_processed = pd.DataFrame(columns=[\"file_name\", \"COGs_created\"])   # A dataframe to keep track of the files that we have transformed into COGs\n\n# Reading the raw netCDF files from local machine\nfor fol_ in fold_names:\n    for name in os.listdir(f\"ODIAC/{fol_}\"):\n        xds = xarray.open_dataarray(f\"ODIAC/{fol_}/{name}\")\n\n        filename = name.split(\"/ \")[-1]\n        filename_elements = re.split(\"[_ .]\", filename)\n        # # insert date of generated COG into filename\n        filename_elements.pop()\n        filename_elements[-1] = fol_ + filename_elements[-1][-2:]\n\n        xds.rio.set_spatial_dims(\"x\", \"y\", inplace=True)\n        xds.rio.write_nodata(-9999, inplace=True)\n        xds.rio.write_crs(\"epsg:4326\", inplace=True)\n\n        cog_filename = \"_\".join(filename_elements)\n        # # add extension\n        cog_filename = f\"{cog_filename}.tif\"\n\n        with tempfile.NamedTemporaryFile() as temp_file:\n            xds.rio.to_raster(\n                temp_file.name,\n                driver=\"COG\",\n            )\n            s3_client.upload_file(\n                Filename=temp_file.name,\n                Bucket=bucket_name,\n                Key=f\"ODIAC_geotiffs_COGs/{cog_filename}\",\n            )\n\n        files_processed = files_processed._append(\n            {\"file_name\": name, \"COGs_created\": cog_filename},\n            ignore_index=True,\n        )\n\n        print(f\"Generated and saved COG: {cog_filename}\")\n\n\n# creating the csv file with the names of files transformed.\nfiles_processed.to_csv(\n    f\"s3://{bucket_name}/ODIAC_COGs/files_converted.csv\",\n)\nprint(\"Done generating COGs\")\n\n\n\n\n Back to top"
  },
  {
    "objectID": "cog_transformation/influx-testbed-ghg-concentrations.html",
    "href": "cog_transformation/influx-testbed-ghg-concentrations.html",
    "title": "Carbon Dioxide and Methane Concentrations from the Indianapolis Flux Experiment (INFLUX)",
    "section": "",
    "text": "This script was used to transform the NIST INFLUX dataset into meaningful csv files for ingestion to vector dataset.\n\nimport pandas as pd\nimport glob\nimport os\nimport zipfile\nimport wget\nfrom collections import defaultdict\nfrom io import StringIO\nimport re\nimport warnings\nimport warnings\nfrom datetime import datetime, timedelta\n# Ignore the FutureWarning\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\n\n\n\nselected_level=\"level1\"\nbase_dir = \"data/\"\noutput_dir = \"output/\"\ndat_file_pattern = f\"{base_dir}/*/*.dat\"\noutput_base_dataset_name = \"PSU_INFLUX_INSITU\" \nconstant_variables = [\"datetime\",\"latitude\",\"longitude\",\"level\",\"elevation_m\",\"intake_height_m\",\"Instr\"]\nvariables =[['CO2(ppm)'],['CH4(ppb)']] # exclude CO\nmetadata_link= \"UrbanTestBed-Metadata - INFLUX.csv\"\n\n\n# Functions\ndef filter_dict(site_dict, selected_level):\n    return {key: [x for x in value if selected_level in x] for key, value in site_dict.items()}\n\ndef flag_desired_level(df, desired_level):\n    df['is_max_height_data'] = df['level']== desired_level\n    return df\n\ndef add_location(link, site_number):\n    meta= pd.read_csv(link)\n    location =meta[meta['Station Code']==f\"Site {site_number[-2:]}\"][['City','State']]#(get the actual site number)\n    return location['City'].item()+\",\"+location['State'].item()\n\ndef convert_to_datetime(row):\n    year = int(row['Year'])\n    doy = int(row['DOY'])\n    hour = int(row['Hour'])\n    \n    # Create a datetime object for the start of the year\n    date = datetime(year, 1, 1) + timedelta(days=doy - 1)\n    # Add the hours\n    datetime_obj = date + timedelta(hours=hour)\n    # Format as yyyy-mm-ddThh:mm:ssZ\n    return datetime_obj.strftime('%Y-%m-%dT%H:%M:%SZ')\n\ndef download_and_extract_zip_files(base_dir, levels):\n    \"\"\"\n    Download, extract, and delete zip files for the specified levels.\n\n    Parameters:\n    base_dir (str): The base directory for storing the downloaded files.\n    levels (list): A list of levels to download and extract.\n    \"\"\"\n    # Ensure the base directory exists\n    os.makedirs(base_dir, exist_ok=True)\n\n    # Loop through the levels and handle the download and extraction\n    for level in levels:\n        download_link = f\"https://www.datacommons.psu.edu/download/meteorology/influx/influx-tower-data/wmo-x2019-scale/level{level}.zip\"\n        fname = download_link.split(\"/\")[-1]\n        target_path = os.path.join(base_dir, fname)\n        \n        # Download the zip file\n        wget.download(download_link, target_path)\n        print(f\"Downloaded {download_link} to {target_path}\")\n\n        # Extract the zip file\n        with zipfile.ZipFile(target_path, 'r') as zip_ref:\n            zip_ref.extractall(base_dir)\n            print(f\"Extracted {fname}\")\n\n        # Delete the zip file after extraction\n        os.remove(target_path)\n\ndef create_site_dict(pattern):\n    \"\"\"\n    Creates a dictionary where keys are site numbers extracted from file paths,\n    and values are lists of file paths corresponding to each site number.\n    \n    Args:\n    - pattern (str): Glob pattern to match files.\n    \n    Returns:\n    - dict: Dictionary mapping site numbers to lists of file paths.\n    \"\"\"\n    all_files = glob.glob(pattern)\n    site_dict = defaultdict(list)\n    \n    for file_path in all_files:\n        site_number = file_path.split('_')[-4]\n        site_dict[site_number].append(file_path)\n    \n    return dict(site_dict)\n\ndef process_site_files(site_number, file_list):\n    \"\"\"\n    Process files for a given site number and save the combined DataFrame to CSV.\n    \n    Args:\n    - site_number (str): Site number to process.\n    - file_list (list): List of file paths corresponding to the site number.\n    \"\"\"\n    df = pd.DataFrame()\n    \n    for file_path in file_list:\n        with open(file_path, 'r') as file:\n            data = file.read()\n            \n        contents = data.split(\"\\nSite\")\n        lat = float((re.search(r'LATITUDE:\\s*([0-9.]+)\\s*[NS]', contents[0])).group(1))\n        lat_hemisphere = (re.search(r'LATITUDE:\\s*([0-9.]+)\\s*[NS]', contents[0])).group(0)[-1]\n        \n        lon = float((re.search(r'LONGITUDE:\\s*([0-9.]+)\\s*[EW]', contents[0])).group(1))\n        lon_hemisphere = (re.search(r'LONGITUDE:\\s*([0-9.]+)\\s*[EW]', contents[0])).group(0)[-1]\n        \n        level= file_path.split(\"/\")[-2]\n        \n        elevation= re.search(r'ALTITUDE:\\s*([0-9.]+)\\s*m\\s*ASL', contents[0]).group(1)\n        intake_height= re.search(r'SAMPLING HEIGHT:\\s*([0-9.]+)\\s*m\\s*AGL', contents[0]).group(1)\n\n        \n        data_io = StringIO(contents[1])\n        tmp_data = pd.read_csv(data_io, delim_whitespace=True)\n        tmp_data = tmp_data.reset_index().rename(columns={'index': 'Site'})\n        tmp= tmp_data.query(\"Flag==1\").copy()# 1 means no known problem, 0 is not recommemded, 9 is instrument issue (unrealistic)\n        #tmp['SiteCode'] = int(re.search(r'\\d+', site_number).group()) \n        tmp['latitude'] = lat\n        tmp['longitude'] = lon\n        tmp['level'] = int(re.search(r'\\d+', level).group())\n        tmp['elevation_m'] = elevation\n        tmp['intake_height_m']= intake_height\n\n        if lat_hemisphere == 'S':\n            tmp['latitude'] = -1* tmp[\"latitude\"]\n        if lon_hemisphere == 'W':\n            tmp['longitude'] = -1* tmp[\"longitude\"]\n\n        df = pd.concat([df, tmp], ignore_index=True)\n\n    # Ensure the output directory exists\n    os.makedirs(output_dir, exist_ok=True)\n    os.makedirs(output_dir+\"PSU_INFLUX_INSITU/\", exist_ok=True)\n    \n\n    df['datetime'] = df[[\"Year\",\"DOY\",\"Hour\"]].apply(convert_to_datetime, axis=1)\n    df.reset_index(drop=True, inplace=True)\n    for v in variables:\n        tmp_file=df[constant_variables + v].copy()\n        tmp_file['unit'] = v[0][-4:-1] #CO2(ppm) get  the unit only\n        \n        tmp_file.rename(columns={v[0]: 'value'}, inplace=True)\n        tmp_file['location']= add_location(metadata_link, site_number)\n        tmp_file = flag_desired_level(tmp_file, 1) # Flagging only level 1 data\n\n        # Remove nan\n        tmp_file.dropna(subset=[\"value\"], inplace=True)\n\n        #filter 0 values\n        tmp_file[tmp_file[\"value\"]!=0].to_csv(f\"{output_dir}/PSU_INFLUX_INSITU/NIST-FLUX-IN-{site_number}-{v[0][:-5]}-hourly-concentrations.csv\", index=False)\n        print(f\"CSV Created for Site {site_number}-{v[0][:-5]}!!!\")\n    return \n\n\n\n\n\n\n# Download and extract zip files\nlevels_to_download = range(1, 5)\n#download_and_extract_zip_files(base_dir=base_dir, levels=levels_to_download)\n\n# Create site dictionary\nsite_dict = create_site_dict(dat_file_pattern)\n\n# Comment if you want data from all levels\n#site_dict = filter_dict(site_dict, selected_level)\n\n# Process each site's files\nfor site_number, file_list in site_dict.items():\n    print(f\"Processing Site Number: {site_number}, Total Files: {len(file_list)}\")\n    process_site_files(site_number, file_list)\n\n\n\n\n Back to top",
    "crumbs": [
      "Data Transformation Notebooks",
      "Greenhouse Gas Concentrations",
      "Carbon Dioxide and Methane Concentrations from the Indianapolis Flux Experiment (INFLUX)"
    ]
  },
  {
    "objectID": "cog_transformation/odiac-ffco2-monthgrid-v2023.html",
    "href": "cog_transformation/odiac-ffco2-monthgrid-v2023.html",
    "title": "ODIAC Fossil Fuel CO₂ Emissions",
    "section": "",
    "text": "This script was used to transform the ODIAC Fossil Fuel CO₂ Emissions dataset from GeoTIFF to Cloud Optimized GeoTIFF (COG) format for display in the Greenhouse Gas (GHG) Center.\n\nimport xarray\nimport re\nimport tempfile\nimport numpy as np\nimport boto3\nimport os\nimport gzip,shutil, wget\nimport s3fs\nimport hashlib\nimport json\n\n\n\nsession = boto3.session.Session()\ns3_client = session.client(\"s3\")\nfs = s3fs.S3FileSystem()\n\ndata_dir = \"data/\"\ndataset_name = \"odiac-ffco2-monthgrid-v2023\"\ncog_data_bucket = \"ghgc-data-store-develop\"\ncog_data_prefix= f\"transformed_cogs/{dataset_name}\"\ncog_checksum_prefix= \"checksum\"\n\n\n# Retrieve the checksum of raw files\nchecksum_dict ={}\nfor year in range(2000,2023):\n    checksum_url = f\"https://db.cger.nies.go.jp/nies_data/10.17595/20170411.001/odiac2023/1km_tiff/{year}/odiac2023_1km_checksum_{year}.md5.txt\"\n    response = requests.get(checksum_url)\n    content = response.text\n    tmp={}\n    \n    # Split the content into lines\n    lines = content.splitlines()\n    \n    for line in lines:\n        checksum, filename = line.split()\n        tmp[filename[:-3]] = checksum\n    checksum_dict.update(tmp)\nchecksum_dict = {k: v for k, v in checksum_dict.items() if k.endswith('.tif')}\n\n\n\ndef calculate_md5(file_path):\n    \"\"\"\n    Calculate the MD5 hash of a file.\n\n    Parameters:\n    file_path (str): The path to the file.\n\n    Returns:\n    str: The MD5 hash of the file.\n    \"\"\"\n    hash_md5 = hashlib.md5()\n    with open(file_path, \"rb\") as f:\n        for chunk in iter(lambda: f.read(4096), b\"\"):\n            hash_md5.update(chunk)\n    return hash_md5.hexdigest()\n\n\n#Code to download raw ODIAC data in your local machine\n\n# Creating  a base directory for ODIAC data\nif not os.path.exists(data_dir):\n        os.makedirs(data_dir)\n\nchecksum_dict_local={}\n# Download and unzip data for the years you want\nfor year in range(2000,2023):\n    year_dir = os.path.join(data_dir, str(year))\n    checksum_download_link = f\"https://db.cger.nies.go.jp/nies_data/10.17595/20170411.001/odiac2023/1km_tiff/{year}/odiac2023_1km_checksum_{year}.md5.txt\"\n    wget.download(checksum_download_link, year_dir)\n    # Make a subfolder for each year\n    if not os.path.exists(year_dir):\n        os.makedirs(year_dir)\n\n    for month in range(1,13):\n        month = f\"{month:02d}\"\n        download_link = f\"https://db.cger.nies.go.jp/nies_data/10.17595/20170411.001/odiac2023/1km_tiff/{year}/odiac2023_1km_excl_intl_{str(year)[-2:]}{month}.tif.gz\"\n        target_folder = f\"{data_dir}/{year}/\"\n        fname = os.path.basename(download_link)\n        target_path = os.path.join(target_folder, fname)\n\n        # Download the file\n        wget.download(download_link, target_path)\n\n        # Unzip the file\n        with gzip.open(target_path, 'rb') as f_in:\n            with open(target_path[:-3], 'wb') as f_out:\n                shutil.copyfileobj(f_in, f_out)\n                \n        # Calculate checksum of the .gz file \n        checksum_dict_local[target_path.split(\"/\")[-1][:-3]]=calculate_md5(target_path)\n        \n        # Remove the zip file\n        os.remove(target_path)\n    \n\n\n# check if the checksums match\nchecksum_dict_local == checksum_dict\n\n\n# List of years you want to run the transformation on\nfold_names=[str(i) for i in range(2020,2023)]\n\nfor fol_ in fold_names:\n    names= os.listdir(f\"{data_dir}{fol_}\")\n    names= [name for name in names if name.endswith('.tif')]\n    print(\"For year: \" ,fol_)\n    for name in names:\n        xds = xarray.open_dataarray(f\"{data_dir}{fol_}/{name}\")\n        filename = name.split(\"/ \")[-1]\n        filename_elements = re.split(\"[_ .]\", filename)\n        \n        # Remove the extension\n        filename_elements.pop()\n        # Extract and insert date of generated COG into filename\n        filename_elements[-1] = fol_ + filename_elements[-1][-2:]\n\n        # Replace 0 values  with -9999\n        xds = xds.where(xds!=0, -9999)\n        xds.rio.set_spatial_dims(\"x\", \"y\", inplace=True)\n        xds.rio.write_nodata(-9999, inplace=True)\n        xds.rio.write_crs(\"epsg:4326\", inplace=True)\n\n        cog_filename = \"_\".join(filename_elements)\n        cog_filename = f\"{cog_filename}.tif\"\n\n        # Write the cog file to s3 \n        with tempfile.NamedTemporaryFile() as temp_file:\n            xds.rio.to_raster(\n                temp_file.name,\n                driver=\"COG\",\n                compress=\"DEFLATE\"\n            )\n            s3_client.upload_file(\n                Filename=temp_file.name,\n                Bucket=cog_data_bucket,\n                Key=f\"{cog_data_prefix}/{cog_filename}\",\n            )\n\n        print(f\"Generated and saved COG: {cog_filename}\")\n\nprint(\"ODIAC COGs generation completed!!!\")\n\n\n# This block is used to calculate the SHA for each COG file and store in a JSON.\n\ndef get_all_s3_keys(bucket, model_name, ext):\n    \"\"\"Get a list of all keys in an S3 bucket.\"\"\"\n    keys = []\n\n    kwargs = {\"Bucket\": bucket, \"Prefix\": f\"{model_name}/\"}\n    while True:\n        resp = s3_client.list_objects_v2(**kwargs)\n        for obj in resp[\"Contents\"]:\n            if obj[\"Key\"].endswith(ext) and \"historical\" not in obj[\"Key\"]:\n                keys.append(obj[\"Key\"])\n\n        try:\n            kwargs[\"ContinuationToken\"] = resp[\"NextContinuationToken\"]\n        except KeyError:\n            break\n\n    return keys\n\nkeys = get_all_s3_keys(cog_data_bucket, cog_data_prefix,\".tif\")\n\n\ndef compute_sha256(url):\n    \"\"\"Compute SHA-256 checksum for a given file.\"\"\"\n    sha256_hash = hashlib.sha256()\n    with fs.open(url) as f:\n        for byte_block in iter(lambda: f.read(4096), b\"\"):\n            sha256_hash.update(byte_block)\n    return sha256_hash.hexdigest()\n\nsha_mapping = {}\nfor key in keys:\n    sha_mapping[key.split(\"/\")[-1]]=compute_sha256(f\"s3://{cog_data_bucket}/{key}\")\n\n\njson_data = json.dumps(sha_mapping, indent=4)\ns3_client.put_object(Bucket=cog_data_bucket, Key=f\"{cog_checksum_prefix}/{dataset_name}.json\", Body=json_data)\n\nprint(\"Checksums created for ODIAC!!!\")\n\n\n\n\n Back to top",
    "crumbs": [
      "Data Transformation Notebooks",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "ODIAC Fossil Fuel CO₂ Emissions"
    ]
  },
  {
    "objectID": "cog_transformation/gosat-based-ch4budget-yeargrid-v1.html",
    "href": "cog_transformation/gosat-based-ch4budget-yeargrid-v1.html",
    "title": "GOSAT-based Top-down Methane Budgets",
    "section": "",
    "text": "This script was used to transform the GOSAT-based Top-down Methane Budgets dataset from netCDF to Cloud Optimized GeoTIFF (COG) format for display in the Greenhouse Gas (GHG) Center.\n\nimport os\nimport xarray\nimport re\nimport pandas as pd\nimport json\nimport tempfile\nimport boto3\nimport rasterio\nfrom datetime import datetime\nfrom dateutil.relativedelta import relativedelta\n\n\nsession = boto3.session.Session()\ns3_client = session.client(\"s3\")\nbucket_name = (\n    \"ghgc-data-store-dev\"  # S3 bucket where the COGs are stored after transformation\n)\nyear_ = datetime(2019, 1, 1)\nfolder_name = \"new_data/CH4-inverse-flux\"\n\nCOG_PROFILE = {\"driver\": \"COG\", \"compress\": \"DEFLATE\"}\n\nfiles_processed = pd.DataFrame(\n    columns=[\"file_name\", \"COGs_created\"]\n)  # A dataframe to keep track of the files that we have transformed into COGs\n\n# Reading the raw netCDF files from local machine\nfor name in os.listdir(folder_name):\n    ds = xarray.open_dataset(\n        f\"{folder_name}/{name}\",\n        engine=\"netcdf4\",\n    )\n\n    ds = ds.rename({\"dimy\": \"lat\", \"dimx\": \"lon\"})\n    # assign coords from dimensions\n    ds = ds.assign_coords(lon=(((ds.lon + 180) % 360) - 180)).sortby(\"lon\")\n    ds = ds.assign_coords(lat=((ds.lat / 180) * 180) - 90).sortby(\"lat\")\n\n    variable = [var for var in ds.data_vars]\n\n    for var in variable[2:]:\n        filename = name.split(\"/ \")[-1]\n        filename_elements = re.split(\"[_ .]\", filename)\n        data = ds[var]\n        filename_elements.pop()\n        filename_elements.insert(2, var)\n        cog_filename = \"_\".join(filename_elements)\n        # # add extension\n        cog_filename = f\"{cog_filename}.tif\"\n\n        data = data.reindex(lat=list(reversed(data.lat)))\n\n        data.rio.set_spatial_dims(\"lon\", \"lat\")\n        data.rio.write_crs(\"epsg:4326\", inplace=True)\n\n        # generate COG\n        COG_PROFILE = {\"driver\": \"COG\", \"compress\": \"DEFLATE\"}\n\n        with tempfile.NamedTemporaryFile() as temp_file:\n            data.rio.to_raster(temp_file.name, **COG_PROFILE)\n            s3_client.upload_file(\n                Filename=temp_file.name,\n                Bucket=bucket_name,\n                Key=f\"ch4_inverse_flux/{cog_filename}\",\n            )\n\n        files_processed = files_processed._append(\n            {\"file_name\": name, \"COGs_created\": cog_filename},\n            ignore_index=True,\n        )\n\n        print(f\"Generated and saved COG: {cog_filename}\")\n\n# Generate the json file with the metadata that is present in the netCDF files.\nwith tempfile.NamedTemporaryFile(mode=\"w+\") as fp:\n    json.dump(ds.attrs, fp)\n    json.dump({\"data_dimensions\": dict(ds.dims)}, fp)\n    json.dump({\"data_variables\": list(ds.data_vars)}, fp)\n    fp.flush()\n\n    s3_client.upload_file(\n        Filename=fp.name,\n        Bucket=bucket_name,\n        Key=\"ch4_inverse_flux/metadata.json\",\n    )\n\n# creating the csv file with the names of files transformed.\nfiles_processed.to_csv(\n    f\"s3://{bucket_name}/ch4_inverse_flux/files_converted.csv\",\n)\nprint(\"Done generating COGs\")\n\n\n\n\n Back to top",
    "crumbs": [
      "Data Transformation Notebooks",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "GOSAT-based Top-down Methane Budgets"
    ]
  },
  {
    "objectID": "cog_transformation/noaa-gggrn-concentrations.html",
    "href": "cog_transformation/noaa-gggrn-concentrations.html",
    "title": "Atmospheric Carbon Dioxide and Methane Concentrations from NOAA Global Monitoring Laboratory",
    "section": "",
    "text": "This script was used to transform the CO₂ and CH₄ datasets in txt format with hourly granularity to JSON in daily and monthly granularity for visualization in the Greenhouse Gas (GHG) Center.\n\nimport sys\nimport json\nimport pandas as pd\n\n\ndef daily_aggregate(filepath):\n    \"\"\"\n    Reads hourly data from a .txt file, aggregates it to daily, and returns a list of JSON objects that can be readily visualized in chart.\n\n    Parameters:\n        filepath (str): The path to the file containing the data to be aggregated.\n\n    Returns:\n        list: A list of dictionaries representing aggregated data, with each dictionary containing\n              'date' and 'value' keys.\n\n    Description:\n        This function reads data from the specified file, aggregates it, and returns a list of JSON objects.\n        The function performs the following steps:\n        - Reads the content of the file.\n        - Extracts the header lines from the file to determine the structure of the data.\n        - Processes the data into a DataFrame.\n        - Filters and aggregates the data.\n        - Converts the aggregated data into a list of JSON objects, where each object contains 'date' and 'value' keys.\n\n    Exceptions:\n        - FileNotFoundError: If the specified file is not found.\n        - Exception: If any other exception occurs during the processing, the exception message is returned.\n\n    Note:\n        - The input file is expected to have a .txt format with header lines indicating the structure of the data.\n        - The function aggregates data from hourly to daily intervals.\n        - The returned JSON list is suitable for use in frontend applications to visualize the aggregated data.\n\n    Example:\n        aggregated_data = daily_aggregate(\"/path/to/data_file.txt\")\n    \"\"\"\n    try:\n        with open(filepath, \"r\", encoding=\"utf-8\") as file:\n            file_content_str = file.read()\n            # split the string text based on new line\n            file_content_list = file_content_str.split(\"\\n\")\n            # get the header lines. its mentioned in the file's first line.\n            header_lines = file_content_list[0].split(\":\")[-1]\n            header_lines = int(header_lines)\n            # Slice the non header part of the data. and the last empty element\n            str_datas = file_content_list[header_lines - 1: -1]\n            data = [data.replace(\"\\n\", \"\").split(\" \") for data in str_datas]\n            # seperate table body and head to form dataframe\n            table_head = data[0]\n            table_body = data[1:]\n            dataframe = pd.DataFrame(table_body, columns=table_head)\n            dataframe['value'] = dataframe['value'].astype(float)\n            # Filter data\n            mask = (dataframe[\"qcflag\"] == \"...\") & (dataframe[\"value\"] != 0) & (dataframe[\"value\"] != -999)\n            filtered_df = dataframe[mask].reset_index(drop=True)\n            # Aggregate data (hourly into daily)\n            aggregated_df = filtered_df.groupby(['year', 'month', 'day'])['value'].mean().reset_index()\n            aggregated_df['value'] = aggregated_df['value'].round(2)\n            # necessary columns, processed df\n            aggregated_df['datetime'] = pd.to_datetime(aggregated_df[['year', 'month', 'day']])\n            aggregated_df['datetime'] = aggregated_df['datetime'].dt.strftime('%Y-%m-%dT%H:%M:%SZ')\n            processed_df = aggregated_df[['datetime', 'value']]\n            processed_df = processed_df.sort_values(by='datetime')\n            # dict formation, needed for frontend [{date: , value: }]\n            json_list = []\n            for _, row in processed_df.iterrows():\n                json_obj = {'date': row['datetime'], 'value': row['value']}\n                json_list.append(json_obj)\n            return json_list\n    except FileNotFoundError:\n        return \"File not found\"\n    except Exception as e:\n        return f\"Exception occured {e}\"\n\n\ndef monthly_aggregate(filepath):\n    \"\"\"\n    Reads hourly data from a .txt file, aggregates it to monthly, and returns a list of JSON objects that can be readily visualized in chart.\n\n    Parameters:\n        filepath (str): The path to the file containing the data to be aggregated.\n\n    Returns:\n        list: A list of dictionaries representing aggregated data, with each dictionary containing\n              'date' and 'value' keys.\n\n    Description:\n        This function reads data from the specified file, aggregates it, and returns a list of JSON objects.\n        The function performs the following steps:\n        - Reads the content of the file.\n        - Extracts the header lines from the file to determine the structure of the data.\n        - Processes the data into a DataFrame.\n        - Filters and aggregates the data.\n        - Converts the aggregated data into a list of JSON objects, where each object contains 'date' and 'value' keys.\n\n    Exceptions:\n        - FileNotFoundError: If the specified file is not found.\n        - Exception: If any other exception occurs during the processing, the exception message is returned.\n\n    Note:\n        - The input file is expected to have a .txt format with header lines indicating the structure of the data.\n        - The function aggregates data from hourly to daily intervals.\n        - The returned JSON list is suitable for use in frontend applications to visualize the aggregated data.\n\n    Example:\n        aggregated_data = monthly_aggregate(\"/path/to/data_file.txt\")\n    \"\"\"\n    try:\n        with open(filepath, \"r\", encoding=\"utf-8\") as file:\n            file_content_str = file.read()\n            # split the string text based on new line\n            file_content_list = file_content_str.split(\"\\n\")\n            # get the header lines. its mentioned in the file's first line.\n            header_lines = file_content_list[0].split(\":\")[-1]\n            header_lines = int(header_lines)\n            # Slice the non header part of the data. and the last empty element\n            str_datas = file_content_list[header_lines - 1: -1]\n            data = [data.replace(\"\\n\", \"\").split(\" \") for data in str_datas]\n            # seperate table body and head to form dataframe\n            table_head = data[0]\n            table_body = data[1:]\n            dataframe = pd.DataFrame(table_body, columns=table_head)\n            dataframe['value'] = dataframe['value'].astype(float)\n            # Filter data\n            mask = (dataframe[\"qcflag\"] == \"...\") & (dataframe[\"value\"] != 0) & (dataframe[\"value\"] != -999)\n            filtered_df = dataframe[mask].reset_index(drop=True)\n            # Aggregate data (hourly into monthly)\n            aggregated_df = filtered_df.groupby(['year', 'month'])['value'].mean().reset_index()\n            aggregated_df['value'] = aggregated_df['value'].round(2)\n            # necessary columns, processed df\n            aggregated_df['datetime'] = pd.to_datetime(aggregated_df[['year', 'month']].assign(day=1))\n            aggregated_df['datetime'] = aggregated_df['datetime'].dt.strftime('%Y-%m-%dT%H:%M:%SZ')\n            processed_df = aggregated_df[['datetime', 'value']]\n            processed_df = processed_df.sort_values(by='datetime')\n            # dict formation, needed for frontend [{date: , value: }]\n            json_list = []\n            for _, row in processed_df.iterrows():\n                json_obj = {'date': row['datetime'], 'value': row['value']}\n                json_list.append(json_obj)\n            return json_list\n    except FileNotFoundError:\n        return \"File not found\"\n    except Exception as e:\n        return f\"Exception occured {e}\"\n\n\nif __name__ == \"__main__\":\n    # Check if filepath argument is provided\n    if len(sys.argv) != 2:\n        print(\"Usage: python aggregrate.py &lt;daily|monthly&gt; &lt;filepath&gt;\")\n        sys.exit(1)\n\n    # Get the filepath from command line argument\n    frequency = sys.argv[1]\n    hourly_data_filepath = sys.argv[2]\n\n    # Call the aggregate function with the provided filepath\n    if (frequency == \"daily\"):\n        result = daily_aggregate(hourly_data_filepath)\n    elif (frequency == \"monthly\"):\n        result = monthly_aggregate(hourly_data_filepath)\n    else:\n        print(\"Usage: python aggregrate.py &lt;daily|monthly&gt; &lt;filepath&gt;\")\n        sys.exit(1)\n\n    if result is not None:\n        print(result)\n        # save the json file for reference\n        out_path = f\"{hourly_data_filepath.split(\"/\")[-1]}.json\"\n        with open(out_path, \"w\", encoding=\"utf-8\") as file:\n            json.dump(result, file)\n\n\n\n\n Back to top",
    "crumbs": [
      "Data Transformation Notebooks",
      "Greenhouse Gas Concentrations",
      "Atmospheric Carbon Dioxide and Methane Concentrations from NOAA Global Monitoring Laboratory"
    ]
  },
  {
    "objectID": "datatransformationcode.html",
    "href": "datatransformationcode.html",
    "title": "U.S. Greenhouse Gas Center: Data Transformation Notebooks",
    "section": "",
    "text": "Welcome to the U.S. Greenhouse Gas (GHG) Center data transformation notebooks, where we harness the power of Cloud Optimized Geotiffs (COGs) to offer a dynamic, cloud-based platform for exploring and analyzing greenhouse gas datasets. Dive into our curated collection of GHG datasets, all optimized for seamless accessibility and analysis.\nDiscover the journey of each dataset from its original format to COGs through the below Jupyter notebooks. The transformation examples are grouped topically. The dataset product type (model output, satellite observation, etc.) is noted next to the notebook name. Click on a notebook to learn more about the dataset and to view the transformation code.\nJoin us in our mission to make data-driven environmental solutions. Explore, analyze, and make a difference with the US GHG Center.\nNote: Not all datasets have a transformation code\nView the US GHG Center Data Catalog",
    "crumbs": [
      "Data Transformation Notebooks"
    ]
  },
  {
    "objectID": "datatransformationcode.html#gridded-anthropogenic-greenhouse-gas-emissions",
    "href": "datatransformationcode.html#gridded-anthropogenic-greenhouse-gas-emissions",
    "title": "U.S. Greenhouse Gas Center: Data Transformation Notebooks",
    "section": "Gridded Anthropogenic Greenhouse Gas Emissions",
    "text": "Gridded Anthropogenic Greenhouse Gas Emissions\n\nOCO-2 MIP Top-Down CO₂ Budgets\nODIAC Fossil Fuel CO₂ Emissions\nTM5-4DVar Isotopic CH₄ Inverse Fluxes\nU.S. Gridded Anthropogenic Methane Emissions Inventory\nVulcan Fossil Fuel CO₂ Emissions",
    "crumbs": [
      "Data Transformation Notebooks"
    ]
  },
  {
    "objectID": "datatransformationcode.html#natural-greenhouse-gas-emissions-and-sinks",
    "href": "datatransformationcode.html#natural-greenhouse-gas-emissions-and-sinks",
    "title": "U.S. Greenhouse Gas Center: Data Transformation Notebooks",
    "section": "Natural Greenhouse Gas Emissions and Sinks",
    "text": "Natural Greenhouse Gas Emissions and Sinks\n\nAir-Sea CO₂ Flux, ECCO-Darwin Model v5\nGOSAT-based Top-down Total and Natural Methane Emissions\nOCO-2 MIP Top-Down CO₂ Budgets\nTM5-4DVar Isotopic CH₄ Inverse Fluxes\nGRA²PES Greenhouse Gas and Air Quality Species",
    "crumbs": [
      "Data Transformation Notebooks"
    ]
  },
  {
    "objectID": "datatransformationcode.html#large-emissions-events",
    "href": "datatransformationcode.html#large-emissions-events",
    "title": "U.S. Greenhouse Gas Center: Data Transformation Notebooks",
    "section": "Large Emissions Events",
    "text": "Large Emissions Events\n\nEMIT Methane Point Source Plume Complexes",
    "crumbs": [
      "Data Transformation Notebooks"
    ]
  },
  {
    "objectID": "datatransformationcode.html#greenhouse-gas-concentrations",
    "href": "datatransformationcode.html#greenhouse-gas-concentrations",
    "title": "U.S. Greenhouse Gas Center: Data Transformation Notebooks",
    "section": "Greenhouse Gas Concentrations",
    "text": "Greenhouse Gas Concentrations\n\nAtmospheric Carbon Dioxide and Methane Concentrations from NOAA Global Monitoring Laboratory\nOCO-2 GEOS Column CO₂ Concentrations\nCarbon Dioxide and Methane Concentrations from the Indianapolis Flux Experiment (INFLUX)\nCarbon Dioxide and Methane Concentrations from the Los Angeles Megacity Carbon Project\nCarbon Dioxide and Methane Concentrations from the Northeast Corridor (NEC) Urban Test Bed",
    "crumbs": [
      "Data Transformation Notebooks"
    ]
  },
  {
    "objectID": "datatransformationcode.html#socioeconomic",
    "href": "datatransformationcode.html#socioeconomic",
    "title": "U.S. Greenhouse Gas Center: Data Transformation Notebooks",
    "section": "Socioeconomic",
    "text": "Socioeconomic\n\nSEDAC Gridded World Population Density",
    "crumbs": [
      "Data Transformation Notebooks"
    ]
  },
  {
    "objectID": "datatransformationcode.html#contact",
    "href": "datatransformationcode.html#contact",
    "title": "U.S. Greenhouse Gas Center: Data Transformation Notebooks",
    "section": "Contact",
    "text": "Contact\nFor technical help or general questions, please contact the support team using the feedback form.",
    "crumbs": [
      "Data Transformation Notebooks"
    ]
  },
  {
    "objectID": "user_data_notebooks/gra2pes-ghg-monthgrid-v1_User_Notebook.html",
    "href": "user_data_notebooks/gra2pes-ghg-monthgrid-v1_User_Notebook.html",
    "title": "GRA²PES Greenhouse Gas and Air Quality Species",
    "section": "",
    "text": "You can launch this notebook in the US GHG Center JupyterHub by clicking the link below.\nLaunch in the US GHG Center JupyterHub (requires access)",
    "crumbs": [
      "Data Usage Notebooks",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "GRA²PES Greenhouse Gas and Air Quality Species"
    ]
  },
  {
    "objectID": "user_data_notebooks/gra2pes-ghg-monthgrid-v1_User_Notebook.html#run-this-notebook",
    "href": "user_data_notebooks/gra2pes-ghg-monthgrid-v1_User_Notebook.html#run-this-notebook",
    "title": "GRA²PES Greenhouse Gas and Air Quality Species",
    "section": "",
    "text": "You can launch this notebook in the US GHG Center JupyterHub by clicking the link below.\nLaunch in the US GHG Center JupyterHub (requires access)",
    "crumbs": [
      "Data Usage Notebooks",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "GRA²PES Greenhouse Gas and Air Quality Species"
    ]
  },
  {
    "objectID": "user_data_notebooks/gra2pes-ghg-monthgrid-v1_User_Notebook.html#approach",
    "href": "user_data_notebooks/gra2pes-ghg-monthgrid-v1_User_Notebook.html#approach",
    "title": "GRA²PES Greenhouse Gas and Air Quality Species",
    "section": "Approach",
    "text": "Approach\n\nIdentify available dates and temporal frequency of observations for the given collection using the GHGC API /stac endpoint. The collection processed in this notebook is the Vulcan Fossil Fuel CO₂ Emissions Data product.\nPass the STAC item into the raster API /stac/tilejson.jsonendpoint.\nUsing folium.plugins.DualMap, we will visualize two tiles (side-by-side), allowing us to compare time points.\nAfter the visualization, we will perform zonal statistics for a given polygon.",
    "crumbs": [
      "Data Usage Notebooks",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "GRA²PES Greenhouse Gas and Air Quality Species"
    ]
  },
  {
    "objectID": "user_data_notebooks/gra2pes-ghg-monthgrid-v1_User_Notebook.html#about-the-data",
    "href": "user_data_notebooks/gra2pes-ghg-monthgrid-v1_User_Notebook.html#about-the-data",
    "title": "GRA²PES Greenhouse Gas and Air Quality Species",
    "section": "About the Data",
    "text": "About the Data\nThe Greenhouse gas And Air Pollutants Emissions System (GRA2PES) dataset at the GHG Center is an aggregated, regridded, monthly high-resolution (0.036 x 0.036°) data product with emissions of both greenhouse gases and air pollutants developed in a consistent framework. The dataset contains emissions over the contiguous United States covering major anthropogenic sectors, including energy, industrial fuel combustion and processes, commercial and residential combustion, oil and gas production, on-road and off-road transportation, etc. (see Table 1 in the Scientific Details section below for a full sector list). Fossil fuel CO2 (ffCO2) emissions are developed along with those of air pollutants including CO, NOx, SOx, and PM2.5 with consistency in spatial and temporal distributions. Emissions by sectors are grouped into point and area sources, reported as column totals in units of metric tons per km2 per month. Spatial-temporal surrogates are developed to distribute CO2 emissions to grid cells to keep consistency between greenhouse gases and air quality species. The current version of GRA2PES is for 2021. Long-term emissions and more greenhouse gas species (e.g., methane) are under development and will be added in the future.\nFor more information regarding this dataset, please visit the GRA2PES Greenhouse Gas and Air Quality Species, Version 1 data overview page.",
    "crumbs": [
      "Data Usage Notebooks",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "GRA²PES Greenhouse Gas and Air Quality Species"
    ]
  },
  {
    "objectID": "user_data_notebooks/gra2pes-ghg-monthgrid-v1_User_Notebook.html#querying-the-stac-api",
    "href": "user_data_notebooks/gra2pes-ghg-monthgrid-v1_User_Notebook.html#querying-the-stac-api",
    "title": "GRA²PES Greenhouse Gas and Air Quality Species",
    "section": "Querying the STAC API",
    "text": "Querying the STAC API\nFirst, we are going to import the required libraries. Once imported, they allow better executing a query in the GHG Center Spatio Temporal Asset Catalog (STAC) Application Programming Interface (API) where the granules for this collection are stored.\n\n# Provide STAC and RASTER API endpoints\nSTAC_API_URL = \"https://earth.gov/ghgcenter/api/stac\"\nRASTER_API_URL = \"https://earth.gov/ghgcenter/api/raster\"\n\n# Please use the collection name similar to the one used in the STAC collection.\n# Name of the collection for Vulcan Fossil Fuel CO₂ Emissions, Version 4. \ncollection_name = \"gra2pes-ghg-monthgrid-v1\"\n\n\n# Fetch the collection from STAC collections using the appropriate endpoint\n# the 'requests' library allows a HTTP request possible\ncollection_graapes = requests.get(f\"{STAC_API_URL}/collections/{collection_name}\").json()\n\nExamining the contents of our collection under the temporal variable, we see that the data is available from January 2010 to December 2021. By looking at the dashboard:time density, we observe that the data is periodic with year time density.\n\n# Create a function that would search for the above data collection in the STAC API\ndef get_item_count(collection_id):\n    count = 0\n    items_url = f\"{STAC_API_URL}/collections/{collection_id}/items\"\n\n    while True:\n        response = requests.get(items_url)\n\n        if not response.ok:\n            print(\"error getting items\")\n            exit()\n\n        stac = response.json()\n        count += int(stac[\"context\"].get(\"returned\", 0))\n        next = [link for link in stac[\"links\"] if link[\"rel\"] == \"next\"]\n\n        if not next:\n            break\n        items_url = next[0][\"href\"]\n\n    return count\n\n\n# Apply the above function and check the total number of items available within the collection\nnumber_of_items = get_item_count(collection_name)\nitems_graapes = requests.get(f\"{STAC_API_URL}/collections/{collection_name}/items?limit={number_of_items}\").json()[\"features\"]\nprint(f\"Found {len(items_vulcan)} items\")\n\nFound 12 items\n\n\n\n# To access the year value from each item more easily, this will let us query more explicitly by year and month (e.g., 2020-02)\nitems = {item[\"properties\"][\"start_datetime\"][:7]: item for item in items_graapes} \n# rh = Heterotrophic Respiration\nasset_name = \"co2\"\n\n\nrescale_values = {\"max\":items[list(items.keys())[0]][\"assets\"][asset_name][\"raster:bands\"][0][\"histogram\"][\"max\"], \"min\":items[list(items.keys())[0]][\"assets\"][asset_name][\"raster:bands\"][0][\"histogram\"][\"min\"]}\n\nNow, we will pass the item id, collection name, asset name, and the rescaling factor to the Raster API endpoint. We will do this twice, once for 2021-01 and again for 2021-05, so that we can visualize each event independently.\n\ncolor_map = \"spectral_r\" # please refer to matplotlib library if you'd prefer choosing a different color ramp.\n# For more information on Colormaps in Matplotlib, please visit https://matplotlib.org/stable/users/explain/colors/colormaps.html\n\n# To change the year and month of the observed parameter, you can modify the \"items['YYYY-MM']\" statement\n# For example, you can change the current statement \"items['2003-12']\" to \"items['2016-10']\" \n_202101_tile = requests.get(\n    f\"{RASTER_API_URL}/collections/{items['2021-01']['collection']}/items/{items['2021-01']['id']}/tilejson.json?collection={items['2021-01']['collection']}&item={items['2021-01']['id']}\"\n\n    f\"&assets={asset_name}\"\n    f\"&color_formula=gamma+r+1.05&colormap_name={color_map}\"\n    f\"&rescale=0,150\", \n).json()\n_202101_tile\n\n{'tilejson': '2.2.0',\n 'version': '1.0.0',\n 'scheme': 'xyz',\n 'tiles': ['https://dev.ghg.center/api/raster/collections/gra2pes-co2-monthgrid-v1/items/gra2pes-co2-monthgrid-v1-202101/tiles/WebMercatorQuad/{z}/{x}/{y}@1x?collection=gra2pes-co2-monthgrid-v1&item=gra2pes-co2-monthgrid-v1-202101&assets=co2&color_formula=gamma+r+1.05&colormap_name=spectral_r&rescale=0%2C150'],\n 'minzoom': 0,\n 'maxzoom': 24,\n 'bounds': [-137.3143, 18.173376, -58.58229999999702, 52.229376000001295],\n 'center': [-97.94829999999851, 35.20137600000065, 0]}\n\n\n\n_202105_tile = requests.get(\n    f\"{RASTER_API_URL}/collections/{items['2021-05']['collection']}/items/{items['2021-05']['id']}/tilejson.json?collection={items['2021-05']['collection']}&item={items['2021-05']['id']}\"\n\n    f\"&assets={asset_name}\"\n    f\"&color_formula=gamma+r+1.05&colormap_name={color_map}\"\n    f\"&rescale=0,150\", \n).json()\n_202105_tile\n\n{'tilejson': '2.2.0',\n 'version': '1.0.0',\n 'scheme': 'xyz',\n 'tiles': ['https://dev.ghg.center/api/raster/collections/gra2pes-co2-monthgrid-v1/items/gra2pes-co2-monthgrid-v1-202105/tiles/WebMercatorQuad/{z}/{x}/{y}@1x?collection=gra2pes-co2-monthgrid-v1&item=gra2pes-co2-monthgrid-v1-202105&assets=co2&color_formula=gamma+r+1.05&colormap_name=spectral_r&rescale=0%2C150'],\n 'minzoom': 0,\n 'maxzoom': 24,\n 'bounds': [-137.3143, 18.173376, -58.58229999999702, 52.229376000001295],\n 'center': [-97.94829999999851, 35.20137600000065, 0]}",
    "crumbs": [
      "Data Usage Notebooks",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "GRA²PES Greenhouse Gas and Air Quality Species"
    ]
  },
  {
    "objectID": "user_data_notebooks/gra2pes-ghg-monthgrid-v1_User_Notebook.html#visualizing-total-fossil-fuel-co₂-emissions",
    "href": "user_data_notebooks/gra2pes-ghg-monthgrid-v1_User_Notebook.html#visualizing-total-fossil-fuel-co₂-emissions",
    "title": "GRA²PES Greenhouse Gas and Air Quality Species",
    "section": "Visualizing Total Fossil Fuel CO₂ Emissions",
    "text": "Visualizing Total Fossil Fuel CO₂ Emissions\n\nmap_ = folium.plugins.DualMap(location=(34, -118), zoom_start=6)\n\n\n# Define the first map layer with the CO2 Flux data for December 2022\nmap_layer_202101 = TileLayer(\n    tiles=_202101_tile[\"tiles\"][0], # Path to retrieve the tile\n    attr=\"GHG\", # Set the attribution \n    name='2021-01 Total CO2 Fossil Fuel Emissions', # Title for the layer\n    overlay=True, # The layer can be overlaid on the map\n    opacity=0.8, # Adjust the transparency of the layer\n)\n# Add the first layer to the Dual Map \nmap_layer_202101.add_to(map_.m1)\n\nmap_layer_202105 = TileLayer(\n    tiles=_202105_tile[\"tiles\"][0], # Path to retrieve the tile\n    attr=\"GHG\", # Set the attribution \n    name='2021-05 Total CO2  Emissions', # Title for the layer\n    overlay=True, # The layer can be overlaid on the map\n    opacity=0.8, # Adjust the transparency of the layer\n)\n# Add the first layer to the Dual Map \nmap_layer_2021.add_to(map_.m2)\n\nmap_\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook",
    "crumbs": [
      "Data Usage Notebooks",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "GRA²PES Greenhouse Gas and Air Quality Species"
    ]
  },
  {
    "objectID": "user_data_notebooks/gra2pes-ghg-monthgrid-v1_User_Notebook.html#summary",
    "href": "user_data_notebooks/gra2pes-ghg-monthgrid-v1_User_Notebook.html#summary",
    "title": "GRA²PES Greenhouse Gas and Air Quality Species",
    "section": "Summary",
    "text": "Summary\nIn this notebook we have successfully explored, analyzed, and visualized the STAC collection for GRA2PES greenhouse gases Emissions, Version 1 dataset.\n\nInstall and import the necessary libraries\nFetch the collection from STAC collections using the appropriate endpoints\nCount the number of existing granules within the collection\nMap and compare the total CO₂ emissions for two distinctive months/years\n\nIf you have any questions regarding this user notebook, please contact us using the feedback form.",
    "crumbs": [
      "Data Usage Notebooks",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "GRA²PES Greenhouse Gas and Air Quality Species"
    ]
  },
  {
    "objectID": "user_data_notebooks/epa-ch4emission-grid-v2express_User_Notebook.html",
    "href": "user_data_notebooks/epa-ch4emission-grid-v2express_User_Notebook.html",
    "title": "Leveraging the U.S. Gridded Anthropogenic Methane Emissions Inventory for Monitoring Trends in Methane Emissions",
    "section": "",
    "text": "You can launch this notebook in the US GHG Center JupyterHub by clicking the link below. If you are a new user, you should first sign up for the hub by filling out this request form and providing the required information.\nAccess the U.S. Gridded Anthropogenic Methane Emissions Inventory notebook in the US GHG Center JupyterHub.",
    "crumbs": [
      "Data Usage Notebooks",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "Leveraging the U.S. Gridded Anthropogenic Methane Emissions Inventory for Monitoring Trends in Methane Emissions"
    ]
  },
  {
    "objectID": "user_data_notebooks/epa-ch4emission-grid-v2express_User_Notebook.html#access-this-notebook",
    "href": "user_data_notebooks/epa-ch4emission-grid-v2express_User_Notebook.html#access-this-notebook",
    "title": "Leveraging the U.S. Gridded Anthropogenic Methane Emissions Inventory for Monitoring Trends in Methane Emissions",
    "section": "",
    "text": "You can launch this notebook in the US GHG Center JupyterHub by clicking the link below. If you are a new user, you should first sign up for the hub by filling out this request form and providing the required information.\nAccess the U.S. Gridded Anthropogenic Methane Emissions Inventory notebook in the US GHG Center JupyterHub.",
    "crumbs": [
      "Data Usage Notebooks",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "Leveraging the U.S. Gridded Anthropogenic Methane Emissions Inventory for Monitoring Trends in Methane Emissions"
    ]
  },
  {
    "objectID": "user_data_notebooks/epa-ch4emission-grid-v2express_User_Notebook.html#table-of-contents",
    "href": "user_data_notebooks/epa-ch4emission-grid-v2express_User_Notebook.html#table-of-contents",
    "title": "Leveraging the U.S. Gridded Anthropogenic Methane Emissions Inventory for Monitoring Trends in Methane Emissions",
    "section": "Table of Contents",
    "text": "Table of Contents\n\nData Summary and Application\nApproach\nAbout the Data\nInstall the Required Libraries\nQuery the STAC API\nVisual Comparison Across Time Periods\nMap Out Selected Tiles\nCalculate Zonal Statistics\nTime-Series Analysis\nSummary",
    "crumbs": [
      "Data Usage Notebooks",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "Leveraging the U.S. Gridded Anthropogenic Methane Emissions Inventory for Monitoring Trends in Methane Emissions"
    ]
  },
  {
    "objectID": "user_data_notebooks/epa-ch4emission-grid-v2express_User_Notebook.html#data-summary-and-application",
    "href": "user_data_notebooks/epa-ch4emission-grid-v2express_User_Notebook.html#data-summary-and-application",
    "title": "Leveraging the U.S. Gridded Anthropogenic Methane Emissions Inventory for Monitoring Trends in Methane Emissions",
    "section": "Data Summary and Application",
    "text": "Data Summary and Application\n\nSpatial coverage: Contiguous United States\nSpatial resolution: 0.1° x 0.1°\nTemporal extent: 2012 - 2020\nTemporal resolution: Annual\nUnit: Megagrams of methane per square kilometer per year (Mg CH₄/km²/yr)\nUtility: Methane Monitoring, Anthropogenic Emissions Analysis, Climate Research\n\nFor more, visit the U.S. Gridded Anthropogenic Methane Emissions Inventory data overview page.",
    "crumbs": [
      "Data Usage Notebooks",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "Leveraging the U.S. Gridded Anthropogenic Methane Emissions Inventory for Monitoring Trends in Methane Emissions"
    ]
  },
  {
    "objectID": "user_data_notebooks/epa-ch4emission-grid-v2express_User_Notebook.html#approach",
    "href": "user_data_notebooks/epa-ch4emission-grid-v2express_User_Notebook.html#approach",
    "title": "Leveraging the U.S. Gridded Anthropogenic Methane Emissions Inventory for Monitoring Trends in Methane Emissions",
    "section": "Approach",
    "text": "Approach\n\nIdentify available dates and temporal frequency of observations for the given collection using the GHGC API /stac endpoint. The collection processed in this notebook is the gridded methane emissions data product.\nPass the STAC item into the raster API /stac/tilejson.jsonendpoint.\nUsing folium.plugins.DualMap, we will visualize two tiles (side-by-side), allowing us to compare time points.\nAfter the visualization, we will perform zonal statistics for a given polygon.",
    "crumbs": [
      "Data Usage Notebooks",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "Leveraging the U.S. Gridded Anthropogenic Methane Emissions Inventory for Monitoring Trends in Methane Emissions"
    ]
  },
  {
    "objectID": "user_data_notebooks/epa-ch4emission-grid-v2express_User_Notebook.html#about-the-data",
    "href": "user_data_notebooks/epa-ch4emission-grid-v2express_User_Notebook.html#about-the-data",
    "title": "Leveraging the U.S. Gridded Anthropogenic Methane Emissions Inventory for Monitoring Trends in Methane Emissions",
    "section": "About the Data",
    "text": "About the Data\nThe gridded EPA U.S. anthropogenic methane greenhouse gas inventory (gridded GHGI) includes spatially disaggregated (0.1 deg x 0.1 deg or approximately 10 x 10 km resolution) maps of annual anthropogenic methane emissions (for the contiguous United States (CONUS)), consistent with national annual U.S. anthropogenic methane emissions reported in the U.S. EPA Inventory of U.S. Greenhouse Gas Emissions and Sinks (U.S. GHGI).\nThis V2 Express Extension dataset contains methane emissions provided as fluxes, in units of molecules of methane per square cm per second, for over 25 individual emission source categories, including those from agriculture, petroleum and natural gas systems, coal mining, and waste. The data have been converted from their original NetCDF format to Cloud-Optimized GeoTIFF (COG) for use in the US GHG Center, thereby enabling user exploration of spatial anthropogenic methane emissions and their trends.\nThe gridded dataset currently includes 34 data layers. The first data layer includes annual 2012-2020 gridded methane emissions fluxes from all anthropogenic sources of methane in the U.S. GHGI (excluding Land Use, Land-Use Change and Forestry (LULUCF) sources, which are not included in the gridded GHGI). The next six data layers include annual 2012-2020 gridded methane fluxes from sources within the aggregate Agriculture, Natural Gas, Petroleum, Waste, Industry, and ‘Other’ source categories. The remaining 27 data layers include annual 2012-2020 gridded methane emissions fluxes from individual emission sectors within each of the aggregate categories.\nFor more information regarding this dataset, please visit the U.S. Gridded Anthropogenic Methane Emissions Inventory data overview page.",
    "crumbs": [
      "Data Usage Notebooks",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "Leveraging the U.S. Gridded Anthropogenic Methane Emissions Inventory for Monitoring Trends in Methane Emissions"
    ]
  },
  {
    "objectID": "user_data_notebooks/epa-ch4emission-grid-v2express_User_Notebook.html#query-the-stac-api",
    "href": "user_data_notebooks/epa-ch4emission-grid-v2express_User_Notebook.html#query-the-stac-api",
    "title": "Leveraging the U.S. Gridded Anthropogenic Methane Emissions Inventory for Monitoring Trends in Methane Emissions",
    "section": "Query the STAC API",
    "text": "Query the STAC API\nFirst, you need to import the required libraries. Once imported, they allow better execution of a query in the GHG Center Spatio Temporal Asset Catalog (STAC) Application Programming Interface (API) where the granules for this collection are stored. You will learn the functionality of each library throughout the notebook.\n\n# Provide the STAC and RASTER API endpoints\n# The endpoint is referring to a location within the API that executes a request on a data collection nesting on the server.\n\n# The STAC API is a catalog of all the existing data collections that are stored in the GHG Center.\nSTAC_API_URL = \"https://earth.gov/ghgcenter/api/stac\"\n\n# The RASTER API is used to fetch collections for visualization\nRASTER_API_URL = \"https://earth.gov/ghgcenter/api/raster\"\n\nSTAC API Collection Names\nNow, you must fetch the dataset from the STAC API by defining its associated STAC API collection ID as a variable. The collection ID, also known as the collection name, for the U.S. Gridded Anthropogenic Methane Emissions Inventory dataset is epa-ch4emission-yeargrid-v2express\n\n# The collection name is used to fetch the dataset from the STAC API. First, we define the collection name as a variable\n# Name of the collection for gridded methane dataset \ncollection_name = \"epa-ch4emission-yeargrid-v2express\"\n\n# Fetch the collection from the STAC API using the appropriate endpoint\n# The 'requests' library allows a HTTP request possible\ncollection = requests.get(f\"{STAC_API_URL}/collections/{collection_name}\").json()\n\n# Print the properties of the collection in a table\n# Adjust display settings\npd.set_option('display.max_colwidth', None)  # Set maximum column width to \"None\" to prevent cutting off text\n\n# Extract the relevant information about the collection\ncollection_info = {\n    \"Title\": collection.get(\"title\", \"N/A\"), # Extract the title of the collection \n    \"Description\": collection.get(\"description\", \"N/A\"), # Extract the dataset description\n    \"Temporal Extent\": collection.get(\"extent\", {}).get(\"temporal\", {}).get(\"interval\", \"N/A\"), # Extract the temporal coverage of the collection\n    \"Spatial Extent\": collection.get(\"extent\", {}).get(\"spatial\", {}).get(\"bbox\", \"N/A\"), # Extract the spatial coverage of the collection\n}\n\n# Convert the derived information into a DataFrame format\nproperties_table = pd.DataFrame(list(collection_info.items()), columns=[\"Collection Summary\", \"\"])\n\n# Display the properties in a table\ncollection_summary = properties_table.style.set_properties(**{'text-align': 'left'}) \\\n                                           .set_table_styles([\n    {\n        'selector': 'th.col0, td.col0',    # Select the first column\n        'props': [('min-width', '200px'),  # Set a minimum width\n                  ('text-align', 'left')]  # Align text to the left\n    },\n    {\n        'selector': 'td.col1',             # Select the second column\n        'props': [('text-align', 'left')]  # Align text to the left\n    }\n])\n\n# Print the collection summary table\ncollection_summary\n\nNext, you will examine the contents of the collection under the temporal variable. You’ll see that the data is available from January 2012 to December 2020. By looking at the dashboard:time density, you can observe that the periodic frequency of these observations is yearly.\n\n# Create a function that would search for a data collection in the US GHG Center STAC API\n\n# First, we need to define the function\n# The name of the function = \"get_item_count\"\n# The argument that will be passed through the defined function = \"collection_id\"\ndef get_item_count(collection_id):\n\n    # Set a counter for the number of items existing in the collection\n    count = 0\n\n    # Define the path to retrieve the granules (items) of the collection of interest in the STAC API\n    items_url = f\"{STAC_API_URL}/collections/{collection_id}/items\"\n\n    # Run a while loop to make HTTP requests until there are no more URLs associated with the collection in the STAC API\n    while True:\n\n        # Retrieve information about the granules by sending a \"get\" request to the STAC API using the defined collection path\n        response = requests.get(items_url)\n\n        # If the items do not exist, print an error message and quit the loop\n        if not response.ok:\n            print(\"error getting items\")\n            exit()\n\n        # Return the results of the HTTP response as JSON\n        stac = response.json()\n\n        # Increase the \"count\" by the number of items (granules) returned in the response\n        count += int(stac[\"context\"].get(\"returned\", 0))\n\n        # Retrieve information about the next URL associated with the collection in the STAC API (if applicable)\n        next = [link for link in stac[\"links\"] if link[\"rel\"] == \"next\"]\n\n        # Exit the loop if there are no other URLs\n        if not next:\n            break\n        \n        # Ensure the information gathered by other STAC API links associated with the collection are added to the original path\n        # \"href\" is the identifier for each of the tiles stored in the STAC API\n        items_url = next[0][\"href\"]\n\n    # Return the information about the total number of granules found associated with the collection\n    return count\n\n\n# Apply the function created above \"get_item_count\" to the data collection\nnumber_of_items = get_item_count(collection_name)\n\n# Get the information about the number of granules found in the collection\nitems = requests.get(f\"{STAC_API_URL}/collections/{collection_name}/items?limit={number_of_items}\").json()[\"features\"]\n\n# Print the total number of items (granules) found\nprint(f\"Found {len(items)} items\")\n\n# Sort the items based on their date-time attribute\nitems_sorted = sorted(items, key=lambda x: x[\"properties\"][\"datetime\"])\n\n# Create an empty list\ntable_data = []\n# Extract the ID and date-time information for each granule and add them to the list\n# By default, only the first 5 items in the collection are extracted to be displayed in the table. \n# To see the date-time of all existing granules in this collection, remove \"5\" from \"item_sorted[:5]\" in the line below. \nfor item in items_sorted[:5]:\n    table_data.append([item['id'], item['properties']['datetime']])\n\n# Define the table headers\nheaders = [\"Item ID\", \"Start Date-Time\"]\n\nprint(\"Below you see the first 5 items in the collection, along with their item IDs and corresponding Start Date-Time.\")\n\n# Print the table using tabulate\nprint(tabulate(table_data, headers=headers, tablefmt=\"fancy_grid\"))\n\nThis makes sense as there are 9 years between 2012 - 2020, meaning 9 records in total.\n\n# Examine the first item in the collection\n# Keep in mind that a list starts from 0, 1, 2... therefore items[0] is referring to the first item in the list/collection\nitems_sorted[0]",
    "crumbs": [
      "Data Usage Notebooks",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "Leveraging the U.S. Gridded Anthropogenic Methane Emissions Inventory for Monitoring Trends in Methane Emissions"
    ]
  },
  {
    "objectID": "user_data_notebooks/epa-ch4emission-grid-v2express_User_Notebook.html#visual-comparison-across-time-periods",
    "href": "user_data_notebooks/epa-ch4emission-grid-v2express_User_Notebook.html#visual-comparison-across-time-periods",
    "title": "Leveraging the U.S. Gridded Anthropogenic Methane Emissions Inventory for Monitoring Trends in Methane Emissions",
    "section": "Visual Comparison Across Time Periods",
    "text": "Visual Comparison Across Time Periods\nIn this notebook, you will explore the impacts of methane emissions and by examining changes over time in urban regions. You will visualize the outputs on a map using folium.\n\n# Now we create a dictionary where the start datetime values for each granule is queried more explicitly by year and month (e.g., 2020-02)\nitems = {item[\"properties\"][\"datetime\"][:7]: item for item in items} \n\n# Next, we need to specify the asset name for this collection\n# The asset name is referring to the raster band containing the pixel values for the parameter of interest\n# For the case of the U.S. Gridded Anthropogenic Methane Emissions Inventory collection, the parameter of interest is “surface-coal”\nasset_name = \"surface-coal\"\n\nBelow, you will enter the minimum and maximum values to provide our upper and lower bounds in the rescale_values.\n\n# Fetching the min and max values for a specific item\nrescale_values = {\"max\":items[list(items.keys())[0]][\"assets\"][asset_name][\"raster:bands\"][0][\"histogram\"][\"max\"], \"min\":items[list(items.keys())[0]][\"assets\"][asset_name][\"raster:bands\"][0][\"histogram\"][\"min\"]}\n\nNow, you will pass the item id, collection name, asset name, and the rescaling factor to the Raster API endpoint. This step is done twice, once for January 2018 and again for January 2012, so that you can visualize each event independently.\n\n# Choose a color map for displaying the first observation (event)\n# Please refer to matplotlib library if you'd prefer choosing a different color ramp.\n# For more information on Colormaps in Matplotlib, please visit https://matplotlib.org/stable/users/explain/colors/colormaps.html\ncolor_map = \"rainbow\" \n\nobservation_date_1 = '2018'\n\n# Don't change anything here\nobservation_1 = f'epa-ch4emission-yeargrid-v2express-{observation_date_1}'\n\n# Make a GET request to retrieve information for the 2018 tile \njanuary_2018_tile = requests.get(\n\n    # Pass the collection name, the item number in the list, and its ID\n    (f\"{RASTER_API_URL}/collections/{collection_name}/items/{observation_1}/WebMercatorQuad/tilejson.json?\"\n\n    # Pass the asset name\n    f\"&assets={asset_name}\"\n\n    # Pass the color formula and colormap for custom visualization\n    f\"&color_formula=gamma+r+1.05&colormap_name={color_map}\"\n\n    # Pass the minimum and maximum values for rescaling\n    f\"&rescale={rescale_values['min']},{rescale_values['max']}\"), \n\n# Return the response in JSON format\n).json()\n\n# Print the properties of the retrieved granule to the console\njanuary_2018_tile\n\n\n# You will repeat the same approach used in the previous step to retrieve the second observation of interest\nobservation_date_2 = '2012'\n\n# Don't change anything here\nobservation_2 = f'epa-ch4emission-yeargrid-v2express-{observation_date_2}'\n\n# Make a GET request to retrieve information for the 2018 tile \njanuary_2012_tile = requests.get(\n\n    # Pass the collection name, the item number in the list, and its ID\n    (f\"{RASTER_API_URL}/collections/{collection_name}/items/{observation_2}/WebMercatorQuad/tilejson.json?\"\n\n    # Pass the asset name\n    f\"&assets={asset_name}\"\n\n    # Pass the color formula and colormap for custom visualization\n    f\"&color_formula=gamma+r+1.05&colormap_name={color_map}\"\n\n    # Pass the minimum and maximum values for rescaling\n    f\"&rescale={rescale_values['min']},{rescale_values['max']}\"), \n\n# Return the response in JSON format\n).json()\n\n# Print the properties of the retrieved granule to the console\njanuary_2012_tile",
    "crumbs": [
      "Data Usage Notebooks",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "Leveraging the U.S. Gridded Anthropogenic Methane Emissions Inventory for Monitoring Trends in Methane Emissions"
    ]
  },
  {
    "objectID": "user_data_notebooks/epa-ch4emission-grid-v2express_User_Notebook.html#map-out-selected-tiles",
    "href": "user_data_notebooks/epa-ch4emission-grid-v2express_User_Notebook.html#map-out-selected-tiles",
    "title": "Leveraging the U.S. Gridded Anthropogenic Methane Emissions Inventory for Monitoring Trends in Methane Emissions",
    "section": "Map Out Selected Tiles",
    "text": "Map Out Selected Tiles\n\n# Set initial zoom and center of map for CH₄ Layer\n# Centre of map [latitude,longitude]\n# 'folium.plugins' allows mapping side-by-side\nmap_ = folium.plugins.DualMap(location=(38.9, -80.0), zoom_start=8)\n\n# Define the first map layer (January 2018)\nmap_layer_2018 = TileLayer(\n    tiles=january_2018_tile[\"tiles\"][0], # Path to retrieve the tile\n    attr=\"GHG\", # Set the attribution\n    name='January 2018', # Title for the layer\n    overlay=True, # The layer can be overlaid on the map\n    opacity=0.7, # Adjust the transparency of the layer\n)\n\n# Add the first layer to the Dual Map\nmap_layer_2018.add_to(map_.m1)\n\n# Define the second map layer (January 2012)\nmap_layer_2012 = TileLayer(\n    tiles=january_2012_tile[\"tiles\"][0], # Path to retrieve the tile\n    attr=\"GHG\", # Set the attribution\n    name='January 2012', # Title for the layer\n    overlay=True, # The layer can be overlaid on the map\n    opacity=0.7, # Adjust the transparency of the layer\n)\n\n# Add the second layer to the Dual Map\nmap_layer_2012.add_to(map_.m2)\n\n# Display data markers (titles) on both maps\nfolium.Marker((40, 5.0), tooltip=\"both\").add_to(map_)\n\n# Add a layer control to switch between map layers\nfolium.LayerControl(collapsed=False).add_to(map_)\n\n# Visualize the Dual Map\nmap_",
    "crumbs": [
      "Data Usage Notebooks",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "Leveraging the U.S. Gridded Anthropogenic Methane Emissions Inventory for Monitoring Trends in Methane Emissions"
    ]
  },
  {
    "objectID": "user_data_notebooks/epa-ch4emission-grid-v2express_User_Notebook.html#time-series-analysis",
    "href": "user_data_notebooks/epa-ch4emission-grid-v2express_User_Notebook.html#time-series-analysis",
    "title": "Leveraging the U.S. Gridded Anthropogenic Methane Emissions Inventory for Monitoring Trends in Methane Emissions",
    "section": "Time-Series Analysis",
    "text": "Time-Series Analysis\nYou can now explore the gridded methane emission (Domestic Wastewater Treatment & Discharge (5D)) time series (January 2000 -December 2021) available for the Pittsburgh Pennsylvania area of the U.S. You can plot the data set using the code below:\n\n# Ensure 'datetime' column is in datetime format\ndf['datetime'] = pd.to_datetime(df['datetime'])\n\n# Sort the DataFrame by the datetime column so the plot displays the values from left to right (2020 -&gt; 2022)\ndf_sorted = df.sort_values(by=\"datetime\")\n\n# Figure size: 20 representing the width, 10 representing the height\nfig = plt.figure(figsize=(20, 10))\n\nplt.plot(\n    df[\"date\"], # X-axis: sorted date\n    df[\"max\"],  # Y-axis: maximum CH4 emission\n    color=\"red\", # Line color\n    linestyle=\"-\", # Line style\n    linewidth=0.5, # Line width\n    label=\"Max CH4 emissions\", # Legend label\n)\n\n# Display legend\nplt.legend()\n\n# Insert label for the X-axis\nplt.xlabel(\"Years\")\n\n# Insert label for the Y-axis\nplt.ylabel(\"CH4 emissions Molecules CH₄/cm²/s\")\n\n# Insert title for the plot\nplt.title(\"CH4 gridded methane emission from Domestic Wastewater Treatment & Discharge (5D) for Pittsburgh, PA (2012-2021)\")\n\n\n# Add data citation\nplt.text(\n    df_sorted[\"datetime\"].iloc[0],           # X-coordinate of the text \n    df_sorted[\"max\"].min(),                  # Y-coordinate of the text \n\n    # Text to be displayed\n    \"Source: EPA Gridded Anthropogenic Methane Emissions Inventory\",                   \n    fontsize=12,                             # Font size\n    horizontalalignment=\"left\",              # Horizontal alignment\n    verticalalignment=\"bottom\",              # Vertical alignment\n    color=\"blue\",                            # Text color\n)\n\n# Plot the time series\nplt.show()\n\n\n# Print the properties for the 3rd item in the collection\nprint(items[2][\"properties\"][\"datetime\"])\n\n\n# You will repeat the same approach used in the previous step to retrieve the second observation of interest\nobservation_date_3 = '2016'\n\n# Don't change anything here\nobservation_3 = f'epa-ch4emission-yeargrid-v2express-{observation_date_3}'\n\n# Make a GET request to retrieve information for the 2018 tile \ntile_2016 = requests.get(\n\n    # Pass the collection name, the item number in the list, and its ID\n    (f\"{RASTER_API_URL}/collections/{collection_name}/items/{observation_3}/WebMercatorQuad/tilejson.json?\"\n\n    # Pass the asset name\n    f\"&assets={asset_name}\"\n\n    # Pass the color formula and colormap for custom visualization\n    f\"&color_formula=gamma+r+1.05&colormap_name={color_map}\"\n\n    # Pass the minimum and maximum values for rescaling\n    f\"&rescale={rescale_values['min']},{rescale_values['max']}\"), \n\n# Return the response in JSON format\n).json()\n\n# Print the properties of the retrieved granule to the console\ntile_2016\n\n\n# Create a new map to display the 2016 tile\naoi_map_bbox = Map(\n\n    # Base map is set to OpenStreetMap\n    tiles=\"OpenStreetMap\",\n\n    # Set the center of the map\n    location=[\n        39.9,-79.4\n    ],\n\n    # Set the zoom value\n    zoom_start=9,\n)\n\n# Define the map layer\nmap_layer = TileLayer(\n\n    # Path to retrieve the tile\n    tiles=tile_2016[\"tiles\"][0],\n\n    # Set the attribution and adjust the transparency of the layer\n    attr=\"GHG\", opacity = 0.5\n)\n\n# Add the layer to the map\nmap_layer.add_to(aoi_map_bbox)\n\n# Visualize the map\naoi_map_bbox",
    "crumbs": [
      "Data Usage Notebooks",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "Leveraging the U.S. Gridded Anthropogenic Methane Emissions Inventory for Monitoring Trends in Methane Emissions"
    ]
  },
  {
    "objectID": "user_data_notebooks/epa-ch4emission-grid-v2express_User_Notebook.html#summary",
    "href": "user_data_notebooks/epa-ch4emission-grid-v2express_User_Notebook.html#summary",
    "title": "Leveraging the U.S. Gridded Anthropogenic Methane Emissions Inventory for Monitoring Trends in Methane Emissions",
    "section": "Summary",
    "text": "Summary\nIn this notebook we have successfully completed the following steps for the STAC collection for the U.S. Gridded Anthropogenic Methane Emissions Inventory dataset:\n\nInstall and import the necessary libraries\nFetch the collection from STAC collections using the appropriate endpoints\nCount the number of existing granules within the collection\nMap and compare the anthropogenic methane emissions for two distinctive months/years\nGenerate zonal statistics for the area of interest (AOI)\nGenerate a time-series graph of the anthropogenic methane emissions for a specified region\n\nIf you have any questions regarding this user notebook, please contact us using the feedback form.",
    "crumbs": [
      "Data Usage Notebooks",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "Leveraging the U.S. Gridded Anthropogenic Methane Emissions Inventory for Monitoring Trends in Methane Emissions"
    ]
  },
  {
    "objectID": "user_data_notebooks/odiac-ffco2-monthgrid-v2022_User_Notebook.html",
    "href": "user_data_notebooks/odiac-ffco2-monthgrid-v2022_User_Notebook.html",
    "title": "ODIAC Fossil Fuel CO₂ Emissions",
    "section": "",
    "text": "You can launch this notebook in the US GHG Center JupyterHub by clicking the link below.\nLaunch in the US GHG Center JupyterHub (requires access)"
  },
  {
    "objectID": "user_data_notebooks/odiac-ffco2-monthgrid-v2022_User_Notebook.html#run-this-notebook",
    "href": "user_data_notebooks/odiac-ffco2-monthgrid-v2022_User_Notebook.html#run-this-notebook",
    "title": "ODIAC Fossil Fuel CO₂ Emissions",
    "section": "",
    "text": "You can launch this notebook in the US GHG Center JupyterHub by clicking the link below.\nLaunch in the US GHG Center JupyterHub (requires access)"
  },
  {
    "objectID": "user_data_notebooks/odiac-ffco2-monthgrid-v2022_User_Notebook.html#approach",
    "href": "user_data_notebooks/odiac-ffco2-monthgrid-v2022_User_Notebook.html#approach",
    "title": "ODIAC Fossil Fuel CO₂ Emissions",
    "section": "Approach",
    "text": "Approach\n\nIdentify available dates and temporal frequency of observations for the given collection using the GHGC API /stac endpoint. Collection processed in this notebook is ODIAC CO₂ emissions version 2022.\nPass the STAC item into raster API /stac/tilejson.json endpoint\nWe’ll visualize two tiles (side-by-side) allowing for comparison of each of the time points using folium.plugins.DualMap\nAfter the visualization, we’ll perform zonal statistics for a given polygon."
  },
  {
    "objectID": "user_data_notebooks/odiac-ffco2-monthgrid-v2022_User_Notebook.html#about-the-data",
    "href": "user_data_notebooks/odiac-ffco2-monthgrid-v2022_User_Notebook.html#about-the-data",
    "title": "ODIAC Fossil Fuel CO₂ Emissions",
    "section": "About the Data",
    "text": "About the Data\nThe Open-Data Inventory for Anthropogenic Carbon dioxide (ODIAC) is a high-spatial resolution global emission data product of CO₂ emissions from fossil fuel combustion (Oda and Maksyutov, 2011). ODIAC pioneered the combined use of space-based nighttime light data and individual power plant emission/location profiles to estimate the global spatial extent of fossil fuel CO₂ emissions. With the innovative emission modeling approach, ODIAC achieved the fine picture of global fossil fuel CO₂ emissions at a 1x1km.\nFor more information regarding this dataset, please visit the ODIAC Fossil Fuel CO₂ Emissions data overview page."
  },
  {
    "objectID": "user_data_notebooks/odiac-ffco2-monthgrid-v2022_User_Notebook.html#querying-the-stac-api",
    "href": "user_data_notebooks/odiac-ffco2-monthgrid-v2022_User_Notebook.html#querying-the-stac-api",
    "title": "ODIAC Fossil Fuel CO₂ Emissions",
    "section": "Querying the STAC API",
    "text": "Querying the STAC API\nFirst, we are going to import the required libraries. Once imported, they allow better executing a query in the GHG Center Spatio Temporal Asset Catalog (STAC) Application Programming Interface (API) where the granules for this collection are stored.\n\n# Import the following libraries\nimport requests\nimport folium\nimport folium.plugins\nfrom folium import Map, TileLayer\nfrom pystac_client import Client\nimport branca\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n\n# Provide the STAC and RASTER API endpoints\n# The endpoint is referring to a location within the API that executes a request on a data collection nesting on the server.\n\n# The STAC API is a catalog of all the existing data collections that are stored in the GHG Center.\nSTAC_API_URL = \"https://earth.gov/ghgcenter/api/stac\"\n\n# The RASTER API is used to fetch collections for visualization\nRASTER_API_URL = \"https://earth.gov/ghgcenter/api/raster\"\n\n# The collection name is used to fetch the dataset from the STAC API. First, we define the collection name as a variable\n# Name of the collection for ODIAC dataset \ncollection_name = \"odiac-ffco2-monthgrid-v2022\"\n\n\n# Fetch the collection from the STAC API using the appropriate endpoint\n# The 'requests' library allows a HTTP request possible\ncollection = requests.get(f\"{STAC_API_URL}/collections/{collection_name}\").json()\n\n# Print the properties of the collection to the console\ncollection\n\nExamining the contents of our collection under summaries we see that the data is available from January 2000 to December 2021. By looking at the dashboard:time density we observe that the periodic frequency of these observations is monthly.\n\n# Create a function that would search for a data collection in the US GHG Center STAC API\n\n# First, we need to define the function\n# The name of the function = \"get_item_count\"\n# The argument that will be passed through the defined function = \"collection_id\"\ndef get_item_count(collection_id):\n\n    # Set a counter for the number of items existing in the collection\n    count = 0\n\n    # Define the path to retrieve the granules (items) of the collection of interest in the STAC API\n    items_url = f\"{STAC_API_URL}/collections/{collection_id}/items\"\n\n    # Run a while loop to make HTTP requests until there are no more URLs associated with the collection in the STAC API\n    while True:\n\n        # Retrieve information about the granules by sending a \"get\" request to the STAC API using the defined collection path\n        response = requests.get(items_url)\n\n        # If the items do not exist, print an error message and quit the loop\n        if not response.ok:\n            print(\"error getting items\")\n            exit()\n\n        # Return the results of the HTTP response as JSON\n        stac = response.json()\n\n        # Increase the \"count\" by the number of items (granules) returned in the response\n        count += int(stac[\"context\"].get(\"returned\", 0))\n\n        # Retrieve information about the next URL associated with the collection in the STAC API (if applicable)\n        next = [link for link in stac[\"links\"] if link[\"rel\"] == \"next\"]\n\n        # Exit the loop if there are no other URLs\n        if not next:\n            break\n        \n        # Ensure the information gathered by other STAC API links associated with the collection are added to the original path\n        # \"href\" is the identifier for each of the tiles stored in the STAC API\n        items_url = next[0][\"href\"]\n\n    # Return the information about the total number of granules found associated with the collection\n    return count\n\n\n# Apply the function created above \"get_item_count\" to the data collection\nnumber_of_items = get_item_count(collection_name)\n\n# Get the information about the number of granules found in the collection\nitems = requests.get(f\"{STAC_API_URL}/collections/{collection_name}/items?limit={number_of_items}\").json()[\"features\"]\n\n# Print the total number of items (granules) found\nprint(f\"Found {len(items)} items\")\n\nThis makes sense as there are 22 years between 2000 - 2021, with 12 months per year, meaning 264 records in total.\n\n# Examine the first item in the collection\n# Keep in mind that a list starts from 0, 1, 2... therefore items[0] is referring to the first item in the list/collection\nitems[0]"
  },
  {
    "objectID": "user_data_notebooks/odiac-ffco2-monthgrid-v2022_User_Notebook.html#exploring-changes-in-carbon-dioxide-co₂-levels-using-the-raster-api",
    "href": "user_data_notebooks/odiac-ffco2-monthgrid-v2022_User_Notebook.html#exploring-changes-in-carbon-dioxide-co₂-levels-using-the-raster-api",
    "title": "ODIAC Fossil Fuel CO₂ Emissions",
    "section": "Exploring Changes in Carbon Dioxide (CO₂) levels using the Raster API",
    "text": "Exploring Changes in Carbon Dioxide (CO₂) levels using the Raster API\nWe will explore changes in fossil fuel emissions in urban egions. In this notebook, we’ll explore the impacts of these emissions and explore these changes over time. We’ll then visualize the outputs on a map using folium.\n\n# Now we create a dictionary where the start datetime values for each granule is queried more explicitly by year and month (e.g., 2020-02)\nitems = {item[\"properties\"][\"start_datetime\"][:7]: item for item in items} \n\n# Next, we need to specify the asset name for this collection\n# The asset name is referring to the raster band containing the pixel values for the parameter of interest\n# For the case of the ODIAC Fossil Fuel CO₂ Emissions collection, the parameter of interest is “co2-emissions”\nasset_name = \"co2-emissions\"\n\nBelow, we are entering the minimum and maximum values to provide our upper and lower bounds in rescale_values.\n\n# Fetching the min and max values for a specific item\nrescale_values = {\"max\":items[list(items.keys())[0]][\"assets\"][asset_name][\"raster:bands\"][0][\"histogram\"][\"max\"], \"min\":items[list(items.keys())[0]][\"assets\"][asset_name][\"raster:bands\"][0][\"histogram\"][\"min\"]}\n\nNow, we will pass the item id, collection name, asset name, and the rescaling factor to the Raster API endpoint. We will do this twice, once for January 2020 and again for January 2000, so that we can visualize each event independently.\n\n# Choose a color map for displaying the first observation (event)\n# Please refer to matplotlib library if you'd prefer choosing a different color ramp.\n# For more information on Colormaps in Matplotlib, please visit https://matplotlib.org/stable/users/explain/colors/colormaps.html\ncolor_map = \"rainbow\" \n\n# Make a GET request to retrieve information for the 2020 tile\n# 2020\njanuary_2020_tile = requests.get(\n\n    # Pass the collection name, the item number in the list, and its ID\n    f\"{RASTER_API_URL}/stac/tilejson.json?collection={items['2020-01']['collection']}&item={items['2020-01']['id']}\"\n\n    # Pass the asset name\n    f\"&assets={asset_name}\"\n\n    # Pass the color formula and colormap for custom visualization\n    f\"&color_formula=gamma+r+1.05&colormap_name={color_map}\"\n\n    # Pass the minimum and maximum values for rescaling\n    f\"&rescale={rescale_values['min']},{rescale_values['max']}\", \n\n# Return the response in JSON format\n).json()\n\n# Print the properties of the retrieved granule to the console\njanuary_2020_tile\n\n\n# Make a GET request to retrieve information for the 2000 tile\n# 2000\njanuary_2000_tile = requests.get(\n\n    # Pass the collection name, the item number in the list, and its ID\n    f\"{RASTER_API_URL}/stac/tilejson.json?collection={items['2000-01']['collection']}&item={items['2000-01']['id']}\"\n\n    # Pass the asset name\n    f\"&assets={asset_name}\"\n\n    # Pass the color formula and colormap for custom visualization\n    f\"&color_formula=gamma+r+1.05&colormap_name={color_map}\"\n\n    # Pass the minimum and maximum values for rescaling\n    f\"&rescale={rescale_values['min']},{rescale_values['max']}\", \n\n# Return the response in JSON format\n).json()\n\n# Print the properties of the retrieved granule to the console\njanuary_2000_tile"
  },
  {
    "objectID": "user_data_notebooks/odiac-ffco2-monthgrid-v2022_User_Notebook.html#visualizing-co₂-emissions",
    "href": "user_data_notebooks/odiac-ffco2-monthgrid-v2022_User_Notebook.html#visualizing-co₂-emissions",
    "title": "ODIAC Fossil Fuel CO₂ Emissions",
    "section": "Visualizing CO₂ emissions",
    "text": "Visualizing CO₂ emissions\n\n# To change the location, you can simply insert the latitude and longitude of the area of your interest in the \"location=(LAT, LONG)\" statement\n\n# Set the initial zoom level and center of map for both tiles\n# 'folium.plugins' allows mapping side-by-side\nmap_ = folium.plugins.DualMap(location=(34, -118), zoom_start=6)\n\n# Define the first map layer (January 2020)\nmap_layer_2020 = TileLayer(\n    tiles=january_2020_tile[\"tiles\"][0], # Path to retrieve the tile\n    attr=\"GHG\", # Set the attribution\n    opacity=0.8, # Adjust the transparency of the layer\n)\n\n# Add the first layer to the Dual Map\nmap_layer_2020.add_to(map_.m1)\n\n# Define the second map layer (January 2000)\nmap_layer_2000 = TileLayer(\n    tiles=january_2000_tile[\"tiles\"][0], # Path to retrieve the tile\n    attr=\"GHG\", # Set the attribution\n    opacity=0.8, # Adjust the transparency of the layer\n)\n\n# Add the second layer to the Dual Map\nmap_layer_2000.add_to(map_.m2)\n\n# Visualize the Dual Map\nmap_"
  },
  {
    "objectID": "user_data_notebooks/odiac-ffco2-monthgrid-v2022_User_Notebook.html#visualizing-the-data-as-a-time-series",
    "href": "user_data_notebooks/odiac-ffco2-monthgrid-v2022_User_Notebook.html#visualizing-the-data-as-a-time-series",
    "title": "ODIAC Fossil Fuel CO₂ Emissions",
    "section": "Visualizing the Data as a Time Series",
    "text": "Visualizing the Data as a Time Series\nWe can now explore the ODIAC fossil fuel emission time series available (January 2000 -December 2021) for the Texas, Dallas area of USA. We can plot the data set using the code below:\n\n# Figure size: 20 representing the width, 10 representing the height\nfig = plt.figure(figsize=(20, 10))\n\n\nplt.plot(\n    df[\"date\"], # X-axis: sorted datetime\n    df[\"max\"], # Y-axis: maximum CO₂ level\n    color=\"red\", # Line color\n    linestyle=\"-\", # Line style\n    linewidth=0.5, # Line width\n    label=\"Max monthly CO₂ emissions\", # Legend label\n)\n\n# Display legend\nplt.legend()\n\n# Insert label for the X-axis\nplt.xlabel(\"Years\")\n\n# Insert label for the Y-axis\nplt.ylabel(\"CO2 emissions gC/m2/d\")\n\n# Insert title for the plot\nplt.title(\"CO2 emission Values for Texas, Dallas (2000-2021)\")\n\n###\n# Add data citation\nplt.text(\n    df[\"date\"].iloc[0],           # X-coordinate of the text\n    df[\"max\"].min(),              # Y-coordinate of the text\n\n\n\n\n    # Text to be displayed\n    \"Source: NASA ODIAC Fossil Fuel CO₂ Emissions\",                  \n    fontsize=12,                             # Font size\n    horizontalalignment=\"right\",             # Horizontal alignment\n    verticalalignment=\"top\",                 # Vertical alignment\n    color=\"blue\",                            # Text color\n)\n\n# Plot the time series\nplt.show()\n\n\n# Print the properties of the 3rd item in the collection\nprint(items[2][\"properties\"][\"start_datetime\"])\n\n\n# A GET request is made for the October tile\noctober_tile = requests.get(\n\n    # Pass the collection name, the item number in the list, and its ID\n    f\"{RASTER_API_URL}/stac/tilejson.json?collection={items[2]['collection']}&item={items[2]['id']}\"\n\n    # Pass the asset name\n    f\"&assets={asset_name}\"\n\n    # Pass the color formula and colormap for custom visualization\n    f\"&color_formula=gamma+r+1.05&colormap_name={color_map}\"\n\n    # Pass the minimum and maximum values for rescaling\n    f\"&rescale={rescale_values['min']},{rescale_values['max']}\",\n\n# Return the response in JSON format\n).json()\n\n# Print the properties of the retrieved granule to the console\noctober_tile\n\n\n# Create a new map to display the October tile\naoi_map_bbox = Map(\n\n    # Base map is set to OpenStreetMap\n    tiles=\"OpenStreetMap\",\n\n    # Set the center of the map\n    location=[\n        30,-100\n    ],\n\n    # Set the zoom value\n    zoom_start=8,\n)\n\n# Define the map layer\nmap_layer = TileLayer(\n\n    # Path to retrieve the tile\n    tiles=october_tile[\"tiles\"][0],\n\n    # Set the attribution and adjust the transparency of the layer\n    attr=\"GHG\", opacity = 0.5\n)\n\n# Add the layer to the map\nmap_layer.add_to(aoi_map_bbox)\n\n# Visualize the map\naoi_map_bbox"
  },
  {
    "objectID": "user_data_notebooks/odiac-ffco2-monthgrid-v2022_User_Notebook.html#summary",
    "href": "user_data_notebooks/odiac-ffco2-monthgrid-v2022_User_Notebook.html#summary",
    "title": "ODIAC Fossil Fuel CO₂ Emissions",
    "section": "Summary",
    "text": "Summary\nIn this notebook we have successfully explored, analysed and visualized STAC collecetion for ODIAC C02 fossisl fuel emission (2022).\n\nInstall and import the necessary libraries\nFetch the collection from STAC collections using the appropriate endpoints\nCount the number of existing granules within the collection\nMap and compare the CO₂ levels for two distinctive months/years\nGenerate zonal statistics for the area of interest (AOI)\nVisualizing the Data as a Time Series\n\nIf you have any questions regarding this user notebook, please contact us using the feedback form."
  },
  {
    "objectID": "user_data_notebooks/micasa-carbonflux-daygrid-v1_User_Notebook.html",
    "href": "user_data_notebooks/micasa-carbonflux-daygrid-v1_User_Notebook.html",
    "title": "MiCASA Land Carbon Flux",
    "section": "",
    "text": "You can launch this notebook in the US GHG Center JupyterHub by clicking the link below.\nLaunch in the US GHG Center JupyterHub (requires access)",
    "crumbs": [
      "Data Usage Notebooks",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "MiCASA Land Carbon Flux"
    ]
  },
  {
    "objectID": "user_data_notebooks/micasa-carbonflux-daygrid-v1_User_Notebook.html#run-this-notebook",
    "href": "user_data_notebooks/micasa-carbonflux-daygrid-v1_User_Notebook.html#run-this-notebook",
    "title": "MiCASA Land Carbon Flux",
    "section": "",
    "text": "You can launch this notebook in the US GHG Center JupyterHub by clicking the link below.\nLaunch in the US GHG Center JupyterHub (requires access)",
    "crumbs": [
      "Data Usage Notebooks",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "MiCASA Land Carbon Flux"
    ]
  },
  {
    "objectID": "user_data_notebooks/micasa-carbonflux-daygrid-v1_User_Notebook.html#approach",
    "href": "user_data_notebooks/micasa-carbonflux-daygrid-v1_User_Notebook.html#approach",
    "title": "MiCASA Land Carbon Flux",
    "section": "Approach",
    "text": "Approach\n\nIdentify available dates and temporal frequency of observations for a given collection using the GHGC API /stac endpoint. The collection processed in this notebook is the Land-Atmosphere Carbon Flux data product\nPass the STAC item into the raster API /stac/tilejson.json endpoint\nUsing folium.plugins.DualMap, visualize two tiles (side-by-side), allowing time point comparison\nAfter the visualization, perform zonal statistics for a given polygon",
    "crumbs": [
      "Data Usage Notebooks",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "MiCASA Land Carbon Flux"
    ]
  },
  {
    "objectID": "user_data_notebooks/micasa-carbonflux-daygrid-v1_User_Notebook.html#about-the-data",
    "href": "user_data_notebooks/micasa-carbonflux-daygrid-v1_User_Notebook.html#about-the-data",
    "title": "MiCASA Land Carbon Flux",
    "section": "About the Data",
    "text": "About the Data\nThis dataset presents a variety of carbon flux parameters derived from the Más Informada Carnegie-Ames-Stanford-Approach (MiCASA) model. The model’s input data includes air temperature, precipitation, incident solar radiation, a soil classification map, and several satellite derived products. All model calculations are driven by analyzed meteorological data from NASA’s Modern-Era Retrospective analysis for Research and Application, Version 2 (MERRA-2). The resulting product provides global, daily data at 0.1 degree resolution from January 2001 through December 2023. It includes carbon flux variables expressed in units of kilograms of carbon per square meter per day (kg Carbon/m²/day) from net primary production (NPP), heterotrophic respiration (Rh), wildfire emissions (FIRE), fuel wood burning emissions (FUEL), net ecosystem exchange (NEE), and net biosphere exchange (NBE). The latter two are derived from the first four (see Scientific Details below). MiCASA is an extensive revision of the CASA – Global Fire Emissions Database, version 3 (CASA-GFED3) product. CASA-GFED3 and earlier versions of MERRA-driven CASA-GFED carbon fluxes have been used in several atmospheric carbon dioxide (CO₂) transport studies, serve as a community standard for priors of flux inversion systems, and through the support of NASA’s Carbon Monitoring System (CMS), help characterize, quantify, understand and predict the evolution of global carbon sources and sinks.\nFor more information regarding this dataset, please visit the U.S. Greenhouse Gas Center.",
    "crumbs": [
      "Data Usage Notebooks",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "MiCASA Land Carbon Flux"
    ]
  },
  {
    "objectID": "user_data_notebooks/micasa-carbonflux-daygrid-v1_User_Notebook.html#query-the-stac-api",
    "href": "user_data_notebooks/micasa-carbonflux-daygrid-v1_User_Notebook.html#query-the-stac-api",
    "title": "MiCASA Land Carbon Flux",
    "section": "Query the STAC API",
    "text": "Query the STAC API\nFirst, we are going to import the required libraries. Once imported, they allow better executing a query in the GHG Center Spatio Temporal Asset Catalog (STAC) Application Programming Interface (API) where the granules for this collection are stored.\n\n# Import the following libraries\nimport requests\nimport folium\nimport folium.plugins\nfrom folium import Map, TileLayer\nfrom pystac_client import Client\nimport branca\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n\n# Provide the STAC and RASTER API endpoints\n# The endpoint is referring to a location within the API that executes a request on a data collection nesting on the server.\n\n# The STAC API is a catalog of all the existing data collections that are stored in the GHG Center.\nSTAC_API_URL = \"https://earth.gov/ghgcenter/api/stac\"\n\n# The RASTER API is used to fetch collections for visualization\nRASTER_API_URL = \"https://earth.gov/ghgcenter/api/raster\"\n\n# The collection name is used to fetch the dataset from the STAC API. First, we define the collection name as a variable\n# Name of the collection for MiCASA Land Carbon Flux\ncollection_name = \"micasa-carbonflux-daygrid-v1\"\n\n# Next, we need to specify the asset name for this collection\n# The asset name is referring to the raster band containing the pixel values for the parameter of interest\n# For the case of the MiCASA Land Carbon Flux collection, the parameter of interest is “rh”\n# rh = Heterotrophic Respiration\nasset_name = \"rh\"\n\n\n# Fetch the collection from the STAC API using the appropriate endpoint\n# The 'requests' library allows a HTTP request possible\ncollection = requests.get(f\"{STAC_API_URL}/collections/{collection_name}\").json()\n\n# Print the properties of the collection to the console\ncollection\n\nExamining the contents of our collection under the temporal variable, we see that the data is available from January 2003 to December 2017. By looking at the dashboard:time density, we observe that the periodic frequency of these observations is monthly.\n\n# Create a function that would search for a data collection in the US GHG Center STAC API\n\n# First, we need to define the function\n# The name of the function = \"get_item_count\"\n# The argument that will be passed through the defined function = \"collection_id\"\ndef get_item_count(collection_id):\n   \n    # Set a counter for the number of items existing in the collection\n    count = 0\n\n    # Define the path to retrieve the granules (items) of the collection of interest (MiCASA Land Carbon Flux) in the STAC API\n    items_url = f\"{STAC_API_URL}/collections/{collection_id}/items\"\n\n    # Run a while loop to make HTTP requests until there are no more URLs associated with the collection in the STAC API\n    while True:\n\n        # Retrieve information about the granules by sending a \"get\" request to the STAC API using the defined collection path\n        response = requests.get(items_url)\n\n        # If the items do not exist, print an error message and quit the loop\n        if not response.ok:\n            print(\"error getting items\")\n            exit()\n\n        # Return the results of the HTTP response as JSON\n        stac = response.json()\n       \n        # Increase the \"count\" by the number of items (granules) returned in the response\n        count += int(stac[\"context\"].get(\"returned\", 0))\n\n        # Retrieve information about the next URL associated with the collection (MiCASA Land Carbon Flux) in the STAC API (if applicable)\n        next = [link for link in stac[\"links\"] if link[\"rel\"] == \"next\"]\n\n        # Exit the loop if there are no other URLs\n        if not next:\n            break\n       \n        # Ensure the information gathered by other STAC API links associated with the collection are added to the original path\n        # \"href\" is the identifier for each of the tiles stored in the STAC API\n        items_url = next[0][\"href\"]\n        # temp = items_url.split('/')\n        # temp.insert(3, 'ghgcenter')\n        # temp.insert(4, 'api')\n        # temp.insert(5, 'stac')\n        # items_url = '/'.join(temp)\n\n    # Return the information about the total number of granules found associated with the collection (MiCASA Land Carbon Flux)\n    return count\n\n\n# Apply the function created above \"get_item_count\" to the data collection\nnumber_of_items = get_item_count(collection_name)\n\n# Get the information about the number of granules found in the collection\nitems = requests.get(f\"{STAC_API_URL}/collections/{collection_name}/items?limit=800\").json()[\"features\"]\n\n# Print the total number of items (granules) found\nprint(f\"Found {len(items)} items\")\n\nFound 800 items\n\n\n\n# Examine the first item in the collection\n# Keep in mind that a list starts from 0, 1, 2... therefore items[0] is referring to the first item in the list/collection\nitems[0]",
    "crumbs": [
      "Data Usage Notebooks",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "MiCASA Land Carbon Flux"
    ]
  },
  {
    "objectID": "user_data_notebooks/micasa-carbonflux-daygrid-v1_User_Notebook.html#explore-changes-in-carbon-flux-levels-using-the-raster-api",
    "href": "user_data_notebooks/micasa-carbonflux-daygrid-v1_User_Notebook.html#explore-changes-in-carbon-flux-levels-using-the-raster-api",
    "title": "MiCASA Land Carbon Flux",
    "section": "Explore Changes in Carbon Flux Levels Using the Raster API",
    "text": "Explore Changes in Carbon Flux Levels Using the Raster API\nWe will explore changes in the land atmosphere Carbon flux Heterotrophic Respiration and examine their impacts over time. We’ll then visualize the outputs on a map using folium.\n\n# Now we create a dictionary where the start datetime values for each granule is queried more explicitly by year and month (e.g., 2020-02)\nitems = {item[\"properties\"][\"datetime\"][:10]: item for item in items}\n\nBelow, we are entering the minimum and maximum values to provide our upper and lower bounds in the rescale_values.\n\n# Fetch the minimum and maximum values for rescaling\nrescale_values = {\"max\":items[list(items.keys())[0]][\"assets\"][asset_name][\"raster:bands\"][0][\"histogram\"][\"max\"], \"min\":items[list(items.keys())[0]][\"assets\"][asset_name][\"raster:bands\"][0][\"histogram\"][\"min\"]}\n\nNow, we will pass the item id, collection name, asset name, and the rescaling factor to the Raster API endpoint. This step is done twice, once for December 2003 and again for December 2017, so that we can visualize each event independently.\n\n# Choose a color for displaying the tiles\n# Please refer to matplotlib library if you'd prefer choosing a different color ramp.\n# For more information on Colormaps in Matplotlib, please visit https://matplotlib.org/stable/users/explain/colors/colormaps.html\ncolor_map = \"purd\"\n\n# Make a GET request to retrieve information for the date mentioned below\ndate1 = '2023-01-01'\ndate1_tile = requests.get(\n\n    # Pass the collection name, collection date, and its ID\n    # To change the year, month and date of the observed parameter, you can modify the date mentioned above.\n    f\"{RASTER_API_URL}/stac/tilejson.json?collection={items[date1]['collection']}&item={items[date1]['id']}\"\n\n    # Pass the asset name\n    f\"&assets={asset_name}\"\n\n    # Pass the color formula and colormap for custom visualization\n    f\"&color_formula=gamma+r+1.05&colormap_name={color_map}\"\n\n    # Pass the minimum and maximum values for rescaling\n    f\"&rescale={rescale_values['min']},{rescale_values['max']}\",\n\n# Return response in JSON format\n).json()\n\n# Print the properties of the retrieved granule to the console\ndate1_tile\n\n{'tilejson': '2.2.0',\n 'version': '1.0.0',\n 'scheme': 'xyz',\n 'tiles': ['https://earth.gov/ghgcenter/api/raster/stac/tiles/WebMercatorQuad/{z}/{x}/{y}@1x?collection=micasa-carbonflux-daygrid-v1&item=micasa-carbonflux-daygrid-v1-20230101&assets=rh&color_formula=gamma+r+1.05&colormap_name=purd&rescale=-0.32319876551628113%2C5.9415082931518555'],\n 'minzoom': 0,\n 'maxzoom': 24,\n 'bounds': [-180.0, -90.0, 179.99999999999994, 90.0],\n 'center': [-2.842170943040401e-14, 0.0, 0]}\n\n\n\n# Make a GET request to retrieve information for the date mentioned below\ndate2 = '2023-01-31'\ndate2_tile = requests.get(\n\n    # Pass the collection name, collection date, and its ID\n    # To change the year, month and date of the observed parameter, you can modify the date mentioned above.\n    f\"{RASTER_API_URL}/stac/tilejson.json?collection={items[date2]['collection']}&item={items[date2]['id']}\"\n\n    # Pass the asset name\n    f\"&assets={asset_name}\"\n\n    # Pass the color formula and colormap for custom visualization\n    f\"&color_formula=gamma+r+1.05&colormap_name={color_map}\"\n\n    # Pass the minimum and maximum values for rescaling\n    f\"&rescale={rescale_values['min']},{rescale_values['max']}\", \n\n# Return response in JSON format\n).json()\n\n# Print the properties of the retrieved granule to the console\ndate2_tile\n\n{'tilejson': '2.2.0',\n 'version': '1.0.0',\n 'scheme': 'xyz',\n 'tiles': ['https://earth.gov/ghgcenter/api/raster/stac/tiles/WebMercatorQuad/{z}/{x}/{y}@1x?collection=micasa-carbonflux-daygrid-v1&item=micasa-carbonflux-daygrid-v1-20230131&assets=rh&color_formula=gamma+r+1.05&colormap_name=purd&rescale=-0.32319876551628113%2C5.9415082931518555'],\n 'minzoom': 0,\n 'maxzoom': 24,\n 'bounds': [-180.0, -90.0, 179.99999999999994, 90.0],\n 'center': [-2.842170943040401e-14, 0.0, 0]}",
    "crumbs": [
      "Data Usage Notebooks",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "MiCASA Land Carbon Flux"
    ]
  },
  {
    "objectID": "user_data_notebooks/micasa-carbonflux-daygrid-v1_User_Notebook.html#visualize-land-atmosphere-carbon-flux-heterotrophic-respiration",
    "href": "user_data_notebooks/micasa-carbonflux-daygrid-v1_User_Notebook.html#visualize-land-atmosphere-carbon-flux-heterotrophic-respiration",
    "title": "MiCASA Land Carbon Flux",
    "section": "Visualize Land-Atmosphere Carbon Flux (Heterotrophic Respiration)",
    "text": "Visualize Land-Atmosphere Carbon Flux (Heterotrophic Respiration)\n\n# For this study we are going to compare the Rh level for date1 and date2 over the State of Texas \n# To change the location, you can simply insert the latitude and longitude of the area of your interest in the \"location=(LAT, LONG)\" statement\n# For example, you can change the current statement \"location=(31.9, -99.9)\" to \"location=(34, -118)\" to monitor the Rh level in California instead of Texas\n\n# Set initial zoom and center of map for CO₂ Layer\n# 'folium.plugins' allows mapping side-by-side\nmap_ = folium.plugins.DualMap(location=(31.9, -99.9), zoom_start=6)\n\n\n# Define the first map layer with Rh level for the tile fetched for date 1\n# The TileLayer library helps in manipulating and displaying raster layers on a map\nmap_layer_date1 = TileLayer(\n    tiles=date1_tile[\"tiles\"][0], # Path to retrieve the tile\n    attr=\"GHG\", # Set the attribution\n    opacity=0.8, # Adjust the transparency of the layer\n    name=f\"{date1} Rh Level\", # Title for the layer\n    overlay= True, # The layer can be overlaid on the map\n    legendEnabled = True # Enable displaying the legend on the map\n)\n\n# Add the first layer to the Dual Map\nmap_layer_date1.add_to(map_.m1)\n\n\n# Define the first map layer with Rh level for the tile fetched for date 2\nmap_layer_date2 = TileLayer(\n    tiles=date2_tile[\"tiles\"][0], # Path to retrieve the tile\n    attr=\"GHG\", # Set the attribution\n    opacity=0.8, # Adjust the transparency of the layer\n    name=f\"{date2} RH Level\", # Title for the layer\n    overlay= True, # The layer can be overlaid on the map\n    legendEnabled = True # Enable displaying the legend on the map\n)\n\n# Add the second layer to the Dual Map\nmap_layer_date2.add_to(map_.m2)\n\n# Display data markers (titles) on both maps\nfolium.Marker((40, 5.0), tooltip=\"both\").add_to(map_)\n\n# Add a layer control to switch between map layers\nfolium.LayerControl(collapsed=False).add_to(map_)\n\n# Add a legend to the dual map using the 'branca' library. \n# Note: the inserted legend is representing the minimum and maximum values for both tiles.\ncolormap = branca.colormap.linear.PuRd_09.scale(0, 0.3) # minimum value = 0, maximum value = 0.3 (kg Carbon/m2/daily)\n\n# Classify the colormap according to specified Rh values \ncolormap = colormap.to_step(index=[0, 0.07, 0.15, 0.22, 0.3])\n\n# Add the data unit as caption\ncolormap.caption = 'Rh Values (gm Carbon/m2/daily)'\n\n# Display the legend and caption on the map\ncolormap.add_to(map_.m1)\n\n# Visualize the Dual Map\nmap_\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook",
    "crumbs": [
      "Data Usage Notebooks",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "MiCASA Land Carbon Flux"
    ]
  },
  {
    "objectID": "user_data_notebooks/micasa-carbonflux-daygrid-v1_User_Notebook.html#generate-the-statistics-for-the-aoi",
    "href": "user_data_notebooks/micasa-carbonflux-daygrid-v1_User_Notebook.html#generate-the-statistics-for-the-aoi",
    "title": "MiCASA Land Carbon Flux",
    "section": "Generate the statistics for the AOI",
    "text": "Generate the statistics for the AOI\n\n%%time\n# %%time = Wall time (execution time) for running the code below\n\n# Generate statistics using the created function \"generate_stats\" within the bounding box defined by the \"texas_dallas_aoi\" polygon\nstats = [generate_stats(item, texas_dallas_aoi) for item in items]\n\n\n# Print the stats for the first item in the collection\nstats[0]\n\n{'statistics': {'b1': {'min': 0.11864250898361206,\n   'max': 1.3311004638671875,\n   'mean': 0.7455709838867187,\n   'count': 150.0,\n   'sum': 111.83564758300781,\n   'std': 0.2550486573615515,\n   'median': 0.7395486831665039,\n   'majority': 0.11864250898361206,\n   'minority': 0.11864250898361206,\n   'unique': 150.0,\n   'histogram': [[3.0, 4.0, 17.0, 22.0, 24.0, 29.0, 20.0, 18.0, 7.0, 6.0],\n    [0.11864250898361206,\n     0.23988831043243408,\n     0.3611341118812561,\n     0.48237988352775574,\n     0.6036257147789001,\n     0.7248715162277222,\n     0.8461172580718994,\n     0.9673630595207214,\n     1.0886088609695435,\n     1.2098547220230103,\n     1.3311004638671875]],\n   'valid_percent': 100.0,\n   'masked_pixels': 0.0,\n   'valid_pixels': 150.0,\n   'percentile_2': 0.24085583359003068,\n   'percentile_98': 1.2310137295722965}},\n 'datetime': '2023-12-31'}\n\n\nCreate a function that goes through every single item in the collection and populates their properties - including the minimum, maximum, and sum of their values - in a table.\n\n# Create a function that converts statistics in JSON format into a pandas DataFrame\ndef clean_stats(stats_json) -&gt; pd.DataFrame:\n\n    # Normalize the JSON data\n    df = pd.json_normalize(stats_json)\n\n    # Replace the naming \"statistics.b1\" in the columns\n    df.columns = [col.replace(\"statistics.b1.\", \"\") for col in df.columns]\n\n    # Set the datetime format\n    df[\"date\"] = pd.to_datetime(df[\"datetime\"])\n\n    # Return the cleaned format\n    return df\n\n# Apply the generated function on the stats data\ndf = clean_stats(stats)\n\n# Display the stats for the first 5 granules in the collection in the table\n# Change the value in the parenthesis to show more or a smaller number of rows in the table\ndf.head(5)\n\n\n\n\n\n\n\n\ndatetime\nmin\nmax\nmean\ncount\nsum\nstd\nmedian\nmajority\nminority\nunique\nhistogram\nvalid_percent\nmasked_pixels\nvalid_pixels\npercentile_2\npercentile_98\ndate\n\n\n\n\n0\n2023-12-31\n0.118643\n1.331100\n0.745571\n150.0\n111.835648\n0.255049\n0.739549\n0.118643\n0.118643\n150.0\n[[3.0, 4.0, 17.0, 22.0, 24.0, 29.0, 20.0, 18.0...\n100.0\n0.0\n150.0\n0.240856\n1.231014\n2023-12-31\n\n\n1\n2023-12-30\n0.118560\n1.329713\n0.744604\n150.0\n111.690636\n0.254805\n0.738541\n0.118560\n0.118560\n150.0\n[[3.0, 4.0, 17.0, 22.0, 24.0, 29.0, 20.0, 18.0...\n100.0\n0.0\n150.0\n0.240662\n1.229237\n2023-12-30\n\n\n2\n2023-12-29\n0.118470\n1.328249\n0.743593\n150.0\n111.538979\n0.254547\n0.737490\n0.118470\n0.118470\n150.0\n[[3.0, 4.0, 17.0, 22.0, 24.0, 29.0, 20.0, 19.0...\n100.0\n0.0\n150.0\n0.240456\n1.227379\n2023-12-29\n\n\n3\n2023-12-28\n0.118373\n1.326706\n0.742537\n150.0\n111.380539\n0.254277\n0.736633\n0.118373\n0.118373\n150.0\n[[3.0, 4.0, 17.0, 23.0, 23.0, 29.0, 20.0, 19.0...\n100.0\n0.0\n150.0\n0.240238\n1.225439\n2023-12-28\n\n\n4\n2023-12-27\n0.118268\n1.325084\n0.741434\n150.0\n111.215126\n0.253992\n0.735755\n0.118268\n0.118268\n150.0\n[[3.0, 4.0, 17.0, 23.0, 23.0, 29.0, 20.0, 19.0...\n100.0\n0.0\n150.0\n0.240007\n1.223415\n2023-12-27",
    "crumbs": [
      "Data Usage Notebooks",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "MiCASA Land Carbon Flux"
    ]
  },
  {
    "objectID": "user_data_notebooks/micasa-carbonflux-daygrid-v1_User_Notebook.html#visualize-the-data-as-a-time-series",
    "href": "user_data_notebooks/micasa-carbonflux-daygrid-v1_User_Notebook.html#visualize-the-data-as-a-time-series",
    "title": "MiCASA Land Carbon Flux",
    "section": "Visualize the Data as a Time Series",
    "text": "Visualize the Data as a Time Series\nWe can now explore the Heterotrophic Respiration time series (October 2021 - January 2024) available for the Dallas, Texas area. We can plot the data set using the code below:\n\n# Determine the width and height of the plot using the 'matplotlib' library\n# Figure size: 20 representing the width, 10 representing the height\nfig = plt.figure(figsize=(20, 10)) \n\n# Plot the time series analysis of the daily Heterotrophic Respiration changes in Dallas, Texas\nplt.plot(\n    df[\"date\"], # X-axis: date\n    df[\"max\"], # Y-axis: Rh value\n    color=\"purple\", # Line color\n    linestyle=\"-\", # Line style\n    linewidth=0.5, # Line width\n    label=\"RH Level\", # Legend label\n)\n\n# Display legend\nplt.legend()\n\n# Insert label for the X-axis\nplt.xlabel(\"Years\")\n\n# Insert label for the Y-axis\nplt.ylabel(\"gm Carbon/m2/day\")\n\n# Insert title for the plot\nplt.title(\"Heterotrophic Respiration Values for Dallas, Texas (October 2021 to January 2024)\")\n\nText(0.5, 1.0, 'Heterotrophic Respiration Values for Dallas, Texas (October 2021 to January 2024)')\n\n\n\n\n\n\n\n\n\nTo take a closer look at the daily Heterotrophic Respiration variability across this region, we are going to retrieve and display data collected during the December, 2023 observation.\n\n# Fetch the third item in the list as the observation item.\n# Considering that a list starts with \"0\", we need to insert \"2\" in the \"items[2]\" statement\n# Print the start Date Time of the third granule in the collection\nprint(items[2][\"properties\"][\"datetime\"]) \n\n2023-12-29T00:00:00+00:00\n\n\n\n# A GET request is made for the observed tile\nobserved_tile = requests.get(\n\n    # Pass the collection name, the item number in the list, and its ID\n    f\"{RASTER_API_URL}/stac/tilejson.json?collection={items[2]['collection']}&item={items[2]['id']}\"\n\n    # Pass the asset name\n    f\"&assets={asset_name}\"\n\n    # Pass the color formula and colormap for custom visualization\n    f\"&color_formula=gamma+r+1.05&colormap_name={color_map}\"\n\n    # Pass the minimum and maximum values for rescaling\n    f\"&rescale={rescale_values['min']},{rescale_values['max']}\",\n\n# Return the response in JSON format\n).json()\n\n# Print the properties of the retrieved granule to the console \nobserved_tile\n\n{'tilejson': '2.2.0',\n 'version': '1.0.0',\n 'scheme': 'xyz',\n 'tiles': ['https://earth.gov/ghgcenter/api/raster/stac/tiles/WebMercatorQuad/{z}/{x}/{y}@1x?collection=micasa-carbonflux-daygrid-v1&item=micasa-carbonflux-daygrid-v1-20231229&assets=rh&color_formula=gamma+r+1.05&colormap_name=purd&rescale=-0.32319876551628113%2C5.9415082931518555'],\n 'minzoom': 0,\n 'maxzoom': 24,\n 'bounds': [-180.0, -90.0, 179.99999999999994, 90.0],\n 'center': [-2.842170943040401e-14, 0.0, 0]}\n\n\n\n# Create a new map to display the Rh level for the Dallas, Texas area for the observed tile timeframe.\naoi_map_bbox = Map(\n\n    # Base map is set to OpenStreetMap\n    tiles=\"OpenStreetMap\",\n\n    # Set the center of the map\n    location=[\n        32.8, # latitude\n        -96.79, # longitude\n    ],\n\n    # Set the zoom value\n    zoom_start=9,\n)\n\n# Define the map layer with the Rh level for observed tile\nmap_layer = TileLayer(\n    tiles=observed_tile[\"tiles\"][0], # Path to retrieve the tile\n\n    # Set the attribution, transparency, and the title along with enabling the visualization of the legend on the map \n    attr=\"GHG\", opacity = 0.7, name=\" Observed tile RH Level\", overlay= True, legendEnabled = True\n)\n\n# Add the layer to the map\nmap_layer.add_to(aoi_map_bbox)\n\n# Display data marker (title) on the map\nfolium.Marker((40, 5.9), tooltip=\"both\").add_to(aoi_map_bbox)\n\n# Add a layer control\nfolium.LayerControl(collapsed=False).add_to(aoi_map_bbox)\n\n# Add a legend using the 'branca' library\ncolormap = branca.colormap.linear.PuRd_09.scale(0, 0.3) # minimum value = 0, maximum value = 0.3 (gm Carbon/m2/daily)\n\n# Classify the colormap according to the specified Rh values\ncolormap = colormap.to_step(index=[0, 0.07, 0.15, 0.22, 0.3])\n\n# Add the data unit as caption\ncolormap.caption = 'Rh Values (gm Carbon/m2/daily)'\n\n# Display the legend and caption on the map\ncolormap.add_to(aoi_map_bbox)\n\n# Visualize the map\naoi_map_bbox\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook",
    "crumbs": [
      "Data Usage Notebooks",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "MiCASA Land Carbon Flux"
    ]
  },
  {
    "objectID": "user_data_notebooks/micasa-carbonflux-daygrid-v1_User_Notebook.html#summary",
    "href": "user_data_notebooks/micasa-carbonflux-daygrid-v1_User_Notebook.html#summary",
    "title": "MiCASA Land Carbon Flux",
    "section": "Summary",
    "text": "Summary\nIn this notebook we have successfully completed the following steps for the STAC collection for MiCASA Land Carbon Flux data: 1. Install and import the necessary libraries 2. Fetch the collection from STAC collections using the appropriate endpoints 3. Count the number of existing granules within the collection 4. Map and compare the Heterotrophic Respiration (Rh) levels over the Dallas, Texas area for two distinctive years 5. Create a table that displays the minimum, maximum, and sum of the Rh values for a specified region 6. Generate a time-series graph of the Rh values for a specified region\nIf you have any questions regarding this user notebook, please contact us using the feedback form.",
    "crumbs": [
      "Data Usage Notebooks",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "MiCASA Land Carbon Flux"
    ]
  },
  {
    "objectID": "user_data_notebooks/lpjeosim-wetlandch4-grid-v2_User_Notebook.html",
    "href": "user_data_notebooks/lpjeosim-wetlandch4-grid-v2_User_Notebook.html",
    "title": "Wetland Methane Emissions, LPJ-EOSIM Model",
    "section": "",
    "text": "You can launch this notebook in the US GHG Center JupyterHub by clicking the link below.\nLaunch in the US GHG Center JupyterHub (requires access)",
    "crumbs": [
      "Data Usage Notebooks",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "Wetland Methane Emissions, LPJ-EOSIM Model"
    ]
  },
  {
    "objectID": "user_data_notebooks/lpjeosim-wetlandch4-grid-v2_User_Notebook.html#run-this-notebook",
    "href": "user_data_notebooks/lpjeosim-wetlandch4-grid-v2_User_Notebook.html#run-this-notebook",
    "title": "Wetland Methane Emissions, LPJ-EOSIM Model",
    "section": "",
    "text": "You can launch this notebook in the US GHG Center JupyterHub by clicking the link below.\nLaunch in the US GHG Center JupyterHub (requires access)",
    "crumbs": [
      "Data Usage Notebooks",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "Wetland Methane Emissions, LPJ-EOSIM Model"
    ]
  },
  {
    "objectID": "user_data_notebooks/lpjeosim-wetlandch4-grid-v2_User_Notebook.html#approach",
    "href": "user_data_notebooks/lpjeosim-wetlandch4-grid-v2_User_Notebook.html#approach",
    "title": "Wetland Methane Emissions, LPJ-EOSIM Model",
    "section": "Approach",
    "text": "Approach\n\nIdentify available dates and temporal frequency of observations for the given collection using the GHGC API /stac endpoint. The collection processed in this notebook is the Wetland Methane Emissions, LPJ-EOSIM Model data product.\nPass the STAC item into the raster API /stac/tilejson.jsonendpoint.\nUsing folium.plugins.DualMap, visualize two tiles (side-by-side), allowing time point comparison.\nAfter the visualization, perform zonal statistics for a given polygon.",
    "crumbs": [
      "Data Usage Notebooks",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "Wetland Methane Emissions, LPJ-EOSIM Model"
    ]
  },
  {
    "objectID": "user_data_notebooks/lpjeosim-wetlandch4-grid-v2_User_Notebook.html#about-the-data",
    "href": "user_data_notebooks/lpjeosim-wetlandch4-grid-v2_User_Notebook.html#about-the-data",
    "title": "Wetland Methane Emissions, LPJ-EOSIM Model",
    "section": "About the Data",
    "text": "About the Data\nMethane (CH₄) emissions from vegetated wetlands are estimated to be the largest natural source of methane in the global CH₄ budget, contributing to roughly one third of the total of natural and anthropogenic emissions. Wetland CH₄ is produced by microbes breaking down organic matter in the oxygen deprived environment of inundated soils. Due to limited data availability, the details of the role of wetland CH₄ emissions have thus far been underrepresented. Using the Earth Observation SIMulator version (LPJ-EOSIM) of the Lund-Potsdam-Jena Dynamic Global Vegetation Model (LPJ-DGVM) global CH₄ emissions from wetlands are estimated at 0.5° x 0.5 degree spatial resolution. By simulating wetland extent and using characteristics of inundated areas, such as wetland soil moisture, temperature, and carbon content, the model provides estimates of CH₄ quantities emitted into the atmosphere. This dataset shows concentrated methane sources from tropical and high latitude ecosystems. The LPJ-EOSIM Wetland Methane Emissions dataset consists of global daily model estimates of terrestrial wetland methane emissions from 1990 to the present, with data added bimonthly. The estimates are regularly used in conjunction with NASA’s Goddard Earth Observing System (GEOS) model to simulate the impact of wetlands and other methane sources on atmospheric methane concentrations, to compare against satellite and airborne data, and to improve understanding and prediction of wetland emissions.\nFor more information regarding this dataset, please visit the U.S. Greenhouse Gas Center.",
    "crumbs": [
      "Data Usage Notebooks",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "Wetland Methane Emissions, LPJ-EOSIM Model"
    ]
  },
  {
    "objectID": "user_data_notebooks/lpjeosim-wetlandch4-grid-v2_User_Notebook.html#query-the-stac-api",
    "href": "user_data_notebooks/lpjeosim-wetlandch4-grid-v2_User_Notebook.html#query-the-stac-api",
    "title": "Wetland Methane Emissions, LPJ-EOSIM Model",
    "section": "Query the STAC API",
    "text": "Query the STAC API\nFirst, we are going to import the required libraries. Once imported, they allow better executing a query in the GHG Center Spatio Temporal Asset Catalog (STAC) Application Programming Interface (API) where the granules for this collection are stored.\n\n# Import the following libraries\nimport requests\nimport folium\nimport folium.plugins\nfrom folium import Map, TileLayer\nfrom pystac_client import Client\nimport branca\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n\n# Provide the STAC and RASTER API endpoints\n# The endpoint is referring to a location within the API that executes a request on a data collection nesting on the server.\n\n# The STAC API is a catalog of all the existing data collections that are stored in the GHG Center.\nSTAC_API_URL = \"https://earth.gov/ghgcenter/api/stac\"\n\n# The RASTER API is used to fetch collections for visualization\nRASTER_API_URL = \"https://earth.gov/ghgcenter/api/raster\"\n\n# The collection name is used to fetch the dataset from the STAC API. First, we define the collection name as a variable\n# Name of the collection for the wetland methane emissions LPJ-EOSIM Model\ncollection_name = \"lpjeosim-wetlandch4-daygrid-v2\"\n\n# Next, we need to specify the asset name for this collection\n# The asset name is referring to the raster band containing the pixel values for the parameter of interest\nasset_name = \"ensemble-mean-ch4-wetlands-emissions\"\n\n\n# Fetch the collection from the STAC API using the appropriate endpoint\n# The 'requests' library allows a HTTP request possible\ncollection = requests.get(f\"{STAC_API_URL}/collections/{collection_name}\").json()\n\n# Print the properties of the collection to the console\ncollection\n\n{'id': 'lpjeosim-wetlandch4-daygrid-v2',\n 'type': 'Collection',\n 'links': [{'rel': 'items',\n   'type': 'application/geo+json',\n   'href': 'https://earth.gov/ghgcenter/api/stac/collections/lpjeosim-wetlandch4-daygrid-v2/items'},\n  {'rel': 'parent',\n   'type': 'application/json',\n   'href': 'https://earth.gov/ghgcenter/api/stac/'},\n  {'rel': 'root',\n   'type': 'application/json',\n   'href': 'https://earth.gov/ghgcenter/api/stac/'},\n  {'rel': 'self',\n   'type': 'application/json',\n   'href': 'https://earth.gov/ghgcenter/api/stac/collections/lpjeosim-wetlandch4-daygrid-v2'}],\n 'title': 'Wetland Methane Emissions, LPJ-EOSIM Model v2',\n 'assets': None,\n 'extent': {'spatial': {'bbox': [[-180, -90, 180, 90]]},\n  'temporal': {'interval': [['1990-01-01T00:00:00+00:00',\n     '2024-02-27T00:00:00+00:00']]}},\n 'license': 'CC0 1.0',\n 'keywords': None,\n 'providers': [{'url': None,\n   'name': 'NASA',\n   'roles': None,\n   'description': None}],\n 'summaries': {'datetime': ['1990-01-01T00:00:00Z', '2024-02-27T00:00:00Z']},\n 'description': 'Global, daily estimates of methane (CH4) emissions from terrestrial wetlands at 0.5 x 0.5 degree spatial resolution using the Earth Observation SIMulator version (LPJ-EOSIM) of the Lund-Potsdam-Jena Dynamic Global Vegetation Model (LPJ-DGVM). Methane emissions from vegetated wetlands are estimated to be the largest natural source of methane in the global CH4 budget, contributing to roughly one third of the total of natural and anthropogenic emissions. Wetland CH4 is produced by microbes breaking down organic matter in the oxygen deprived environment of inundated soils. Due to limited data availability, the details of the role of wetland CH4 emissions have thus far been underrepresented. The LPJ-EOSIM model estimates wetland methane emissions by simulating wetland extent and using characteristics of these inundated areas such as soil moisture, temperature, and carbon content to estimate CH4 quantities emitted into the atmosphere. Input climate forcing data comes from Modern-Era Retrospective analysis for Research and Applications Version 2 (MERRA-2) data and ECMWF Re-Analysis data (ERA5). An ensemble layer provides the result of the mean of the MERRA-2 and ERA5 layers.',\n 'item_assets': {'era5-ch4-wetlands-emissions': {'type': 'image/tiff; application=geotiff; profile=cloud-optimized',\n   'roles': ['data', 'layer'],\n   'title': 'Wetland Methane Emissions, ERA5 LPJ-EOSIM Model v2',\n   'description': 'Methane emissions from wetlands in units of grams of methane per meter squared per day. ECMWF Re-Analysis (ERA5) as input to LPJ-EOSIM model.'},\n  'merra2-ch4-wetlands-emissions': {'type': 'image/tiff; application=geotiff; profile=cloud-optimized',\n   'roles': ['data', 'layer'],\n   'title': 'Wetland Methane Emissions, MERRA-2 LPJ-EOSIM Model v2',\n   'description': 'Methane emissions from wetlands in units of grams of methane per meter squared per day. Modern-Era Retrospective analysis for Research and Applications Version 2 (MERRA-2) data as input to LPJ-EOSIM model.'},\n  'ensemble-mean-ch4-wetlands-emissions': {'type': 'image/tiff; application=geotiff; profile=cloud-optimized',\n   'roles': ['data', 'layer'],\n   'title': 'Wetland Methane Emissions, Ensemble Mean LPJ-EOSIM Model v2',\n   'description': 'Methane emissions from wetlands in units of grams of methane per meter squared per day. Ensemble of multiple climate forcing data sources input to LPJ-EOSIM model.'}},\n 'stac_version': '1.0.0',\n 'stac_extensions': None,\n 'dashboard:is_periodic': True,\n 'dashboard:time_density': 'day'}\n\n\nExamining the contents of our collection under summaries, we see that the data is available from January 1990 to December 2024. By looking at dashboard: time density, we can see that these observations are collected monthly.\n\n# Create a function that would search for a data collection in the US GHG Center STAC API\n\n# First, we need to define the function\n# The name of the function = \"get_item_count\"\n# The argument that will be passed through the defined function = \"collection_id\"\n\ndef get_item_count(collection_id):\n\n    # Set a counter for the number of items existing in the collection\n    count = 0\n\n    # Define the path to retrieve the granules (items) of the collection of interest in the STAC API\n    items_url = f\"{STAC_API_URL}/collections/{collection_id}/items\"\n\n    # Run a while loop to make HTTP requests until there are no more URLs associated with the collection in the STAC API\n    while True:\n\n        # Retrieve information about the granules by sending a \"get\" request to the STAC API using the defined collection path\n        response = requests.get(items_url)\n\n        # If the items do not exist, print an error message and quit the loop\n        if not response.ok:\n            print(\"error getting items\")\n            exit()\n\n        # Return the results of the HTTP response as JSON\n        stac = response.json()\n\n        # Increase the \"count\" by the number of items (granules) returned in the response\n        count += int(stac[\"context\"].get(\"returned\", 0))\n\n        # Retrieve information about the next URL associated with the collection in the STAC API (if applicable)\n        next = [link for link in stac[\"links\"] if link[\"rel\"] == \"next\"]\n\n        # Exit the loop if there are no other URLs\n        if not next:\n            break\n        \n        # Ensure the information gathered by other STAC API links associated with the collection are added to the original path\n        # \"href\" is the identifier for each of the tiles stored in the STAC API\n        items_url = next[0][\"href\"]\n        # temp = items_url.split('/')\n        # temp.insert(3, 'ghgcenter')\n        # temp.insert(4, 'api')\n        # temp.insert(5, 'stac')\n        # items_url = '/'.join(temp)\n\n    # Return the information about the total number of granules found associated with the collection\n    return count\n\n\n# Apply the function created above \"get_item_count\" to the data collection\nnumber_of_items = get_item_count(collection_name)\n\n# Get the information about the number of granules found in the collection\nitems = requests.get(f\"{STAC_API_URL}/collections/{collection_name}/items?limit=800\"\n).json()[\"features\"]\n\n# Print the total number of items (granules) found\nprint(f\"Found {len(items)} items\")\n\nFound 800 items\n\n\n\n# Examine the first item in the collection\n# Keep in mind that a list starts from 0, 1, 2... therefore items[0] is referring to the first item in the list/collection\nitems[0]\n\nBelow, we are entering the minimum and maximum values to provide our upper and lower bounds in the rescale_values.\n\n# Fetch the minimum and maximum values for rescaling\nrescale_values = {'max': 0.0003, 'min': 0.0}",
    "crumbs": [
      "Data Usage Notebooks",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "Wetland Methane Emissions, LPJ-EOSIM Model"
    ]
  },
  {
    "objectID": "user_data_notebooks/lpjeosim-wetlandch4-grid-v2_User_Notebook.html#explore-changes-in-methane-ch4-emission-levels-using-the-raster-api",
    "href": "user_data_notebooks/lpjeosim-wetlandch4-grid-v2_User_Notebook.html#explore-changes-in-methane-ch4-emission-levels-using-the-raster-api",
    "title": "Wetland Methane Emissions, LPJ-EOSIM Model",
    "section": "Explore Changes in Methane (CH4) Emission Levels Using the Raster API",
    "text": "Explore Changes in Methane (CH4) Emission Levels Using the Raster API\nIn this notebook, we will explore the temporal impacts of methane emissions. We will visualize the outputs on a map using folium.\n\n# Now we create a dictionary where the start datetime values for each granule is queried more explicitly by year and month (e.g., 2020-02)\nitems = {item[\"properties\"][\"datetime\"][:10]: item for item in items} \n\nNow, we will pass the item id, collection name, and rescaling_factor to the Raster API endpoint. We will do this twice, once for date 1 mentioned in the next cell and again for date 2, so we can visualize each event independently.\n\n# Choose a color for displaying the tiles\n# Please refer to matplotlib library if you'd prefer choosing a different color ramp.\n# For more information on Colormaps in Matplotlib, please visit https://matplotlib.org/stable/users/explain/colors/colormaps.html\ncolor_map = \"magma\" \n\n# Make a GET request to retrieve information for the date mentioned below\ndate1 = '2024-01-01'\ndate1_tile = requests.get(\n\n    # Pass the collection name, collection date, and its ID\n    # To change the year, month and date of the observed parameter, you can modify the date2 variable above\n    f\"{RASTER_API_URL}/stac/tilejson.json?collection={items[date1]['collection']}&item={items[date1]['id']}\"\n\n    # Pass the asset name\n    f\"&assets={asset_name}\"\n\n    # Pass the color formula and colormap for custom visualization\n    f\"&color_formula=gamma+r+1.05&colormap_name={color_map}\"\n\n    # Pass the minimum and maximum values for rescaling\n    f\"&rescale={rescale_values['min']},{rescale_values['max']}\", \n\n# Return response in JSON format\n).json()\n\n# Print the properties of the retrieved granule to the console\ndate1_tile\n\n{'tilejson': '2.2.0',\n 'version': '1.0.0',\n 'scheme': 'xyz',\n 'tiles': ['https://earth.gov/ghgcenter/api/raster/stac/tiles/WebMercatorQuad/{z}/{x}/{y}@1x?collection=lpjeosim-wetlandch4-daygrid-v2&item=lpjeosim-wetlandch4-daygrid-v2-20240101day&assets=ensemble-mean-ch4-wetlands-emissions&color_formula=gamma+r+1.05&colormap_name=magma&rescale=0.0%2C0.0003'],\n 'minzoom': 0,\n 'maxzoom': 24,\n 'bounds': [-180.0, -90.0, 180.0, 90.0],\n 'center': [0.0, 0.0, 0]}\n\n\n\n# Make a GET request to retrieve information for date mentioned below\ndate2 = '2024-01-30'\ndate2_tile = requests.get(\n\n    # Pass the collection name, collection date, and its ID\n    # To change the year, month and date of the observed parameter, you can modify the date2 variable above\n    f\"{RASTER_API_URL}/stac/tilejson.json?collection={items[date2]['collection']}&item={items[date2]['id']}\"\n\n    # Pass the asset name\n    f\"&assets={asset_name}\"\n\n    # Pass the color formula and colormap for custom visualization\n    f\"&color_formula=gamma+r+1.05&colormap_name={color_map}\"\n\n    # Pass the minimum and maximum values for rescaling\n    f\"&rescale={rescale_values['min']},{rescale_values['max']}\",\n\n# Return response in JSON format \n).json()\n\n# Print the properties of the retrieved granule to the console\ndate2_tile\n\n{'tilejson': '2.2.0',\n 'version': '1.0.0',\n 'scheme': 'xyz',\n 'tiles': ['https://earth.gov/ghgcenter/api/raster/stac/tiles/WebMercatorQuad/{z}/{x}/{y}@1x?collection=lpjeosim-wetlandch4-daygrid-v2&item=lpjeosim-wetlandch4-daygrid-v2-20240130day&assets=ensemble-mean-ch4-wetlands-emissions&color_formula=gamma+r+1.05&colormap_name=magma&rescale=0.0%2C0.0003'],\n 'minzoom': 0,\n 'maxzoom': 24,\n 'bounds': [-180.0, -90.0, 180.0, 90.0],\n 'center': [0.0, 0.0, 0]}",
    "crumbs": [
      "Data Usage Notebooks",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "Wetland Methane Emissions, LPJ-EOSIM Model"
    ]
  },
  {
    "objectID": "user_data_notebooks/lpjeosim-wetlandch4-grid-v2_User_Notebook.html#visualize-ch₄-emissions",
    "href": "user_data_notebooks/lpjeosim-wetlandch4-grid-v2_User_Notebook.html#visualize-ch₄-emissions",
    "title": "Wetland Methane Emissions, LPJ-EOSIM Model",
    "section": "Visualize CH₄ Emissions",
    "text": "Visualize CH₄ Emissions\n\n# For this study we are going to compare the CH₄ Emissions in date1 and date2 along the coast of California\n# To change the location, you can simply insert the latitude and longitude of the area of your interest in the \"location=(LAT, LONG)\" statement\n\n# Set initial zoom and center of map\n# 'folium.plugins' allows mapping side-by-side\nmap_ = folium.plugins.DualMap(location=(34, -118), zoom_start=6)\n\n# Define the first map layer for tile fetched for date 1\n# The TileLayer library helps in manipulating and displaying raster layers on a map\nmap_layer_date1 = TileLayer(\n    tiles=date1_tile[\"tiles\"][0], # Path to retrieve the tile\n    attr=\"GHG\", # Set the attribution\n    opacity=0.5, # Adjust the transparency of the layer\n)\n\n# Add the first layer to the Dual Map\nmap_layer_date1.add_to(map_.m1)\n\n\n# Define the second map layer for the tile fetched for date 2\nmap_layer_date2 = TileLayer(\n    tiles=date2_tile[\"tiles\"][0], # Path to retrieve the tile\n    attr=\"GHG\", # Set the attribution\n    opacity=0.5, # Adjust the transparency of the layer\n)\n\n# Add the second layer to the Dual Map\nmap_layer_date2.add_to(map_.m2)\n\n# Visualize the Dual Map\nmap_\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook",
    "crumbs": [
      "Data Usage Notebooks",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "Wetland Methane Emissions, LPJ-EOSIM Model"
    ]
  },
  {
    "objectID": "user_data_notebooks/lpjeosim-wetlandch4-grid-v2_User_Notebook.html#visualize-the-data-as-a-time-series",
    "href": "user_data_notebooks/lpjeosim-wetlandch4-grid-v2_User_Notebook.html#visualize-the-data-as-a-time-series",
    "title": "Wetland Methane Emissions, LPJ-EOSIM Model",
    "section": "Visualize the Data as a Time Series",
    "text": "Visualize the Data as a Time Series\nWe can now explore the wetland methane emissions time series (January 1990 – December 2024) available for the Texas area of the U.S. We can plot the data set using the code below:\n\n# Determine the width and height of the plot using the 'matplotlib' library\n# Figure size: 20 representing the width, 10 representing the height\nfig = plt.figure(figsize=(20, 10))\n\n# Plot the time series\nplt.plot(\n    df[\"date\"], # X-axis: date\n    df[\"max\"], # Y-axis: CH₄ value\n    color=\"red\", # Line color\n    linestyle=\"-\", # Line style\n    linewidth=0.5, # Line width\n    label=\"Max daily CH₄ emissions\", # Legend label\n)\n\n# Display legend\nplt.legend()\n\n# Insert label for the X-axis\nplt.xlabel(\"Years\")\n\n# Insert label for the Y-axis\nplt.ylabel(\"Daily CH4 emissions g/m2\")\n\n# Insert title for the plot\nplt.title(\"Daily CH4 emission Values for Texas, January 2022- March 2024\")\n\nText(0.5, 1.0, 'Daily CH4 emission Values for Texas, January 2022- March 2024')\n\n\n\n\n\n\n\n\n\nTo take a closer look at the CH4 variability across this region, we are going to retrieve and display data collected during the February, 2024 observation.\n\n# The 2024-02-25 observation is the 3rd item in the list\n# Considering that a list starts with \"0\", we need to insert \"2\" in the \"items[2]\" statement\n# Print the start Date Time of the third granule in the collection\nprint(items[2][\"properties\"][\"datetime\"])\n\n2024-02-25T00:00:00+00:00\n\n\n\n# A GET request is made for the 3rd item in the collection\nobserved_tile = requests.get(\n\n    # Pass the collection name, the item number in the list, and its ID\n    f\"{RASTER_API_URL}/stac/tilejson.json?collection={items[2]['collection']}&item={items[2]['id']}&assets={asset_name}\"\n\n    # Pass the color formula and colormap for custom visualization\n    f\"&color_formula=gamma+r+1.05&colormap_name={color_map}\"\n\n    # Pass the minimum and maximum values for rescaling\n    f\"&rescale={rescale_values['min']},{rescale_values['max']}\",\n\n# Return the response in JSON format\n).json()\n\n# Print the properties of the retrieved granule to the console\nobserved_tile\n\n{'tilejson': '2.2.0',\n 'version': '1.0.0',\n 'scheme': 'xyz',\n 'tiles': ['https://earth.gov/ghgcenter/api/raster/stac/tiles/WebMercatorQuad/{z}/{x}/{y}@1x?collection=lpjeosim-wetlandch4-daygrid-v2&item=lpjeosim-wetlandch4-daygrid-v2-20240225day&assets=ensemble-mean-ch4-wetlands-emissions&color_formula=gamma+r+1.05&colormap_name=magma&rescale=0.0%2C0.0003'],\n 'minzoom': 0,\n 'maxzoom': 24,\n 'bounds': [-180.0, -90.0, 180.0, 90.0],\n 'center': [0.0, 0.0, 0]}\n\n\n\n# Create a new map to display the CH4 variability for the Texas region for Observed tile timeframe\naoi_map_bbox = Map(\n\n    # Base map is set to OpenStreetMap\n    tiles=\"OpenStreetMap\",\n\n    # Set the center of the map\n    location=[\n        30,-100\n    ],\n\n    # Set the zoom value\n    zoom_start=8,\n)\n\n# Define the map layer\nmap_layer = TileLayer(\n    tiles=observed_tile[\"tiles\"][0], # Path to retrieve the tile\n    attr=\"GHG\", opacity = 0.5 # Set the attribution and transparency\n)\n\n# Add the layer to the map\nmap_layer.add_to(aoi_map_bbox)\n\n# Visualize the map\naoi_map_bbox\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook",
    "crumbs": [
      "Data Usage Notebooks",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "Wetland Methane Emissions, LPJ-EOSIM Model"
    ]
  },
  {
    "objectID": "user_data_notebooks/lpjeosim-wetlandch4-grid-v2_User_Notebook.html#summary",
    "href": "user_data_notebooks/lpjeosim-wetlandch4-grid-v2_User_Notebook.html#summary",
    "title": "Wetland Methane Emissions, LPJ-EOSIM Model",
    "section": "Summary",
    "text": "Summary\nIn this notebook we have successfully completed the following steps for the STAC collection for the Daily Wetland Methane Emissions, LPJ-EOSIM Model data: 1. Install and import the necessary libraries 2. Fetch the collection from STAC collections using the appropriate endpoints 3. Count the number of existing granules within the collection 4. Map and compare the CH4 levels over the Texas region for two distinctive years 5. Create a table that displays the minimum, maximum, and sum of the CH4 levels for a specified region 6. Generate a time-series graph of the CH4 levels for a specified region\nIf you have any questions regarding this user notebook, please contact us using the feedback form.",
    "crumbs": [
      "Data Usage Notebooks",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "Wetland Methane Emissions, LPJ-EOSIM Model"
    ]
  },
  {
    "objectID": "user_data_notebooks/noaa-insitu_User_Notebook.html",
    "href": "user_data_notebooks/noaa-insitu_User_Notebook.html",
    "title": "Atmospheric Carbon Dioxide Concentrations from NOAA Global Monitoring Laboratory",
    "section": "",
    "text": "You can launch this notebook in the US GHG Center JupyterHub by clicking the link below.\nLaunch in the US GHG Center JupyterHub (requires access)",
    "crumbs": [
      "Data Usage Notebooks",
      "Greenhouse Gas Concentrations",
      "Atmospheric Carbon Dioxide Concentrations from NOAA Global Monitoring Laboratory"
    ]
  },
  {
    "objectID": "user_data_notebooks/noaa-insitu_User_Notebook.html#run-this-notebook",
    "href": "user_data_notebooks/noaa-insitu_User_Notebook.html#run-this-notebook",
    "title": "Atmospheric Carbon Dioxide Concentrations from NOAA Global Monitoring Laboratory",
    "section": "",
    "text": "You can launch this notebook in the US GHG Center JupyterHub by clicking the link below.\nLaunch in the US GHG Center JupyterHub (requires access)",
    "crumbs": [
      "Data Usage Notebooks",
      "Greenhouse Gas Concentrations",
      "Atmospheric Carbon Dioxide Concentrations from NOAA Global Monitoring Laboratory"
    ]
  },
  {
    "objectID": "user_data_notebooks/noaa-insitu_User_Notebook.html#approach",
    "href": "user_data_notebooks/noaa-insitu_User_Notebook.html#approach",
    "title": "Atmospheric Carbon Dioxide Concentrations from NOAA Global Monitoring Laboratory",
    "section": "Approach",
    "text": "Approach\n\nIdentify available dates and temporal frequency of observations for the given data. The collection processed in this notebook is the Atmospheric Carbon Dioxide Concentrations from NOAA Global Monitoring Laboratory.\nVisualize the time series data",
    "crumbs": [
      "Data Usage Notebooks",
      "Greenhouse Gas Concentrations",
      "Atmospheric Carbon Dioxide Concentrations from NOAA Global Monitoring Laboratory"
    ]
  },
  {
    "objectID": "user_data_notebooks/noaa-insitu_User_Notebook.html#about-the-data",
    "href": "user_data_notebooks/noaa-insitu_User_Notebook.html#about-the-data",
    "title": "Atmospheric Carbon Dioxide Concentrations from NOAA Global Monitoring Laboratory",
    "section": "About the Data",
    "text": "About the Data\nThe Global Greenhouse Gas Reference Network (GGGRN) for the Carbon Cycle and Greenhouse Gases (CCGG) Group is part of NOAA’S Global Monitoring Laboratory (GML) in Boulder, CO. The Reference Network measures the atmospheric distribution and trends of the three main long-term drivers of climate change, carbon dioxide (CO₂), methane (CH₄), and nitrous oxide (N2O), as well as carbon monoxide (CO) and many other trace gases which help interpretation of the main GHGs. The Reference Network measurement program includes continuous in-situ measurements at 4 baseline observatories (global background sites) and 8 tall towers, as well as flask-air samples collected by volunteers at over 50 additional regional background sites and from small aircraft conducting regular vertical profiles. The air samples are returned to GML for analysis where measurements of about 55 trace gases are done. NOAA’s GGGRN maintains the World Meteorological Organization international calibration scales for CO₂, CH₄, CO, N2O, and SF6 in air. The measurements from the GGGRN serve as a comparison with measurements made by many other international laboratories, and with regional studies. They are widely used in modeling studies that infer space-time patterns of emissions and removals of greenhouse gases that are optimally consistent with the atmospheric observations, given wind patterns. These data serve as an early warning for climate “surprises”. The measurements are also helpful for the ongoing evaluation of remote sensing technologies.\nFor more information regarding this dataset, please visit the Atmospheric Carbon Dioxide Concentrations from NOAA GML data overview page.",
    "crumbs": [
      "Data Usage Notebooks",
      "Greenhouse Gas Concentrations",
      "Atmospheric Carbon Dioxide Concentrations from NOAA Global Monitoring Laboratory"
    ]
  },
  {
    "objectID": "user_data_notebooks/noaa-insitu_User_Notebook.html#reading-the-noaa-data-from-github-repo",
    "href": "user_data_notebooks/noaa-insitu_User_Notebook.html#reading-the-noaa-data-from-github-repo",
    "title": "Atmospheric Carbon Dioxide Concentrations from NOAA Global Monitoring Laboratory",
    "section": "Reading the NOAA data from GitHub repo",
    "text": "Reading the NOAA data from GitHub repo\n\ngithub_repo_owner = \"NASA-IMPACT\"\ngithub_repo_name = \"noaa-viz\"\nfolder_path_ch4, folder_path_co2 = \"flask/ch4\", \"flask/c02\"\ncombined_df_co2, combined_df_ch4 = pd.DataFrame(), pd.DataFrame()\n\n\n# Function to fetch and append a file from GitHub\ndef append_github_file(file_url):\n    response = requests.get(file_url)\n    response.raise_for_status()\n    return response.text\n\n# Get the list of CH4 files in the specified directory using GitHub API\ngithub_api_url = f\"https://api.github.com/repos/{github_repo_owner}/{github_repo_name}/contents/{folder_path_ch4}\"\nresponse = requests.get(github_api_url)\nresponse.raise_for_status()\nfile_list_ch4 = response.json()\n\n# Get the list of CO2 files in the specified directory using GitHub API\ngithub_api_url = f\"https://api.github.com/repos/{github_repo_owner}/{github_repo_name}/contents/{folder_path_ch4}\"\nresponse = requests.get(github_api_url)\nresponse.raise_for_status()\nfile_list_co2 = response.json()",
    "crumbs": [
      "Data Usage Notebooks",
      "Greenhouse Gas Concentrations",
      "Atmospheric Carbon Dioxide Concentrations from NOAA Global Monitoring Laboratory"
    ]
  },
  {
    "objectID": "user_data_notebooks/noaa-insitu_User_Notebook.html#concatenating-the-ch4-data-into-a-single-dataframe",
    "href": "user_data_notebooks/noaa-insitu_User_Notebook.html#concatenating-the-ch4-data-into-a-single-dataframe",
    "title": "Atmospheric Carbon Dioxide Concentrations from NOAA Global Monitoring Laboratory",
    "section": "Concatenating the CH4 data into a single DataFrame",
    "text": "Concatenating the CH4 data into a single DataFrame\n\nfor file_info in file_list_ch4:\n    if file_info[\"name\"].endswith(\"txt\"):\n        file_content = append_github_file(file_info[\"download_url\"])\n        Lines = file_content.splitlines()\n        index = Lines.index(\"# VARIABLE ORDER\")+2\n        df = pd.read_csv(StringIO(\"\\n\".join(Lines[index:])), delim_whitespace=True)\n        combined_df_ch4 = pd.concat([combined_df_ch4, df], ignore_index=True)",
    "crumbs": [
      "Data Usage Notebooks",
      "Greenhouse Gas Concentrations",
      "Atmospheric Carbon Dioxide Concentrations from NOAA Global Monitoring Laboratory"
    ]
  },
  {
    "objectID": "user_data_notebooks/noaa-insitu_User_Notebook.html#concatenating-the-co2-data-into-a-single-dataframe",
    "href": "user_data_notebooks/noaa-insitu_User_Notebook.html#concatenating-the-co2-data-into-a-single-dataframe",
    "title": "Atmospheric Carbon Dioxide Concentrations from NOAA Global Monitoring Laboratory",
    "section": "Concatenating the CO2 data into a single DataFrame",
    "text": "Concatenating the CO2 data into a single DataFrame\n\nfor file_info in file_list_co2:\n    if file_info[\"name\"].endswith(\"txt\"):\n        file_content = append_github_file(file_info[\"download_url\"])\n        Lines = file_content.splitlines()\n        index = Lines.index(\"# VARIABLE ORDER\")+2\n        df = pd.read_csv(StringIO(\"\\n\".join(Lines[index:])), delim_whitespace=True)\n        combined_df_co2 = pd.concat([combined_df_co2, df], ignore_index=True)",
    "crumbs": [
      "Data Usage Notebooks",
      "Greenhouse Gas Concentrations",
      "Atmospheric Carbon Dioxide Concentrations from NOAA Global Monitoring Laboratory"
    ]
  },
  {
    "objectID": "user_data_notebooks/noaa-insitu_User_Notebook.html#visualizing-the-noaa-data-for-ch4-and-co2",
    "href": "user_data_notebooks/noaa-insitu_User_Notebook.html#visualizing-the-noaa-data-for-ch4-and-co2",
    "title": "Atmospheric Carbon Dioxide Concentrations from NOAA Global Monitoring Laboratory",
    "section": "Visualizing the NOAA data for CH4 and CO2",
    "text": "Visualizing the NOAA data for CH4 and CO2\n\nsite_to_filter = 'ABP'\nfiltered_df = combined_df_co2[combined_df_co2['site_code'] == site_to_filter]\n\nfiltered_df['datetime'] = pd.to_datetime(filtered_df['datetime'])\n\n# Set the \"Date\" column as the index\nfiltered_df.set_index('datetime', inplace=True)\n\n# Create a time series plot for 'Data' and 'Value'\nplt.figure(figsize=(12, 6))\nplt.plot(filtered_df.index, filtered_df['value'], label='Carbon Dioxide(CO2) Concentration (ppm)')\nplt.xlabel(\"Observed Date/Time\")\nplt.ylabel(\"Carbon Dioxide(CO2) Concentration (ppm)\")\nplt.title(f\"Observed Co2 Concentration {site_to_filter}\")\nplt.legend()\nplt.grid(True)\n# plt.show()\n\n/var/folders/7b/5rrvrjx51l54jchgs0tqps0c0000gn/T/ipykernel_70808/2606016741.py:4: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  filtered_df['datetime'] = pd.to_datetime(filtered_df['datetime'])\n\n\n\n\n\n\n\n\n\n\nsite_to_filter = 'ABP'\nfiltered_df = combined_df_ch4[combined_df_ch4['site_code'] == site_to_filter]\nfiltered_df['datetime'] = pd.to_datetime(filtered_df['datetime'])\n\n# Set the \"Date\" column as the index\nfiltered_df.set_index('datetime', inplace=True)\n\n# Create a time series plot for 'Data' and 'Value'\nplt.figure(figsize=(12, 6))\nplt.plot(filtered_df.index, filtered_df['value'], label='Methane Ch4 Concentration (ppb)')\nplt.xlabel(\"Observation Date/Time\")\nplt.ylabel(\"Methane Ch4 Concentration (ppb)\")\nplt.title(f\"Observed CH4 Concentration {site_to_filter}\")\nplt.legend()\nplt.grid(True)\nplt.show()\n\n/var/folders/7b/5rrvrjx51l54jchgs0tqps0c0000gn/T/ipykernel_70808/1635934907.py:3: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  filtered_df['datetime'] = pd.to_datetime(filtered_df['datetime'])",
    "crumbs": [
      "Data Usage Notebooks",
      "Greenhouse Gas Concentrations",
      "Atmospheric Carbon Dioxide Concentrations from NOAA Global Monitoring Laboratory"
    ]
  },
  {
    "objectID": "user_data_notebooks/noaa-insitu_User_Notebook.html#summary",
    "href": "user_data_notebooks/noaa-insitu_User_Notebook.html#summary",
    "title": "Atmospheric Carbon Dioxide Concentrations from NOAA Global Monitoring Laboratory",
    "section": "Summary",
    "text": "Summary\nIn this notebook we have successfully visualized the data for Atmospheric Carbon Dioxide Concentrations from NOAA Global Monitoring Laboratory.\n\nInstall and import the necessary libraries\nFetch the collection from GitHub API using the appropriate endpoints\nConcatenating the CO2 and CH4 data into a single DataFrame\nVisualizing the NOAA data for CO2 and CH4\n\nIf you have any questions regarding this user notebook, please contact us using the feedback form.",
    "crumbs": [
      "Data Usage Notebooks",
      "Greenhouse Gas Concentrations",
      "Atmospheric Carbon Dioxide Concentrations from NOAA Global Monitoring Laboratory"
    ]
  },
  {
    "objectID": "user_data_notebooks/odiac-ffco2-monthgrid-v2023_User_Notebook.html",
    "href": "user_data_notebooks/odiac-ffco2-monthgrid-v2023_User_Notebook.html",
    "title": "ODIAC Fossil Fuel CO₂ Emissions",
    "section": "",
    "text": "You can launch this notebook in the US GHG Center JupyterHub by clicking the link below.\nLaunch in the US GHG Center JupyterHub (requires access)",
    "crumbs": [
      "Data Usage Notebooks",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "ODIAC Fossil Fuel CO₂ Emissions"
    ]
  },
  {
    "objectID": "user_data_notebooks/odiac-ffco2-monthgrid-v2023_User_Notebook.html#run-this-notebook",
    "href": "user_data_notebooks/odiac-ffco2-monthgrid-v2023_User_Notebook.html#run-this-notebook",
    "title": "ODIAC Fossil Fuel CO₂ Emissions",
    "section": "",
    "text": "You can launch this notebook in the US GHG Center JupyterHub by clicking the link below.\nLaunch in the US GHG Center JupyterHub (requires access)",
    "crumbs": [
      "Data Usage Notebooks",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "ODIAC Fossil Fuel CO₂ Emissions"
    ]
  },
  {
    "objectID": "user_data_notebooks/odiac-ffco2-monthgrid-v2023_User_Notebook.html#approach",
    "href": "user_data_notebooks/odiac-ffco2-monthgrid-v2023_User_Notebook.html#approach",
    "title": "ODIAC Fossil Fuel CO₂ Emissions",
    "section": "Approach",
    "text": "Approach\n\nIdentify available dates and temporal frequency of observations for the given collection using the GHGC API /stac endpoint. Collection processed in this notebook is ODIAC CO₂ emissions version 2023.\nPass the STAC item into raster API /stac/tilejson.json endpoint\nWe’ll visualize two tiles (side-by-side) allowing for comparison of each of the time points using folium.plugins.DualMap\nAfter the visualization, we’ll perform zonal statistics for a given polygon.",
    "crumbs": [
      "Data Usage Notebooks",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "ODIAC Fossil Fuel CO₂ Emissions"
    ]
  },
  {
    "objectID": "user_data_notebooks/odiac-ffco2-monthgrid-v2023_User_Notebook.html#about-the-data",
    "href": "user_data_notebooks/odiac-ffco2-monthgrid-v2023_User_Notebook.html#about-the-data",
    "title": "ODIAC Fossil Fuel CO₂ Emissions",
    "section": "About the Data",
    "text": "About the Data\nThe Open-Data Inventory for Anthropogenic Carbon dioxide (ODIAC) is a high-spatial resolution global emission data product of CO₂ emissions from fossil fuel combustion (Oda and Maksyutov, 2011). ODIAC pioneered the combined use of space-based nighttime light data and individual power plant emission/location profiles to estimate the global spatial extent of fossil fuel CO₂ emissions. With the innovative emission modeling approach, ODIAC achieved the fine picture of global fossil fuel CO₂ emissions at a 1x1km.\nFor more information regarding this dataset, please visit the ODIAC Fossil Fuel CO₂ Emissions data overview page.",
    "crumbs": [
      "Data Usage Notebooks",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "ODIAC Fossil Fuel CO₂ Emissions"
    ]
  },
  {
    "objectID": "user_data_notebooks/odiac-ffco2-monthgrid-v2023_User_Notebook.html#querying-the-stac-api",
    "href": "user_data_notebooks/odiac-ffco2-monthgrid-v2023_User_Notebook.html#querying-the-stac-api",
    "title": "ODIAC Fossil Fuel CO₂ Emissions",
    "section": "Querying the STAC API",
    "text": "Querying the STAC API\nFirst, we are going to import the required libraries. Once imported, they allow better executing a query in the GHG Center Spatio Temporal Asset Catalog (STAC) Application Programming Interface (API) where the granules for this collection are stored.\n\n# Import the following libraries\nimport requests\nimport folium\nimport folium.plugins\nfrom folium import Map, TileLayer\nfrom pystac_client import Client\nimport branca\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n\n# Provide the STAC and RASTER API endpoints\n# The endpoint is referring to a location within the API that executes a request on a data collection nesting on the server.\n\n# The STAC API is a catalog of all the existing data collections that are stored in the GHG Center.\nSTAC_API_URL = \"https://earth.gov/ghgcenter/api/stac\"\n\n# The RASTER API is used to fetch collections for visualization\nRASTER_API_URL = \"https://earth.gov/ghgcenter/api/raster\"\n\n# The collection name is used to fetch the dataset from the STAC API. First, we define the collection name as a variable\n# Name of the collection for ODIAC dataset \ncollection_name = \"odiac-ffco2-monthgrid-v2023\"\n\n\n# Fetch the collection from the STAC API using the appropriate endpoint\n# The 'requests' library allows a HTTP request possible\ncollection = requests.get(f\"{STAC_API_URL}/collections/{collection_name}\").json()\n\n# Print the properties of the collection to the console\ncollection\n\nExamining the contents of our collection under summaries we see that the data is available from January 2000 to December 2022. By looking at the dashboard:time density we observe that the periodic frequency of these observations is monthly.\n\n# Create a function that would search for a data collection in the US GHG Center STAC API\n\n# First, we need to define the function\n# The name of the function = \"get_item_count\"\n# The argument that will be passed through the defined function = \"collection_id\"\ndef get_item_count(collection_id):\n\n    # Set a counter for the number of items existing in the collection\n    count = 0\n\n    # Define the path to retrieve the granules (items) of the collection of interest in the STAC API\n    items_url = f\"{STAC_API_URL}/collections/{collection_id}/items\"\n\n    # Run a while loop to make HTTP requests until there are no more URLs associated with the collection in the STAC API\n    while True:\n\n        # Retrieve information about the granules by sending a \"get\" request to the STAC API using the defined collection path\n        response = requests.get(items_url)\n\n        # If the items do not exist, print an error message and quit the loop\n        if not response.ok:\n            print(\"error getting items\")\n            exit()\n\n        # Return the results of the HTTP response as JSON\n        stac = response.json()\n\n        # Increase the \"count\" by the number of items (granules) returned in the response\n        count += int(stac[\"context\"].get(\"returned\", 0))\n\n        # Retrieve information about the next URL associated with the collection in the STAC API (if applicable)\n        next = [link for link in stac[\"links\"] if link[\"rel\"] == \"next\"]\n\n        # Exit the loop if there are no other URLs\n        if not next:\n            break\n        \n        # Ensure the information gathered by other STAC API links associated with the collection are added to the original path\n        # \"href\" is the identifier for each of the tiles stored in the STAC API\n        items_url = next[0][\"href\"]\n\n    # Return the information about the total number of granules found associated with the collection\n    return count\n\n\n# Apply the function created above \"get_item_count\" to the data collection\nnumber_of_items = get_item_count(collection_name)\n\n# Get the information about the number of granules found in the collection\nitems = requests.get(f\"{STAC_API_URL}/collections/{collection_name}/items?limit={number_of_items}\").json()[\"features\"]\n\n# Print the total number of items (granules) found\nprint(f\"Found {len(items)} items\")\n\nThis makes sense as there are 23 years between 2000 - 2023, with 12 months per year, meaning 276 records in total.\n\n# Examine the first item in the collection\n# Keep in mind that a list starts from 0, 1, 2... therefore items[0] is referring to the first item in the list/collection\nitems[0]",
    "crumbs": [
      "Data Usage Notebooks",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "ODIAC Fossil Fuel CO₂ Emissions"
    ]
  },
  {
    "objectID": "user_data_notebooks/odiac-ffco2-monthgrid-v2023_User_Notebook.html#exploring-changes-in-carbon-dioxide-co₂-levels-using-the-raster-api",
    "href": "user_data_notebooks/odiac-ffco2-monthgrid-v2023_User_Notebook.html#exploring-changes-in-carbon-dioxide-co₂-levels-using-the-raster-api",
    "title": "ODIAC Fossil Fuel CO₂ Emissions",
    "section": "Exploring Changes in Carbon Dioxide (CO₂) levels using the Raster API",
    "text": "Exploring Changes in Carbon Dioxide (CO₂) levels using the Raster API\nWe will explore changes in fossil fuel emissions in urban egions. In this notebook, we’ll explore the impacts of these emissions and explore these changes over time. We’ll then visualize the outputs on a map using folium.\n\n# Now we create a dictionary where the start datetime values for each granule is queried more explicitly by year and month (e.g., 2020-02)\nitems = {item[\"properties\"][\"start_datetime\"][:7]: item for item in items} \n\n# Next, we need to specify the asset name for this collection\n# The asset name is referring to the raster band containing the pixel values for the parameter of interest\n# For the case of the ODIAC Fossil Fuel CO₂ Emissions collection, the parameter of interest is “co2-emissions”\nasset_name = \"co2-emissions\"\n\nBelow, we are entering the minimum and maximum values to provide our upper and lower bounds in rescale_values.\n\n# Fetching the min and max values for a specific item\nrescale_values = {\"max\":items[list(items.keys())[0]][\"assets\"][asset_name][\"raster:bands\"][0][\"histogram\"][\"max\"], \"min\":items[list(items.keys())[0]][\"assets\"][asset_name][\"raster:bands\"][0][\"histogram\"][\"min\"]}\n\nNow, we will pass the item id, collection name, asset name, and the rescaling factor to the Raster API endpoint. We will do this twice, once for January 2020 and again for January 2000, so that we can visualize each event independently.\n\n# Choose a color map for displaying the first observation (event)\n# Please refer to matplotlib library if you'd prefer choosing a different color ramp.\n# For more information on Colormaps in Matplotlib, please visit https://matplotlib.org/stable/users/explain/colors/colormaps.html\ncolor_map = \"rainbow\" \n\n# Make a GET request to retrieve information for the 2020 tile\n# 2020\njanuary_2020_tile = requests.get(\n\n    # Pass the collection name, the item number in the list, and its ID\n    f\"{RASTER_API_URL}/stac/tilejson.json?collection={items['2020-01']['collection']}&item={items['2020-01']['id']}\"\n\n    # Pass the asset name\n    f\"&assets={asset_name}\"\n\n    # Pass the color formula and colormap for custom visualization\n    f\"&color_formula=gamma+r+1.05&colormap_name={color_map}\"\n\n    # Pass the minimum and maximum values for rescaling\n    f\"&rescale={rescale_values['min']},{rescale_values['max']}\", \n\n# Return the response in JSON format\n).json()\n\n# Print the properties of the retrieved granule to the console\njanuary_2020_tile\n\n\n# Make a GET request to retrieve information for the 2000 tile\n# 2000\njanuary_2000_tile = requests.get(\n\n    # Pass the collection name, the item number in the list, and its ID\n    f\"{RASTER_API_URL}/stac/tilejson.json?collection={items['2000-01']['collection']}&item={items['2000-01']['id']}\"\n\n    # Pass the asset name\n    f\"&assets={asset_name}\"\n\n    # Pass the color formula and colormap for custom visualization\n    f\"&color_formula=gamma+r+1.05&colormap_name={color_map}\"\n\n    # Pass the minimum and maximum values for rescaling\n    f\"&rescale={rescale_values['min']},{rescale_values['max']}\", \n\n# Return the response in JSON format\n).json()\n\n# Print the properties of the retrieved granule to the console\njanuary_2000_tile",
    "crumbs": [
      "Data Usage Notebooks",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "ODIAC Fossil Fuel CO₂ Emissions"
    ]
  },
  {
    "objectID": "user_data_notebooks/odiac-ffco2-monthgrid-v2023_User_Notebook.html#visualizing-co₂-emissions",
    "href": "user_data_notebooks/odiac-ffco2-monthgrid-v2023_User_Notebook.html#visualizing-co₂-emissions",
    "title": "ODIAC Fossil Fuel CO₂ Emissions",
    "section": "Visualizing CO₂ emissions",
    "text": "Visualizing CO₂ emissions\n\n# To change the location, you can simply insert the latitude and longitude of the area of your interest in the \"location=(LAT, LONG)\" statement\n\n# Set the initial zoom level and center of map for both tiles\n# 'folium.plugins' allows mapping side-by-side\nmap_ = folium.plugins.DualMap(location=(34, -118), zoom_start=6)\n\n# Define the first map layer (January 2020)\nmap_layer_2020 = TileLayer(\n    tiles=january_2020_tile[\"tiles\"][0], # Path to retrieve the tile\n    attr=\"GHG\", # Set the attribution\n    opacity=0.8, # Adjust the transparency of the layer\n)\n\n# Add the first layer to the Dual Map\nmap_layer_2020.add_to(map_.m1)\n\n# Define the second map layer (January 2000)\nmap_layer_2000 = TileLayer(\n    tiles=january_2000_tile[\"tiles\"][0], # Path to retrieve the tile\n    attr=\"GHG\", # Set the attribution\n    opacity=0.8, # Adjust the transparency of the layer\n)\n\n# Add the second layer to the Dual Map\nmap_layer_2000.add_to(map_.m2)\n\n# Visualize the Dual Map\nmap_",
    "crumbs": [
      "Data Usage Notebooks",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "ODIAC Fossil Fuel CO₂ Emissions"
    ]
  },
  {
    "objectID": "user_data_notebooks/odiac-ffco2-monthgrid-v2023_User_Notebook.html#visualizing-the-data-as-a-time-series",
    "href": "user_data_notebooks/odiac-ffco2-monthgrid-v2023_User_Notebook.html#visualizing-the-data-as-a-time-series",
    "title": "ODIAC Fossil Fuel CO₂ Emissions",
    "section": "Visualizing the Data as a Time Series",
    "text": "Visualizing the Data as a Time Series\nWe can now explore the ODIAC fossil fuel emission time series available (January 2000 -December 2022) for the Texas, Dallas area of USA. We can plot the data set using the code below:\n\n# Figure size: 20 representing the width, 10 representing the height\nfig = plt.figure(figsize=(20, 10))\n\n\nplt.plot(\n    df[\"date\"], # X-axis: sorted datetime\n    df[\"max\"], # Y-axis: maximum CO₂ level\n    color=\"red\", # Line color\n    linestyle=\"-\", # Line style\n    linewidth=0.5, # Line width\n    label=\"Max monthly CO₂ emissions\", # Legend label\n)\n\n# Display legend\nplt.legend()\n\n# Insert label for the X-axis\nplt.xlabel(\"Years\")\n\n# Insert label for the Y-axis\nplt.ylabel(\"CO2 emissions gC/m2/d\")\n\n# Insert title for the plot\nplt.title(\"CO2 emission Values for Texas, Dallas (2000-2022)\")\n\n###\n# Add data citation\nplt.text(\n    df[\"date\"].iloc[0],           # X-coordinate of the text\n    df[\"max\"].min(),              # Y-coordinate of the text\n\n\n\n\n    # Text to be displayed\n    \"Source: NASA ODIAC Fossil Fuel CO₂ Emissions\",                  \n    fontsize=12,                             # Font size\n    horizontalalignment=\"right\",             # Horizontal alignment\n    verticalalignment=\"top\",                 # Vertical alignment\n    color=\"blue\",                            # Text color\n)\n\n# Plot the time series\nplt.show()\n\n\n# Print the properties of the 3rd item in the collection\nprint(items[2][\"properties\"][\"start_datetime\"])\n\n\n# A GET request is made for the October tile\noctober_tile = requests.get(\n\n    # Pass the collection name, the item number in the list, and its ID\n    f\"{RASTER_API_URL}/stac/tilejson.json?collection={items[2]['collection']}&item={items[2]['id']}\"\n\n    # Pass the asset name\n    f\"&assets={asset_name}\"\n\n    # Pass the color formula and colormap for custom visualization\n    f\"&color_formula=gamma+r+1.05&colormap_name={color_map}\"\n\n    # Pass the minimum and maximum values for rescaling\n    f\"&rescale={rescale_values['min']},{rescale_values['max']}\",\n\n# Return the response in JSON format\n).json()\n\n# Print the properties of the retrieved granule to the console\noctober_tile\n\n\n# Create a new map to display the October tile\naoi_map_bbox = Map(\n\n    # Base map is set to OpenStreetMap\n    tiles=\"OpenStreetMap\",\n\n    # Set the center of the map\n    location=[\n        30,-100\n    ],\n\n    # Set the zoom value\n    zoom_start=8,\n)\n\n# Define the map layer\nmap_layer = TileLayer(\n\n    # Path to retrieve the tile\n    tiles=october_tile[\"tiles\"][0],\n\n    # Set the attribution and adjust the transparency of the layer\n    attr=\"GHG\", opacity = 0.5\n)\n\n# Add the layer to the map\nmap_layer.add_to(aoi_map_bbox)\n\n# Visualize the map\naoi_map_bbox",
    "crumbs": [
      "Data Usage Notebooks",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "ODIAC Fossil Fuel CO₂ Emissions"
    ]
  },
  {
    "objectID": "user_data_notebooks/odiac-ffco2-monthgrid-v2023_User_Notebook.html#summary",
    "href": "user_data_notebooks/odiac-ffco2-monthgrid-v2023_User_Notebook.html#summary",
    "title": "ODIAC Fossil Fuel CO₂ Emissions",
    "section": "Summary",
    "text": "Summary\nIn this notebook we have successfully explored, analysed and visualized STAC collecetion for ODIAC C02 fossisl fuel emission (2023).\n\nInstall and import the necessary libraries\nFetch the collection from STAC collections using the appropriate endpoints\nCount the number of existing granules within the collection\nMap and compare the CO₂ levels for two distinctive months/years\nGenerate zonal statistics for the area of interest (AOI)\nVisualizing the Data as a Time Series\n\nIf you have any questions regarding this user notebook, please contact us using the feedback form.",
    "crumbs": [
      "Data Usage Notebooks",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "ODIAC Fossil Fuel CO₂ Emissions"
    ]
  },
  {
    "objectID": "user_data_notebooks/casagfed-carbonflux-monthgrid-v3_User_Notebook.html",
    "href": "user_data_notebooks/casagfed-carbonflux-monthgrid-v3_User_Notebook.html",
    "title": "CASA-GFED3 Land Carbon Flux",
    "section": "",
    "text": "You can launch this notebook in the US GHG Center JupyterHub by clicking the link below.\nLaunch in the US GHG Center JupyterHub (requires access)"
  },
  {
    "objectID": "user_data_notebooks/casagfed-carbonflux-monthgrid-v3_User_Notebook.html#run-this-notebook",
    "href": "user_data_notebooks/casagfed-carbonflux-monthgrid-v3_User_Notebook.html#run-this-notebook",
    "title": "CASA-GFED3 Land Carbon Flux",
    "section": "",
    "text": "You can launch this notebook in the US GHG Center JupyterHub by clicking the link below.\nLaunch in the US GHG Center JupyterHub (requires access)"
  },
  {
    "objectID": "user_data_notebooks/casagfed-carbonflux-monthgrid-v3_User_Notebook.html#approach",
    "href": "user_data_notebooks/casagfed-carbonflux-monthgrid-v3_User_Notebook.html#approach",
    "title": "CASA-GFED3 Land Carbon Flux",
    "section": "Approach",
    "text": "Approach\n\nIdentify available dates and temporal frequency of observations for a given collection using the GHGC API /stac endpoint. The collection processed in this notebook is the Land-Atmosphere Carbon Flux data product.\nPass the STAC item into the raster API /stac/tilejson.jsonendpoint.\nUsing folium.plugins.DualMap, visualize two tiles (side-by-side), allowing time point comparison.\nAfter the visualization, perform zonal statistics for a given polygon."
  },
  {
    "objectID": "user_data_notebooks/casagfed-carbonflux-monthgrid-v3_User_Notebook.html#about-the-data",
    "href": "user_data_notebooks/casagfed-carbonflux-monthgrid-v3_User_Notebook.html#about-the-data",
    "title": "CASA-GFED3 Land Carbon Flux",
    "section": "About the Data",
    "text": "About the Data\nThis dataset presents a variety of carbon flux parameters derived from the Carnegie-Ames-Stanford-Approach – Global Fire Emissions Database version 3 (CASA-GFED3) model. The model’s input data includes air temperature, precipitation, incident solar radiation, a soil classification map, and a number of satellite derived products. All model calculations are driven by analyzed meteorological data from NASA’s Modern-Era Retrospective analysis for Research and Application, Version 2 (MERRA-2). The resulting product provides monthly, global data at 0.5 degree resolution from January 2003 through December 2017. It includes the following carbon flux variables expressed in units of kilograms of carbon per square meter per month (kg Carbon m²/mon) from the following sources: net primary production (NPP), net ecosystem exchange (NEE), heterotrophic respiration (Rh), wildfire emissions (FIRE), and fuel wood burning emissions (FUEL). This product and earlier versions of MERRA-driven CASA-GFED carbon fluxes have been used in a number of atmospheric CO₂ transport studies, and through the support of NASA’s Carbon Monitoring System (CMS), it helps characterize, quantify, understand and predict the evolution of global carbon sources and sinks."
  },
  {
    "objectID": "user_data_notebooks/casagfed-carbonflux-monthgrid-v3_User_Notebook.html#querying-the-stac-api",
    "href": "user_data_notebooks/casagfed-carbonflux-monthgrid-v3_User_Notebook.html#querying-the-stac-api",
    "title": "CASA-GFED3 Land Carbon Flux",
    "section": "Querying the STAC API",
    "text": "Querying the STAC API\nPlease run the next cell to import the required libraries.\n\nimport requests\nimport folium\nimport folium.plugins\nfrom folium import Map, TileLayer \nfrom pystac_client import Client \nimport branca \nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n\n# Provide STAC and RASTER API endpoints\nSTAC_API_URL = \"https://earth.gov/ghgcenter/api/stac\"\nRASTER_API_URL = \"https://earth.gov/ghgcenter/api/raster\"\n\n# Please use the collection name similar to the one used in the STAC collection.\n# Name of the collection for CASA GFED Land-Atmosphere Carbon Flux monthly emissions. \ncollection_name = \"casagfed-carbonflux-monthgrid-v3\"\n\n\n# Fetch the collection from STAC collections using the appropriate endpoint\n# the 'requests' library allows a HTTP request possible\ncollection = requests.get(f\"{STAC_API_URL}/collections/{collection_name}\").json()\ncollection\n\nExamining the contents of our collection under the temporal variable, we see that the data is available from January 2003 to December 2017. By looking at the dashboard:time density, we observe that the periodic frequency of these observations is monthly.\n\n# Create a function that would search for the above data collection in the STAC API\ndef get_item_count(collection_id):\n    count = 0\n    items_url = f\"{STAC_API_URL}/collections/{collection_id}/items\"\n\n    while True:\n        response = requests.get(items_url)\n\n        if not response.ok:\n            print(\"error getting items\")\n            exit()\n\n        stac = response.json()\n        count += int(stac[\"context\"].get(\"returned\", 0))\n        next = [link for link in stac[\"links\"] if link[\"rel\"] == \"next\"]\n\n        if not next:\n            break\n        items_url = next[0][\"href\"]\n\n    return count\n\n\n# Apply the above function and check the total number of items available within the collection\nnumber_of_items = get_item_count(collection_name)\nitems = requests.get(f\"{STAC_API_URL}/collections/{collection_name}/items?limit={number_of_items}\").json()[\"features\"]\nprint(f\"Found {len(items)} items\")\n\n\n# Examine the first item in the collection\nitems[0]"
  },
  {
    "objectID": "user_data_notebooks/casagfed-carbonflux-monthgrid-v3_User_Notebook.html#exploring-changes-in-carbon-flux-levels-using-the-raster-api",
    "href": "user_data_notebooks/casagfed-carbonflux-monthgrid-v3_User_Notebook.html#exploring-changes-in-carbon-flux-levels-using-the-raster-api",
    "title": "CASA-GFED3 Land Carbon Flux",
    "section": "Exploring Changes in Carbon Flux Levels Using the Raster API",
    "text": "Exploring Changes in Carbon Flux Levels Using the Raster API\nWe will explore changes in the land atmosphere Carbon flux Heterotrophic Respiration and examine their impacts over time. We’ll then visualize the outputs on a map using folium.\n\n# To access the year value from each item more easily, this will let us query more explicitly by year and month (e.g., 2020-02)\nitems = {item[\"properties\"][\"start_datetime\"][:7]: item for item in items} \n# rh = Heterotrophic Respiration\nasset_name = \"rh\"\n\nBelow, we are entering the minimum and maximum values to provide our upper and lower bounds in rescale_values.\n\nrescale_values = {\"max\":items[list(items.keys())[0]][\"assets\"][asset_name][\"raster:bands\"][0][\"histogram\"][\"max\"], \"min\":items[list(items.keys())[0]][\"assets\"][asset_name][\"raster:bands\"][0][\"histogram\"][\"min\"]}\n\nNow, we will pass the item id, collection name, and rescaling_factor to the Raster API endpoint. We will do this twice, once for December 2003 and again for December 2017, so that we can visualize each event independently.\n\ncolor_map = \"purd\" # please refer to matplotlib library if you'd prefer choosing a different color ramp.\n# For more information on Colormaps in Matplotlib, please visit https://matplotlib.org/stable/users/explain/colors/colormaps.html\n\n# To change the year and month of the observed parameter, you can modify the \"items['YYYY-MM']\" statement\n# For example, you can change the current statement \"items['2003-12']\" to \"items['2016-10']\" \ndecember_2003_tile = requests.get(\n    f\"{RASTER_API_URL}/stac/tilejson.json?collection={items['2003-12']['collection']}&item={items['2003-12']['id']}\"\n    f\"&assets={asset_name}\"\n    f\"&color_formula=gamma+r+1.05&colormap_name={color_map}\"\n    f\"&rescale={rescale_values['min']},{rescale_values['max']}\", \n).json()\ndecember_2003_tile\n\n\n# Now we apply the same process used in the previous step for the December 2017 tile\ndecember_2017_tile = requests.get(\n    f\"{RASTER_API_URL}/stac/tilejson.json?collection={items['2017-12']['collection']}&item={items['2017-12']['id']}\"\n    f\"&assets={asset_name}\"\n    f\"&color_formula=gamma+r+1.05&colormap_name={color_map}\"\n    f\"&rescale={rescale_values['min']},{rescale_values['max']}\", \n).json()\ndecember_2017_tile"
  },
  {
    "objectID": "user_data_notebooks/casagfed-carbonflux-monthgrid-v3_User_Notebook.html#visualizing-land-atmosphere-carbon-flux-heterotrophic-respiration",
    "href": "user_data_notebooks/casagfed-carbonflux-monthgrid-v3_User_Notebook.html#visualizing-land-atmosphere-carbon-flux-heterotrophic-respiration",
    "title": "CASA-GFED3 Land Carbon Flux",
    "section": "Visualizing Land-Atmosphere Carbon Flux (Heterotrophic Respiration)",
    "text": "Visualizing Land-Atmosphere Carbon Flux (Heterotrophic Respiration)\n\n# For this study we are going to compare the RH level in 2003 and 2017 over the State of Texas \n# To change the location, you can simply insert the latitude and longitude of the area of your interest in the \"location=(LAT, LONG)\" statement\n# For example, you can change the current statement \"location=(31.9, -99.9)\" to \"location=(34, -118)\" to monitor the RH level in California instead of Texas\n\n# Set initial zoom and center of map for CO₂ Layer\n# 'folium.plugins' allows mapping side-by-side\nmap_ = folium.plugins.DualMap(location=(31.9, -99.9), zoom_start=6)\n\n# The TileLayer library helps in manipulating and displaying raster layers on a map\n# December 2003\nmap_layer_2003 = TileLayer(\n    tiles=december_2003_tile[\"tiles\"][0],\n    attr=\"GHG\",\n    opacity=0.8,\n    name=\"December 2003 RH Level\",\n    overlay= True,\n    legendEnabled = True\n)\nmap_layer_2003.add_to(map_.m1)\n\n\n# December 2017\nmap_layer_2017 = TileLayer(\n    tiles=december_2017_tile[\"tiles\"][0],\n    attr=\"GHG\",\n    opacity=0.8,\n    name=\"December 2017 RH Level\",\n    overlay= True,\n    legendEnabled = True\n)\nmap_layer_2017.add_to(map_.m2)\n\n\n# Display data markers (titles) on both maps\nfolium.Marker((40, 5.0), tooltip=\"both\").add_to(map_)\nfolium.LayerControl(collapsed=False).add_to(map_)\n\n\n# Add a legend to the dual map using the 'branca' library. \n# Note: the inserted legend is representing the minimum and maximum values for both tiles.\ncolormap = branca.colormap.linear.PuRd_09.scale(0, 0.3) # minimum value = 0, maximum value = 0.3 (kg Carbon/m2/month)\ncolormap = colormap.to_step(index=[0, 0.07, 0.15, 0.22, 0.3])\ncolormap.caption = 'Rh Values (kg Carbon/m2/month)'\n\ncolormap.add_to(map_.m1)\n\n\n# Visualizing the map\nmap_"
  },
  {
    "objectID": "user_data_notebooks/casagfed-carbonflux-monthgrid-v3_User_Notebook.html#visualizing-the-data-as-a-time-series",
    "href": "user_data_notebooks/casagfed-carbonflux-monthgrid-v3_User_Notebook.html#visualizing-the-data-as-a-time-series",
    "title": "CASA-GFED3 Land Carbon Flux",
    "section": "Visualizing the Data as a Time Series",
    "text": "Visualizing the Data as a Time Series\nWe can now explore the Heterotrophic Respiration time series (January 2003 -December 2017) available for the Dallas, Texas area. We can plot the data set using the code below:\n\nfig = plt.figure(figsize=(20, 10)) #determine the width and height of the plot using the 'matplotlib' library\n\nplt.plot(\n    df[\"date\"],\n    df[\"max\"],\n    color=\"purple\",\n    linestyle=\"-\",\n    linewidth=0.5,\n    label=\"Max monthly Carbon emissions\",\n)\n\nplt.legend()\nplt.xlabel(\"Years\")\nplt.ylabel(\"kg Carbon/m2/month\")\nplt.title(\"Heterotrophic Respiration Values for Dallas, Texas (2003-2017)\")\n\n\n# Now let's examine the Rh level for the 3rd item in the collection for Dallas, Texas area\n# Keep in mind that a list starts from 0, 1, 2,... therefore items[2] is referring to the third item in the list/collection\nprint(items[2][\"properties\"][\"start_datetime\"]) #print the start Date Time of the third granule in the collection!\n\n\n# Fetch the third granule in the collection and set the color scheme and rescale values. \noctober_tile = requests.get(\n    f\"{RASTER_API_URL}/stac/tilejson.json?collection={items[2]['collection']}&item={items[2]['id']}\"\n    f\"&assets={asset_name}\"\n    f\"&color_formula=gamma+r+1.05&colormap_name={color_map}\"\n    f\"&rescale={rescale_values['min']},{rescale_values['max']}\",\n).json()\noctober_tile\n\n\n# Map the Rh level for the Dallas, Texas area for the October, 2017 timeframe\naoi_map_bbox = Map(\n    tiles=\"OpenStreetMap\",\n    location=[\n        32.8, # latitude\n        -96.79, # longitude\n    ],\n    zoom_start=9,\n)\n\nmap_layer = TileLayer(\n    tiles=october_tile[\"tiles\"][0],\n    attr=\"GHG\", opacity = 0.7, name=\"October 2017 RH Level\", overlay= True, legendEnabled = True\n)\n\nmap_layer.add_to(aoi_map_bbox)\n\n# Display data marker (title) on the map\nfolium.Marker((40, 5.9), tooltip=\"both\").add_to(aoi_map_bbox)\nfolium.LayerControl(collapsed=False).add_to(aoi_map_bbox)\n\n# Add a legend\ncolormap = branca.colormap.linear.PuRd_09.scale(0, 0.3) # minimum value = 0, maximum value = 0.3 (kg Carbon/m2/month)\ncolormap = colormap.to_step(index=[0, 0.07, 0.15, 0.22, 0.3])\ncolormap.caption = 'Rh Values (kg Carbon/m2/month)'\n\ncolormap.add_to(aoi_map_bbox)\n\naoi_map_bbox"
  },
  {
    "objectID": "user_data_notebooks/casagfed-carbonflux-monthgrid-v3_User_Notebook.html#summary",
    "href": "user_data_notebooks/casagfed-carbonflux-monthgrid-v3_User_Notebook.html#summary",
    "title": "CASA-GFED3 Land Carbon Flux",
    "section": "Summary",
    "text": "Summary\nIn this notebook we have successfully completed the following steps for the STAC collection for CASA GFED Land-Atmosphere Carbon Flux data: 1. Install and import the necessary libraries 2. Fetch the collection from STAC collections using the appropriate endpoints 3. Count the number of existing granules within the collection 4. Map and compare the Heterotrophic Respiration (Rh) levels over the Dallas, Texas area for two distinctive years 5. Create a table that displays the minimum, maximum, and sum of the Rh values for a specified region 6. Generate a time-series graph of the Rh values for a specified region\nIf you have any questions regarding this user notebook, please contact us using the feedback form."
  },
  {
    "objectID": "user_data_notebooks/tm54dvar-ch4flux-monthgrid-v1_User_Notebook.html",
    "href": "user_data_notebooks/tm54dvar-ch4flux-monthgrid-v1_User_Notebook.html",
    "title": "TM5-4DVar Isotopic CH₄ Inverse Fluxes",
    "section": "",
    "text": "You can launch this notebook in the US GHG Center JupyterHub by clicking the link below.\nLaunch in the US GHG Center JupyterHub (requires access)",
    "crumbs": [
      "Data Usage Notebooks",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "TM5-4DVar Isotopic CH₄ Inverse Fluxes"
    ]
  },
  {
    "objectID": "user_data_notebooks/tm54dvar-ch4flux-monthgrid-v1_User_Notebook.html#run-this-notebook",
    "href": "user_data_notebooks/tm54dvar-ch4flux-monthgrid-v1_User_Notebook.html#run-this-notebook",
    "title": "TM5-4DVar Isotopic CH₄ Inverse Fluxes",
    "section": "",
    "text": "You can launch this notebook in the US GHG Center JupyterHub by clicking the link below.\nLaunch in the US GHG Center JupyterHub (requires access)",
    "crumbs": [
      "Data Usage Notebooks",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "TM5-4DVar Isotopic CH₄ Inverse Fluxes"
    ]
  },
  {
    "objectID": "user_data_notebooks/tm54dvar-ch4flux-monthgrid-v1_User_Notebook.html#approach",
    "href": "user_data_notebooks/tm54dvar-ch4flux-monthgrid-v1_User_Notebook.html#approach",
    "title": "TM5-4DVar Isotopic CH₄ Inverse Fluxes",
    "section": "Approach",
    "text": "Approach\n\nIdentify available dates and temporal frequency of observations for the given collection using the GHGC API /stac endpoint. The collection processed in this notebook is the TM5-4DVar Isotopic CH₄ Inverse Fluxes Data product.\nPass the STAC item into the raster API /stac/tilejson.jsonendpoint.\nUsing folium.plugins.DualMap, we will visualize two tiles (side-by-side), allowing us to compare time points.\nAfter the visualization, we will perform zonal statistics for a given polygon.",
    "crumbs": [
      "Data Usage Notebooks",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "TM5-4DVar Isotopic CH₄ Inverse Fluxes"
    ]
  },
  {
    "objectID": "user_data_notebooks/tm54dvar-ch4flux-monthgrid-v1_User_Notebook.html#about-the-data",
    "href": "user_data_notebooks/tm54dvar-ch4flux-monthgrid-v1_User_Notebook.html#about-the-data",
    "title": "TM5-4DVar Isotopic CH₄ Inverse Fluxes",
    "section": "About the Data",
    "text": "About the Data\nSurface methane (CH₄) emissions are derived from atmospheric measurements of methane and its ¹³C carbon isotope content. Different sources of methane contain different ratios of the two stable isotopologues, ¹²CH₄ and ¹³CH₄. This makes normally indistinguishable collocated sources of methane, say from agriculture and oil and gas exploration, distinguishable. The National Oceanic and Atmospheric Administration (NOAA) collects whole air samples from its global cooperative network of flasks (https://gml.noaa.gov/ccgg/about.html), which are then analyzed for methane and other trace gasses. A subset of those flasks are also analyzed for ¹³C of methane in collaboration with the Institute of Arctic and Alpine Research at the University of Colorado Boulder. Scientists at the National Aeronautics and Space Administration (NASA) and NOAA used those measurements of methane and ¹³C of methane in conjunction with a model of atmospheric circulation to estimate emissions of methane separated by three source types, microbial, fossil and pyrogenic.\nFor more information regarding this dataset, please visit the TM5-4DVar Isotopic CH₄ Inverse Fluxes data overview page.",
    "crumbs": [
      "Data Usage Notebooks",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "TM5-4DVar Isotopic CH₄ Inverse Fluxes"
    ]
  },
  {
    "objectID": "user_data_notebooks/tm54dvar-ch4flux-monthgrid-v1_User_Notebook.html#querying-the-stac-api",
    "href": "user_data_notebooks/tm54dvar-ch4flux-monthgrid-v1_User_Notebook.html#querying-the-stac-api",
    "title": "TM5-4DVar Isotopic CH₄ Inverse Fluxes",
    "section": "Querying the STAC API",
    "text": "Querying the STAC API\nFirst, we are going to import the required libraries. Once imported, they allow better executing a query in the GHG Center Spatio Temporal Asset Catalog (STAC) Application Programming Interface (API) where the granules for this collection are stored.\n\n# Provide the STAC and RASTER API endpoints\n# The endpoint is referring to a location within the API that executes a request on a data collection nesting on the server.\n\n# The STAC API is a catalog of all the existing data collections that are stored in the GHG Center.\nSTAC_API_URL = \"https://earth.gov/ghgcenter/api/stac\"\n\n# The RASTER API is used to fetch collections for visualization\nRASTER_API_URL = \"https://earth.gov/ghgcenter/api/raster\"\n\n# The collection name is used to fetch the dataset from the STAC API. First, we define the collection name as a variable\n# Name of the collection for TM5 CH₄ inverse flux dataset \ncollection_name = \"tm54dvar-ch4flux-monthgrid-v1\"\n\n\n# Fetch the collection from the STAC API using the appropriate endpoint\n# The 'requests' library allows a HTTP request possible\ncollection = requests.get(f\"{STAC_API_URL}/collections/{collection_name}\").json()\n\n# Print the properties of the collection to the console\ncollection\n\nExamining the contents of our collection under the temporal variable, we see that the data is available from January 1999 to December 2016. By looking at the dashboard:time density, we observe that the data is periodic with monthly time density.\n\n# Create a function that would search for a data collection in the US GHG Center STAC API\n\n# First, we need to define the function\n# The name of the function = \"get_item_count\"\n# The argument that will be passed through the defined function = \"collection_id\"\ndef get_item_count(collection_id):\n\n    # Set a counter for the number of items existing in the collection\n    count = 0\n\n    # Define the path to retrieve the granules (items) of the collection of interest in the STAC API\n    items_url = f\"{STAC_API_URL}/collections/{collection_id}/items\"\n\n    # Run a while loop to make HTTP requests until there are no more URLs associated with the collection in the STAC API\n    while True:\n\n        # Retrieve information about the granules by sending a \"get\" request to the STAC API using the defined collection path\n        response = requests.get(items_url)\n\n        # If the items do not exist, print an error message and quit the loop\n        if not response.ok:\n            print(\"error getting items\")\n            exit()\n\n        # Return the results of the HTTP response as JSON\n        stac = response.json()\n\n        # Increase the \"count\" by the number of items (granules) returned in the response\n        count += int(stac[\"context\"].get(\"returned\", 0))\n\n        # Retrieve information about the next URL associated with the collection in the STAC API (if applicable)\n        next = [link for link in stac[\"links\"] if link[\"rel\"] == \"next\"]\n\n        # Exit the loop if there are no other URLs\n        if not next:\n            break\n        \n        # Ensure the information gathered by other STAC API links associated with the collection are added to the original path\n        # \"href\" is the identifier for each of the tiles stored in the STAC API\n        items_url = next[0][\"href\"]\n\n    # Return the information about the total number of granules found associated with the collection\n    return count\n\n\n# Apply the function created above \"get_item_count\" to the data collection\nnumber_of_items = get_item_count(collection_name)\n\n# Get the information about the number of granules found in the collection\nitems = requests.get(f\"{STAC_API_URL}/collections/{collection_name}/items?limit={number_of_items}\").json()[\"features\"]\n\n# Print the total number of items (granules) found\nprint(f\"Found {len(items)} items\")\n\n\n# Examine the first item in the collection\n# Keep in mind that a list starts from 0, 1, 2... therefore items[0] is referring to the first item in the list/collection\nitems[0]",
    "crumbs": [
      "Data Usage Notebooks",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "TM5-4DVar Isotopic CH₄ Inverse Fluxes"
    ]
  },
  {
    "objectID": "user_data_notebooks/tm54dvar-ch4flux-monthgrid-v1_User_Notebook.html#exploring-changes-in-ch₄-flux-levels-using-the-raster-api",
    "href": "user_data_notebooks/tm54dvar-ch4flux-monthgrid-v1_User_Notebook.html#exploring-changes-in-ch₄-flux-levels-using-the-raster-api",
    "title": "TM5-4DVar Isotopic CH₄ Inverse Fluxes",
    "section": "Exploring Changes in CH₄ flux Levels Using the Raster API",
    "text": "Exploring Changes in CH₄ flux Levels Using the Raster API\nIn this notebook, we will explore the global changes of CH₄ flux over time in urban regions. We will visualize the outputs on a map using folium.\n\n# Now we create a dictionary where the start datetime values for each granule is queried more explicitly by year and month (e.g., 2020-02)\nitems = {item[\"properties\"][\"start_datetime\"][:10]: item for item in items} \n\n# Next, we need to specify the asset name for this collection\n# The asset name is referring to the raster band containing the pixel values for the parameter of interest\n# For the case of the TM5-4DVar Isotopic CH₄ Inverse Fluxes collection, the parameter of interest is “fossil”\nasset_name = \"fossil\" #fossil fuel\n\nBelow, we are entering the minimum and maximum values to provide our upper and lower bounds in the rescale_values.\n\n# Fetching the min and max values for a specific item\nrescale_values = {\"max\":items[list(items.keys())[0]][\"assets\"][asset_name][\"raster:bands\"][0][\"histogram\"][\"max\"], \"min\":items[list(items.keys())[0]][\"assets\"][asset_name][\"raster:bands\"][0][\"histogram\"][\"min\"]}\n\nNow, we will pass the item id, collection name, asset name, and the rescaling factor to the Raster API endpoint. We will do this twice, once for 2016 and again for 1999, so that we can visualize each event independently.\n\n# Choose a color map for displaying the first observation (event)\n# Please refer to matplotlib library if you'd prefer choosing a different color ramp.\n# For more information on Colormaps in Matplotlib, please visit https://matplotlib.org/stable/users/explain/colors/colormaps.html\ncolor_map = \"purd\"\n\n# Make a GET request to retrieve information for the 2016 tile\nch4_flux_1 = requests.get(\n\n    # Pass the collection name, the item number in the list, and its ID\n    f\"{RASTER_API_URL}/stac/tilejson.json?collection={items['2016-12-01']['collection']}&item={items['2016-12-01']['id']}\"\n\n    # Pass the asset name\n    f\"&assets={asset_name}\"\n\n    # Pass the color formula and colormap for custom visualization\n    f\"&color_formula=gamma+r+1.05&colormap_name={color_map}\"\n\n    # Pass the minimum and maximum values for rescaling\n    f\"&rescale={rescale_values['min']},{rescale_values['max']}\", \n\n# Return the response in JSON format\n).json()\n\n# Print the properties of the retrieved granule to the console\nch4_flux_1\n\n\n# Make a GET request to retrieve information for the 1999 tile\nch4_flux_2 = requests.get(\n\n    # Pass the collection name, the item number in the list, and its ID\n    f\"{RASTER_API_URL}/stac/tilejson.json?collection={items['1999-12-01']['collection']}&item={items['1999-12-01']['id']}\"\n\n    # Pass the asset name\n    f\"&assets={asset_name}\"\n\n    # Pass the color formula and colormap for custom visualization\n    f\"&color_formula=gamma+r+1.05&colormap_name={color_map}\"\n\n    # Pass the minimum and maximum values for rescaling\n    f\"&rescale={rescale_values['min']},{rescale_values['max']}\", \n\n# Return the response in JSON format\n).json()\n\n# Print the properties of the retrieved granule to the console\nch4_flux_2",
    "crumbs": [
      "Data Usage Notebooks",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "TM5-4DVar Isotopic CH₄ Inverse Fluxes"
    ]
  },
  {
    "objectID": "user_data_notebooks/tm54dvar-ch4flux-monthgrid-v1_User_Notebook.html#visualizing-ch₄-flux-emissions-from-fossil-fuel",
    "href": "user_data_notebooks/tm54dvar-ch4flux-monthgrid-v1_User_Notebook.html#visualizing-ch₄-flux-emissions-from-fossil-fuel",
    "title": "TM5-4DVar Isotopic CH₄ Inverse Fluxes",
    "section": "Visualizing CH₄ flux Emissions from Fossil Fuel",
    "text": "Visualizing CH₄ flux Emissions from Fossil Fuel\n\n# For this study we are going to compare CH4 fluxes from fossil fuels in 2016 and 1999 along the coast of California\n# To change the location, you can simply insert the latitude and longitude of the area of your interest in the \"location=(LAT, LONG)\" statement\n\n# Set the initial zoom level and center of map for both tiles\n# 'folium.plugins' allows mapping side-by-side\nmap_ = folium.plugins.DualMap(location=(34, -118), zoom_start=6)\n\n# Define the first map layer (2016)\nmap_layer_2016 = TileLayer(\n    tiles=ch4_flux_1[\"tiles\"][0], # Path to retrieve the tile\n    attr=\"GHG\", # Set the attribution\n    opacity=0.8, # Adjust the transparency of the layer\n)\n# Add the first layer to the Dual Map\nmap_layer_2016.add_to(map_.m1)\n\n\n# Define the second map layer (1999)\nmap_layer_1999 = TileLayer(\n    tiles=ch4_flux_2[\"tiles\"][0], # Path to retrieve the tile\n    attr=\"GHG\", # Set the attribution\n    opacity=0.8, # Adjust the transparency of the layer\n)\n\n# Add the second layer to the Dual Map\nmap_layer_1999.add_to(map_.m2)\n\n# Visualize the Dual Map\nmap_",
    "crumbs": [
      "Data Usage Notebooks",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "TM5-4DVar Isotopic CH₄ Inverse Fluxes"
    ]
  },
  {
    "objectID": "user_data_notebooks/tm54dvar-ch4flux-monthgrid-v1_User_Notebook.html#visualizing-the-data-as-a-time-series",
    "href": "user_data_notebooks/tm54dvar-ch4flux-monthgrid-v1_User_Notebook.html#visualizing-the-data-as-a-time-series",
    "title": "TM5-4DVar Isotopic CH₄ Inverse Fluxes",
    "section": "Visualizing the Data as a Time Series",
    "text": "Visualizing the Data as a Time Series\nWe can now explore the fossil fuel emission time series (January 1999 -December 2016) available for the Dallas, Texas area of the U.S. We can plot the data set using the code below:\n\n# Figure size: 20 representing the width, 10 representing the height\nfig = plt.figure(figsize=(20, 10))\n\nplt.plot(\n    df[\"datetime\"], # X-axis: sorted datetime\n    df[\"max\"], # Y-axis: maximum CH4 flux\n    color=\"red\", # Line color\n    linestyle=\"-\", # Line style\n    linewidth=0.5, # Line width\n    label=\"CH4 emissions\", # Legend label\n)\n\n# Display legend\nplt.legend()\n\n# Insert label for the X-axis\nplt.xlabel(\"Years\")\n\n# Insert label for the Y-axis\nplt.ylabel(\"g CH₄/m²/year\")\nplt.xticks(rotation = 90)\n\n# Insert title for the plot\nplt.title(\"CH4 emission Values for Texas, Dallas (1999-2016)\")\n\n# Add data citation\nplt.text(\n    df[\"datetime\"].iloc[0],           # X-coordinate of the text\n    df[\"max\"].min(),                  # Y-coordinate of the text\n\n\n\n\n    # Text to be displayed\n    \"Source: NASA/NOAA TM5-4DVar Isotopic CH₄ Inverse Fluxes\",                  \n    fontsize=12,                             # Font size\n    horizontalalignment=\"left\",              # Horizontal alignment\n    verticalalignment=\"top\",                 # Vertical alignment\n    color=\"blue\",                            # Text color\n)\n\n\n# Plot the time series\nplt.show()\n\n\n# Print the properties for the 3rd item in the collection\nprint(items[2][\"properties\"][\"start_datetime\"])\n\n\n# A GET request is made for the 3rd granule\nch4_flux_3 = requests.get(\n\n    # Pass the collection name, the item number in the list, and its ID\n    f\"{RASTER_API_URL}/stac/tilejson.json?collection={items[2]['collection']}&item={items[2]['id']}\"\n\n    # Pass the asset name\n    f\"&assets={asset_name}\"\n\n    # Pass the color formula and colormap for custom visualization\n    f\"&color_formula=gamma+r+1.05&colormap_name={color_map}\"\n\n    # Pass the minimum and maximum values for rescaling\n    f\"&rescale={rescale_values['min']},{rescale_values['max']}\",\n\n# Return the response in JSON format\n).json()\n\n# Print the properties of the retrieved granule to the console\nch4_flux_3\n\n\n# Create a new map to display the tile\naoi_map_bbox = Map(\n\n    # Base map is set to OpenStreetMap\n    tiles=\"OpenStreetMap\",\n\n    # Set the center of the map\n    location=[\n        30,-100\n    ],\n\n    # Set the zoom value\n    zoom_start=6.8,\n)\n\n# Define the map layer\nmap_layer = TileLayer(\n\n    # Path to retrieve the tile\n    tiles=ch4_flux_3[\"tiles\"][0],\n\n    # Set the attribution and adjust the transparency of the layer\n    attr=\"GHG\", opacity = 0.7\n)\n\n# Add the layer to the map\nmap_layer.add_to(aoi_map_bbox)\n\n# Visualize the map\naoi_map_bbox",
    "crumbs": [
      "Data Usage Notebooks",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "TM5-4DVar Isotopic CH₄ Inverse Fluxes"
    ]
  },
  {
    "objectID": "user_data_notebooks/tm54dvar-ch4flux-monthgrid-v1_User_Notebook.html#summary",
    "href": "user_data_notebooks/tm54dvar-ch4flux-monthgrid-v1_User_Notebook.html#summary",
    "title": "TM5-4DVar Isotopic CH₄ Inverse Fluxes",
    "section": "Summary",
    "text": "Summary\nIn this notebook we have successfully explored, analyzed, and visualized the STAC collection for TM5-4DVar Isotopic CH₄ Inverse Fluxes dataset.\n\nInstall and import the necessary libraries\nFetch the collection from STAC collections using the appropriate endpoints\nCount the number of existing granules within the collection\nMap and compare the CH₄ inverse fluxes for two distinctive months/years\nGenerate zonal statistics for the area of interest (AOI)\nVisualizing the Data as a Time Series\n\nIf you have any questions regarding this user notebook, please contact us using the feedback form.",
    "crumbs": [
      "Data Usage Notebooks",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "TM5-4DVar Isotopic CH₄ Inverse Fluxes"
    ]
  },
  {
    "objectID": "user_data_notebooks/nec-testbed-ghg-concentrations_User_Notebook.html",
    "href": "user_data_notebooks/nec-testbed-ghg-concentrations_User_Notebook.html",
    "title": "Carbon Dioxide and Methane Concentrations from the Northeast Corridor (NEC) Urban Test Bed",
    "section": "",
    "text": "Identify available dates and temporal frequency of observations for the given data. The collection processed in this notebook is the Atmospheric concentrations of carbon dioxide (CO₂) and methane (CH₄) collected at NIST Urban Test Bed tower sites in the Northeastern U.S.\nVisualize the time series data",
    "crumbs": [
      "Data Usage Notebooks",
      "Greenhouse Gas Concentrations",
      "Carbon Dioxide and Methane Concentrations from the Northeast Corridor (NEC) Urban Test Bed"
    ]
  },
  {
    "objectID": "user_data_notebooks/nec-testbed-ghg-concentrations_User_Notebook.html#approach",
    "href": "user_data_notebooks/nec-testbed-ghg-concentrations_User_Notebook.html#approach",
    "title": "Carbon Dioxide and Methane Concentrations from the Northeast Corridor (NEC) Urban Test Bed",
    "section": "",
    "text": "Identify available dates and temporal frequency of observations for the given data. The collection processed in this notebook is the Atmospheric concentrations of carbon dioxide (CO₂) and methane (CH₄) collected at NIST Urban Test Bed tower sites in the Northeastern U.S.\nVisualize the time series data",
    "crumbs": [
      "Data Usage Notebooks",
      "Greenhouse Gas Concentrations",
      "Carbon Dioxide and Methane Concentrations from the Northeast Corridor (NEC) Urban Test Bed"
    ]
  },
  {
    "objectID": "user_data_notebooks/nec-testbed-ghg-concentrations_User_Notebook.html#about-the-data",
    "href": "user_data_notebooks/nec-testbed-ghg-concentrations_User_Notebook.html#about-the-data",
    "title": "Carbon Dioxide and Methane Concentrations from the Northeast Corridor (NEC) Urban Test Bed",
    "section": "About the Data",
    "text": "About the Data\nNIST is engaged in research to improve measurement of greenhouse gas emissions in areas containing multiple emission sources and sinks, such as UNYies. NIST’s objective is to develop measurement tools supporting independent means to increase the accuracy of greenhouse gas emissions data at urban and regional geospatial scales. NIST has established three test beds in U.S. UNYies to develop and evaluate the performance of advanced measurement capabilities for emissions independent of their origin. Located in Indianapolis, Indiana, the Los Angeles air basin of California, and the U.S. Northeast corridor (beginning with the Baltimore/Washington D.C. region), the test beds have been selected for their varying meteorology, terrain and emissions characteristics. These test beds will serve as a means to independently diagnose the accuracy of emissions data obtained directly from emission or uptake sources.\nFor more information regarding this dataset, please visit the Carbon Dioxide and Methane Concentrations from the Northeast Corridor (NEC) Urban Test Bed data overview page.",
    "crumbs": [
      "Data Usage Notebooks",
      "Greenhouse Gas Concentrations",
      "Carbon Dioxide and Methane Concentrations from the Northeast Corridor (NEC) Urban Test Bed"
    ]
  },
  {
    "objectID": "user_data_notebooks/nec-testbed-ghg-concentrations_User_Notebook.html#querying-the-feature-vector-api",
    "href": "user_data_notebooks/nec-testbed-ghg-concentrations_User_Notebook.html#querying-the-feature-vector-api",
    "title": "Carbon Dioxide and Methane Concentrations from the Northeast Corridor (NEC) Urban Test Bed",
    "section": "Querying the Feature Vector API",
    "text": "Querying the Feature Vector API\nFirst, we are going to import the required libraries. Once imported, they allow better executing a query in the GHG Center Feature Vector Application Programming Interface (API) where the items for this collection are stored.\n\nFEATURE_API_URL=\"https://earth.gov/ghgcenter/api/features\"\n\n\n# Function to fetch CSV data for a station with a limit parameter\ndef get_station_data_csv(station_code, gas_type, frequency, elevation_m, limit=10000):\n    # Use the ?f=csv and limit query to get more rows\n    url = f\"https://earth.gov/ghgcenter/api/features/collections/public.nist_testbed_nec_{station_code}_{gas_type}_{frequency}_concentrations/items?f=csv&elevation_m={elevation_m}&limit={limit}\"\n    print(url)\n    try:\n        response = requests.get(url)\n        print(response)\n        # Check if the response is successful\n        if response.status_code != 200:\n            print(f\"Failed to fetch data for {station_code}. Status code: {response.status_code}\")\n            return pd.DataFrame()\n\n        # Check if the content type is CSV\n        content_type = response.headers.get('Content-Type')\n        if 'text/csv' not in content_type:\n            print(f\"Unexpected content type for {station_code}: {content_type}\")\n            print(\"Response content:\", response.text)\n            return pd.DataFrame()\n\n        # Read the CSV content into a pandas DataFrame\n        csv_data = StringIO(response.text)\n        return pd.read_csv(csv_data)\n    \n    except requests.exceptions.RequestException as e:\n        print(f\"Request failed: {e}\")\n        return pd.DataFrame()",
    "crumbs": [
      "Data Usage Notebooks",
      "Greenhouse Gas Concentrations",
      "Carbon Dioxide and Methane Concentrations from the Northeast Corridor (NEC) Urban Test Bed"
    ]
  },
  {
    "objectID": "user_data_notebooks/nec-testbed-ghg-concentrations_User_Notebook.html#visualizing-the-ch₄-data-for-two-nec-stations",
    "href": "user_data_notebooks/nec-testbed-ghg-concentrations_User_Notebook.html#visualizing-the-ch₄-data-for-two-nec-stations",
    "title": "Carbon Dioxide and Methane Concentrations from the Northeast Corridor (NEC) Urban Test Bed",
    "section": "Visualizing the CH₄ data for two NEC stations",
    "text": "Visualizing the CH₄ data for two NEC stations\n\n# Get station name and elevation from metdata dataframe\n# Fetch data for UNY (elevation 230) and TMD (elevation 489), using limit=10000\n# ch4/co2 select the ghg \nuny_data = get_station_data_csv('uny', 'ch4', 'hourly', 483, limit=10000)\ntmd_data = get_station_data_csv('tmd', 'ch4', 'hourly', 561, limit=10000)\n\n# Check if data was successfully retrieved before proceeding\nif uny_data.empty or tmd_data.empty:\n    print(\"No data available for one or both stations. Exiting.\")\nelse:\n    # Convert the 'datetime' column to datetime for plotting\n    uny_data['datetime'] = pd.to_datetime(uny_data['datetime'], format='%Y-%m-%dT%H:%M:%SZ')\n    tmd_data['datetime'] = pd.to_datetime(tmd_data['datetime'], format='%Y-%m-%dT%H:%M:%SZ')\n\n    # Plot the data\n    plt.figure(figsize=(10, 6))\n    plt.plot(uny_data['datetime'], uny_data['value'], label='UNY (230m)', color='blue', marker='o')\n    plt.plot(tmd_data['datetime'], tmd_data['value'], label='TMD (489m)', color='green', marker='o')\n\n    plt.title('Methane (CH₄) Hourly Concentrations Over Time for UNY and TMD Stations')\n    plt.xlabel('Time')\n    plt.ylabel('CH4 Concentration (ppb)')\n    plt.legend()\n    plt.grid(True)\n\n    # Show plot\n    plt.show()\n\nhttps://earth.gov/ghgcenter/api/features/collections/public.nist_testbed_nec_uny_ch4_hourly_concentrations/items?f=csv&elevation_m=483&limit=10000\n&lt;Response [200]&gt;\nhttps://earth.gov/ghgcenter/api/features/collections/public.nist_testbed_nec_tmd_ch4_hourly_concentrations/items?f=csv&elevation_m=561&limit=10000\n&lt;Response [200]&gt;",
    "crumbs": [
      "Data Usage Notebooks",
      "Greenhouse Gas Concentrations",
      "Carbon Dioxide and Methane Concentrations from the Northeast Corridor (NEC) Urban Test Bed"
    ]
  },
  {
    "objectID": "user_data_notebooks/lpjeosim-wetlandch4-monthgrid-v2_User_Notebook.html",
    "href": "user_data_notebooks/lpjeosim-wetlandch4-monthgrid-v2_User_Notebook.html",
    "title": "Wetland Methane Emissions, LPJ-EOSIM Model",
    "section": "",
    "text": "You can launch this notebook in the US GHG Center JupyterHub by clicking the link below.\nLaunch in the US GHG Center JupyterHub (requires access)"
  },
  {
    "objectID": "user_data_notebooks/lpjeosim-wetlandch4-monthgrid-v2_User_Notebook.html#run-this-notebook",
    "href": "user_data_notebooks/lpjeosim-wetlandch4-monthgrid-v2_User_Notebook.html#run-this-notebook",
    "title": "Wetland Methane Emissions, LPJ-EOSIM Model",
    "section": "",
    "text": "You can launch this notebook in the US GHG Center JupyterHub by clicking the link below.\nLaunch in the US GHG Center JupyterHub (requires access)"
  },
  {
    "objectID": "user_data_notebooks/lpjeosim-wetlandch4-monthgrid-v2_User_Notebook.html#approach",
    "href": "user_data_notebooks/lpjeosim-wetlandch4-monthgrid-v2_User_Notebook.html#approach",
    "title": "Wetland Methane Emissions, LPJ-EOSIM Model",
    "section": "Approach",
    "text": "Approach\n\nIdentify available dates and temporal frequency of observations for the given collection using the GHGC API /stac endpoint. The collection processed in this notebook is the Wetland Methane Emissions, LPJ-EOSIM Model data product.\nPass the STAC item into the raster API /stac/tilejson.jsonendpoint.\nUsing folium.plugins.DualMap, visualize two tiles (side-by-side), allowing time point comparison.\nAfter the visualization, perform zonal statistics for a given polygon."
  },
  {
    "objectID": "user_data_notebooks/lpjeosim-wetlandch4-monthgrid-v2_User_Notebook.html#about-the-data",
    "href": "user_data_notebooks/lpjeosim-wetlandch4-monthgrid-v2_User_Notebook.html#about-the-data",
    "title": "Wetland Methane Emissions, LPJ-EOSIM Model",
    "section": "About the Data",
    "text": "About the Data\nMethane (CH₄) emissions from vegetated wetlands are estimated to be the largest natural source of methane in the global CH₄ budget, contributing to roughly one third of the total of natural and anthropogenic emissions. Wetland CH₄ is produced by microbes breaking down organic matter in the oxygen deprived environment of inundated soils. Due to limited data availability, the details of the role of wetland CH₄ emissions have thus far been underrepresented. Using the Earth Observation SIMulator version (LPJ-EOSIM) of the Lund-Potsdam-Jena Dynamic Global Vegetation Model (LPJ-DGVM) global CH₄ emissions from wetlands are estimated at 0.5° x 0.5 degree spatial resolution. By simulating wetland extent and using characteristics of inundated areas, such as wetland soil moisture, temperature, and carbon content, the model provides estimates of CH₄ quantities emitted into the atmosphere. This dataset shows concentrated methane sources from tropical and high latitude ecosystems. The LPJ-EOSIM Wetland Methane Emissions dataset consists of global daily model estimates of terrestrial wetland methane emissions from 1990 to the present, with data added bimonthly. The monthly data has been curated by aggregating the daily files. The estimates are regularly used in conjunction with NASA’s Goddard Earth Observing System (GEOS) model to simulate the impact of wetlands and other methane sources on atmospheric methane concentrations, to compare against satellite and airborne data, and to improve understanding and prediction of wetland emissions.\nFor more information regarding this dataset, please visit the U.S. Greenhouse Gas Center."
  },
  {
    "objectID": "user_data_notebooks/lpjeosim-wetlandch4-monthgrid-v2_User_Notebook.html#query-the-stac-api",
    "href": "user_data_notebooks/lpjeosim-wetlandch4-monthgrid-v2_User_Notebook.html#query-the-stac-api",
    "title": "Wetland Methane Emissions, LPJ-EOSIM Model",
    "section": "Query the STAC API",
    "text": "Query the STAC API\nFirst, we are going to import the required libraries. Once imported, they allow better executing a query in the GHG Center Spatio Temporal Asset Catalog (STAC) Application Programming Interface (API) where the granules for this collection are stored.\n\n# Import the following libraries\nimport requests\nimport folium\nimport folium.plugins\nfrom folium import Map, TileLayer\nfrom pystac_client import Client\nimport branca\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n\n# Provide the STAC and RASTER API endpoints\n# The endpoint is referring to a location within the API that executes a request on a data collection nesting on the server.\n\n# The STAC API is a catalog of all the existing data collections that are stored in the GHG Center.\nSTAC_API_URL = \"https://earth.gov/ghgcenter/api/stac\"\n\n# The RASTER API is used to fetch collections for visualization\nRASTER_API_URL = \"https://earth.gov/ghgcenter/api/raster\"\n\n# The collection name is used to fetch the dataset from the STAC API. First, we define the collection name as a variable\n# Name of the collection for the wetland methane emissions LPJ-EOSIM Model\ncollection_name = \"lpjeosim-wetlandch4-monthgrid-v2\"\n\n# Next, we need to specify the asset name for this collection\n# The asset name is referring to the raster band containing the pixel values for the parameter of interest\nasset_name = \"ensemble-mean-ch4-wetlands-emissions\"\n\n\n# Fetch the collection from the STAC API using the appropriate endpoint\n# The 'requests' library allows a HTTP request possible\ncollection = requests.get(f\"{STAC_API_URL}/collections/{collection_name}\").json()\n\n# Print the properties of the collection to the console\ncollection\n\nExamining the contents of our collection under summaries, we see that the data is available from January 1990 to December 2024. By looking at dashboard: time density, we can see that these observations are collected monthly.\n\n# Create a function that would search for a data collection in the US GHG Center STAC API\n\n# First, we need to define the function\n# The name of the function = \"get_item_count\"\n# The argument that will be passed through the defined function = \"collection_id\"\n\ndef get_item_count(collection_id):\n\n    # Set a counter for the number of items existing in the collection\n    count = 0\n\n    # Define the path to retrieve the granules (items) of the collection of interest in the STAC API\n    items_url = f\"{STAC_API_URL}/collections/{collection_id}/items\"\n\n    # Run a while loop to make HTTP requests until there are no more URLs associated with the collection in the STAC API\n    while True:\n\n        # Retrieve information about the granules by sending a \"get\" request to the STAC API using the defined collection path\n        response = requests.get(items_url)\n\n        # If the items do not exist, print an error message and quit the loop\n        if not response.ok:\n            print(\"error getting items\")\n            exit()\n\n        # Return the results of the HTTP response as JSON\n        stac = response.json()\n\n        # Increase the \"count\" by the number of items (granules) returned in the response\n        count += int(stac[\"context\"].get(\"returned\", 0))\n\n        # Retrieve information about the next URL associated with the collection in the STAC API (if applicable)\n        next = [link for link in stac[\"links\"] if link[\"rel\"] == \"next\"]\n\n        # Exit the loop if there are no other URLs\n        if not next:\n            break\n        \n        # Ensure the information gathered by other STAC API links associated with the collection are added to the original path\n        # \"href\" is the identifier for each of the tiles stored in the STAC API\n        items_url = next[0][\"href\"]\n        # temp = items_url.split('/')\n        # temp.insert(3, 'ghgcenter')\n        # temp.insert(4, 'api')\n        # temp.insert(5, 'stac')\n        # items_url = '/'.join(temp)\n\n    # Return the information about the total number of granules found associated with the collection\n    return count\n\n\n# Apply the function created above \"get_item_count\" to the data collection\nnumber_of_items = get_item_count(collection_name)\n\n# Get the information about the number of granules found in the collection\nitems = requests.get(f\"{STAC_API_URL}/collections/{collection_name}/items?limit={number_of_items}\"\n).json()[\"features\"]\n\n# Print the total number of items (granules) found\nprint(f\"Found {len(items)} items\")\n\nFound 409 items\n\n\n\n# Examine the first item in the collection\n# Keep in mind that a list starts from 0, 1, 2... therefore items[0] is referring to the first item in the list/collection\nitems[0]\n\nBelow, we are entering the minimum and maximum values to provide our upper and lower bounds in the rescale_values.\n\n# Fetch the minimum and maximum values for rescaling\nrescale_values = {'max': 0.0003, 'min': 0.0}"
  },
  {
    "objectID": "user_data_notebooks/lpjeosim-wetlandch4-monthgrid-v2_User_Notebook.html#explore-changes-in-methane-ch4-emission-levels-using-the-raster-api",
    "href": "user_data_notebooks/lpjeosim-wetlandch4-monthgrid-v2_User_Notebook.html#explore-changes-in-methane-ch4-emission-levels-using-the-raster-api",
    "title": "Wetland Methane Emissions, LPJ-EOSIM Model",
    "section": "Explore Changes in Methane (CH4) Emission Levels Using the Raster API",
    "text": "Explore Changes in Methane (CH4) Emission Levels Using the Raster API\nIn this notebook, we will explore the temporal impacts of methane emissions. We will visualize the outputs on a map using folium.\n\n# Now we create a dictionary where the start datetime values for each granule is queried more explicitly by year and month (e.g., 2020-02)\nitems = {item[\"properties\"][\"start_datetime\"][:7]: item for item in items} \n\nNow, we will pass the item id, collection name, and rescaling_factor to the Raster API endpoint. We will do this twice, once for month 1 mentioned in the next cell and again for month 2, so we can visualize each event independently.\n\n# Choose a color for displaying the tiles\n# Please refer to matplotlib library if you'd prefer choosing a different color ramp.\n# For more information on Colormaps in Matplotlib, please visit https://matplotlib.org/stable/users/explain/colors/colormaps.html\ncolor_map = \"magma\" \n\n# Make a GET request to retrieve information for the date mentioned below\nmonth1 = '1990-01'\nmonth1_tile = requests.get(\n\n    # Pass the collection name, collection date, and its ID\n    # To change the year and month of the observed parameter, you can modify month mentioned above.\n    f\"{RASTER_API_URL}/stac/tilejson.json?collection={items[month1]['collection']}&item={items[month1]['id']}\"\n\n    # Pass the asset name\n    f\"&assets={asset_name}\"\n\n    # Pass the color formula and colormap for custom visualization\n    f\"&color_formula=gamma+r+1.05&colormap_name={color_map}\"\n\n    # Pass the minimum and maximum values for rescaling\n    f\"&rescale={rescale_values['min']},{rescale_values['max']}\", \n\n# Return response in JSON format\n).json()\n\n# Print the properties of the retrieved granule to the console\nmonth1_tile\n\n{'tilejson': '2.2.0',\n 'version': '1.0.0',\n 'scheme': 'xyz',\n 'tiles': ['https://earth.gov/ghgcenter/api/raster/stac/tiles/WebMercatorQuad/{z}/{x}/{y}@1x?collection=lpjeosim-wetlandch4-monthgrid-v2&item=lpjeosim-wetlandch4-monthgrid-v2-199001&assets=ensemble-mean-ch4-wetlands-emissions&color_formula=gamma+r+1.05&colormap_name=magma&rescale=0.0%2C0.0003'],\n 'minzoom': 0,\n 'maxzoom': 24,\n 'bounds': [-180.0, -90.0, 180.0, 90.0],\n 'center': [0.0, 0.0, 0]}\n\n\n\n# Make a GET request to retrieve information for date mentioned below\nmonth2 = '1990-08'\nmonth2_tile = requests.get(\n\n    # Pass the collection name, collection date, and its ID\n    # To change the year and month of the observed parameter, you can modify the month mentioned above.\n    f\"{RASTER_API_URL}/stac/tilejson.json?collection={items[month2]['collection']}&item={items[month2]['id']}\"\n\n    # Pass the asset name\n    f\"&assets={asset_name}\"\n\n    # Pass the color formula and colormap for custom visualization\n    f\"&color_formula=gamma+r+1.05&colormap_name={color_map}\"\n\n    # Pass the minimum and maximum values for rescaling\n    f\"&rescale={rescale_values['min']},{rescale_values['max']}\",\n\n# Return response in JSON format \n).json()\n\n# Print the properties of the retrieved granule to the console\nmonth2_tile\n\n{'tilejson': '2.2.0',\n 'version': '1.0.0',\n 'scheme': 'xyz',\n 'tiles': ['https://earth.gov/ghgcenter/api/raster/stac/tiles/WebMercatorQuad/{z}/{x}/{y}@1x?collection=lpjeosim-wetlandch4-monthgrid-v2&item=lpjeosim-wetlandch4-monthgrid-v2-199008&assets=ensemble-mean-ch4-wetlands-emissions&color_formula=gamma+r+1.05&colormap_name=magma&rescale=0.0%2C0.0003'],\n 'minzoom': 0,\n 'maxzoom': 24,\n 'bounds': [-180.0, -90.0, 180.0, 90.0],\n 'center': [0.0, 0.0, 0]}"
  },
  {
    "objectID": "user_data_notebooks/lpjeosim-wetlandch4-monthgrid-v2_User_Notebook.html#visualize-ch₄-emissions",
    "href": "user_data_notebooks/lpjeosim-wetlandch4-monthgrid-v2_User_Notebook.html#visualize-ch₄-emissions",
    "title": "Wetland Methane Emissions, LPJ-EOSIM Model",
    "section": "Visualize CH₄ Emissions",
    "text": "Visualize CH₄ Emissions\n\n# For this study we are going to compare the CH₄ Emissions for month1 and month2 along the coast of California\n# To change the location, you can simply insert the latitude and longitude of the area of your interest in the \"location=(LAT, LONG)\" statement\n\n# Set initial zoom and center of map\n# 'folium.plugins' allows mapping side-by-side\nmap_ = folium.plugins.DualMap(location=(34, -118), zoom_start=6)\n\n# Define the first map layer for tile fetched for month 1\n# The TileLayer library helps in manipulating and displaying raster layers on a map\nmap_layer_month1 = TileLayer(\n    tiles=month1_tile[\"tiles\"][0], # Path to retrieve the tile\n    attr=\"GHG\", # Set the attribution\n    opacity=0.5, # Adjust the transparency of the layer\n)\n\n# Add the first layer to the Dual Map\nmap_layer_month1.add_to(map_.m1)\n\n\n# Define the second map layer for the tile fetched for month 2\nmap_layer_month2 = TileLayer(\n    tiles=month2_tile[\"tiles\"][0], # Path to retrieve the tile\n    attr=\"GHG\", # Set the attribution\n    opacity=0.5, # Adjust the transparency of the layer\n)\n\n# Add the second layer to the Dual Map\nmap_layer_month2.add_to(map_.m2)\n\n# Visualize the Dual Map\nmap_\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook"
  },
  {
    "objectID": "user_data_notebooks/lpjeosim-wetlandch4-monthgrid-v2_User_Notebook.html#visualize-the-data-as-a-time-series",
    "href": "user_data_notebooks/lpjeosim-wetlandch4-monthgrid-v2_User_Notebook.html#visualize-the-data-as-a-time-series",
    "title": "Wetland Methane Emissions, LPJ-EOSIM Model",
    "section": "Visualize the Data as a Time Series",
    "text": "Visualize the Data as a Time Series\nWe can now explore the wetland methane emissions time series (January 1990 – December 2024) available for the Texas area of the U.S. We can plot the data set using the code below:\n\n# Determine the width and height of the plot using the 'matplotlib' library\n# Figure size: 20 representing the width, 10 representing the height\nfig = plt.figure(figsize=(20, 10))\n\n# Plot the time series\nplt.plot(\n    df[\"date\"], # X-axis: date\n    df[\"max\"], # Y-axis: CH₄ value\n    color=\"red\", # Line color\n    linestyle=\"-\", # Line style\n    linewidth=0.5, # Line width\n    label=\"Max monthly CH₄ emissions\", # Legend label\n)\n\n# Display legend\nplt.legend()\n\n# Insert label for the X-axis\nplt.xlabel(\"Years\")\n\n# Insert label for the Y-axis\nplt.ylabel(\"Monthly CH4 emissions g/m2\")\n\n# Insert title for the plot\nplt.title(\"Monthly CH4 emission Values for Texas, 1990-2024\")\n\nText(0.5, 1.0, 'Monthly CH4 emission Values for Texas, 1990-2024')\n\n\n\n\n\n\n\n\n\nTo take a closer look at the CH4 variability across this region, we are going to retrieve and display data collected for the observation mentioned below.\n\n# The 2023-11-01 observation is the 3rd item in the list\n# Considering that a list starts with \"0\", we need to insert \"2\" in the \"items[2]\" statement\n# Print the start Date Time of the third granule in the collection\nprint(items[2][\"properties\"][\"start_datetime\"])\n\n2023-11-01T00:00:00+00:00\n\n\n\n# A GET request is made for the 3rd item in the collection\nobserved_tile = requests.get(\n\n    # Pass the collection name, the item number in the list, and its ID\n    f\"{RASTER_API_URL}/stac/tilejson.json?collection={items[2]['collection']}&item={items[2]['id']}&assets={asset_name}\"\n\n    # Pass the color formula and colormap for custom visualization\n    f\"&color_formula=gamma+r+1.05&colormap_name={color_map}\"\n\n    # Pass the minimum and maximum values for rescaling\n    f\"&rescale={rescale_values['min']},{rescale_values['max']}\",\n\n# Return the response in JSON format\n).json()\n\n# Print the properties of the retrieved granule to the console\nobserved_tile\n\n{'tilejson': '2.2.0',\n 'version': '1.0.0',\n 'scheme': 'xyz',\n 'tiles': ['https://earth.gov/ghgcenter/api/raster/stac/tiles/WebMercatorQuad/{z}/{x}/{y}@1x?collection=lpjeosim-wetlandch4-monthgrid-v2&item=lpjeosim-wetlandch4-monthgrid-v2-202311&assets=ensemble-mean-ch4-wetlands-emissions&color_formula=gamma+r+1.05&colormap_name=magma&rescale=0.0%2C0.0003'],\n 'minzoom': 0,\n 'maxzoom': 24,\n 'bounds': [-180.0, -90.0, 180.0, 90.0],\n 'center': [0.0, 0.0, 0]}\n\n\n\n# Create a new map to display the CH4 variability for the Texas region for the time in previous cell.\naoi_map_bbox = Map(\n\n    # Base map is set to OpenStreetMap\n    tiles=\"OpenStreetMap\",\n\n    # Set the center of the map\n    location=[\n        30,-100\n    ],\n\n    # Set the zoom value\n    zoom_start=8,\n)\n\n# Define the map layer\nmap_layer = TileLayer(\n    tiles=observed_tile[\"tiles\"][0], # Path to retrieve the tile\n    attr=\"GHG\", opacity = 0.5 # Set the attribution and transparency\n)\n\n# Add the layer to the map\nmap_layer.add_to(aoi_map_bbox)\n\n# Visualize the map\naoi_map_bbox\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook"
  },
  {
    "objectID": "user_data_notebooks/lpjeosim-wetlandch4-monthgrid-v2_User_Notebook.html#summary",
    "href": "user_data_notebooks/lpjeosim-wetlandch4-monthgrid-v2_User_Notebook.html#summary",
    "title": "Wetland Methane Emissions, LPJ-EOSIM Model",
    "section": "Summary",
    "text": "Summary\nIn this notebook we have successfully completed the following steps for the STAC collection for the Monthly Wetland Methane Emissions, LPJ-EOSIM Model data: 1. Install and import the necessary libraries 2. Fetch the collection from STAC collections using the appropriate endpoints 3. Count the number of existing granules within the collection 4. Map and compare the CH4 levels over the Texas region for two distinctive years 5. Create a table that displays the minimum, maximum, and sum of the CH4 levels for a specified region 6. Generate a time-series graph of the CH4 levels for a specified region\nIf you have any questions regarding this user notebook, please contact us using the feedback form."
  },
  {
    "objectID": "processing_and_verification_reports/micasa-carbonflux-daygrid-v1_Processing and Verification Report.html",
    "href": "processing_and_verification_reports/micasa-carbonflux-daygrid-v1_Processing and Verification Report.html",
    "title": "MiCASA Land Carbon Flux",
    "section": "",
    "text": "This browser does not support PDFs. Please download the PDF to view it: Download PDF.\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Processing and Verification Reports",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "MiCASA Land Carbon Flux"
    ]
  },
  {
    "objectID": "processing_and_verification_reports/noaa-gggrn-ch4-concentrations_Processing and Verification Report.html",
    "href": "processing_and_verification_reports/noaa-gggrn-ch4-concentrations_Processing and Verification Report.html",
    "title": "Atmospheric Methane Concentrations from the NOAA Global Monitoring Laboratory",
    "section": "",
    "text": "This browser does not support PDFs. Please download the PDF to view it: Download PDF.\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Processing and Verification Reports",
      "Greenhouse Gas Concentrations",
      "Atmospheric Methane Concentrations from the NOAA Global Monitoring Laboratory"
    ]
  },
  {
    "objectID": "processing_and_verification_reports/oco2-mip-co2budget-yeargrid-v1_Processing and Verification Report.html",
    "href": "processing_and_verification_reports/oco2-mip-co2budget-yeargrid-v1_Processing and Verification Report.html",
    "title": "OCO-2 MIP Top-Down CO₂ Budgets",
    "section": "",
    "text": "This browser does not support PDFs. Please download the PDF to view it: Download PDF.\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Processing and Verification Reports",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "OCO-2 MIP Top-Down CO₂ Budgets"
    ]
  },
  {
    "objectID": "processing_and_verification_reports/emit-ch4plume-v1_Processing and Verification Report.html",
    "href": "processing_and_verification_reports/emit-ch4plume-v1_Processing and Verification Report.html",
    "title": "EMIT Methane Point Source Plume Complexes",
    "section": "",
    "text": "This browser does not support PDFs. Please download the PDF to view it: Download PDF.\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Processing and Verification Reports",
      "Large Emissions Events",
      "EMIT Methane Point Source Plume Complexes"
    ]
  },
  {
    "objectID": "processing_and_verification_reports/tm54dvar-ch4flux-monthgrid-v1_Processing and Verification Report.html",
    "href": "processing_and_verification_reports/tm54dvar-ch4flux-monthgrid-v1_Processing and Verification Report.html",
    "title": "TM5-4DVar Isotopic CH₄ Inverse Fluxes",
    "section": "",
    "text": "This browser does not support PDFs. Please download the PDF to view it: Download PDF.\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Processing and Verification Reports",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "TM5-4DVar Isotopic CH₄ Inverse Fluxes"
    ]
  },
  {
    "objectID": "processing_and_verification_reports/gra2pes-ghg-monthgrid-v1_Processing and Verification Report.html",
    "href": "processing_and_verification_reports/gra2pes-ghg-monthgrid-v1_Processing and Verification Report.html",
    "title": "GRA²PES Greenhouse Gas and Air Quality Species",
    "section": "",
    "text": "This browser does not support PDFs. Please download the PDF to view it: Download PDF.\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Processing and Verification Reports",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "GRA²PES Greenhouse Gas and Air Quality Species"
    ]
  },
  {
    "objectID": "processing_and_verification_reports/sedac-popdensity-yeargrid5yr-v4.11_Processing and Verification Report.html",
    "href": "processing_and_verification_reports/sedac-popdensity-yeargrid5yr-v4.11_Processing and Verification Report.html",
    "title": "SEDAC Gridded World Population Density",
    "section": "",
    "text": "This browser does not support PDFs. Please download the PDF to view it: Download PDF.\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Processing and Verification Reports",
      "Socioeconomic",
      "SEDAC Gridded World Population Density"
    ]
  },
  {
    "objectID": "processing_and_verification_reports/noaa-gggrn-co2-concentrations_Processing and Verification Report.html",
    "href": "processing_and_verification_reports/noaa-gggrn-co2-concentrations_Processing and Verification Report.html",
    "title": "Atmospheric Carbon Dioxide Concentrations from NOAA Global Monitoring Laboratory",
    "section": "",
    "text": "This browser does not support PDFs. Please download the PDF to view it: Download PDF.\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Processing and Verification Reports",
      "Greenhouse Gas Concentrations",
      "Atmospheric Carbon Dioxide Concentrations from NOAA Global Monitoring Laboratory"
    ]
  },
  {
    "objectID": "processing_and_verification_reports/gosat-based-ch4budget-yeargrid-v1_Processing and Verification Report.html",
    "href": "processing_and_verification_reports/gosat-based-ch4budget-yeargrid-v1_Processing and Verification Report.html",
    "title": "GOSAT-based Top-down Total and Natural Methane Emissions",
    "section": "",
    "text": "This browser does not support PDFs. Please download the PDF to view it: Download PDF.\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Processing and Verification Reports",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "GOSAT-based Top-down Total and Natural Methane Emissions"
    ]
  },
  {
    "objectID": "services/apis.html",
    "href": "services/apis.html",
    "title": "APIs",
    "section": "",
    "text": "Please note: while some of our services are already very mature, the US GHG Center platform is currently in the beta phase and will undergo many changes in coming months.",
    "crumbs": [
      "User Services",
      "APIs"
    ]
  },
  {
    "objectID": "services/apis.html#open-source",
    "href": "services/apis.html#open-source",
    "title": "APIs",
    "section": "Open Source",
    "text": "Open Source\nMost of the US GHG Center APIs are hosted out of a single project (veda-backend) that combines multiple standalone services.",
    "crumbs": [
      "User Services",
      "APIs"
    ]
  }
]