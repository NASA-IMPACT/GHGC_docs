[
  {
    "objectID": "services/apis.html",
    "href": "services/apis.html",
    "title": "APIs",
    "section": "",
    "text": "Please note: while some of our services are already very mature, the US GHG Center platform is currently in the beta phase and will undergo many changes in coming months.",
    "crumbs": [
      "User Services",
      "APIs"
    ]
  },
  {
    "objectID": "services/apis.html#open-source",
    "href": "services/apis.html#open-source",
    "title": "APIs",
    "section": "Open Source",
    "text": "Open Source\nMost of the US GHG Center APIs are hosted out of a single project (ghgc-backend) that combines multiple standalone services.",
    "crumbs": [
      "User Services",
      "APIs"
    ]
  },
  {
    "objectID": "processingreport.html",
    "href": "processingreport.html",
    "title": "U.S. Greenhouse Gas Center: Processing and Verification Reports",
    "section": "",
    "text": "Welcome to the U.S. Greenhouse Gas (GHG) Center processing and verification reports. These reports verify that the accuracy and integrity of each dataset in the US GHG Center is maintained once it is processed into the Center.\nThe reports are grouped topically and labeled by dataset name. The dataset product type (model output, satellite observation, etc.) is noted next to each. Click on a dataset name to view the processing and verification report for that dataset.\nExamples of processing that may occur include transforming data from its source format into a could-optimized format, converting the units of the source data into a more common or standard unit, and flagging “nodata” values to ensure accurate data visualization. We strive to handle all data with extreme care, and share these reports to provide transparency and insight into any processing that is applied, while ensuring accuracy and reliability every step of the way.\nJoin us in our mission to make data-driven environmental solutions accessible. Explore, analyze, and make a difference with the US GHG Center.\nView the US GHG Center Data Catalog",
    "crumbs": [
      "Processing and Verification Reports"
    ]
  },
  {
    "objectID": "processingreport.html#gridded-anthropogenic-greenhouse-gas-emissions",
    "href": "processingreport.html#gridded-anthropogenic-greenhouse-gas-emissions",
    "title": "U.S. Greenhouse Gas Center: Processing and Verification Reports",
    "section": "Gridded Anthropogenic Greenhouse Gas Emissions",
    "text": "Gridded Anthropogenic Greenhouse Gas Emissions\n\nOCO-2 MIP Top-Down CO₂ Budgets - Model Output\nODIAC Fossil Fuel CO₂ Emissions - Model Output\nTM5-4DVar Isotopic CH₄ Inverse Fluxes - Model Output, Ground Measurements\nU.S. Gridded Anthropogenic Methane Emissions Inventory - Gridded Inventory",
    "crumbs": [
      "Processing and Verification Reports"
    ]
  },
  {
    "objectID": "processingreport.html#natural-greenhouse-gas-emissions-and-sinks",
    "href": "processingreport.html#natural-greenhouse-gas-emissions-and-sinks",
    "title": "U.S. Greenhouse Gas Center: Processing and Verification Reports",
    "section": "Natural Greenhouse Gas Emissions and Sinks",
    "text": "Natural Greenhouse Gas Emissions and Sinks\n\nAir-Sea CO₂ Flux, ECCO-Darwin Model v5 - Model Output\nCASA-GFED3 Land Carbon Flux - Model Output\nGOSAT-based Top-down Total and Natural Methane Emissions - Model Output\nOCO-2 MIP Top-Down CO₂ Budgets - Model Output\nTM5-4DVar Isotopic CH₄ Inverse Fluxes - Model Output, Ground Measurements\nWetland Methane Emissions from the LPJ-wsl model - Model Output",
    "crumbs": [
      "Processing and Verification Reports"
    ]
  },
  {
    "objectID": "processingreport.html#large-emissions-events",
    "href": "processingreport.html#large-emissions-events",
    "title": "U.S. Greenhouse Gas Center: Processing and Verification Reports",
    "section": "Large Emissions Events",
    "text": "Large Emissions Events\n\nEMIT Methane Point Source Plume Complexes - Satellite Observations",
    "crumbs": [
      "Processing and Verification Reports"
    ]
  },
  {
    "objectID": "processingreport.html#greenhouse-gas-concentrations",
    "href": "processingreport.html#greenhouse-gas-concentrations",
    "title": "U.S. Greenhouse Gas Center: Processing and Verification Reports",
    "section": "Greenhouse Gas Concentrations",
    "text": "Greenhouse Gas Concentrations\n\nOCO-2 GEOS Column CO₂ Concentrations - Model Output, Satellite Observations",
    "crumbs": [
      "Processing and Verification Reports"
    ]
  },
  {
    "objectID": "processingreport.html#socioeconomic",
    "href": "processingreport.html#socioeconomic",
    "title": "U.S. Greenhouse Gas Center: Processing and Verification Reports",
    "section": "Socioeconomic",
    "text": "Socioeconomic\n\nSEDAC Gridded World Population Density - Model Output",
    "crumbs": [
      "Processing and Verification Reports"
    ]
  },
  {
    "objectID": "processingreport.html#contact",
    "href": "processingreport.html#contact",
    "title": "U.S. Greenhouse Gas Center: Processing and Verification Reports",
    "section": "Contact",
    "text": "Contact\nFor technical help or general questions, please contact the support team using the feedback form.",
    "crumbs": [
      "Processing and Verification Reports"
    ]
  },
  {
    "objectID": "processing_and_verification_reports/tm54dvar-ch4flux-monthgrid-v1_Processing and Verification Report.html",
    "href": "processing_and_verification_reports/tm54dvar-ch4flux-monthgrid-v1_Processing and Verification Report.html",
    "title": "TM5-4DVar Isotopic CH₄ Inverse Fluxes",
    "section": "",
    "text": "This browser does not support PDFs. Please download the PDF to view it: Download PDF.\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Processing and Verification Reports",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "TM5-4DVar Isotopic CH₄ Inverse Fluxes"
    ]
  },
  {
    "objectID": "processing_and_verification_reports/emit-ch4plume-v1_Processing and Verification Report.html",
    "href": "processing_and_verification_reports/emit-ch4plume-v1_Processing and Verification Report.html",
    "title": "EMIT Methane Point Source Plume Complexes",
    "section": "",
    "text": "This browser does not support PDFs. Please download the PDF to view it: Download PDF.\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Processing and Verification Reports",
      "Large Emissions Events",
      "EMIT Methane Point Source Plume Complexes"
    ]
  },
  {
    "objectID": "processing_and_verification_reports/eccodarwin-co2flux-monthgrid-v5_Processing and Verification Report.html",
    "href": "processing_and_verification_reports/eccodarwin-co2flux-monthgrid-v5_Processing and Verification Report.html",
    "title": "Air-Sea CO₂ Flux, ECCO-Darwin Model v5",
    "section": "",
    "text": "This browser does not support PDFs. Please download the PDF to view it: Download PDF.\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Processing and Verification Reports",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "Air-Sea CO₂ Flux, ECCO-Darwin Model v5"
    ]
  },
  {
    "objectID": "processing_and_verification_reports/casagfed-carbonflux-monthgrid-v3_Processing and Verification Report.html",
    "href": "processing_and_verification_reports/casagfed-carbonflux-monthgrid-v3_Processing and Verification Report.html",
    "title": "CASA-GFED3 Land Carbon Flux",
    "section": "",
    "text": "This browser does not support PDFs. Please download the PDF to view it: Download PDF.\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Processing and Verification Reports",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "CASA-GFED3 Land Carbon Flux"
    ]
  },
  {
    "objectID": "processing_and_verification_reports/oco2-mip-co2budget-yeargrid-v1_Processing and Verification Report.html",
    "href": "processing_and_verification_reports/oco2-mip-co2budget-yeargrid-v1_Processing and Verification Report.html",
    "title": "OCO-2 MIP Top-Down CO₂ Budgets",
    "section": "",
    "text": "This browser does not support PDFs. Please download the PDF to view it: Download PDF.\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Processing and Verification Reports",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "OCO-2 MIP Top-Down CO₂ Budgets"
    ]
  },
  {
    "objectID": "processing_and_verification_reports/oco2geos-co2-daygrid-v10r_Processing and Verification Report.html",
    "href": "processing_and_verification_reports/oco2geos-co2-daygrid-v10r_Processing and Verification Report.html",
    "title": "OCO-2 GEOS Column CO₂ Concentrations",
    "section": "",
    "text": "This browser does not support PDFs. Please download the PDF to view it: Download PDF.\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Processing and Verification Reports",
      "Greenhouse Gas Concentrations",
      "OCO-2 GEOS Column CO₂ Concentrations"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "U.S. Greenhouse Gas Center: Documentation",
    "section": "",
    "text": "The U.S. Greenhouse Gas (GHG) Center provides a cloud-based system for exploring and analyzing U.S. government and other curated greenhouse gas datasets.\nOn this site, you can find the technical documentation for the services the center provides, how to load the datasets, and how the datasets were transformed from their source formats (eg. netCDF, HDF, etc.) into cloud-optimized formats that enable efficient cloud data access and visualization.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#welcome",
    "href": "index.html#welcome",
    "title": "U.S. Greenhouse Gas Center: Documentation",
    "section": "",
    "text": "The U.S. Greenhouse Gas (GHG) Center provides a cloud-based system for exploring and analyzing U.S. government and other curated greenhouse gas datasets.\nOn this site, you can find the technical documentation for the services the center provides, how to load the datasets, and how the datasets were transformed from their source formats (eg. netCDF, HDF, etc.) into cloud-optimized formats that enable efficient cloud data access and visualization.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#contents",
    "href": "index.html#contents",
    "title": "U.S. Greenhouse Gas Center: Documentation",
    "section": "Contents",
    "text": "Contents\n\nServices provided for accessing and analyzing the US GHG Center datasets, such as the JupyterHub environment for interactive computing.\nDataset usage examples, e.g. for the Wetland Methane Emissions from the LPJ-wsl model dataset, that shows how to load the dataset in Python in JupyterHub.\nDataset transformation scripts, such as the CASA-GFED3 Land Carbon Flux dataset.\nData processing and verification reports that openly present the process we used to check and verify that any transformation did not alter the original source data. An example is the CEOS CH₄ budget yearly dataset.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#contact",
    "href": "index.html#contact",
    "title": "U.S. Greenhouse Gas Center: Documentation",
    "section": "Contact",
    "text": "Contact\nFor technical help or general questions, please contact the support team using the feedback form.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "cog_transformation/lpjwsl-wetlandch4-monthgrid-v1.html",
    "href": "cog_transformation/lpjwsl-wetlandch4-monthgrid-v1.html",
    "title": "Wetland Methane Emissions, LPJ-wsl Model",
    "section": "",
    "text": "This script was used to transform the Wetland Methane Emissions, LPJ-wsl Model dataset from netCDF to Cloud Optimized GeoTIFF (COG) format for display in the Greenhouse Gas (GHG) Center.\n\nimport os\nimport xarray\nimport re\nimport pandas as pd\nimport json\nimport tempfile\nimport boto3\n\n\nsession = boto3.session.Session()\ns3_client = session.client(\"s3\")\nbucket_name = (\n    \"ghgc-data-store-dev\"  # S3 bucket where the COGs are stored after transformation\n)\nFOLDER_NAME = \"NASA_GSFC_ch4_wetlands_monthly\"\ndirectory = \"ch4_wetlands_monthly\"\n\nfiles_processed = pd.DataFrame(\n    columns=[\"file_name\", \"COGs_created\"]\n)  # A dataframe to keep track of the files that we have transformed into COGs\n\n# Reading the raw netCDF files from local machine\nfor name in os.listdir(directory):\n    xds = xarray.open_dataset(\n        f\"{directory}/{name}\", engine=\"netcdf4\", decode_times=False\n    )\n    xds = xds.assign_coords(longitude=(((xds.longitude + 180) % 360) - 180)).sortby(\n        \"longitude\"\n    )\n    variable = [var for var in xds.data_vars]\n    filename = name.split(\"/ \")[-1]\n    filename_elements = re.split(\"[_ .]\", filename)\n\n    for time_increment in range(0, len(xds.time)):\n        for var in variable:\n            filename = name.split(\"/ \")[-1]\n            filename_elements = re.split(\"[_ .]\", filename)\n            data = getattr(xds.isel(time=time_increment), var)\n            data = data.isel(latitude=slice(None, None, -1))\n            data = data * 1000\n            data.rio.set_spatial_dims(\"longitude\", \"latitude\", inplace=True)\n            data.rio.write_crs(\"epsg:4326\", inplace=True)\n\n            date = (\n                f\"0{int((data.time.item(0)/732)+1)}\"\n                if len(str(int((data.time.item(0) / 732) + 1))) == 1\n                else f\"{int((data.time.item(0)/732)+1)}\"\n            )\n            # # insert date of generated COG into filename\n            filename_elements.pop()\n            filename_elements[-1] = filename_elements[-1] + date\n            filename_elements.insert(2, var)\n            cog_filename = \"_\".join(filename_elements)\n            # # add extension\n            cog_filename = f\"{cog_filename}.tif\"\n\n            with tempfile.NamedTemporaryFile() as temp_file:\n                data.rio.to_raster(\n                    temp_file.name,\n                    driver=\"COG\",\n                )\n                s3_client.upload_file(\n                    Filename=temp_file.name,\n                    Bucket=bucket_name,\n                    Key=f\"{FOLDER_NAME}/{cog_filename}\",\n                )\n\n            files_processed = files_processed._append(\n                {\"file_name\": name, \"COGs_created\": cog_filename},\n                ignore_index=True,\n            )\n\n            print(f\"Generated and saved COG: {cog_filename}\")\n\n# Generate the json file with the metadata that is present in the netCDF files.\nwith tempfile.NamedTemporaryFile(mode=\"w+\") as fp:\n    json.dump(xds.attrs, fp)\n    json.dump({\"data_dimensions\": dict(xds.dims)}, fp)\n    json.dump({\"data_variables\": list(xds.data_vars)}, fp)\n    fp.flush()\n\n    s3_client.upload_file(\n        Filename=fp.name,\n        Bucket=bucket_name,\n        Key=f\"{FOLDER_NAME}/metadata.json\",\n    )\n\n# creating the csv file with the names of files transformed.\nfiles_processed.to_csv(\n    f\"s3://{bucket_name}/{FOLDER_NAME}/files_converted.csv\",\n)\nprint(\"Done generating COGs\")\n\n\n\n\n Back to top"
  },
  {
    "objectID": "cog_transformation/gosat-based-ch4budget-yeargrid-v1.html",
    "href": "cog_transformation/gosat-based-ch4budget-yeargrid-v1.html",
    "title": "GOSAT-based Top-down Methane Budgets",
    "section": "",
    "text": "This script was used to transform the GOSAT-based Top-down Methane Budgets dataset from netCDF to Cloud Optimized GeoTIFF (COG) format for display in the Greenhouse Gas (GHG) Center.\n\nimport os\nimport xarray\nimport re\nimport pandas as pd\nimport json\nimport tempfile\nimport boto3\nimport rasterio\nfrom datetime import datetime\nfrom dateutil.relativedelta import relativedelta\n\n\nsession = boto3.session.Session()\ns3_client = session.client(\"s3\")\nbucket_name = (\n    \"ghgc-data-store-dev\"  # S3 bucket where the COGs are stored after transformation\n)\nyear_ = datetime(2019, 1, 1)\nfolder_name = \"new_data/CH4-inverse-flux\"\n\nCOG_PROFILE = {\"driver\": \"COG\", \"compress\": \"DEFLATE\"}\n\nfiles_processed = pd.DataFrame(\n    columns=[\"file_name\", \"COGs_created\"]\n)  # A dataframe to keep track of the files that we have transformed into COGs\n\n# Reading the raw netCDF files from local machine\nfor name in os.listdir(folder_name):\n    ds = xarray.open_dataset(\n        f\"{folder_name}/{name}\",\n        engine=\"netcdf4\",\n    )\n\n    ds = ds.rename({\"dimy\": \"lat\", \"dimx\": \"lon\"})\n    # assign coords from dimensions\n    ds = ds.assign_coords(lon=(((ds.lon + 180) % 360) - 180)).sortby(\"lon\")\n    ds = ds.assign_coords(lat=((ds.lat / 180) * 180) - 90).sortby(\"lat\")\n\n    variable = [var for var in ds.data_vars]\n\n    for var in variable[2:]:\n        filename = name.split(\"/ \")[-1]\n        filename_elements = re.split(\"[_ .]\", filename)\n        data = ds[var]\n        filename_elements.pop()\n        filename_elements.insert(2, var)\n        cog_filename = \"_\".join(filename_elements)\n        # # add extension\n        cog_filename = f\"{cog_filename}.tif\"\n\n        data = data.reindex(lat=list(reversed(data.lat)))\n\n        data.rio.set_spatial_dims(\"lon\", \"lat\")\n        data.rio.write_crs(\"epsg:4326\", inplace=True)\n\n        # generate COG\n        COG_PROFILE = {\"driver\": \"COG\", \"compress\": \"DEFLATE\"}\n\n        with tempfile.NamedTemporaryFile() as temp_file:\n            data.rio.to_raster(temp_file.name, **COG_PROFILE)\n            s3_client.upload_file(\n                Filename=temp_file.name,\n                Bucket=bucket_name,\n                Key=f\"ch4_inverse_flux/{cog_filename}\",\n            )\n\n        files_processed = files_processed._append(\n            {\"file_name\": name, \"COGs_created\": cog_filename},\n            ignore_index=True,\n        )\n\n        print(f\"Generated and saved COG: {cog_filename}\")\n\n# Generate the json file with the metadata that is present in the netCDF files.\nwith tempfile.NamedTemporaryFile(mode=\"w+\") as fp:\n    json.dump(ds.attrs, fp)\n    json.dump({\"data_dimensions\": dict(ds.dims)}, fp)\n    json.dump({\"data_variables\": list(ds.data_vars)}, fp)\n    fp.flush()\n\n    s3_client.upload_file(\n        Filename=fp.name,\n        Bucket=bucket_name,\n        Key=\"ch4_inverse_flux/metadata.json\",\n    )\n\n# creating the csv file with the names of files transformed.\nfiles_processed.to_csv(\n    f\"s3://{bucket_name}/ch4_inverse_flux/files_converted.csv\",\n)\nprint(\"Done generating COGs\")\n\n\n\n\n Back to top"
  },
  {
    "objectID": "cog_transformation/oco2geos-co2-daygrid-v10r.html",
    "href": "cog_transformation/oco2geos-co2-daygrid-v10r.html",
    "title": "OCO-2 GEOS Column CO₂ Concentrations",
    "section": "",
    "text": "This script was used to transform the OCO-2 GEOS Column CO₂ Concentrations dataset from netCDF to Cloud Optimized GeoTIFF (COG) format for display in the Greenhouse Gas (GHG) Center.\n\nimport xarray\nimport re\nimport pandas as pd\nimport json\nimport tempfile\nimport boto3\nimport os\n\n\nsession = boto3.Session()\ns3_client = session.client(\"s3\")\nbucket_name = (\n    \"ghgc-data-store-dev\"  # S3 bucket where the COGs are stored after transformation\n)\nFOLDER_NAME = \"earth_data/geos_oco2\"\ns3_folder_name = \"geos-oco2\"\n\nerror_files = []\ncount = 0\nfiles_processed = pd.DataFrame(\n    columns=[\"file_name\", \"COGs_created\"]\n)  # A dataframe to keep track of the files that we have transformed into COGs\n\n# Reading the raw netCDF files from local machine\nfor name in os.listdir(FOLDER_NAME):\n    try:\n        xds = xarray.open_dataset(f\"{FOLDER_NAME}/{name}\", engine=\"netcdf4\")\n        xds = xds.assign_coords(lon=(((xds.lon + 180) % 360) - 180)).sortby(\"lon\")\n        variable = [var for var in xds.data_vars]\n        filename = name.split(\"/ \")[-1]\n        filename_elements = re.split(\"[_ .]\", filename)\n\n        for time_increment in range(0, len(xds.time)):\n            for var in variable:\n                filename = name.split(\"/ \")[-1]\n                filename_elements = re.split(\"[_ .]\", filename)\n                data = getattr(xds.isel(time=time_increment), var)\n                data = data.isel(lat=slice(None, None, -1))\n                data.rio.set_spatial_dims(\"lon\", \"lat\", inplace=True)\n                data.rio.write_crs(\"epsg:4326\", inplace=True)\n\n                # # insert date of generated COG into filename\n                filename_elements[-1] = filename_elements[-3]\n                filename_elements.insert(2, var)\n                filename_elements.pop(-3)\n                cog_filename = \"_\".join(filename_elements)\n                # # add extension\n                cog_filename = f\"{cog_filename}.tif\"\n\n                with tempfile.NamedTemporaryFile() as temp_file:\n                    data.rio.to_raster(\n                        temp_file.name,\n                        driver=\"COG\",\n                    )\n                    s3_client.upload_file(\n                        Filename=temp_file.name,\n                        Bucket=bucket_name,\n                        Key=f\"{s3_folder_name}/{cog_filename}\",\n                    )\n\n                files_processed = files_processed._append(\n                    {\"file_name\": name, \"COGs_created\": cog_filename},\n                    ignore_index=True,\n                )\n        count += 1\n        print(f\"Generated and saved COG: {cog_filename}\")\n    except OSError:\n        error_files.append(name)\n        pass\n\n# Generate the json file with the metadata that is present in the netCDF files.\nwith tempfile.NamedTemporaryFile(mode=\"w+\") as fp:\n    json.dump(xds.attrs, fp)\n    json.dump({\"data_dimensions\": dict(xds.dims)}, fp)\n    json.dump({\"data_variables\": list(xds.data_vars)}, fp)\n    fp.flush()\n\n    s3_client.upload_file(\n        Filename=fp.name,\n        Bucket=bucket_name,\n        Key=f\"{s3_folder_name}/metadata.json\",\n    )\n\n# creating the csv file with the names of files transformed.\nfiles_processed.to_csv(\n    f\"s3://{bucket_name}/{s3_folder_name}/files_converted.csv\",\n)\nprint(\"Done generating COGs\")\n\n\n\n\n Back to top"
  },
  {
    "objectID": "cog_transformation/tm54dvar-ch4flux-monthgrid-v1.html",
    "href": "cog_transformation/tm54dvar-ch4flux-monthgrid-v1.html",
    "title": "TM5-4DVar Isotopic CH₄ Inverse Fluxes",
    "section": "",
    "text": "This script was used to transform the TM5-4DVar Isotopic CH₄ Inverse Fluxes dataset from netCDF to Cloud Optimized GeoTIFF (COG) format for display in the Greenhouse Gas (GHG) Center.\n\nimport os\nimport xarray\nimport re\nimport pandas as pd\nimport json\nimport tempfile\nimport boto3\nfrom datetime import datetime\n\n\nsession = boto3.session.Session()\ns3_client = session.client(\"s3\")\nbucket_name = (\n    \"ghgc-data-store-dev\"  # S3 bucket where the COGs are stored after transformation\n)\nFOLDER_NAME = \"tm5-ch4-inverse-flux\"\n\nfiles_processed = pd.DataFrame(\n    columns=[\"file_name\", \"COGs_created\"]\n)  # A dataframe to keep track of the files that we have transformed into COGs\n\n# Reading the raw netCDF files from local machine\nfor name in os.listdir(FOLDER_NAME):\n    xds = xarray.open_dataset(f\"{FOLDER_NAME}/{name}\", engine=\"netcdf4\")\n    xds = xds.rename({\"latitude\": \"lat\", \"longitude\": \"lon\"})\n    xds = xds.assign_coords(lon=(((xds.lon + 180) % 360) - 180)).sortby(\"lon\")\n    variable = [var for var in xds.data_vars if \"global\" not in var]\n\n    for time_increment in range(0, len(xds.months)):\n        filename = name.split(\"/ \")[-1]\n        filename_elements = re.split(\"[_ .]\", filename)\n        start_time = datetime(int(filename_elements[-2]), time_increment + 1, 1)\n        for var in variable:\n            data = getattr(xds.isel(months=time_increment), var)\n            data = data.isel(lat=slice(None, None, -1))\n            data.rio.set_spatial_dims(\"lon\", \"lat\", inplace=True)\n            data.rio.write_crs(\"epsg:4326\", inplace=True)\n\n            # # insert date of generated COG into filename\n            filename_elements.pop()\n            filename_elements[-1] = start_time.strftime(\"%Y%m\")\n            filename_elements.insert(2, var)\n            cog_filename = \"_\".join(filename_elements)\n            # # add extension\n            cog_filename = f\"{cog_filename}.tif\"\n\n            with tempfile.NamedTemporaryFile() as temp_file:\n                data.rio.to_raster(\n                    temp_file.name,\n                    driver=\"COG\",\n                )\n                s3_client.upload_file(\n                    Filename=temp_file.name,\n                    Bucket=bucket_name,\n                    Key=f\"{FOLDER_NAME}/{cog_filename}\",\n                )\n\n            files_processed = files_processed._append(\n                {\"file_name\": name, \"COGs_created\": cog_filename},\n                ignore_index=True,\n            )\n\n            print(f\"Generated and saved COG: {cog_filename}\")\n\n# Generate the json file with the metadata that is present in the netCDF files.\nwith tempfile.NamedTemporaryFile(mode=\"w+\") as fp:\n    json.dump(xds.attrs, fp)\n    json.dump({\"data_dimensions\": dict(xds.dims)}, fp)\n    json.dump({\"data_variables\": list(xds.data_vars)}, fp)\n    fp.flush()\n\n    s3_client.upload_file(\n        Filename=fp.name,\n        Bucket=bucket_name,\n        Key=f\"{FOLDER_NAME}/metadata.json\",\n    )\n\n# creating the csv file with the names of files transformed.\nfiles_processed.to_csv(\n    f\"s3://{bucket_name}/{FOLDER_NAME}/files_converted.csv\",\n)\nprint(\"Done generating COGs\")\n\n\n\n\n Back to top"
  },
  {
    "objectID": "cog_transformation/odiac-ffco2-monthgrid-v2022.html",
    "href": "cog_transformation/odiac-ffco2-monthgrid-v2022.html",
    "title": "ODIAC Fossil Fuel CO₂ Emissions",
    "section": "",
    "text": "This script was used to transform the ODIAC Fossil Fuel CO₂ Emissions dataset from GeoTIFF to Cloud Optimized GeoTIFF (COG) format for display in the Greenhouse Gas (GHG) Center.\n\nimport os\nimport xarray\nimport re\nimport pandas as pd\n\nimport tempfile\nimport boto3\n\n\nsession = boto3.session.Session()\ns3_client = session.client(\"s3\")\nbucket_name = \"ghgc-data-store-dev\" # S3 bucket where the COGs are stored after transformation\n\nfold_names = os.listdir(\"ODIAC\")\n\nfiles_processed = pd.DataFrame(columns=[\"file_name\", \"COGs_created\"])   # A dataframe to keep track of the files that we have transformed into COGs\n\n# Reading the raw netCDF files from local machine\nfor fol_ in fold_names:\n    for name in os.listdir(f\"ODIAC/{fol_}\"):\n        xds = xarray.open_dataarray(f\"ODIAC/{fol_}/{name}\")\n\n        filename = name.split(\"/ \")[-1]\n        filename_elements = re.split(\"[_ .]\", filename)\n        # # insert date of generated COG into filename\n        filename_elements.pop()\n        filename_elements[-1] = fol_ + filename_elements[-1][-2:]\n\n        xds.rio.set_spatial_dims(\"x\", \"y\", inplace=True)\n        xds.rio.write_nodata(-9999, inplace=True)\n        xds.rio.write_crs(\"epsg:4326\", inplace=True)\n\n        cog_filename = \"_\".join(filename_elements)\n        # # add extension\n        cog_filename = f\"{cog_filename}.tif\"\n\n        with tempfile.NamedTemporaryFile() as temp_file:\n            xds.rio.to_raster(\n                temp_file.name,\n                driver=\"COG\",\n            )\n            s3_client.upload_file(\n                Filename=temp_file.name,\n                Bucket=bucket_name,\n                Key=f\"ODIAC_geotiffs_COGs/{cog_filename}\",\n            )\n\n        files_processed = files_processed._append(\n            {\"file_name\": name, \"COGs_created\": cog_filename},\n            ignore_index=True,\n        )\n\n        print(f\"Generated and saved COG: {cog_filename}\")\n\n\n# creating the csv file with the names of files transformed.\nfiles_processed.to_csv(\n    f\"s3://{bucket_name}/ODIAC_COGs/files_converted.csv\",\n)\nprint(\"Done generating COGs\")\n\n\n\n\n Back to top"
  },
  {
    "objectID": "cog_transformation/emit-ch4plume-v1.html",
    "href": "cog_transformation/emit-ch4plume-v1.html",
    "title": "EMIT Methane Point Source Plume Complexes",
    "section": "",
    "text": "This script was used to read the EMIT Methane Point Source Plume Complexes dataset provided in Cloud Optimized GeoTIFF (COG) format for display in the Greenhouse Gas (GHG) Center.\n\nimport re\nimport pandas as pd\nimport json\nimport tempfile\nimport boto3\n\n\nsession_ghgc = boto3.session.Session(profile_name=\"ghg_user\")\ns3_client_ghgc = session_ghgc.client(\"s3\")\nsession_veda_smce = boto3.session.Session()\ns3_client_veda_smce = session_veda_smce.client(\"s3\")\n\n# Since the plume emissions were already COGs, we just had to transform their naming convention to be stored in the STAC collection.\nSOURCE_BUCKET_NAME = \"ghgc-data-staging-uah\"\nTARGET_BUCKET_NAME = \"ghgc-data-store-dev\"\n\n\nkeys = []\nresp = s3_client_ghgc.list_objects_v2(Bucket=SOURCE_BUCKET_NAME)\nfor obj in resp[\"Contents\"]:\n    if \"l3\" in obj[\"Key\"]:\n        keys.append(obj[\"Key\"])\n\nfor key in keys:\n    s3_obj = s3_client_ghgc.get_object(Bucket=SOURCE_BUCKET_NAME, Key=key)[\n        \"Body\"\n    ]\n    filename = key.split(\"/\")[-1]\n    filename_elements = re.split(\"[_ .]\", filename)\n\n    date = re.search(\"t\\d\\d\\d\\d\\d\\d\\d\\dt\", key).group(0)\n    filename_elements.insert(-1, date[1:-1])\n    filename_elements.pop()\n\n    cog_filename = \"_\".join(filename_elements)\n    # # add extension\n    cog_filename = f\"{cog_filename}.tif\"\n    s3_client_veda_smce.upload_fileobj(\n        Fileobj=s3_obj,\n        Bucket=TARGET_BUCKET_NAME,\n        Key=f\"plum_data/{cog_filename}\",\n    )\n\n\n\n\n Back to top"
  },
  {
    "objectID": "cog_transformation/epa-ch4emission-grid-v2express_layers_update.html",
    "href": "cog_transformation/epa-ch4emission-grid-v2express_layers_update.html",
    "title": "Gridded Anthropogenic Methane Emissions Inventory",
    "section": "",
    "text": "This script was used to add concatenated layers and transform Gridded Anthropogenic Methane Emissions Inventory dataset from netCDF to Cloud Optimized GeoTIFF (COG) format for display in the Greenhouse Gas (GHG) Center.\n\nimport os\nimport xarray\nimport re\nimport pandas as pd\nimport json\nimport tempfile\nimport boto3\nfrom datetime import datetime\nimport numpy as np\n\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\nTrue\n\n\n\n# session = boto3.session.Session()\nsession = boto3.Session(\n    aws_access_key_id=os.environ.get(\"AWS_ACCESS_KEY_ID\"),\n    aws_secret_access_key=os.environ.get(\"AWS_SECRET_ACCESS_KEY\"),\n    aws_session_token=os.environ.get(\"AWS_SESSION_TOKEN\"),\n)\ns3_client = session.client(\"s3\")\nbucket_name = (\n    \"ghgc-data-store-dev\"  # S3 bucket where the COGs are stored after transformation\n)\nFOLDER_NAME = \"../data/epa_emissions_express_extension\"\ns3_folder_name = \"epa_express_extension_Mg_km2_yr\"\n\nfiles_processed = pd.DataFrame(\n    columns=[\"file_name\", \"COGs_created\"]\n)  # A dataframe to keep track of the files that we have transformed into COGs\n\n# Reading the raw netCDF files from local machine\nfor name in os.listdir(FOLDER_NAME):\n    xds = xarray.open_dataset(f\"{FOLDER_NAME}/{name}\", engine=\"netcdf4\")\n    xds = xds.assign_coords(lon=(((xds.lon + 180) % 360) - 180)).sortby(\"lon\")\n    variable = [var for var in xds.data_vars]\n    new_variables = {\n        \"all-variables\": variable[:-1],\n        \"agriculture\": variable[17:21],\n        \"natural-gas-systems\": variable[10:15] + [variable[26]],\n        \"petroleum-systems\": variable[5:9],\n        \"waste\": variable[21:26],\n        \"coal-mines\": variable[2:5],\n        \"other\": variable[:2] + [variable[9]] + variable[15:17],\n    }\n    filename = name.split(\"/ \")[-1]\n    filename_elements = re.split(\"[_ .]\", filename)\n    start_time = datetime(int(filename_elements[-2]), 1, 1)\n\n    for time_increment in range(0, len(xds.time)):\n        for key, value in new_variables.items():\n            data = np.zeros(dtype=np.float32, shape=(len(xds.lat), len(xds.lon)))\n            filename = name.split(\"/ \")[-1]\n            filename_elements = re.split(\"[_ .]\", filename)\n            for var in value:\n                data = data + getattr(xds.isel(time=time_increment), var)\n            # data = np.round(data / pow(10, 9), 2)\n            data.values[data.values==0] = np.nan\n            data = data*((1/(6.022*pow(10,23)))*(16.04*pow(10,-6))*366*pow(10,10)*86400)\n            data = data.fillna(-9999)\n            data = data.isel(lat=slice(None, None, -1))\n            data.rio.set_spatial_dims(\"lon\", \"lat\", inplace=True)\n            data.rio.write_crs(\"epsg:4326\", inplace=True)\n\n            # # insert date of generated COG into filename\n            filename_elements.pop()\n            filename_elements[-1] = start_time.strftime(\"%Y\")\n            filename_elements.insert(2, key)\n            cog_filename = \"_\".join(filename_elements)\n            # # add extension\n            cog_filename = f\"{cog_filename}.tif\"\n\n            with tempfile.NamedTemporaryFile() as temp_file:\n                data.rio.to_raster(\n                    temp_file.name,\n                    driver=\"COG\",\n                )\n                s3_client.upload_file(\n                    Filename=temp_file.name,\n                    Bucket=bucket_name,\n                    Key=f\"{s3_folder_name}/{cog_filename}\",\n                )\n\n                files_processed = files_processed._append(\n                    {\"file_name\": name, \"COGs_created\": cog_filename},\n                    ignore_index=True,\n                )\n\n                print(f\"Generated and saved COG: {cog_filename}\")\nprint(\"Done generating COGs\")\n\nTraceback (most recent call last):\n  File \"_pydevd_bundle/pydevd_cython.pyx\", line 1078, in _pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\n  File \"_pydevd_bundle/pydevd_cython.pyx\", line 297, in _pydevd_bundle.pydevd_cython.PyDBFrame.do_wait_suspend\n  File \"/Users/vgaur/miniconda3/envs/cmip6/lib/python3.9/site-packages/debugpy/_vendored/pydevd/pydevd.py\", line 1976, in do_wait_suspend\n    keep_suspended = self._do_wait_suspend(thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\n  File \"/Users/vgaur/miniconda3/envs/cmip6/lib/python3.9/site-packages/debugpy/_vendored/pydevd/pydevd.py\", line 2011, in _do_wait_suspend\n    time.sleep(0.01)\nKeyboardInterrupt\n\n\nKeyboardInterrupt: \n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "user_data_notebooks/tm54dvar-ch4flux-monthgrid-v1_User_Notebook.html",
    "href": "user_data_notebooks/tm54dvar-ch4flux-monthgrid-v1_User_Notebook.html",
    "title": "TM5-4DVar Isotopic CH₄ Inverse Fluxes",
    "section": "",
    "text": "Identify available dates and temporal frequency of observations for the given collection using the GHGC API /stac endpoint. The collection processed in this notebook is the TM5-4DVar Isotopic CH₄ Inverse Fluxes Data product.\nPass the STAC item into the raster API /stac/tilejson.jsonendpoint.\nUsing folium.plugins.DualMap, we will visualize two tiles (side-by-side), allowing us to compare time points.\nAfter the visualization, we will perform zonal statistics for a given polygon.",
    "crumbs": [
      "Data Usage Notebooks",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "TM5-4DVar Isotopic CH₄ Inverse Fluxes"
    ]
  },
  {
    "objectID": "user_data_notebooks/tm54dvar-ch4flux-monthgrid-v1_User_Notebook.html#approach",
    "href": "user_data_notebooks/tm54dvar-ch4flux-monthgrid-v1_User_Notebook.html#approach",
    "title": "TM5-4DVar Isotopic CH₄ Inverse Fluxes",
    "section": "",
    "text": "Identify available dates and temporal frequency of observations for the given collection using the GHGC API /stac endpoint. The collection processed in this notebook is the TM5-4DVar Isotopic CH₄ Inverse Fluxes Data product.\nPass the STAC item into the raster API /stac/tilejson.jsonendpoint.\nUsing folium.plugins.DualMap, we will visualize two tiles (side-by-side), allowing us to compare time points.\nAfter the visualization, we will perform zonal statistics for a given polygon.",
    "crumbs": [
      "Data Usage Notebooks",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "TM5-4DVar Isotopic CH₄ Inverse Fluxes"
    ]
  },
  {
    "objectID": "user_data_notebooks/tm54dvar-ch4flux-monthgrid-v1_User_Notebook.html#about-the-data",
    "href": "user_data_notebooks/tm54dvar-ch4flux-monthgrid-v1_User_Notebook.html#about-the-data",
    "title": "TM5-4DVar Isotopic CH₄ Inverse Fluxes",
    "section": "About the Data",
    "text": "About the Data\nSurface methane (CH₄) emissions are derived from atmospheric measurements of methane and its ¹³C carbon isotope content. Different sources of methane contain different ratios of the two stable isotopologues, ¹²CH₄ and ¹³CH₄. This makes normally indistinguishable collocated sources of methane, say from agriculture and oil and gas exploration, distinguishable. The National Oceanic and Atmospheric Administration (NOAA) collects whole air samples from its global cooperative network of flasks (https://gml.noaa.gov/ccgg/about.html), which are then analyzed for methane and other trace gasses. A subset of those flasks are also analyzed for ¹³C of methane in collaboration with the Institute of Arctic and Alpine Research at the University of Colorado Boulder. Scientists at the National Aeronautics and Space Administration (NASA) and NOAA used those measurements of methane and ¹³C of methane in conjunction with a model of atmospheric circulation to estimate emissions of methane separated by three source types, microbial, fossil and pyrogenic.",
    "crumbs": [
      "Data Usage Notebooks",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "TM5-4DVar Isotopic CH₄ Inverse Fluxes"
    ]
  },
  {
    "objectID": "user_data_notebooks/tm54dvar-ch4flux-monthgrid-v1_User_Notebook.html#installing-the-required-libraries",
    "href": "user_data_notebooks/tm54dvar-ch4flux-monthgrid-v1_User_Notebook.html#installing-the-required-libraries",
    "title": "TM5-4DVar Isotopic CH₄ Inverse Fluxes",
    "section": "Installing the required libraries",
    "text": "Installing the required libraries\nPlease run the cell below to install the libraries required to run this notebook.\n%pip install requests %pip install folium %pip install rasterstats %pip install pystac_client",
    "crumbs": [
      "Data Usage Notebooks",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "TM5-4DVar Isotopic CH₄ Inverse Fluxes"
    ]
  },
  {
    "objectID": "user_data_notebooks/tm54dvar-ch4flux-monthgrid-v1_User_Notebook.html#querying-the-stac-api",
    "href": "user_data_notebooks/tm54dvar-ch4flux-monthgrid-v1_User_Notebook.html#querying-the-stac-api",
    "title": "TM5-4DVar Isotopic CH₄ Inverse Fluxes",
    "section": "Querying the STAC API",
    "text": "Querying the STAC API\n\nimport requests\nfrom folium import Map, TileLayer\nfrom pystac_client import Client\n\n\n# Provide STAC and RASTER API endpoints\nSTAC_API_URL = \"http://ghg.center/api/stac\"\nRASTER_API_URL = \"https://ghg.center/api/raster\"\n\n# Please use the collection name similar to the one used in STAC collection.\n# Name of the collection for TM5 CH₄ inverse flux dataset. \ncollection_name = \"tm54dvar-ch4flux-monthgrid-v1\"\n\n\n# Fetching the collection from STAC collections using appropriate endpoint.\ncollection = requests.get(f\"{STAC_API_URL}/collections/{collection_name}\").json()\ncollection\n\nExamining the contents of our collection under the temporal variable, we see that the data is available from January 1999 to December 2016. By looking at the dashboard:time density, we observe that the data is periodic with monthly time density.\n\ndef get_item_count(collection_id):\n    count = 0\n    items_url = f\"{STAC_API_URL}/collections/{collection_id}/items\"\n\n    while True:\n        response = requests.get(items_url)\n\n        if not response.ok:\n            print(\"error getting items\")\n            exit()\n\n        stac = response.json()\n        count += int(stac[\"context\"].get(\"returned\", 0))\n        next = [link for link in stac[\"links\"] if link[\"rel\"] == \"next\"]\n\n        if not next:\n            break\n        items_url = next[0][\"href\"]\n\n    return count\n\n\n# Check total number of items available\nnumber_of_items = get_item_count(collection_name)\nitems = requests.get(f\"{STAC_API_URL}/collections/{collection_name}/items?limit={number_of_items}\").json()[\"features\"]\nprint(f\"Found {len(items)} items\")\n\n\n# Examining the first item in the collection\nitems[0]\n\nBelow, we are entering the minimum and maximum values to provide our upper and lower bounds in rescale_values.",
    "crumbs": [
      "Data Usage Notebooks",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "TM5-4DVar Isotopic CH₄ Inverse Fluxes"
    ]
  },
  {
    "objectID": "user_data_notebooks/tm54dvar-ch4flux-monthgrid-v1_User_Notebook.html#exploring-changes-in-ch₄-flux-levels-using-the-raster-api",
    "href": "user_data_notebooks/tm54dvar-ch4flux-monthgrid-v1_User_Notebook.html#exploring-changes-in-ch₄-flux-levels-using-the-raster-api",
    "title": "TM5-4DVar Isotopic CH₄ Inverse Fluxes",
    "section": "Exploring Changes in CH₄ flux Levels Using the Raster API",
    "text": "Exploring Changes in CH₄ flux Levels Using the Raster API\nIn this notebook, we will explore the global changes of CH₄ flux over time in urban regions. We will visualize the outputs on a map using folium.\n\n# to access the year value from each item more easily, this will let us query more explicity by year and month (e.g., 2020-02)\nitems = {item[\"properties\"][\"start_datetime\"][:10]: item for item in items} \nasset_name = \"fossil\" #fossil fuel\n\n\n# Fetching the min and max values for a specific item\nrescale_values = {\"max\":items[list(items.keys())[0]][\"assets\"][asset_name][\"raster:bands\"][0][\"histogram\"][\"max\"], \"min\":items[list(items.keys())[0]][\"assets\"][asset_name][\"raster:bands\"][0][\"histogram\"][\"min\"]}\n\nNow, we will pass the item id, collection name, and rescaling_factor to the Raster API endpoint. We will do this twice, once for 2020 and again for 2019, so that we can visualize each event independently.\n\ncolor_map = \"purd\"\nco2_flux_1 = requests.get(\n    f\"{RASTER_API_URL}/stac/tilejson.json?collection={items['2016-12-01']['collection']}&item={items['2016-12-01']['id']}\"\n    f\"&assets={asset_name}\"\n    f\"&color_formula=gamma+r+1.05&colormap_name={color_map}\"\n    f\"&rescale={rescale_values['min']},{rescale_values['max']}\", \n).json()\nco2_flux_1\n\n\nco2_flux_2 = requests.get(\n    f\"{RASTER_API_URL}/stac/tilejson.json?collection={items['1999-12-01']['collection']}&item={items['1999-12-01']['id']}\"\n    f\"&assets={asset_name}\"\n    f\"&color_formula=gamma+r+1.05&colormap_name={color_map}\"\n    f\"&rescale={rescale_values['min']},{rescale_values['max']}\", \n).json()\nco2_flux_2",
    "crumbs": [
      "Data Usage Notebooks",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "TM5-4DVar Isotopic CH₄ Inverse Fluxes"
    ]
  },
  {
    "objectID": "user_data_notebooks/tm54dvar-ch4flux-monthgrid-v1_User_Notebook.html#visualizing-ch₄-flux-emissions-from-fossil-fuel",
    "href": "user_data_notebooks/tm54dvar-ch4flux-monthgrid-v1_User_Notebook.html#visualizing-ch₄-flux-emissions-from-fossil-fuel",
    "title": "TM5-4DVar Isotopic CH₄ Inverse Fluxes",
    "section": "Visualizing CH₄ flux Emissions from Fossil Fuel",
    "text": "Visualizing CH₄ flux Emissions from Fossil Fuel\n\n# We'll import folium to map and folium.plugins to allow mapping side-by-side\nimport folium\nimport folium.plugins\n\n# Set initial zoom and center of map for CO₂ Layer\n# Centre of map [latitude,longitude]\nmap_ = folium.plugins.DualMap(location=(34, -118), zoom_start=6)\n\n\nmap_layer_2016 = TileLayer(\n    tiles=co2_flux_1[\"tiles\"][0],\n    attr=\"GHG\",\n    opacity=0.8,\n)\nmap_layer_2016.add_to(map_.m1)\n\nmap_layer_1999 = TileLayer(\n    tiles=co2_flux_2[\"tiles\"][0],\n    attr=\"GHG\",\n    opacity=0.8,\n)\nmap_layer_1999.add_to(map_.m2)\n\n# visualising the map\nmap_",
    "crumbs": [
      "Data Usage Notebooks",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "TM5-4DVar Isotopic CH₄ Inverse Fluxes"
    ]
  },
  {
    "objectID": "user_data_notebooks/tm54dvar-ch4flux-monthgrid-v1_User_Notebook.html#visualizing-the-data-as-a-time-series",
    "href": "user_data_notebooks/tm54dvar-ch4flux-monthgrid-v1_User_Notebook.html#visualizing-the-data-as-a-time-series",
    "title": "TM5-4DVar Isotopic CH₄ Inverse Fluxes",
    "section": "Visualizing the Data as a Time Series",
    "text": "Visualizing the Data as a Time Series\nWe can now explore the fossil fuel emission time series (January 1999 -December 2016) available for the Dallas, Texas area of the U.S. We can plot the data set using the code below:\n\nimport matplotlib.pyplot as plt\n\nfig = plt.figure(figsize=(20, 10))\n\n\nplt.plot(\n    df[\"datetime\"],\n    df[\"max\"],\n    color=\"red\",\n    linestyle=\"-\",\n    linewidth=0.5,\n    label=\"CH4 emissions\",\n)\n\nplt.legend()\nplt.xlabel(\"Years\")\nplt.ylabel(\"g CH₄/m²/year\")\nplt.xticks(rotation = 90)\nplt.title(\"CH4 emission Values for Texas, Dallas (2015-2020)\")\n\n\nprint(items[2][\"properties\"][\"start_datetime\"])\n\n\nco2_flux_3 = requests.get(\n    f\"{RASTER_API_URL}/stac/tilejson.json?collection={items[2]['collection']}&item={items[2]['id']}\"\n    f\"&assets={asset_name}\"\n    f\"&color_formula=gamma+r+1.05&colormap_name={color_map}\"\n    f\"&rescale={rescale_values['min']},{rescale_values['max']}\",\n).json()\nco2_flux_3\n\n\n# Use bbox initial zoom and map\n# Set up a map located w/in event bounds\nimport folium\n\naoi_map_bbox = Map(\n    tiles=\"OpenStreetMap\",\n    location=[\n        30,-100\n    ],\n    zoom_start=6.8,\n)\n\nmap_layer = TileLayer(\n    tiles=co2_flux_3[\"tiles\"][0],\n    attr=\"GHG\", opacity = 0.7\n)\n\nmap_layer.add_to(aoi_map_bbox)\n\naoi_map_bbox",
    "crumbs": [
      "Data Usage Notebooks",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "TM5-4DVar Isotopic CH₄ Inverse Fluxes"
    ]
  },
  {
    "objectID": "user_data_notebooks/tm54dvar-ch4flux-monthgrid-v1_User_Notebook.html#summary",
    "href": "user_data_notebooks/tm54dvar-ch4flux-monthgrid-v1_User_Notebook.html#summary",
    "title": "TM5-4DVar Isotopic CH₄ Inverse Fluxes",
    "section": "Summary",
    "text": "Summary\nIn this notebook we have successfully explored, analyzed, and visualized the STAC collection for TM5-4DVar Isotopic CH₄ Inverse Fluxes dataset.",
    "crumbs": [
      "Data Usage Notebooks",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "TM5-4DVar Isotopic CH₄ Inverse Fluxes"
    ]
  },
  {
    "objectID": "user_data_notebooks/noaa-insitu_User_Notebook.html",
    "href": "user_data_notebooks/noaa-insitu_User_Notebook.html",
    "title": "Atmospheric Carbon Dioxide Concentrations from NOAA Global Monitoring Laboratory",
    "section": "",
    "text": "Identify available dates and temporal frequency of observations for the given data. The collection processed in this notebook is the Atmospheric Carbon Dioxide Concentrations from NOAA Global Monitoring Laboratory.\nVisualize the time series data",
    "crumbs": [
      "Data Usage Notebooks",
      "Greenhouse Gas Concentrations",
      "Atmospheric Carbon Dioxide Concentrations from NOAA Global Monitoring Laboratory"
    ]
  },
  {
    "objectID": "user_data_notebooks/noaa-insitu_User_Notebook.html#approach",
    "href": "user_data_notebooks/noaa-insitu_User_Notebook.html#approach",
    "title": "Atmospheric Carbon Dioxide Concentrations from NOAA Global Monitoring Laboratory",
    "section": "",
    "text": "Identify available dates and temporal frequency of observations for the given data. The collection processed in this notebook is the Atmospheric Carbon Dioxide Concentrations from NOAA Global Monitoring Laboratory.\nVisualize the time series data",
    "crumbs": [
      "Data Usage Notebooks",
      "Greenhouse Gas Concentrations",
      "Atmospheric Carbon Dioxide Concentrations from NOAA Global Monitoring Laboratory"
    ]
  },
  {
    "objectID": "user_data_notebooks/noaa-insitu_User_Notebook.html#about-the-data",
    "href": "user_data_notebooks/noaa-insitu_User_Notebook.html#about-the-data",
    "title": "Atmospheric Carbon Dioxide Concentrations from NOAA Global Monitoring Laboratory",
    "section": "About the Data",
    "text": "About the Data\nThe Global Greenhouse Gas Reference Network (GGGRN) for the Carbon Cycle and Greenhouse Gases (CCGG) Group is part of NOAA’S Global Monitoring Laboratory (GML) in Boulder, CO. The Reference Network measures the atmospheric distribution and trends of the three main long-term drivers of climate change, carbon dioxide (CO₂), methane (CH₄), and nitrous oxide (N2O), as well as carbon monoxide (CO) and many other trace gases which help interpretation of the main GHGs. The Reference Network measurement program includes continuous in-situ measurements at 4 baseline observatories (global background sites) and 8 tall towers, as well as flask-air samples collected by volunteers at over 50 additional regional background sites and from small aircraft conducting regular vertical profiles. The air samples are returned to GML for analysis where measurements of about 55 trace gases are done. NOAA’s GGGRN maintains the World Meteorological Organization international calibration scales for CO₂, CH₄, CO, N2O, and SF6 in air. The measurements from the GGGRN serve as a comparison with measurements made by many other international laboratories, and with regional studies. They are widely used in modeling studies that infer space-time patterns of emissions and removals of greenhouse gases that are optimally consistent with the atmospheric observations, given wind patterns. These data serve as an early warning for climate “surprises”. The measurements are also helpful for the ongoing evaluation of remote sensing technologies.",
    "crumbs": [
      "Data Usage Notebooks",
      "Greenhouse Gas Concentrations",
      "Atmospheric Carbon Dioxide Concentrations from NOAA Global Monitoring Laboratory"
    ]
  },
  {
    "objectID": "user_data_notebooks/noaa-insitu_User_Notebook.html#installing-the-required-libraries",
    "href": "user_data_notebooks/noaa-insitu_User_Notebook.html#installing-the-required-libraries",
    "title": "Atmospheric Carbon Dioxide Concentrations from NOAA Global Monitoring Laboratory",
    "section": "Installing the required libraries",
    "text": "Installing the required libraries\nPlease run the cell below to install the libraries required to run this notebook.\n\n%pip install matplotlib\n%pip install pandas\n%pip install requests\n\nRequirement already satisfied: matplotlib in /Users/vgaur/miniconda3/envs/cmip6/lib/python3.9/site-packages (3.7.1)\nRequirement already satisfied: pyparsing&gt;=2.3.1 in /Users/vgaur/miniconda3/envs/cmip6/lib/python3.9/site-packages (from matplotlib) (3.0.9)\nRequirement already satisfied: importlib-resources&gt;=3.2.0 in /Users/vgaur/miniconda3/envs/cmip6/lib/python3.9/site-packages (from matplotlib) (5.12.0)\nRequirement already satisfied: kiwisolver&gt;=1.0.1 in /Users/vgaur/miniconda3/envs/cmip6/lib/python3.9/site-packages (from matplotlib) (1.4.4)\nRequirement already satisfied: contourpy&gt;=1.0.1 in /Users/vgaur/miniconda3/envs/cmip6/lib/python3.9/site-packages (from matplotlib) (1.0.5)\nRequirement already satisfied: numpy&gt;=1.20 in /Users/vgaur/miniconda3/envs/cmip6/lib/python3.9/site-packages (from matplotlib) (1.24.3)\nRequirement already satisfied: fonttools&gt;=4.22.0 in /Users/vgaur/miniconda3/envs/cmip6/lib/python3.9/site-packages (from matplotlib) (4.25.0)\nRequirement already satisfied: python-dateutil&gt;=2.7 in /Users/vgaur/miniconda3/envs/cmip6/lib/python3.9/site-packages (from matplotlib) (2.8.2)\nRequirement already satisfied: pillow&gt;=6.2.0 in /Users/vgaur/miniconda3/envs/cmip6/lib/python3.9/site-packages (from matplotlib) (9.5.0)\nRequirement already satisfied: packaging&gt;=20.0 in /Users/vgaur/miniconda3/envs/cmip6/lib/python3.9/site-packages (from matplotlib) (23.1)\nRequirement already satisfied: cycler&gt;=0.10 in /Users/vgaur/miniconda3/envs/cmip6/lib/python3.9/site-packages (from matplotlib) (0.11.0)\nRequirement already satisfied: zipp&gt;=3.1.0 in /Users/vgaur/miniconda3/envs/cmip6/lib/python3.9/site-packages (from importlib-resources&gt;=3.2.0-&gt;matplotlib) (3.15.0)\nRequirement already satisfied: six&gt;=1.5 in /Users/vgaur/miniconda3/envs/cmip6/lib/python3.9/site-packages (from python-dateutil&gt;=2.7-&gt;matplotlib) (1.16.0)\nNote: you may need to restart the kernel to use updated packages.\nRequirement already satisfied: pandas in /Users/vgaur/miniconda3/envs/cmip6/lib/python3.9/site-packages (2.0.3)\nRequirement already satisfied: numpy&gt;=1.20.3 in /Users/vgaur/miniconda3/envs/cmip6/lib/python3.9/site-packages (from pandas) (1.24.3)\nRequirement already satisfied: tzdata&gt;=2022.1 in /Users/vgaur/miniconda3/envs/cmip6/lib/python3.9/site-packages (from pandas) (2023.3)\nRequirement already satisfied: pytz&gt;=2020.1 in /Users/vgaur/miniconda3/envs/cmip6/lib/python3.9/site-packages (from pandas) (2023.3)\nRequirement already satisfied: python-dateutil&gt;=2.8.2 in /Users/vgaur/miniconda3/envs/cmip6/lib/python3.9/site-packages (from pandas) (2.8.2)\nRequirement already satisfied: six&gt;=1.5 in /Users/vgaur/miniconda3/envs/cmip6/lib/python3.9/site-packages (from python-dateutil&gt;=2.8.2-&gt;pandas) (1.16.0)\nNote: you may need to restart the kernel to use updated packages.\nRequirement already satisfied: requests in /Users/vgaur/miniconda3/envs/cmip6/lib/python3.9/site-packages (2.31.0)\nRequirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in /Users/vgaur/miniconda3/envs/cmip6/lib/python3.9/site-packages (from requests) (1.26.16)\nRequirement already satisfied: idna&lt;4,&gt;=2.5 in /Users/vgaur/miniconda3/envs/cmip6/lib/python3.9/site-packages (from requests) (3.4)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /Users/vgaur/miniconda3/envs/cmip6/lib/python3.9/site-packages (from requests) (2023.7.22)\nRequirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /Users/vgaur/miniconda3/envs/cmip6/lib/python3.9/site-packages (from requests) (3.1.0)\nNote: you may need to restart the kernel to use updated packages.\n\n\n\nImporting required libraries\n\nimport numpy as np\nimport pandas as pd\nfrom glob import glob\nfrom io import StringIO\nimport matplotlib.pyplot as plt\nimport requests",
    "crumbs": [
      "Data Usage Notebooks",
      "Greenhouse Gas Concentrations",
      "Atmospheric Carbon Dioxide Concentrations from NOAA Global Monitoring Laboratory"
    ]
  },
  {
    "objectID": "user_data_notebooks/noaa-insitu_User_Notebook.html#reading-the-noaa-data-from-github-repo",
    "href": "user_data_notebooks/noaa-insitu_User_Notebook.html#reading-the-noaa-data-from-github-repo",
    "title": "Atmospheric Carbon Dioxide Concentrations from NOAA Global Monitoring Laboratory",
    "section": "Reading the NOAA data from GitHub repo",
    "text": "Reading the NOAA data from GitHub repo\n\ngithub_repo_owner = \"NASA-IMPACT\"\ngithub_repo_name = \"noaa-viz\"\nfolder_path_ch4, folder_path_co2 = \"flask/ch4\", \"flask/c02\"\ncombined_df_co2, combined_df_ch4 = pd.DataFrame(), pd.DataFrame()\n\n\n# Function to fetch and append a file from GitHub\ndef append_github_file(file_url):\n    response = requests.get(file_url)\n    response.raise_for_status()\n    return response.text\n\n# Get the list of CH4 files in the specified directory using GitHub API\ngithub_api_url = f\"https://api.github.com/repos/{github_repo_owner}/{github_repo_name}/contents/{folder_path_ch4}\"\nresponse = requests.get(github_api_url)\nresponse.raise_for_status()\nfile_list_ch4 = response.json()\n\n# Get the list of CO2 files in the specified directory using GitHub API\ngithub_api_url = f\"https://api.github.com/repos/{github_repo_owner}/{github_repo_name}/contents/{folder_path_ch4}\"\nresponse = requests.get(github_api_url)\nresponse.raise_for_status()\nfile_list_co2 = response.json()",
    "crumbs": [
      "Data Usage Notebooks",
      "Greenhouse Gas Concentrations",
      "Atmospheric Carbon Dioxide Concentrations from NOAA Global Monitoring Laboratory"
    ]
  },
  {
    "objectID": "user_data_notebooks/noaa-insitu_User_Notebook.html#concatenating-the-ch4-data-into-a-single-dataframe",
    "href": "user_data_notebooks/noaa-insitu_User_Notebook.html#concatenating-the-ch4-data-into-a-single-dataframe",
    "title": "Atmospheric Carbon Dioxide Concentrations from NOAA Global Monitoring Laboratory",
    "section": "Concatenating the CH4 data into a single DataFrame",
    "text": "Concatenating the CH4 data into a single DataFrame\n\nfor file_info in file_list_ch4:\n    if file_info[\"name\"].endswith(\"txt\"):\n        file_content = append_github_file(file_info[\"download_url\"])\n        Lines = file_content.splitlines()\n        index = Lines.index(\"# VARIABLE ORDER\")+2\n        df = pd.read_csv(StringIO(\"\\n\".join(Lines[index:])), delim_whitespace=True)\n        combined_df_ch4 = pd.concat([combined_df_ch4, df], ignore_index=True)",
    "crumbs": [
      "Data Usage Notebooks",
      "Greenhouse Gas Concentrations",
      "Atmospheric Carbon Dioxide Concentrations from NOAA Global Monitoring Laboratory"
    ]
  },
  {
    "objectID": "user_data_notebooks/noaa-insitu_User_Notebook.html#concatenating-the-co2-data-into-a-single-dataframe",
    "href": "user_data_notebooks/noaa-insitu_User_Notebook.html#concatenating-the-co2-data-into-a-single-dataframe",
    "title": "Atmospheric Carbon Dioxide Concentrations from NOAA Global Monitoring Laboratory",
    "section": "Concatenating the CO2 data into a single DataFrame",
    "text": "Concatenating the CO2 data into a single DataFrame\n\nfor file_info in file_list_co2:\n    if file_info[\"name\"].endswith(\"txt\"):\n        file_content = append_github_file(file_info[\"download_url\"])\n        Lines = file_content.splitlines()\n        index = Lines.index(\"# VARIABLE ORDER\")+2\n        df = pd.read_csv(StringIO(\"\\n\".join(Lines[index:])), delim_whitespace=True)\n        combined_df_co2 = pd.concat([combined_df_co2, df], ignore_index=True)",
    "crumbs": [
      "Data Usage Notebooks",
      "Greenhouse Gas Concentrations",
      "Atmospheric Carbon Dioxide Concentrations from NOAA Global Monitoring Laboratory"
    ]
  },
  {
    "objectID": "user_data_notebooks/noaa-insitu_User_Notebook.html#visualizing-the-noaa-data-for-ch4-and-co2",
    "href": "user_data_notebooks/noaa-insitu_User_Notebook.html#visualizing-the-noaa-data-for-ch4-and-co2",
    "title": "Atmospheric Carbon Dioxide Concentrations from NOAA Global Monitoring Laboratory",
    "section": "Visualizing the NOAA data for CH4 and CO2",
    "text": "Visualizing the NOAA data for CH4 and CO2\n\nsite_to_filter = 'ABP'\nfiltered_df = combined_df_co2[combined_df_co2['site_code'] == site_to_filter]\n\nfiltered_df['datetime'] = pd.to_datetime(filtered_df['datetime'])\n\n# Set the \"Date\" column as the index\nfiltered_df.set_index('datetime', inplace=True)\n\n# Create a time series plot for 'Data' and 'Value'\nplt.figure(figsize=(12, 6))\nplt.plot(filtered_df.index, filtered_df['value'], label='Carbon Dioxide(CO2) Concentration (ppm)')\nplt.xlabel(\"Observed Date/Time\")\nplt.ylabel(\"Carbon Dioxide(CO2) Concentration (ppm)\")\nplt.title(f\"Observed Co2 Concentration {site_to_filter}\")\nplt.legend()\nplt.grid(True)\n# plt.show()\n\n/var/folders/7b/5rrvrjx51l54jchgs0tqps0c0000gn/T/ipykernel_70808/2606016741.py:4: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  filtered_df['datetime'] = pd.to_datetime(filtered_df['datetime'])\n\n\n\n\n\n\n\n\n\n\nsite_to_filter = 'ABP'\nfiltered_df = combined_df_ch4[combined_df_ch4['site_code'] == site_to_filter]\nfiltered_df['datetime'] = pd.to_datetime(filtered_df['datetime'])\n\n# Set the \"Date\" column as the index\nfiltered_df.set_index('datetime', inplace=True)\n\n# Create a time series plot for 'Data' and 'Value'\nplt.figure(figsize=(12, 6))\nplt.plot(filtered_df.index, filtered_df['value'], label='Methane Ch4 Concentration (ppb)')\nplt.xlabel(\"Observation Date/Time\")\nplt.ylabel(\"Methane Ch4 Concentration (ppb)\")\nplt.title(f\"Observed CH4 Concentration {site_to_filter}\")\nplt.legend()\nplt.grid(True)\nplt.show()\n\n/var/folders/7b/5rrvrjx51l54jchgs0tqps0c0000gn/T/ipykernel_70808/1635934907.py:3: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  filtered_df['datetime'] = pd.to_datetime(filtered_df['datetime'])",
    "crumbs": [
      "Data Usage Notebooks",
      "Greenhouse Gas Concentrations",
      "Atmospheric Carbon Dioxide Concentrations from NOAA Global Monitoring Laboratory"
    ]
  },
  {
    "objectID": "user_data_notebooks/noaa-insitu_User_Notebook.html#summary",
    "href": "user_data_notebooks/noaa-insitu_User_Notebook.html#summary",
    "title": "Atmospheric Carbon Dioxide Concentrations from NOAA Global Monitoring Laboratory",
    "section": "Summary",
    "text": "Summary\nIn this notebook we have successfully visualized the data for Atmospheric Carbon Dioxide Concentrations from NOAA Global Monitoring Laboratory.",
    "crumbs": [
      "Data Usage Notebooks",
      "Greenhouse Gas Concentrations",
      "Atmospheric Carbon Dioxide Concentrations from NOAA Global Monitoring Laboratory"
    ]
  },
  {
    "objectID": "user_data_notebooks/gosat-based-ch4budget-yeargrid-v1_User_Notebook.html",
    "href": "user_data_notebooks/gosat-based-ch4budget-yeargrid-v1_User_Notebook.html",
    "title": "GOSAT-based Top-down Total and Natural Methane Emissions",
    "section": "",
    "text": "Identify available dates and temporal frequency of observations for the given collection using the GHGC API /stac endpoint. The collection processed in this notebook is the gridded methane emissions data product.\nPass the STAC item into the raster API /stac/tilejson.jsonendpoint.\nUsing folium.plugins.DualMap, we will visualize two tiles (side-by-side), allowing us to compare time points.\nAfter the visualization, we will perform zonal statistics for a given polygon.",
    "crumbs": [
      "Data Usage Notebooks",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "GOSAT-based Top-down Total and Natural Methane Emissions"
    ]
  },
  {
    "objectID": "user_data_notebooks/gosat-based-ch4budget-yeargrid-v1_User_Notebook.html#approach",
    "href": "user_data_notebooks/gosat-based-ch4budget-yeargrid-v1_User_Notebook.html#approach",
    "title": "GOSAT-based Top-down Total and Natural Methane Emissions",
    "section": "",
    "text": "Identify available dates and temporal frequency of observations for the given collection using the GHGC API /stac endpoint. The collection processed in this notebook is the gridded methane emissions data product.\nPass the STAC item into the raster API /stac/tilejson.jsonendpoint.\nUsing folium.plugins.DualMap, we will visualize two tiles (side-by-side), allowing us to compare time points.\nAfter the visualization, we will perform zonal statistics for a given polygon.",
    "crumbs": [
      "Data Usage Notebooks",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "GOSAT-based Top-down Total and Natural Methane Emissions"
    ]
  },
  {
    "objectID": "user_data_notebooks/gosat-based-ch4budget-yeargrid-v1_User_Notebook.html#about-the-data",
    "href": "user_data_notebooks/gosat-based-ch4budget-yeargrid-v1_User_Notebook.html#about-the-data",
    "title": "GOSAT-based Top-down Total and Natural Methane Emissions",
    "section": "About the Data",
    "text": "About the Data\nThe NASA Carbon Monitoring System Flux (CMS-Flux) team analyzed remote sensing observations from Japan’s Greenhouse gases Observing SATellite (GOSAT) to produce the global Committee on Earth Observation Satellites (CEOS) CH₄ Emissions data product. They used an analytic Bayesian inversion approach and the GEOS-Chem global chemistry transport model to quantify annual methane (CH₄) emissions and their uncertainties at a spatial resolution of 1° by 1° and then projected these to each country for 2019.",
    "crumbs": [
      "Data Usage Notebooks",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "GOSAT-based Top-down Total and Natural Methane Emissions"
    ]
  },
  {
    "objectID": "user_data_notebooks/gosat-based-ch4budget-yeargrid-v1_User_Notebook.html#querying-the-stac-api",
    "href": "user_data_notebooks/gosat-based-ch4budget-yeargrid-v1_User_Notebook.html#querying-the-stac-api",
    "title": "GOSAT-based Top-down Total and Natural Methane Emissions",
    "section": "Querying the STAC API",
    "text": "Querying the STAC API\n\nimport requests\nfrom folium import Map, TileLayer\nfrom pystac_client import Client\n\n\n# Provide STAC and RASTER API endpoints\nSTAC_API_URL = \"http://ghg.center/api/stac\"\nRASTER_API_URL = \"https://ghg.center/api/raster\"\n\n# Please use the collection name similar to the one used in STAC collection.\n\n# Name of the collection for gosat budget methane. \ncollection_name = \"gosat-based-ch4budget-yeargrid-v1\"\n\n\n# Fetching the collection from STAC collections using appropriate endpoint.\ncollection = requests.get(f\"{STAC_API_URL}/collections/{collection_name}\").json()\ncollection\n\nExamining the contents of our collection under the temporal variable, we see that the data is available from January 2012 to December 2018. By looking at the dashboard:time density, we observe that the data is available for only one year, i.e. 2019.\n\ndef get_item_count(collection_id):\n    count = 0\n    items_url = f\"{STAC_API_URL}/collections/{collection_id}/items\"\n\n    while True:\n        response = requests.get(items_url)\n\n        if not response.ok:\n            print(\"error getting items\")\n            exit()\n\n        stac = response.json()\n        count += int(stac[\"context\"].get(\"returned\", 0))\n        next = [link for link in stac[\"links\"] if link[\"rel\"] == \"next\"]\n\n        if not next:\n            break\n        items_url = next[0][\"href\"]\n\n    return count\n\n\n# Check total number of items available\nnumber_of_items = get_item_count(collection_name)\nitems = requests.get(f\"{STAC_API_URL}/collections/{collection_name}/items?limit={number_of_items}\").json()[\"features\"]\nprint(f\"Found {len(items)} items\")\n\n\n# Examining the first item in the collection\nitems[0]\n\nBelow, we enter minimum and maximum values to provide our upper and lower bounds in rescale_values.",
    "crumbs": [
      "Data Usage Notebooks",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "GOSAT-based Top-down Total and Natural Methane Emissions"
    ]
  },
  {
    "objectID": "user_data_notebooks/gosat-based-ch4budget-yeargrid-v1_User_Notebook.html#exploring-changes-in-gosat-methane-budgets-ch4-levels-using-the-raster-api",
    "href": "user_data_notebooks/gosat-based-ch4budget-yeargrid-v1_User_Notebook.html#exploring-changes-in-gosat-methane-budgets-ch4-levels-using-the-raster-api",
    "title": "GOSAT-based Top-down Total and Natural Methane Emissions",
    "section": "Exploring Changes in GOSAT Methane budgets (CH4) Levels Using the Raster API",
    "text": "Exploring Changes in GOSAT Methane budgets (CH4) Levels Using the Raster API\nIn this notebook, we will explore the impacts of methane emissions and by examining changes over time in urban regions. We will visualize the outputs on a map using folium.\n\n# To access the year value from each item more easily, this will let us query more explicity by year and month (e.g., 2020-02)\nitems = {item[\"properties\"][\"start_datetime\"][:10]: item for item in items} \nasset_name = \"prior-total\"\n\n\n# Fetching the min and max values for a specific item\nrescale_values = {\"max\":items[list(items.keys())[0]][\"assets\"][asset_name][\"raster:bands\"][0][\"histogram\"][\"max\"], \"min\":items[list(items.keys())[0]][\"assets\"][asset_name][\"raster:bands\"][0][\"histogram\"][\"min\"]}\n\n\nitems.keys()\n\nNow, we will pass the item id, collection name, and rescaling_factor to the Raster API endpoint. We will do this for first January 2019.\n\ncolor_map = \"rainbow\" # please select the color ramp from matplotlib library.\njanuary_2019_tile = requests.get(\n    f\"{RASTER_API_URL}/stac/tilejson.json?collection={items['2019-01-01']['collection']}&item={items['2019-01-01']['id']}\"\n    f\"&assets={asset_name}\"\n    f\"&color_formula=gamma+r+1.05&colormap_name={color_map}\"\n    f\"&rescale={rescale_values['min']},{rescale_values['max']}\", \n).json()\njanuary_2019_tile",
    "crumbs": [
      "Data Usage Notebooks",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "GOSAT-based Top-down Total and Natural Methane Emissions"
    ]
  },
  {
    "objectID": "user_data_notebooks/gosat-based-ch4budget-yeargrid-v1_User_Notebook.html#visualizing-ch₄-emissions",
    "href": "user_data_notebooks/gosat-based-ch4budget-yeargrid-v1_User_Notebook.html#visualizing-ch₄-emissions",
    "title": "GOSAT-based Top-down Total and Natural Methane Emissions",
    "section": "Visualizing CH₄ Emissions",
    "text": "Visualizing CH₄ Emissions\n\n# We will import folium to map and folium.plugins to allow side-by-side mapping\nimport folium\nimport folium.plugins\n\n# Set initial zoom and center of map for CH₄ Layer\n# Centre of map [latitude,longitude]\nmap_ = folium.Map(location=(34, -118), zoom_start=6)\n\n# January 2019\nmap_layer_2019 = TileLayer(\n    tiles=january_2019_tile[\"tiles\"][0],\n    attr=\"GHG\",\n    opacity=0.7,\n)\nmap_layer_2019.add_to(map_)\n\n# # January 2012\n# map_layer_2012 = TileLayer(\n#     tiles=january_2012_tile[\"tiles\"][0],\n#     attr=\"GHG\",\n#     opacity=0.7,\n# )\n# map_layer_2012.add_to(map_.m2)\n\n# visualising the map\nmap_",
    "crumbs": [
      "Data Usage Notebooks",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "GOSAT-based Top-down Total and Natural Methane Emissions"
    ]
  },
  {
    "objectID": "user_data_notebooks/gosat-based-ch4budget-yeargrid-v1_User_Notebook.html#summary",
    "href": "user_data_notebooks/gosat-based-ch4budget-yeargrid-v1_User_Notebook.html#summary",
    "title": "GOSAT-based Top-down Total and Natural Methane Emissions",
    "section": "Summary",
    "text": "Summary\nIn this notebook we have successfully explored, analyzed, and visualized the STAC collection for GOSAT-based Top-down Total and Natural Methane Emissions.",
    "crumbs": [
      "Data Usage Notebooks",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "GOSAT-based Top-down Total and Natural Methane Emissions"
    ]
  },
  {
    "objectID": "user_data_notebooks/sedac-popdensity-yeargrid5yr-v4.11_User_Notebook.html",
    "href": "user_data_notebooks/sedac-popdensity-yeargrid5yr-v4.11_User_Notebook.html",
    "title": "SEDAC Gridded World Population Density",
    "section": "",
    "text": "Identify available dates and temporal frequency of observations for the given collection using the GHGC API /stac endpoint. Collection processed in this notebook is SEDAC gridded population density.\nPass the STAC item into raster API /stac/tilejson.json endpoint\nWe’ll visualize two tiles (side-by-side) allowing for comparison of each of the time points using folium.plugins.DualMap\nAfter the visualization, we’ll perform zonal statistics for a given polygon.",
    "crumbs": [
      "Data Usage Notebooks",
      "Socioeconomic",
      "SEDAC Gridded World Population Density"
    ]
  },
  {
    "objectID": "user_data_notebooks/sedac-popdensity-yeargrid5yr-v4.11_User_Notebook.html#approach",
    "href": "user_data_notebooks/sedac-popdensity-yeargrid5yr-v4.11_User_Notebook.html#approach",
    "title": "SEDAC Gridded World Population Density",
    "section": "",
    "text": "Identify available dates and temporal frequency of observations for the given collection using the GHGC API /stac endpoint. Collection processed in this notebook is SEDAC gridded population density.\nPass the STAC item into raster API /stac/tilejson.json endpoint\nWe’ll visualize two tiles (side-by-side) allowing for comparison of each of the time points using folium.plugins.DualMap\nAfter the visualization, we’ll perform zonal statistics for a given polygon.",
    "crumbs": [
      "Data Usage Notebooks",
      "Socioeconomic",
      "SEDAC Gridded World Population Density"
    ]
  },
  {
    "objectID": "user_data_notebooks/sedac-popdensity-yeargrid5yr-v4.11_User_Notebook.html#about-the-data",
    "href": "user_data_notebooks/sedac-popdensity-yeargrid5yr-v4.11_User_Notebook.html#about-the-data",
    "title": "SEDAC Gridded World Population Density",
    "section": "About the Data",
    "text": "About the Data\nThe SEDAC Gridded Population of the World: Population Density, v4.11 dataset provides annual estimates of population density for the years 2000, 2005, 2010, 2015, and 2020 on a 30 arc-second (~1 km) grid. These data can be used for assessing disaster impacts, risk mapping, and any other applications that include a human dimension. This population density dataset is provided by NASA’s Socioeconomic Data and Applications Center (SEDAC) hosted by the Center for International Earth Science Information Network (CIESIN) at Columbia University. The population estimates are provided as a continuous raster for the entire globe.",
    "crumbs": [
      "Data Usage Notebooks",
      "Socioeconomic",
      "SEDAC Gridded World Population Density"
    ]
  },
  {
    "objectID": "user_data_notebooks/sedac-popdensity-yeargrid5yr-v4.11_User_Notebook.html#querying-the-stac-api",
    "href": "user_data_notebooks/sedac-popdensity-yeargrid5yr-v4.11_User_Notebook.html#querying-the-stac-api",
    "title": "SEDAC Gridded World Population Density",
    "section": "Querying the STAC API",
    "text": "Querying the STAC API\n\nimport requests\nfrom folium import Map, TileLayer\nfrom pystac_client import Client\n\n\n# Provide STAC and RASTER API endpoints\nSTAC_API_URL = \"http://ghg.center/api/stac\"\nRASTER_API_URL = \"https://ghg.center/api/raster\"\n\n#Please use the collection name similar to the one used in STAC collection.\n# Name of the collection for SEDAC population density dataset. \ncollection_name = \"sedac-popdensity-yeargrid5yr-v4.11\"\n\n\n# Fetching the collection from STAC collections using appropriate endpoint.\ncollection = requests.get(f\"{STAC_API_URL}/collections/{collection_name}\").json()\ncollection\n\nExamining the contents of our collection under summaries we see that the data is available from January 2000 to December 2020. By looking at the dashboard:time density we observe that the data is available for the years 2000, 2005, 2010, 2015, 2020.\n\ndef get_item_count(collection_id):\n    count = 0\n    items_url = f\"{STAC_API_URL}/collections/{collection_id}/items\"\n\n    while True:\n        response = requests.get(items_url)\n\n        if not response.ok:\n            print(\"error getting items\")\n            exit()\n\n        stac = response.json()\n        count += int(stac[\"context\"].get(\"returned\", 0))\n        next = [link for link in stac[\"links\"] if link[\"rel\"] == \"next\"]\n\n        if not next:\n            break\n        items_url = next[0][\"href\"]\n\n    return count\n\n\n# Check total number of items available\nnumber_of_items = get_item_count(collection_name)\nitems = requests.get(f\"{STAC_API_URL}/collections/{collection_name}/items?limit={number_of_items}\").json()[\"features\"]\nprint(f\"Found {len(items)} items\")\n\n\nitems[0]\n\nBelow, we are entering the minimum and maximum values to provide our upper and lower bounds in rescale_values.",
    "crumbs": [
      "Data Usage Notebooks",
      "Socioeconomic",
      "SEDAC Gridded World Population Density"
    ]
  },
  {
    "objectID": "user_data_notebooks/sedac-popdensity-yeargrid5yr-v4.11_User_Notebook.html#exploring-changes-in-the-world-population-density-using-the-raster-api",
    "href": "user_data_notebooks/sedac-popdensity-yeargrid5yr-v4.11_User_Notebook.html#exploring-changes-in-the-world-population-density-using-the-raster-api",
    "title": "SEDAC Gridded World Population Density",
    "section": "Exploring Changes in the World Population Density using the Raster API",
    "text": "Exploring Changes in the World Population Density using the Raster API\nWe will explore changes in population density in urban regions. In this notebook, we’ll explore the changes in population density over time. We’ll then visualize the outputs on a map using folium.\n\n# to access the year value from each item more easily, this will let us query more explicity by year and month (e.g., 2020-02)\nitems = {item[\"properties\"][\"start_datetime\"][:7]: item for item in items} \nasset_name = \"population-density\"\n\n\nrescale_values = {\"max\":items[list(items.keys())[0]][\"assets\"][asset_name][\"raster:bands\"][0][\"histogram\"][\"max\"], \"min\":items[list(items.keys())[0]][\"assets\"][asset_name][\"raster:bands\"][0][\"histogram\"][\"min\"]}\n\nNow we will pass the item id, collection name, and rescaling_factor to the Raster API endpoint. We will do this twice, once for January 2000 and again for January 2020, so that we can visualize each event independently.\n\ncolor_map = \"rainbow\" # please select the color ramp from matplotlib library.\njanuary_2020_tile = requests.get(\n    f\"{RASTER_API_URL}/stac/tilejson.json?collection={items['2020-01']['collection']}&item={items['2020-01']['id']}\"\n    f\"&assets={asset_name}\"\n    f\"&color_formula=gamma+r+1.05&colormap_name={color_map}\"\n    f\"&rescale={rescale_values['min']},{rescale_values['max']}\", \n).json()\njanuary_2020_tile\n\n\njanuary_2000_tile = requests.get(\n    f\"{RASTER_API_URL}/stac/tilejson.json?collection={items['2000-01']['collection']}&item={items['2000-01']['id']}\"\n    f\"&assets={asset_name}\"\n    f\"&color_formula=gamma+r+1.05&colormap_name={color_map}\"\n    f\"&rescale={rescale_values['min']},{rescale_values['max']}\", \n).json()\njanuary_2000_tile",
    "crumbs": [
      "Data Usage Notebooks",
      "Socioeconomic",
      "SEDAC Gridded World Population Density"
    ]
  },
  {
    "objectID": "user_data_notebooks/sedac-popdensity-yeargrid5yr-v4.11_User_Notebook.html#visualizing-population-density.",
    "href": "user_data_notebooks/sedac-popdensity-yeargrid5yr-v4.11_User_Notebook.html#visualizing-population-density.",
    "title": "SEDAC Gridded World Population Density",
    "section": "Visualizing Population Density.",
    "text": "Visualizing Population Density.\n\n# We'll import folium to map and folium.plugins to allow mapping side-by-side\nimport folium\nimport folium.plugins\n\n# Set initial zoom and center of map for population density Layer\nmap_ = folium.plugins.DualMap(location=(34, -118), zoom_start=6)\n\n# January 2020\nmap_layer_2020 = TileLayer(\n    tiles=january_2020_tile[\"tiles\"][0],\n    attr=\"GHG\",\n    opacity=1,\n)\nmap_layer_2020.add_to(map_.m1)\n\n# January 2000\nmap_layer_2000 = TileLayer(\n    tiles=january_2000_tile[\"tiles\"][0],\n    attr=\"GHG\",\n    opacity=1,\n)\nmap_layer_2000.add_to(map_.m2)\n\n# visualising the map\nmap_",
    "crumbs": [
      "Data Usage Notebooks",
      "Socioeconomic",
      "SEDAC Gridded World Population Density"
    ]
  },
  {
    "objectID": "user_data_notebooks/sedac-popdensity-yeargrid5yr-v4.11_User_Notebook.html#section",
    "href": "user_data_notebooks/sedac-popdensity-yeargrid5yr-v4.11_User_Notebook.html#section",
    "title": "SEDAC Gridded World Population Density",
    "section": "",
    "text": "# Texas, USA\ntexas_aoi = {\n    \"type\": \"Feature\",\n    \"properties\": {},\n    \"geometry\": {\n        \"coordinates\": [\n            [\n                # [13.686159004559698, -21.700046934333145],\n                # [13.686159004559698, -23.241974326585833],\n                # [14.753560168039911, -23.241974326585833],\n                # [14.753560168039911, -21.700046934333145],\n                # [13.686159004559698, -21.700046934333145],\n                [-95, 29],\n                [-95, 33],\n                [-104, 33],\n                [-104,29],\n                [-95, 29]\n            ]\n        ],\n        \"type\": \"Polygon\",\n    },\n}\n\n\n# We'll plug in the coordinates for a location\n# central to the study area and a reasonable zoom level\n\nimport folium\n\naoi_map = Map(\n    tiles=\"OpenStreetMap\",\n    location=[\n        30,-100\n    ],\n    zoom_start=6,\n)\n\nfolium.GeoJson(texas_aoi, name=\"Texas, USA\").add_to(aoi_map)\naoi_map\n\n\n# Check total number of items available\nitems = requests.get(\n    f\"{STAC_API_URL}/collections/{collection_name}/items?limit=300\"\n).json()[\"features\"]\nprint(f\"Found {len(items)} items\")\n\n\n# Explore one item to see what it contains\nitems[0]\n\n\n# the bounding box should be passed to the geojson param as a geojson Feature or FeatureCollection\ndef generate_stats(item, geojson):\n    result = requests.post(\n        f\"{RASTER_API_URL}/cog/statistics\",\n        params={\"url\": item[\"assets\"][asset_name][\"href\"]},\n        json=geojson,\n    ).json()\n    return {\n        **result[\"properties\"],\n        \"start_datetime\": item[\"properties\"][\"start_datetime\"],\n    }\n\nWith the function above we can generate the statistics for the AOI.\n\n%%time\nstats = [generate_stats(item, texas_aoi) for item in items]\n\n\nstats[0]\n\n\nimport pandas as pd\n\n\ndef clean_stats(stats_json) -&gt; pd.DataFrame:\n    df = pd.json_normalize(stats_json)\n    df.columns = [col.replace(\"statistics.b1.\", \"\") for col in df.columns]\n    df[\"date\"] = pd.to_datetime(df[\"start_datetime\"])\n    return df\n\n\ndf = clean_stats(stats)\ndf.head(5)",
    "crumbs": [
      "Data Usage Notebooks",
      "Socioeconomic",
      "SEDAC Gridded World Population Density"
    ]
  },
  {
    "objectID": "user_data_notebooks/sedac-popdensity-yeargrid5yr-v4.11_User_Notebook.html#visualizing-the-data-as-a-time-series",
    "href": "user_data_notebooks/sedac-popdensity-yeargrid5yr-v4.11_User_Notebook.html#visualizing-the-data-as-a-time-series",
    "title": "SEDAC Gridded World Population Density",
    "section": "Visualizing the Data as a Time Series",
    "text": "Visualizing the Data as a Time Series\nWe can now explore the SEDAC population density dataset time series available for the Texas, Dallas area of USA. We can plot the dataset using the code below:\n\nimport matplotlib.pyplot as plt\n\nfig = plt.figure(figsize=(20, 10))\n\n\nplt.plot(\n    df[\"date\"],\n    df[\"max\"],\n    color=\"red\",\n    linestyle=\"-\",\n    linewidth=0.5,\n    label=\"Population density over the years\",\n)\n\nplt.legend()\nplt.xlabel(\"Years\")\nplt.ylabel(\"Population density\")\nplt.title(\"Population density over Texas, Dallas (2000-2020)\")\n\n\nprint(items[2][\"properties\"][\"start_datetime\"])\n\n\noctober_tile = requests.get(\n    f\"{RASTER_API_URL}/stac/tilejson.json?collection={items[2]['collection']}&item={items[2]['id']}\"\n    f\"&assets={asset_name}\"\n    f\"&color_formula=gamma+r+1.05&colormap_name={color_map}\"\n    f\"&rescale={rescale_values['min']},{rescale_values['max']}\",\n).json()\noctober_tile\n\n\n# Use bbox initial zoom and map\n# Set up a map located w/in event bounds\nimport folium\n\naoi_map_bbox = Map(\n    tiles=\"OpenStreetMap\",\n    location=[\n        30,-100\n    ],\n    zoom_start=8,\n)\n\nmap_layer = TileLayer(\n    tiles=october_tile[\"tiles\"][0],\n    attr=\"GHG\", opacity = 0.5\n)\n\nmap_layer.add_to(aoi_map_bbox)\n\naoi_map_bbox",
    "crumbs": [
      "Data Usage Notebooks",
      "Socioeconomic",
      "SEDAC Gridded World Population Density"
    ]
  },
  {
    "objectID": "user_data_notebooks/sedac-popdensity-yeargrid5yr-v4.11_User_Notebook.html#summary",
    "href": "user_data_notebooks/sedac-popdensity-yeargrid5yr-v4.11_User_Notebook.html#summary",
    "title": "SEDAC Gridded World Population Density",
    "section": "Summary",
    "text": "Summary\nIn this notebook we have successfully explored, analyzed and visualized the STAC collection for the SEDAC Gridded World Population Density dataset.",
    "crumbs": [
      "Data Usage Notebooks",
      "Socioeconomic",
      "SEDAC Gridded World Population Density"
    ]
  },
  {
    "objectID": "user_data_notebooks/emit-ch4plume-v1_User_Notebook.html",
    "href": "user_data_notebooks/emit-ch4plume-v1_User_Notebook.html",
    "title": "EMIT Methane Point Source Plume Complexes",
    "section": "",
    "text": "Identify available dates and temporal frequency of observations for the given collection using the GHGC API /stac endpoint. The collection processed in this notebook is the Earth Surface Mineral Dust Source Investigation (EMIT) methane emission plumes data product.\nPass the STAC item into the raster API /stac/tilejson.json endpoint.\nUsing folium.Map, visualize the plumes.\nAfter the visualization, perform zonal statistics for a given polygon.",
    "crumbs": [
      "Data Usage Notebooks",
      "Large Emissions Events",
      "EMIT Methane Point Source Plume Complexes"
    ]
  },
  {
    "objectID": "user_data_notebooks/emit-ch4plume-v1_User_Notebook.html#approach",
    "href": "user_data_notebooks/emit-ch4plume-v1_User_Notebook.html#approach",
    "title": "EMIT Methane Point Source Plume Complexes",
    "section": "",
    "text": "Identify available dates and temporal frequency of observations for the given collection using the GHGC API /stac endpoint. The collection processed in this notebook is the Earth Surface Mineral Dust Source Investigation (EMIT) methane emission plumes data product.\nPass the STAC item into the raster API /stac/tilejson.json endpoint.\nUsing folium.Map, visualize the plumes.\nAfter the visualization, perform zonal statistics for a given polygon.",
    "crumbs": [
      "Data Usage Notebooks",
      "Large Emissions Events",
      "EMIT Methane Point Source Plume Complexes"
    ]
  },
  {
    "objectID": "user_data_notebooks/emit-ch4plume-v1_User_Notebook.html#about-the-data",
    "href": "user_data_notebooks/emit-ch4plume-v1_User_Notebook.html#about-the-data",
    "title": "EMIT Methane Point Source Plume Complexes",
    "section": "About the Data",
    "text": "About the Data\nThe EMIT instrument builds upon NASA’s long history of developing advanced imaging spectrometers for new science and applications. EMIT launched to the International Space Station (ISS) on July 14, 2022. The data shows high-confidence research grade methane plumes from point source emitters - updated as they are identified - in keeping with JPL Open Science and Open Data policy.",
    "crumbs": [
      "Data Usage Notebooks",
      "Large Emissions Events",
      "EMIT Methane Point Source Plume Complexes"
    ]
  },
  {
    "objectID": "user_data_notebooks/emit-ch4plume-v1_User_Notebook.html#querying-the-stac-api",
    "href": "user_data_notebooks/emit-ch4plume-v1_User_Notebook.html#querying-the-stac-api",
    "title": "EMIT Methane Point Source Plume Complexes",
    "section": "Querying the STAC API",
    "text": "Querying the STAC API\n\nimport requests\nfrom folium import Map, TileLayer\nfrom pystac_client import Client\n\n\n# Provide STAC and RASTER API endpoints\nSTAC_API_URL = \"http://ghg.center/api/stac\"\nRASTER_API_URL = \"https://ghg.center/api/raster\"\n\n#Please use the collection name similar to the one used in STAC collection.\n\n# Name of the collection for methane emission plumes. \ncollection_name = \"emit-ch4plume-v1\"\n\n\n# Fetching the collection from STAC collections using appropriate endpoint.\ncollection = requests.get(f\"{STAC_API_URL}/collections/{collection_name}\").json()\ncollection\n\nExamining the contents of our collection under the temporal variable, we note that data is available from August 2022 to May 2023. By looking at the dashboard: time density, we can see that observations are conducted daily and non-periodically (i.e., there are plumes emissions for multiple places on the same dates).\n\ndef get_item_count(collection_id):\n    count = 0\n    items_url = f\"{STAC_API_URL}/collections/{collection_id}/items\"\n\n    while True:\n        response = requests.get(items_url)\n\n        if not response.ok:\n            print(\"error getting items\")\n            exit()\n\n        stac = response.json()\n        count += int(stac[\"context\"].get(\"returned\", 0))\n        next = [link for link in stac[\"links\"] if link[\"rel\"] == \"next\"]\n\n        if not next:\n            break\n        items_url = next[0][\"href\"]\n\n    return count\n\n\n# Check total number of items available\nnumber_of_items = get_item_count(collection_name)\nitems = requests.get(f\"{STAC_API_URL}/collections/{collection_name}/items?limit={number_of_items}\").json()[\"features\"]\nprint(f\"Found {len(items)} items\")\n\n\n# Examining the first item in the collection\nitems[0]\n\nBelow, we are entering the minimum and maximum values to provide our upper and lower bounds in rescale_values.",
    "crumbs": [
      "Data Usage Notebooks",
      "Large Emissions Events",
      "EMIT Methane Point Source Plume Complexes"
    ]
  },
  {
    "objectID": "user_data_notebooks/emit-ch4plume-v1_User_Notebook.html#exploring-methane-emission-plumes-ch₄-using-the-raster-api",
    "href": "user_data_notebooks/emit-ch4plume-v1_User_Notebook.html#exploring-methane-emission-plumes-ch₄-using-the-raster-api",
    "title": "EMIT Methane Point Source Plume Complexes",
    "section": "Exploring Methane Emission Plumes (CH₄) using the Raster API",
    "text": "Exploring Methane Emission Plumes (CH₄) using the Raster API\nIn this notebook, we will explore global methane emission plumes from point sources. We will visualize the outputs on a map using folium.\n\n# To access the year value from each item more easily, this will let us query more explicity by year and month (e.g., 2020-02)\nitems = {item[\"id\"][20:]: item for item in items} \nasset_name = \"ch4-plume-emissions\"\n\n\n# Fetching the min and max values for a specific item\nrescale_values = {\"max\":items[list(items.keys())[0]][\"assets\"][asset_name][\"raster:bands\"][0][\"histogram\"][\"max\"], \"min\":items[list(items.keys())[0]][\"assets\"][asset_name][\"raster:bands\"][0][\"histogram\"][\"min\"]}\n\nNow we will pass the item id, collection name, and rescaling_factor to the Raster API endpoint. We will do this for only one item so that we can visualize the event.\n\n# Select the item ID which you want to visualize. Item ID is in the format yyyymmdd followed by the timestamp. This ID can be extracted from the COG name as well.\nitem_id = \"20230418T200118_000829\"\ncolor_map = \"magma\"\nmethane_plume_tile = requests.get(\n    f\"{RASTER_API_URL}/stac/tilejson.json?collection={items[item_id]['collection']}&item={items[item_id]['id']}\"\n    f\"&assets={asset_name}\"\n    f\"&color_formula=gamma+r+1.05&colormap_name={color_map}\"\n    f\"&rescale={rescale_values['min']},{rescale_values['max']}\", \n).json()\nmethane_plume_tile",
    "crumbs": [
      "Data Usage Notebooks",
      "Large Emissions Events",
      "EMIT Methane Point Source Plume Complexes"
    ]
  },
  {
    "objectID": "user_data_notebooks/emit-ch4plume-v1_User_Notebook.html#visualizing-ch₄-emission-plume",
    "href": "user_data_notebooks/emit-ch4plume-v1_User_Notebook.html#visualizing-ch₄-emission-plume",
    "title": "EMIT Methane Point Source Plume Complexes",
    "section": "Visualizing CH₄ Emission Plume",
    "text": "Visualizing CH₄ Emission Plume\n\n# We will import folium to map and folium.plugins to allow side-by-side mapping\nimport folium\nimport folium.plugins\n\n# Set initial zoom and center of map for plume Layer\nmap_ = folium.Map(location=(methane_plume_tile[\"center\"][1], methane_plume_tile[\"center\"][0]), zoom_start=13)\n\n# December 2001\nmap_layer = TileLayer(\n    tiles=methane_plume_tile[\"tiles\"][0],\n    attr=\"GHG\",\n    opacity=1,\n)\nmap_layer.add_to(map_)\n\n# visualising the map\nmap_",
    "crumbs": [
      "Data Usage Notebooks",
      "Large Emissions Events",
      "EMIT Methane Point Source Plume Complexes"
    ]
  },
  {
    "objectID": "user_data_notebooks/emit-ch4plume-v1_User_Notebook.html#summary",
    "href": "user_data_notebooks/emit-ch4plume-v1_User_Notebook.html#summary",
    "title": "EMIT Methane Point Source Plume Complexes",
    "section": "Summary",
    "text": "Summary\nIn this notebook we have successfully explored, analyzed, and visualized the STAC collection for EMIT methane emission plumes.",
    "crumbs": [
      "Data Usage Notebooks",
      "Large Emissions Events",
      "EMIT Methane Point Source Plume Complexes"
    ]
  },
  {
    "objectID": "user_data_notebooks/oco2-mip-co2budget-yeargrid-v1_User_Notebook.html",
    "href": "user_data_notebooks/oco2-mip-co2budget-yeargrid-v1_User_Notebook.html",
    "title": "OCO-2 MIP Top-Down CO₂ Budgets",
    "section": "",
    "text": "Identify available dates and temporal frequency of observations for the given collection using the GHGC API /stac endpoint. The collection processed in this notebook is the OCO-2 MIP Top-Down CO₂ Budgets data product.\nPass the STAC item into the raster API /stac/tilejson.jsonendpoint.\nUsing folium.plugins.DualMap, we will visualize two tiles (side-by-side), allowing us to compare time points.\nAfter the visualization, we will perform zonal statistics for a given polygon.",
    "crumbs": [
      "Data Usage Notebooks",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "OCO-2 MIP Top-Down CO₂ Budgets"
    ]
  },
  {
    "objectID": "user_data_notebooks/oco2-mip-co2budget-yeargrid-v1_User_Notebook.html#approach",
    "href": "user_data_notebooks/oco2-mip-co2budget-yeargrid-v1_User_Notebook.html#approach",
    "title": "OCO-2 MIP Top-Down CO₂ Budgets",
    "section": "",
    "text": "Identify available dates and temporal frequency of observations for the given collection using the GHGC API /stac endpoint. The collection processed in this notebook is the OCO-2 MIP Top-Down CO₂ Budgets data product.\nPass the STAC item into the raster API /stac/tilejson.jsonendpoint.\nUsing folium.plugins.DualMap, we will visualize two tiles (side-by-side), allowing us to compare time points.\nAfter the visualization, we will perform zonal statistics for a given polygon.",
    "crumbs": [
      "Data Usage Notebooks",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "OCO-2 MIP Top-Down CO₂ Budgets"
    ]
  },
  {
    "objectID": "user_data_notebooks/oco2-mip-co2budget-yeargrid-v1_User_Notebook.html#about-the-data",
    "href": "user_data_notebooks/oco2-mip-co2budget-yeargrid-v1_User_Notebook.html#about-the-data",
    "title": "OCO-2 MIP Top-Down CO₂ Budgets",
    "section": "About the Data",
    "text": "About the Data\nThe Committee on Earth Observation Satellites (CEOS) Atmospheric Composition - Virtual Constellation (AC-VC) Greenhouse Gas (GHG) team has generated the CEOS CO₂ Budgets dataset, which provides annual top-down carbon dioxide (CO2) emissions and removals from 2015 - 2020 gridded globally at 1° resolution, and as national totals. Data is provided in units of grams of carbon dioxide per square meter per year (g CO2/m2/yr). Only a subset of the full dataset is displayed in the GHG Center explore view.",
    "crumbs": [
      "Data Usage Notebooks",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "OCO-2 MIP Top-Down CO₂ Budgets"
    ]
  },
  {
    "objectID": "user_data_notebooks/oco2-mip-co2budget-yeargrid-v1_User_Notebook.html#installing-the-required-libraries",
    "href": "user_data_notebooks/oco2-mip-co2budget-yeargrid-v1_User_Notebook.html#installing-the-required-libraries",
    "title": "OCO-2 MIP Top-Down CO₂ Budgets",
    "section": "Installing the required libraries",
    "text": "Installing the required libraries\nPlease run the cell below to install the libraries required to run this notebook.\n\n%pip install requests --quiet\n%pip install folium --quiet\n%pip install rasterstats --quiet\n%pip install pystac_client --quiet\n%pip install pandas --quiet\n%pip install matplotlib --quiet\n\nNote: you may need to restart the kernel to use updated packages.\nNote: you may need to restart the kernel to use updated packages.\nNote: you may need to restart the kernel to use updated packages.\nNote: you may need to restart the kernel to use updated packages.\nNote: you may need to restart the kernel to use updated packages.\nNote: you may need to restart the kernel to use updated packages.",
    "crumbs": [
      "Data Usage Notebooks",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "OCO-2 MIP Top-Down CO₂ Budgets"
    ]
  },
  {
    "objectID": "user_data_notebooks/oco2-mip-co2budget-yeargrid-v1_User_Notebook.html#querying-the-stac-api",
    "href": "user_data_notebooks/oco2-mip-co2budget-yeargrid-v1_User_Notebook.html#querying-the-stac-api",
    "title": "OCO-2 MIP Top-Down CO₂ Budgets",
    "section": "Querying the STAC API",
    "text": "Querying the STAC API\n\nimport requests\nfrom folium import Map, TileLayer\nfrom pystac_client import Client\n\n\n# Provide STAC and RASTER API endpoints\nSTAC_API_URL = \"http://ghg.center/api/stac\"\nRASTER_API_URL = \"https://ghg.center/api/raster\"\n\n# Please use the collection name similar to the one used in STAC collection.\n# Name of the collection for CEOS National Top-Down CO₂ Budgets dataset. \ncollection_name = \"oco2-mip-co2budget-yeargrid-v1\"\n\n\n# Fetching the collection from STAC collections using appropriate endpoint.\ncollection = requests.get(f\"{STAC_API_URL}/collections/{collection_name}\").json()\n#collection\n\nExamining the contents of our collection under the temporal variable, we see that the data is available from January 2015 to December 2020. By looking at the dashboard:time density, we observe that the periodic frequency of these observations is yearly.\n\ndef get_item_count(collection_id):\n    count = 0\n    items_url = f\"{STAC_API_URL}/collections/{collection_id}/items\"\n\n    while True:\n        response = requests.get(items_url)\n\n        if not response.ok:\n            print(\"error getting items\")\n            exit()\n\n        stac = response.json()\n        count += int(stac[\"context\"].get(\"returned\", 0))\n        next = [link for link in stac[\"links\"] if link[\"rel\"] == \"next\"]\n\n        if not next:\n            break\n        items_url = next[0][\"href\"]\n\n    return count\n\n\n# Check total number of items available\nnumber_of_items = get_item_count(collection_name)\nitems = requests.get(f\"{STAC_API_URL}/collections/{collection_name}/items?limit={number_of_items}\").json()[\"features\"]\nprint(f\"Found {len(items)} items\")\n\nFound 6 items\n\n\n\n# Examining the first item in the collection\n#items[0]\n\nBelow, we are entering the minimum and maximum values to provide our upper and lower bounds in rescale_values.",
    "crumbs": [
      "Data Usage Notebooks",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "OCO-2 MIP Top-Down CO₂ Budgets"
    ]
  },
  {
    "objectID": "user_data_notebooks/oco2-mip-co2budget-yeargrid-v1_User_Notebook.html#exploring-changes-in-co₂-levels-using-the-raster-api",
    "href": "user_data_notebooks/oco2-mip-co2budget-yeargrid-v1_User_Notebook.html#exploring-changes-in-co₂-levels-using-the-raster-api",
    "title": "OCO-2 MIP Top-Down CO₂ Budgets",
    "section": "Exploring Changes in CO₂ Levels Using the Raster API",
    "text": "Exploring Changes in CO₂ Levels Using the Raster API\nIn this notebook, we will explore the global changes of CO₂ budgets over time in urban regions. We will visualize the outputs on a map using folium.\n\n# to access the year value from each item more easily, this will let us query more explicity by year and month (e.g., 2020-02)\nitems = {item[\"properties\"][\"start_datetime\"]: item for item in items} \nasset_name = \"ff\" #fossil fuel\n\n\n# Fetching the min and max values for a specific item\nrescale_values = {\"max\":items[list(items.keys())[0]][\"assets\"][asset_name][\"raster:bands\"][0][\"histogram\"][\"max\"], \"min\":items[list(items.keys())[0]][\"assets\"][asset_name][\"raster:bands\"][0][\"histogram\"][\"min\"]}\n#Hardcoding the min and max values to match the scale in the GHG Center dashboard\nrescale_values = {\"max\": 450, \"min\": 0}\n\nNow, we will pass the item id, collection name, and rescaling_factor to the Raster API endpoint. We will do this twice, once for 2020 and again for 2019, so that we can visualize each event independently.\n\ncolor_map = \"purd\"\nco2_flux_1 = requests.get(\n    f\"{RASTER_API_URL}/stac/tilejson.json?collection={items[list(items.keys())[0]]['collection']}&item={items[list(items.keys())[0]]['id']}\"\n    f\"&assets={asset_name}\"\n    f\"&color_formula=gamma+r+1.05&colormap_name={color_map}\"\n    f\"&rescale={rescale_values['min']},{rescale_values['max']}\", \n).json()\nco2_flux_1\n\n{'tilejson': '2.2.0',\n 'version': '1.0.0',\n 'scheme': 'xyz',\n 'tiles': ['https://ghg.center/api/raster/stac/tiles/WebMercatorQuad/{z}/{x}/{y}@1x?collection=oco2-mip-co2budget-yeargrid-v1&item=oco2-mip-co2budget-yeargrid-v1-2020&assets=ff&color_formula=gamma+r+1.05&colormap_name=purd&rescale=0%2C450'],\n 'minzoom': 0,\n 'maxzoom': 24,\n 'bounds': [-180.0, -90.0, 180.0, 90.0],\n 'center': [0.0, 0.0, 0]}\n\n\n\nco2_flux_2 = requests.get(\n    f\"{RASTER_API_URL}/stac/tilejson.json?collection={items[list(items.keys())[1]]['collection']}&item={items[list(items.keys())[1]]['id']}\"\n    f\"&assets={asset_name}\"\n    f\"&color_formula=gamma+r+1.05&colormap_name={color_map}\"\n    f\"&rescale={rescale_values['min']},{rescale_values['max']}\", \n).json()\nco2_flux_2\n\n{'tilejson': '2.2.0',\n 'version': '1.0.0',\n 'scheme': 'xyz',\n 'tiles': ['https://ghg.center/api/raster/stac/tiles/WebMercatorQuad/{z}/{x}/{y}@1x?collection=oco2-mip-co2budget-yeargrid-v1&item=oco2-mip-co2budget-yeargrid-v1-2019&assets=ff&color_formula=gamma+r+1.05&colormap_name=purd&rescale=0%2C450'],\n 'minzoom': 0,\n 'maxzoom': 24,\n 'bounds': [-180.0, -90.0, 180.0, 90.0],\n 'center': [0.0, 0.0, 0]}",
    "crumbs": [
      "Data Usage Notebooks",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "OCO-2 MIP Top-Down CO₂ Budgets"
    ]
  },
  {
    "objectID": "user_data_notebooks/oco2-mip-co2budget-yeargrid-v1_User_Notebook.html#visualizing-co₂-emissions",
    "href": "user_data_notebooks/oco2-mip-co2budget-yeargrid-v1_User_Notebook.html#visualizing-co₂-emissions",
    "title": "OCO-2 MIP Top-Down CO₂ Budgets",
    "section": "Visualizing CO₂ Emissions",
    "text": "Visualizing CO₂ Emissions\n\n# We'll import folium to map and folium.plugins to allow mapping side-by-side\nimport folium\nimport folium.plugins\n\n# Set initial zoom and center of map for CO₂ Layer\n# Centre of map [latitude,longitude]\nmap_ = folium.plugins.DualMap(location=(34, -118), zoom_start=6)\n\n\nmap_layer_2020 = TileLayer(\n    tiles=co2_flux_1[\"tiles\"][0],\n    attr=\"GHG\",\n    opacity=0.5,\n)\nmap_layer_2020.add_to(map_.m1)\n\nmap_layer_2019 = TileLayer(\n    tiles=co2_flux_2[\"tiles\"][0],\n    attr=\"GHG\",\n    opacity=0.5,\n)\nmap_layer_2019.add_to(map_.m2)\n\n# visualising the map\nmap_\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook",
    "crumbs": [
      "Data Usage Notebooks",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "OCO-2 MIP Top-Down CO₂ Budgets"
    ]
  },
  {
    "objectID": "user_data_notebooks/oco2-mip-co2budget-yeargrid-v1_User_Notebook.html#visualizing-the-data-as-a-time-series",
    "href": "user_data_notebooks/oco2-mip-co2budget-yeargrid-v1_User_Notebook.html#visualizing-the-data-as-a-time-series",
    "title": "OCO-2 MIP Top-Down CO₂ Budgets",
    "section": "Visualizing the Data as a Time Series",
    "text": "Visualizing the Data as a Time Series\nWe can now explore the fossil fuel emission time series (January 2015 -December 2020) available for the Dallas, Texas area of the U.S. We can plot the data set using the code below:\n\nimport matplotlib.pyplot as plt\n\nfig = plt.figure(figsize=(20, 10))\n\n\nplt.plot(\n    df[\"datetime\"],\n    df[\"max\"],\n    color=\"red\",\n    linestyle=\"-\",\n    linewidth=0.5,\n    label=\"CO2 emissions\",\n)\n\nplt.legend()\nplt.xlabel(\"Years\")\nplt.ylabel(\"CO2 emissions gC/m2/year1\")\nplt.title(\"CO2 emission Values for Texas, Dallas (2015-2020)\")\n\nText(0.5, 1.0, 'CO2 emission Values for Texas, Dallas (2015-2020)')\n\n\n\n\n\n\n\n\n\n\nprint(items[2][\"properties\"][\"start_datetime\"])\n\n2018-01-01T00:00:00+00:00\n\n\n\nco2_flux_3 = requests.get(\n    f\"{RASTER_API_URL}/stac/tilejson.json?collection={items[2]['collection']}&item={items[2]['id']}\"\n    f\"&assets={asset_name}\"\n    f\"&color_formula=gamma+r+1.05&colormap_name={color_map}\"\n    f\"&rescale={rescale_values['min']},{rescale_values['max']}\",\n).json()\nco2_flux_3\n\n{'tilejson': '2.2.0',\n 'version': '1.0.0',\n 'scheme': 'xyz',\n 'tiles': ['https://ghg.center/api/raster/stac/tiles/WebMercatorQuad/{z}/{x}/{y}@1x?collection=oco2-mip-co2budget-yeargrid-v1&item=oco2-mip-co2budget-yeargrid-v1-2018&assets=ff&color_formula=gamma+r+1.05&colormap_name=purd&rescale=0%2C450'],\n 'minzoom': 0,\n 'maxzoom': 24,\n 'bounds': [-180.0, -90.0, 180.0, 90.0],\n 'center': [0.0, 0.0, 0]}\n\n\n\n# Use bbox initial zoom and map\n# Set up a map located w/in event bounds\nimport folium\n\naoi_map_bbox = Map(\n    tiles=\"OpenStreetMap\",\n    location=[\n        30,-100\n    ],\n    zoom_start=6.8,\n)\n\nmap_layer = TileLayer(\n    tiles=co2_flux_3[\"tiles\"][0],\n    attr=\"GHG\", opacity = 0.7\n)\n\nmap_layer.add_to(aoi_map_bbox)\n\naoi_map_bbox\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook",
    "crumbs": [
      "Data Usage Notebooks",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "OCO-2 MIP Top-Down CO₂ Budgets"
    ]
  },
  {
    "objectID": "user_data_notebooks/oco2-mip-co2budget-yeargrid-v1_User_Notebook.html#summary",
    "href": "user_data_notebooks/oco2-mip-co2budget-yeargrid-v1_User_Notebook.html#summary",
    "title": "OCO-2 MIP Top-Down CO₂ Budgets",
    "section": "Summary",
    "text": "Summary\nIn this notebook we have successfully explored, analyzed, and visualized the STAC collection for OCO-2 MIP Top-Down CO₂ Budgets.",
    "crumbs": [
      "Data Usage Notebooks",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "OCO-2 MIP Top-Down CO₂ Budgets"
    ]
  },
  {
    "objectID": "user_data_notebooks/eccodarwin-co2flux-monthgrid-v5_User_Notebook.html",
    "href": "user_data_notebooks/eccodarwin-co2flux-monthgrid-v5_User_Notebook.html",
    "title": "Air-Sea CO₂ Flux, ECCO-Darwin Model v5",
    "section": "",
    "text": "Identify available dates and temporal frequency of observations for the given collection using the GHGC API /stac endpoint. The collection processed in this notebook is the Air-Sea CO₂ Flux, ECCO-Darwin Model v5 Data product.\nPass the STAC item into the raster API /stac/tilejson.jsonendpoint.\nUsing folium.plugins.DualMap, we will visualize two tiles (side-by-side), allowing us to compare time points.\nAfter the visualization, we will perform zonal statistics for a given polygon.",
    "crumbs": [
      "Data Usage Notebooks",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "Air-Sea CO₂ Flux, ECCO-Darwin Model v5"
    ]
  },
  {
    "objectID": "user_data_notebooks/eccodarwin-co2flux-monthgrid-v5_User_Notebook.html#approach",
    "href": "user_data_notebooks/eccodarwin-co2flux-monthgrid-v5_User_Notebook.html#approach",
    "title": "Air-Sea CO₂ Flux, ECCO-Darwin Model v5",
    "section": "",
    "text": "Identify available dates and temporal frequency of observations for the given collection using the GHGC API /stac endpoint. The collection processed in this notebook is the Air-Sea CO₂ Flux, ECCO-Darwin Model v5 Data product.\nPass the STAC item into the raster API /stac/tilejson.jsonendpoint.\nUsing folium.plugins.DualMap, we will visualize two tiles (side-by-side), allowing us to compare time points.\nAfter the visualization, we will perform zonal statistics for a given polygon.",
    "crumbs": [
      "Data Usage Notebooks",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "Air-Sea CO₂ Flux, ECCO-Darwin Model v5"
    ]
  },
  {
    "objectID": "user_data_notebooks/eccodarwin-co2flux-monthgrid-v5_User_Notebook.html#about-the-data",
    "href": "user_data_notebooks/eccodarwin-co2flux-monthgrid-v5_User_Notebook.html#about-the-data",
    "title": "Air-Sea CO₂ Flux, ECCO-Darwin Model v5",
    "section": "About the Data",
    "text": "About the Data\nThe ocean is a major sink for atmospheric carbon dioxide (CO2), largely due to the presence of phytoplankton that use the CO₂ to grow. Studies have shown that global ocean CO₂ uptake has increased over recent decades however there is uncertainty in the various mechanisms that affect ocean CO₂ flux and storage and how the ocean carbon sink will respond to future climate change. Because CO₂ fluxes can vary significantly across space and time, combined with deficiencies in ocean and atmosphere CO₂ observations, there is a need for models that can thoroughly represent these processes. Ocean biogeochemical models (OBMs) have the ability to resolve the physical and biogeochemical mechanisms contributing to spatial and temporal variations in air-sea CO₂ fluxes but previous OBMs do not integrate observations to improve model accuracy and have not be able to operate on the seasonal and multi-decadal timescales needed to adequately characterize these processes. The ECCO-Darwin model is an OBM that assimilates Estimating the Circulation and Climate of the Ocean (ECCO) consortium ocean circulation estimates and biogeochemical processes from the Massachusetts Institute of Technology (MIT) Darwin Project. A pilot study using ECCO-Darwin was completed by Brix et al. (2015) however an improved version of the model was developed by Carroll et al. (2020) in which issues present in the first model were addressed using data assimilation and adjustments were made to initial conditions and biogeochemical parameters. The updated ECCO-Darwin model was compared with interpolation-based products to estimate surface ocean partial pressure (pCO2) and air-sea CO₂ flux. This dataset contains the gridded global, monthly mean air-sea CO₂ fluxes from version 5 of the ECCO-Darwin model. The data are available at ~1/3° horizontal resolution at the equator (~18 km at high latitudes) from January 2020 through December 2022.",
    "crumbs": [
      "Data Usage Notebooks",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "Air-Sea CO₂ Flux, ECCO-Darwin Model v5"
    ]
  },
  {
    "objectID": "user_data_notebooks/eccodarwin-co2flux-monthgrid-v5_User_Notebook.html#installing-the-required-libraries",
    "href": "user_data_notebooks/eccodarwin-co2flux-monthgrid-v5_User_Notebook.html#installing-the-required-libraries",
    "title": "Air-Sea CO₂ Flux, ECCO-Darwin Model v5",
    "section": "Installing the required libraries",
    "text": "Installing the required libraries\nPlease run the cell below to install the libraries required to run this notebook.\n\n%pip install requests\n%pip install folium\n%pip install pystac_client",
    "crumbs": [
      "Data Usage Notebooks",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "Air-Sea CO₂ Flux, ECCO-Darwin Model v5"
    ]
  },
  {
    "objectID": "user_data_notebooks/eccodarwin-co2flux-monthgrid-v5_User_Notebook.html#querying-the-stac-api",
    "href": "user_data_notebooks/eccodarwin-co2flux-monthgrid-v5_User_Notebook.html#querying-the-stac-api",
    "title": "Air-Sea CO₂ Flux, ECCO-Darwin Model v5",
    "section": "Querying the STAC API",
    "text": "Querying the STAC API\n\nimport requests\nfrom folium import Map, TileLayer\nfrom pystac_client import Client\n\n\n# Provide STAC and RASTER API endpoints\nSTAC_API_URL = \"http://ghg.center/api/stac\"\nRASTER_API_URL = \"https://ghg.center/api/raster\"\n\n# Please use the collection name similar to the one used in STAC collection.\n# Name of the collection for Ecco Darwin CO₂ flux dataset. \ncollection_name = \"eccodarwin-co2flux-monthgrid-v5\"\n\n\n# Fetching the collection from STAC collections using appropriate endpoint.\ncollection = requests.get(f\"{STAC_API_URL}/collections/{collection_name}\").json()\ncollection\n\nExamining the contents of our collection under the temporal variable, we see that the data is available from January 2020 to December 2022. By looking at the dashboard:time density, we observe that the data is periodic with monthly time density.\n\ndef get_item_count(collection_id):\n    count = 0\n    items_url = f\"{STAC_API_URL}/collections/{collection_id}/items\"\n\n    while True:\n        response = requests.get(items_url)\n\n        if not response.ok:\n            print(\"error getting items\")\n            exit()\n\n        stac = response.json()\n        count += int(stac[\"context\"].get(\"returned\", 0))\n        next = [link for link in stac[\"links\"] if link[\"rel\"] == \"next\"]\n\n        if not next:\n            break\n        items_url = next[0][\"href\"]\n\n    return count\n\n\n# Check total number of items available\nnumber_of_items = get_item_count(collection_name)\nitems = requests.get(f\"{STAC_API_URL}/collections/{collection_name}/items?limit={number_of_items}\").json()[\"features\"]\nprint(f\"Found {len(items)} items\")\n\n\n# Examining the first item in the collection\nitems[0]\n\nBelow, we are entering the minimum and maximum values to provide our upper and lower bounds in rescale_values.",
    "crumbs": [
      "Data Usage Notebooks",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "Air-Sea CO₂ Flux, ECCO-Darwin Model v5"
    ]
  },
  {
    "objectID": "user_data_notebooks/eccodarwin-co2flux-monthgrid-v5_User_Notebook.html#exploring-changes-in-co₂-levels-using-the-raster-api",
    "href": "user_data_notebooks/eccodarwin-co2flux-monthgrid-v5_User_Notebook.html#exploring-changes-in-co₂-levels-using-the-raster-api",
    "title": "Air-Sea CO₂ Flux, ECCO-Darwin Model v5",
    "section": "Exploring Changes in CO₂ Levels Using the Raster API",
    "text": "Exploring Changes in CO₂ Levels Using the Raster API\nIn this notebook, we will explore the global changes of CO₂ flux over time in urban regions. We will visualize the outputs on a map using folium.\n\n# to access the year value from each item more easily, this will let us query more explicity by year and month (e.g., 2020-02)\nitems = {item[\"properties\"][\"start_datetime\"]: item for item in items} \nasset_name = \"co2\"\n\n\n# Fetching the min and max values for a specific item\nrescale_values = {\"max\":0.05544506255821962, \"min\":-0.0560546997598733}\n\nNow, we will pass the item id, collection name, and rescaling_factor to the Raster API endpoint. We will do this twice so that we can visualize each event independently.\n\ncolor_map = \"magma\"\nco2_flux_1 = requests.get(\n    f\"{RASTER_API_URL}/stac/tilejson.json?collection={items[list(items.keys())[0]]['collection']}&item={items[list(items.keys())[0]]['id']}\"\n    f\"&assets={asset_name}\"\n    f\"&color_formula=gamma+r+1.05&colormap_name={color_map}\"\n    f\"&rescale={rescale_values['min']},{rescale_values['max']}\", \n).json()\nco2_flux_1\n\n\nco2_flux_2 = requests.get(\n    f\"{RASTER_API_URL}/stac/tilejson.json?collection={items[list(items.keys())[20]]['collection']}&item={items[list(items.keys())[20]]['id']}\"\n    f\"&assets={asset_name}\"\n    f\"&color_formula=gamma+r+1.05&colormap_name={color_map}\"\n    f\"&rescale={rescale_values['min']},{rescale_values['max']}\", \n).json()\nco2_flux_2",
    "crumbs": [
      "Data Usage Notebooks",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "Air-Sea CO₂ Flux, ECCO-Darwin Model v5"
    ]
  },
  {
    "objectID": "user_data_notebooks/eccodarwin-co2flux-monthgrid-v5_User_Notebook.html#visualizing-co₂-flux-emissions",
    "href": "user_data_notebooks/eccodarwin-co2flux-monthgrid-v5_User_Notebook.html#visualizing-co₂-flux-emissions",
    "title": "Air-Sea CO₂ Flux, ECCO-Darwin Model v5",
    "section": "Visualizing CO₂ flux Emissions",
    "text": "Visualizing CO₂ flux Emissions\n\n# We'll import folium to map and folium.plugins to allow mapping side-by-side\nimport folium\nimport folium.plugins\n\n# Set initial zoom and center of map for CO₂ Layer\n# Centre of map [latitude,longitude]\nmap_ = folium.plugins.DualMap(location=(34, -118), zoom_start=6)\n\n\nmap_layer_1 = TileLayer(\n    tiles=co2_flux_1[\"tiles\"][0],\n    attr=\"GHG\",\n    opacity=0.8,\n)\nmap_layer_1.add_to(map_.m1)\n\nmap_layer_2 = TileLayer(\n    tiles=co2_flux_2[\"tiles\"][0],\n    attr=\"GHG\",\n    opacity=0.8,\n)\nmap_layer_2.add_to(map_.m2)\n\n# visualising the map\nmap_",
    "crumbs": [
      "Data Usage Notebooks",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "Air-Sea CO₂ Flux, ECCO-Darwin Model v5"
    ]
  },
  {
    "objectID": "user_data_notebooks/eccodarwin-co2flux-monthgrid-v5_User_Notebook.html#visualizing-the-data-as-a-time-series",
    "href": "user_data_notebooks/eccodarwin-co2flux-monthgrid-v5_User_Notebook.html#visualizing-the-data-as-a-time-series",
    "title": "Air-Sea CO₂ Flux, ECCO-Darwin Model v5",
    "section": "Visualizing the Data as a Time Series",
    "text": "Visualizing the Data as a Time Series\nWe can now explore the fossil fuel emission time series (January 2020 -December 2022) available for the Dallas, Texas area of the U.S. We can plot the data set using the code below:\n\nimport matplotlib.pyplot as plt\n\nfig = plt.figure(figsize=(20, 10))\n\n\nplt.plot(\n    df[\"datetime\"],\n    df[\"max\"],\n    color=\"red\",\n    linestyle=\"-\",\n    linewidth=0.5,\n    label=\"CO2 emissions\",\n)\n\nplt.legend()\nplt.xlabel(\"Years\")\nplt.ylabel(\"CO2 emissions mmol m²/s\")\nplt.title(\"CO2 emission Values for Gulf of Mexico (2020-2022)\")\n\n\nprint(items[2][\"properties\"][\"start_datetime\"])\n\n\nco2_flux_3 = requests.get(\n    f\"{RASTER_API_URL}/stac/tilejson.json?collection={items[2]['collection']}&item={items[2]['id']}\"\n    f\"&assets={asset_name}\"\n    f\"&color_formula=gamma+r+1.05&colormap_name={color_map}\"\n    f\"&rescale={rescale_values['min']},{rescale_values['max']}\",\n).json()\nco2_flux_3\n\n\n# Use bbox initial zoom and map\n# Set up a map located w/in event bounds\nimport folium\n\naoi_map_bbox = Map(\n    tiles=\"OpenStreetMap\",\n    location=[\n        30,-100\n    ],\n    zoom_start=6.8,\n)\n\nmap_layer = TileLayer(\n    tiles=co2_flux_3[\"tiles\"][0],\n    attr=\"GHG\", opacity = 0.7\n)\n\nmap_layer.add_to(aoi_map_bbox)\n\naoi_map_bbox",
    "crumbs": [
      "Data Usage Notebooks",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "Air-Sea CO₂ Flux, ECCO-Darwin Model v5"
    ]
  },
  {
    "objectID": "user_data_notebooks/eccodarwin-co2flux-monthgrid-v5_User_Notebook.html#summary",
    "href": "user_data_notebooks/eccodarwin-co2flux-monthgrid-v5_User_Notebook.html#summary",
    "title": "Air-Sea CO₂ Flux, ECCO-Darwin Model v5",
    "section": "Summary",
    "text": "Summary\nIn this notebook we have successfully explored, analyzed, and visualized the STAC collection for ECCO Darwin CO₂ flux dataset",
    "crumbs": [
      "Data Usage Notebooks",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "Air-Sea CO₂ Flux, ECCO-Darwin Model v5"
    ]
  },
  {
    "objectID": "data_workflow/odiac-ffco2-monthgrid-v2022_Data_Flow.html",
    "href": "data_workflow/odiac-ffco2-monthgrid-v2022_Data_Flow.html",
    "title": "ODIAC Fossil Fuel CO₂ Emissions",
    "section": "",
    "text": "ODIAC Fossil Fuel CO₂ Emissions\n\n\n\nData Flow Diagram Extending From Acquisition/Creation to User Delivery\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Data Workflow Documentation",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "ODIAC Fossil Fuel CO₂ Emissions"
    ]
  },
  {
    "objectID": "data_workflow/eccodarwin-co2flux-monthgrid-v5_Data_Flow.html",
    "href": "data_workflow/eccodarwin-co2flux-monthgrid-v5_Data_Flow.html",
    "title": "Air-Sea CO₂ Flux, ECCO-Darwin Model v5",
    "section": "",
    "text": "Air-Sea CO₂ Flux, ECCO-Darwin Model v5\n\n\n\nData Flow Diagram Extending From Acquisition/Creation to User Delivery\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Data Workflow Documentation",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "Air-Sea CO₂ Flux, ECCO-Darwin Model v5"
    ]
  },
  {
    "objectID": "data_workflow/oco2geos-co2-daygrid-v10r_Data_Flow.html",
    "href": "data_workflow/oco2geos-co2-daygrid-v10r_Data_Flow.html",
    "title": "OCO-2 GEOS Column CO₂ Concentrations",
    "section": "",
    "text": "OCO-2 GEOS Column CO₂ Concentrations\n\n\n\nData Flow Diagram Extending From Acquisition/Creation to User Delivery\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Data Workflow Documentation",
      "Greenhouse Gas Concentrations",
      "OCO-2 GEOS Column CO₂ Concentrations"
    ]
  },
  {
    "objectID": "data_workflow/oco2-mip-co2budget-yeargrid-v1_Data_Flow.html",
    "href": "data_workflow/oco2-mip-co2budget-yeargrid-v1_Data_Flow.html",
    "title": "OCO-2 MIP Top-Down CO₂ Budgets",
    "section": "",
    "text": "OCO-2 MIP Top-Down CO₂ Budgets\n\n\n\nData Flow Diagram Extending From Acquisition/Creation to User Delivery\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Data Workflow Documentation",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "OCO-2 MIP Top-Down CO₂ Budgets"
    ]
  },
  {
    "objectID": "data_workflow/casagfed-carbonflux-monthgrid-v3_Data_Flow.html",
    "href": "data_workflow/casagfed-carbonflux-monthgrid-v3_Data_Flow.html",
    "title": "CASA-GFED3 Land Carbon Flux - Data Workflow",
    "section": "",
    "text": "CASA-GFED3 Land Carbon Flux - Data Workflow\n\n\n\nData Flow Diagram Extending From Acquisition/Creation to User Delivery\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Data Workflow Documentation",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "CASA-GFED3 Land Carbon Flux - Data Workflow"
    ]
  },
  {
    "objectID": "data_workflow/sedac-popdensity-yeargrid5yr-v4.11_Data_Flow.html",
    "href": "data_workflow/sedac-popdensity-yeargrid5yr-v4.11_Data_Flow.html",
    "title": "SEDAC Gridded World Population Data",
    "section": "",
    "text": "SEDAC Gridded World Population Data\n\n\n\nData Flow Diagram Extending From Acquisition/Creation to User Delivery\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Data Workflow Documentation",
      "Socioeconomic",
      "SEDAC Gridded World Population Data"
    ]
  },
  {
    "objectID": "data_workflow/emit-ch4plume-v1_Data_Flow.html",
    "href": "data_workflow/emit-ch4plume-v1_Data_Flow.html",
    "title": "EMIT methane point source plume complexes",
    "section": "",
    "text": "EMIT methane point source plume complexes\n\n\n\nData Flow Diagram Extending From Acquisition/Creation to User Delivery\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Data Workflow Documentation",
      "Large Emissions Events",
      "EMIT methane point source plume complexes"
    ]
  },
  {
    "objectID": "data_workflow/gosat-based-ch4budget-yeargrid-v1_Data_Flow.html",
    "href": "data_workflow/gosat-based-ch4budget-yeargrid-v1_Data_Flow.html",
    "title": "GOSAT-based Top-down Total and Natural Methane Emissions",
    "section": "",
    "text": "GOSAT-based Top-down Total and Natural Methane Emissions\n\n\n\nData Flow Diagram Extending From Acquisition/Creation to User Delivery\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Data Workflow Documentation",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "GOSAT-based Top-down Total and Natural Methane Emissions"
    ]
  },
  {
    "objectID": "data_workflow/epa-ch4emission-grid-v2express_Data_Flow.html",
    "href": "data_workflow/epa-ch4emission-grid-v2express_Data_Flow.html",
    "title": "Gridded Anthropogenic Methane Emissions Inventory",
    "section": "",
    "text": "Gridded Anthropogenic Methane Emissions Inventory\n\n\n\nData Flow Diagram Extending From Acquisition/Creation to User Delivery\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Data Workflow Documentation",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "Gridded Anthropogenic Methane Emissions Inventory"
    ]
  },
  {
    "objectID": "data_workflow/tm54dvar-ch4flux-monthgrid-v1_Data_Flow.html",
    "href": "data_workflow/tm54dvar-ch4flux-monthgrid-v1_Data_Flow.html",
    "title": "TM5-4DVar Isotopic CH₄ Inverse Fluxes",
    "section": "",
    "text": "TM5-4DVar Isotopic CH₄ Inverse Fluxes\n\n\n\nData Flow Diagram Extending From Acquisition/Creation to User Delivery\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Data Workflow Documentation",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "TM5-4DVar Isotopic CH₄ Inverse Fluxes"
    ]
  },
  {
    "objectID": "data_workflow/lpjwsl-wetlandch4-grid-v1_Data_Flow.html",
    "href": "data_workflow/lpjwsl-wetlandch4-grid-v1_Data_Flow.html",
    "title": "Wetland Methane Emissions, LPJ-wsl Model",
    "section": "",
    "text": "Wetland Methane Emissions, LPJ-wsl Model\n\n\n\nData Flow Diagram Extending From Acquisition/Creation to User Delivery\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Data Workflow Documentation",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "Wetland Methane Emissions, LPJ-wsl Model"
    ]
  },
  {
    "objectID": "data_workflow/noaa-insitu_Data_Flow.html",
    "href": "data_workflow/noaa-insitu_Data_Flow.html",
    "title": "Atmospheric Carbon Dioxide Concentrations from the NOAA Global Monitoring Laboratory",
    "section": "",
    "text": "Atmospheric Carbon Dioxide Concentrations from the NOAA Global Monitoring Laboratory\n\n\n\nData Flow Diagram Extending From Acquisition/Creation to User Delivery\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Data Workflow Documentation",
      "Greenhouse Gas Concentrations",
      "Atmospheric Carbon Dioxide Concentrations from the NOAA Global Monitoring Laboratory"
    ]
  },
  {
    "objectID": "datatransformation.html",
    "href": "datatransformation.html",
    "title": "U.S. Greenhouse Gas Center: Data Transformation Notebooks",
    "section": "",
    "text": "Welcome to the U.S. Greenhouse Gas (GHG) Center data transformation notebooks, where we harness the power of Cloud Optimized Geotiffs (COGs) to offer a dynamic, cloud-based platform for exploring and analyzing greenhouse gas datasets. Dive into our curated collection of GHG datasets, all optimized for seamless accessibility and analysis.\nDiscover the journey of each dataset from its original format to COGs through the below Jupyter notebooks. The transformation examples are grouped topically. The dataset product type (model output, satellite observation, etc.) is noted next to the notebook name. Click on a notebook to learn more about the dataset and to view the transformation code.\nJoin us in our mission to make data-driven environmental solutions. Explore, analyze, and make a difference with the US GHG Center.\nView the US GHG Center Data Catalog",
    "crumbs": [
      "Data Transformation Notebooks"
    ]
  },
  {
    "objectID": "datatransformation.html#gridded-anthropogenic-greenhouse-gas-emissions",
    "href": "datatransformation.html#gridded-anthropogenic-greenhouse-gas-emissions",
    "title": "U.S. Greenhouse Gas Center: Data Transformation Notebooks",
    "section": "Gridded Anthropogenic Greenhouse Gas Emissions",
    "text": "Gridded Anthropogenic Greenhouse Gas Emissions\n\nOCO-2 MIP Top-Down CO₂ Budgets - Model Output\nODIAC Fossil Fuel CO₂ Emissions - Model Output\nTM5-4DVar Isotopic CH₄ Inverse Fluxes - Model Output, Ground Measurements\nU.S. Gridded Anthropogenic Methane Emissions Inventory - Gridded Inventory",
    "crumbs": [
      "Data Transformation Notebooks"
    ]
  },
  {
    "objectID": "datatransformation.html#natural-greenhouse-gas-emissions-and-sinks",
    "href": "datatransformation.html#natural-greenhouse-gas-emissions-and-sinks",
    "title": "U.S. Greenhouse Gas Center: Data Transformation Notebooks",
    "section": "Natural Greenhouse Gas Emissions and Sinks",
    "text": "Natural Greenhouse Gas Emissions and Sinks\n\nAir-Sea CO₂ Flux, ECCO-Darwin Model v5 - Model Output\nCASA-GFED3 Land Carbon Flux - Model Output\nGOSAT-based Top-down Total and Natural Methane Emissions - Model Output\nOCO-2 MIP Top-Down CO₂ Budgets - Model Output\nTM5-4DVar Isotopic CH₄ Inverse Fluxes - Model Output, Ground Measurements",
    "crumbs": [
      "Data Transformation Notebooks"
    ]
  },
  {
    "objectID": "datatransformation.html#large-emissions-events",
    "href": "datatransformation.html#large-emissions-events",
    "title": "U.S. Greenhouse Gas Center: Data Transformation Notebooks",
    "section": "Large Emissions Events",
    "text": "Large Emissions Events\n\nEMIT Methane Point Source Plume Complexes - Satellite Observations",
    "crumbs": [
      "Data Transformation Notebooks"
    ]
  },
  {
    "objectID": "datatransformation.html#greenhouse-gas-concentrations",
    "href": "datatransformation.html#greenhouse-gas-concentrations",
    "title": "U.S. Greenhouse Gas Center: Data Transformation Notebooks",
    "section": "Greenhouse Gas Concentrations",
    "text": "Greenhouse Gas Concentrations\n\nOCO-2 GEOS Column CO₂ Concentrations - Model Output, Satellite Observations",
    "crumbs": [
      "Data Transformation Notebooks"
    ]
  },
  {
    "objectID": "datatransformation.html#socioeconomic",
    "href": "datatransformation.html#socioeconomic",
    "title": "U.S. Greenhouse Gas Center: Data Transformation Notebooks",
    "section": "Socioeconomic",
    "text": "Socioeconomic\n\nSEDAC Gridded World Population Density - Model Output",
    "crumbs": [
      "Data Transformation Notebooks"
    ]
  },
  {
    "objectID": "datatransformation.html#contact",
    "href": "datatransformation.html#contact",
    "title": "U.S. Greenhouse Gas Center: Data Transformation Notebooks",
    "section": "Contact",
    "text": "Contact\nFor technical help or general questions, please contact the support team using the feedback form.",
    "crumbs": [
      "Data Transformation Notebooks"
    ]
  },
  {
    "objectID": "user_data_notebooks/epa-ch4emission-grid-v2express_User_Notebook.html",
    "href": "user_data_notebooks/epa-ch4emission-grid-v2express_User_Notebook.html",
    "title": "U.S. Gridded Anthropogenic Methane Emissions Inventory",
    "section": "",
    "text": "Identify available dates and temporal frequency of observations for the given collection using the GHGC API /stac endpoint. The collection processed in this notebook is the gridded methane emissions data product.\nPass the STAC item into the raster API /stac/tilejson.jsonendpoint.\nUsing folium.plugins.DualMap, we will visualize two tiles (side-by-side), allowing us to compare time points.\nAfter the visualization, we will perform zonal statistics for a given polygon.",
    "crumbs": [
      "Data Usage Notebooks",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "U.S. Gridded Anthropogenic Methane Emissions Inventory"
    ]
  },
  {
    "objectID": "user_data_notebooks/epa-ch4emission-grid-v2express_User_Notebook.html#approach",
    "href": "user_data_notebooks/epa-ch4emission-grid-v2express_User_Notebook.html#approach",
    "title": "U.S. Gridded Anthropogenic Methane Emissions Inventory",
    "section": "",
    "text": "Identify available dates and temporal frequency of observations for the given collection using the GHGC API /stac endpoint. The collection processed in this notebook is the gridded methane emissions data product.\nPass the STAC item into the raster API /stac/tilejson.jsonendpoint.\nUsing folium.plugins.DualMap, we will visualize two tiles (side-by-side), allowing us to compare time points.\nAfter the visualization, we will perform zonal statistics for a given polygon.",
    "crumbs": [
      "Data Usage Notebooks",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "U.S. Gridded Anthropogenic Methane Emissions Inventory"
    ]
  },
  {
    "objectID": "user_data_notebooks/epa-ch4emission-grid-v2express_User_Notebook.html#about-the-data",
    "href": "user_data_notebooks/epa-ch4emission-grid-v2express_User_Notebook.html#about-the-data",
    "title": "U.S. Gridded Anthropogenic Methane Emissions Inventory",
    "section": "About the Data",
    "text": "About the Data\nThe gridded EPA U.S. anthropogenic methane greenhouse gas inventory (gridded GHGI) includes spatially disaggregated (0.1 deg x 0.1 deg or approximately 10 x 10 km resolution) maps of annual anthropogenic methane emissions (for the contiguous United States (CONUS), consistent with national annual U.S. anthropogenic methane emissions reported in the U.S. EPA Inventory of U.S. Greenhouse Gas Emissions and Sinks (U.S. GHGI). This V2 Express Extension dataset contains methane emissions provided as fluxes, in units of molecules of methane per square cm per second, for over 25 individual emission source categories, including those from agriculture, petroleum and natural gas systems, coal mining, and waste. The data have been converted from their original NetCDF format to Cloud-Optimized GeoTIFF (COG) for use in the US GHG Center, thereby enabling user exploration of spatial anthropogenic methane emissions and their trends.",
    "crumbs": [
      "Data Usage Notebooks",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "U.S. Gridded Anthropogenic Methane Emissions Inventory"
    ]
  },
  {
    "objectID": "user_data_notebooks/epa-ch4emission-grid-v2express_User_Notebook.html#querying-the-stac-api",
    "href": "user_data_notebooks/epa-ch4emission-grid-v2express_User_Notebook.html#querying-the-stac-api",
    "title": "U.S. Gridded Anthropogenic Methane Emissions Inventory",
    "section": "Querying the STAC API",
    "text": "Querying the STAC API\n\nimport requests\nfrom folium import Map, TileLayer\nfrom pystac_client import Client\n\n\n# Provide STAC and RASTER API endpoints\nSTAC_API_URL = \"http://ghg.center/api/stac\"\nRASTER_API_URL = \"https://ghg.center/api/raster\"\n\n# Please use the collection name similar to the one used in STAC collection.\n\n# Name of the collection for gridded methane dataset. \ncollection_name = \"epa-ch4emission-yeargrid-v2\"\n\n\n# Fetching the collection from STAC collections using appropriate endpoint.\ncollection = requests.get(f\"{STAC_API_URL}/collections/{collection_name}\").json()\ncollection\n\nExamining the contents of our collection under the temporal variable, we see that the data is available from January 2012 to December 2020. By looking at the dashboard:time density, we observe that the periodic frequency of these observations is yearly.\n\ndef get_item_count(collection_id):\n    count = 0\n    items_url = f\"{STAC_API_URL}/collections/{collection_id}/items\"\n\n    while True:\n        response = requests.get(items_url)\n\n        if not response.ok:\n            print(\"error getting items\")\n            exit()\n\n        stac = response.json()\n        count += int(stac[\"context\"].get(\"returned\", 0))\n        next = [link for link in stac[\"links\"] if link[\"rel\"] == \"next\"]\n\n        if not next:\n            break\n        items_url = next[0][\"href\"]\n\n    return count\n\n\n# Check total number of items available\nnumber_of_items = get_item_count(collection_name)\nitems = requests.get(f\"{STAC_API_URL}/collections/{collection_name}/items?limit={number_of_items}\").json()[\"features\"]\nprint(f\"Found {len(items)} items\")\n\n\n# Examining the first item in the collection\nitems[0]\n\nThis makes sense as there are 9 years between 2012 - 2020, meaning 9 records in total.\nBelow, we enter minimum and maximum values to provide our upper and lower bounds in rescale_values.",
    "crumbs": [
      "Data Usage Notebooks",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "U.S. Gridded Anthropogenic Methane Emissions Inventory"
    ]
  },
  {
    "objectID": "user_data_notebooks/epa-ch4emission-grid-v2express_User_Notebook.html#exploring-changes-in-methane-ch4-levels-using-the-raster-api",
    "href": "user_data_notebooks/epa-ch4emission-grid-v2express_User_Notebook.html#exploring-changes-in-methane-ch4-levels-using-the-raster-api",
    "title": "U.S. Gridded Anthropogenic Methane Emissions Inventory",
    "section": "Exploring Changes in Methane (CH4) Levels Using the Raster API",
    "text": "Exploring Changes in Methane (CH4) Levels Using the Raster API\nIn this notebook, we will explore the impacts of methane emissions and by examining changes over time in urban regions. We will visualize the outputs on a map using folium.\n\n# To access the year value from each item more easily, this will let us query more explicity by year and month (e.g., 2020-02)\nitems = {item[\"properties\"][\"datetime\"][:7]: item for item in items} \nasset_name = \"surface-coal\"\n\n\n# Fetching the min and max values for a specific item\nrescale_values = {\"max\":items[list(items.keys())[0]][\"assets\"][asset_name][\"raster:bands\"][0][\"histogram\"][\"max\"], \"min\":items[list(items.keys())[0]][\"assets\"][asset_name][\"raster:bands\"][0][\"histogram\"][\"min\"]}\n\n\nitems\n\nNow, we will pass the item id, collection name, and rescaling_factor to the Raster API endpoint. We will do this twice, once for January 2018 and again for January 2012, so that we can visualize each event independently.\n\ncolor_map = \"rainbow\" # please select the color ramp from matplotlib library.\njanuary_2018_tile = requests.get(\n    f\"{RASTER_API_URL}/stac/tilejson.json?collection={items['2018-01']['collection']}&item={items['2018-01']['id']}\"\n    f\"&assets={asset_name}\"\n    f\"&color_formula=gamma+r+1.05&colormap_name={color_map}\"\n    f\"&rescale={rescale_values['min']},{rescale_values['max']}\", \n).json()\njanuary_2018_tile\n\n\njanuary_2012_tile = requests.get(\n    f\"{RASTER_API_URL}/stac/tilejson.json?collection={items['2012-01']['collection']}&item={items['2012-01']['id']}\"\n    f\"&assets={asset_name}\"\n    f\"&color_formula=gamma+r+1.05&colormap_name={color_map}\"\n    f\"&rescale={rescale_values['min']},{rescale_values['max']}\", \n).json()\njanuary_2012_tile",
    "crumbs": [
      "Data Usage Notebooks",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "U.S. Gridded Anthropogenic Methane Emissions Inventory"
    ]
  },
  {
    "objectID": "user_data_notebooks/epa-ch4emission-grid-v2express_User_Notebook.html#visualizing-ch₄-emissions",
    "href": "user_data_notebooks/epa-ch4emission-grid-v2express_User_Notebook.html#visualizing-ch₄-emissions",
    "title": "U.S. Gridded Anthropogenic Methane Emissions Inventory",
    "section": "Visualizing CH₄ emissions",
    "text": "Visualizing CH₄ emissions\n\n# We will import folium to map and folium.plugins to allow side-by-side mapping\nimport folium\nimport folium.plugins\n\n# Set initial zoom and center of map for CH₄ Layer\n# Centre of map [latitude,longitude]\nmap_ = folium.plugins.DualMap(location=(34, -118), zoom_start=6)\n\n# January 2018\nmap_layer_2018 = TileLayer(\n    tiles=january_2018_tile[\"tiles\"][0],\n    attr=\"GHG\",\n    opacity=0.7,\n)\nmap_layer_2018.add_to(map_.m1)\n\n# January 2012\nmap_layer_2012 = TileLayer(\n    tiles=january_2012_tile[\"tiles\"][0],\n    attr=\"GHG\",\n    opacity=0.7,\n)\nmap_layer_2012.add_to(map_.m2)\n\n# visualising the map\nmap_",
    "crumbs": [
      "Data Usage Notebooks",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "U.S. Gridded Anthropogenic Methane Emissions Inventory"
    ]
  },
  {
    "objectID": "user_data_notebooks/epa-ch4emission-grid-v2express_User_Notebook.html#visualizing-the-data-as-a-time-series",
    "href": "user_data_notebooks/epa-ch4emission-grid-v2express_User_Notebook.html#visualizing-the-data-as-a-time-series",
    "title": "U.S. Gridded Anthropogenic Methane Emissions Inventory",
    "section": "Visualizing the Data as a Time Series",
    "text": "Visualizing the Data as a Time Series\nWe can now explore the gridded methane emission (Domestic Wastewater Treatment & Discharge (5D)) time series (January 2000 -December 2021) available for the Dallas, Texas area of the U.S. We can plot the data set using the code below:\n\nimport matplotlib.pyplot as plt\n\nfig = plt.figure(figsize=(20, 10))\n\n\nplt.plot(\n    df[\"date\"],\n    df[\"max\"],\n    color=\"red\",\n    linestyle=\"-\",\n    linewidth=0.5,\n    label=\"Max monthly CO₂ emissions\",\n)\n\nplt.legend()\nplt.xlabel(\"Years\")\nplt.ylabel(\"CH4 emissions Molecules CH₄/cm²/s\")\nplt.title(\"CH4 gridded methane emission from Domestic Wastewater Treatment & Discharge (5D) for Texas, Dallas (2012-202)\")\n\n\nprint(items[2][\"properties\"][\"datetime\"])\n\n\ntile_2016 = requests.get(\n    f\"{RASTER_API_URL}/stac/tilejson.json?collection={items[2]['collection']}&item={items[2]['id']}\"\n    f\"&assets={asset_name}\"\n    f\"&color_formula=gamma+r+1.05&colormap_name={color_map}\"\n    f\"&rescale={rescale_values['min']},{rescale_values['max']}\",\n).json()\ntile_2016\n\n\n# Use bbox initial zoom and map\n# Set up a map located w/in event bounds\nimport folium\n\naoi_map_bbox = Map(\n    tiles=\"OpenStreetMap\",\n    location=[\n        30,-100\n    ],\n    zoom_start=8,\n)\n\nmap_layer = TileLayer(\n    tiles=tile_2016[\"tiles\"][0],\n    attr=\"GHG\", opacity = 0.5\n)\n\nmap_layer.add_to(aoi_map_bbox)\n\naoi_map_bbox",
    "crumbs": [
      "Data Usage Notebooks",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "U.S. Gridded Anthropogenic Methane Emissions Inventory"
    ]
  },
  {
    "objectID": "user_data_notebooks/epa-ch4emission-grid-v2express_User_Notebook.html#summary",
    "href": "user_data_notebooks/epa-ch4emission-grid-v2express_User_Notebook.html#summary",
    "title": "U.S. Gridded Anthropogenic Methane Emissions Inventory",
    "section": "Summary",
    "text": "Summary\nIn this notebook we have successfully explored, analyzed, and visualized the STAC collection for gridded methane emissions.",
    "crumbs": [
      "Data Usage Notebooks",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "U.S. Gridded Anthropogenic Methane Emissions Inventory"
    ]
  },
  {
    "objectID": "user_data_notebooks/oco2-mip-National-co2budget.html",
    "href": "user_data_notebooks/oco2-mip-National-co2budget.html",
    "title": "OCO-2 MIP National Top-Down CO2 Budgets",
    "section": "",
    "text": "Read in National CO2 Budgets using Pandas\nSub-select the data structure using Pandas\nVisualize the CO2 budgets for a country\nInvestigate uncertainties and metrics for understanding the dataset\n\n\n\n\nThis tutorial shows how to explore and plot the dataset reported in:\nByrne et al.: National CO2 budgets (2015–2020) inferred from atmospheric CO2 observations in support of the global stocktake, Earth Syst. Sci. Data, 15, 963–1004, https://doi.org/10.5194/essd-15-963-2023, 2023.\n\n\n\nFirst we will need to import the relevant python modules:\n\nimport pandas as pd # for manipulating csv dataset\nimport numpy as np\nimport matplotlib.pyplot as plt # make plots\nfrom scipy.stats import norm # We will use this for understanding significance\n\n\n\n\nNow we will read in the csv dataset from https://ceos.org/gst/carbon-dioxide.html\n\nurl ='https://ceos.org/gst/files/pilot_topdown_CO2_Budget_countries_v1.csv'\ndf_all = pd.read_csv(url, skiprows=52)\n\n\n\n\nTo simplify the analysis, let’s subselect the results for a single experiment. The experiments are: - IS: estimates fluxes from in situ CO2 measurements - LNLG: estimates fluxes from OCO-2 land CO2 data - LNLGIS: combines in situ and OCO-2 land CO2 data - LNLGOGIS: combines in situ and OCO-2 land and ocean CO2 data\nWe would like to use the experiment that uses the most high-quality CO2 data. There are some concerns about small residual biases in OCO-2 ocean data (Byrne et al., 2023), so let’s use the LNLGIS experiment.\n\n# Choose one experiment from the list ['IS', 'LNLG', 'LNLGIS', 'LNLGOGIS']\nexperiment = 'LNLGIS'\n\n# Subset of columns for a given experiment\nif experiment == 'IS':\n    df = df_all.drop(df_all.columns[[4,5,6,7,8,9,12,13,14,15,16,17,20,21,22,23,24,25,34,35,36]], axis=1)\nif experiment == 'LNLG':\n    df = df_all.drop(df_all.columns[[2,3,6,7,8,9,10,11,14,15,16,17,18,19,22,23,24,25,33,35,36]], axis=1)\nif experiment == 'LNLGIS':\n    df = df_all.drop(df_all.columns[[2,3,4,5,8,9,10,11,12,13,16,17,18,19,20,21,24,25,33,34,36]], axis=1)\nif experiment == 'LNLGOGIS':\n    df = df_all.drop(df_all.columns[[2,3,4,5,6,7,10,11,12,13,14,15,18,19,20,21,22,23,33,34,35]], axis=1)\n\n# We can now look at the colums of data\ndf.head()\n\n\n\n\n\n\n\n\n\nAlpha 3 Code\nYear\nLNLGIS dC_loss (TgCO2)\nLNLGIS dC_loss unc (TgCO2)\nLNLGIS NBE (TgCO2)\nLNLGIS NBE unc (TgCO2)\nLNLGIS NCE (TgCO2)\nLNLGIS NCE unc (TgCO2)\nRivers (TgCO2)\nRiver unc (TgCO2)\nWood+Crop (TgCO2)\nWood+Crop unc (TgCO2)\nFF (TgCO2)\nFF unc (TgCO2)\nZ-statistic\nFUR LNLGIS\n\n\n\n\n0\nAFG\n2015\n39.3407\n153.746\n40.9643\n153.746\n60.3537\n153.744\n-2.43286\n1.69832\n4.05648\n1.21694\n19.3894\n0.797698\n0.37\n0.19\n\n\n1\nAFG\n2016\n50.6167\n175.454\n52.5114\n175.454\n73.0333\n175.452\n-2.16185\n2.24033\n4.05648\n1.21694\n20.5220\n0.678080\n0.31\n0.19\n\n\n2\nAFG\n2017\n54.5096\n179.794\n56.4726\n179.794\n77.5355\n179.793\n-2.09349\n2.37705\n4.05648\n1.21694\n21.0629\n0.695856\n0.47\n0.19\n\n\n3\nAFG\n2018\n116.4260\n243.057\n118.4610\n243.057\n143.9580\n243.056\n-2.02199\n2.52005\n4.05648\n1.21694\n25.4974\n0.695856\n0.39\n0.19\n\n\n4\nAFG\n2019\n64.0162\n181.516\n66.0388\n181.516\n93.8974\n181.514\n-2.03383\n2.49637\n4.05648\n1.21694\n27.8585\n0.797698\n0.49\n0.19\n\n\n\n\n\n\n\n\n\n\n\nLet’s further filter the dataset to look at a specific country. Choose a country by entering the alpha code in the country_name variable below\n\n# Choose a country\ncountry_name = 'USA' \n\n# We can sub-select the data for the country\ncountry_data = df[df['Alpha 3 Code'] == country_name]\n\n# Now we can look at the data for a specific experiment and country\ncountry_data.head()\n\n\n\n\n\n\n\n\n\nAlpha 3 Code\nYear\nLNLGIS dC_loss (TgCO2)\nLNLGIS dC_loss unc (TgCO2)\nLNLGIS NBE (TgCO2)\nLNLGIS NBE unc (TgCO2)\nLNLGIS NCE (TgCO2)\nLNLGIS NCE unc (TgCO2)\nRivers (TgCO2)\nRiver unc (TgCO2)\nWood+Crop (TgCO2)\nWood+Crop unc (TgCO2)\nFF (TgCO2)\nFF unc (TgCO2)\nZ-statistic\nFUR LNLGIS\n\n\n\n\n1232\nUSA\n2015\n-1031.83\n721.213\n-1346.46\n721.213\n4017.31\n713.897\n-165.430\n71.7453\n-149.196\n-44.7589\n5363.77\n102.4670\n-0.81\n0.91\n\n\n1233\nUSA\n2016\n-1419.92\n399.738\n-1743.80\n399.738\n3529.45\n387.079\n-174.684\n53.2375\n-149.196\n-44.7589\n5273.24\n99.8012\n0.04\n0.91\n\n\n1234\nUSA\n2017\n-1375.12\n1034.010\n-1696.63\n1034.010\n3515.14\n1029.250\n-172.308\n57.9894\n-149.196\n-44.7589\n5211.76\n99.0981\n0.67\n0.91\n\n\n1235\nUSA\n2018\n-1018.89\n784.463\n-1333.83\n784.463\n4036.65\n778.179\n-165.747\n71.1117\n-149.196\n-44.7589\n5370.48\n99.0981\n-0.20\n0.91\n\n\n1236\nUSA\n2019\n-1161.41\n718.054\n-1504.61\n718.054\n3728.95\n710.705\n-194.005\n14.5948\n-149.196\n-44.7589\n5233.56\n102.4670\n-0.38\n0.91\n\n\n\n\n\n\n\n\n#This dataset contains fluxes over a five year period, 2015-2020.\nLet’s look at a plot of the annual net terrestrial carbon stock loss (ΔCloss) for each year.\n\n# Make plot\nfig, ax1 = plt.subplots(1, 1, figsize=(6, 4))\nax1.errorbar(country_data['Year'],country_data[experiment+' dC_loss (TgCO2)'],\n                    yerr=country_data[experiment+' dC_loss unc (TgCO2)'],label=experiment,capsize=10)\nax1.legend(loc='upper right')\nax1.set_ylabel('$\\Delta$C$_\\mathrm{loss}$ (TgCO$_2$ year$^{-1}$)')\nax1.set_xlabel('Year')\nax1.set_title('$\\Delta$C$_\\mathrm{loss}$ for '+country_name)\nymin, ymax = ax1.get_ylim()\nmax_abs_y = max(abs(ymin), abs(ymax))\nax1.set_ylim([-max_abs_y, max_abs_y])\nxmin, xmax = ax1.get_xlim()\nax1.plot([xmin,xmax],[0,0],'k',linewidth=0.5)\nax1.set_xlim([xmin, xmax])\n\n\n\n\n\n\n\n\nNext, we can look at the full carbon budget for a given year.\nThe code below creates a plot similar to Fig 13 of Byrne et al. (2023). Each of the bars on the left side of the dashed vertical line (Fossil fuel emissions, lateral C transport by rivers, lateral C transport in crop and wood products, and the net terrestrial carbon stock loss combined to give the net carbon exchange (net surface-atmosphere CO2 flux) shown on the right.\n\n# Pick a specifc year (or mean year)\nyear='mean'\n\n# Make plot\ncountry_data_mean = country_data[country_data['Year'] == year]\na=country_data_mean['Wood+Crop (TgCO2)']\nb=country_data_mean['Wood+Crop unc (TgCO2)']\nprint(b)\n#\nplt.bar(1, country_data_mean['FF (TgCO2)'], yerr=country_data_mean['FF unc (TgCO2)'], label='FF', alpha=0.5)\nplt.bar(2, country_data_mean['Rivers (TgCO2)'], yerr=country_data_mean['River unc (TgCO2)'], label='Rivers', alpha=0.5)\nplt.bar(3, country_data_mean['Wood+Crop (TgCO2)'], yerr=abs(country_data_mean['Wood+Crop unc (TgCO2)']), label='WoodCrop', alpha=0.5)\nplt.bar(4, country_data_mean[experiment+' dC_loss (TgCO2)'], yerr=country_data_mean['LNLGIS dC_loss unc (TgCO2)'], label='dC', alpha=0.5)\nplt.bar(6, country_data_mean[experiment+' NCE (TgCO2)'], yerr=country_data_mean['LNLGIS NCE unc (TgCO2)'], label='NCE', alpha=0.5)\nax = plt.gca()\nymin, ymax = ax.get_ylim()\nplt.plot([5,5],[ymin,ymax],'k:')\nxmin, xmax = ax.get_xlim()\nplt.plot([xmin,xmax],[0,0],'k',linewidth=0.5)\nplt.xlim([xmin,xmax])\nplt.ylim([ymin,ymax])\n#\nplt.xticks([1,2,3,4,6], ['Fossil\\nFuels','Rivers','Wood+\\nCrops','$\\mathrm{\\Delta C _{loss}}$','NCE'])\nplt.title(country_name+' '+year)\nplt.ylabel('CO$_2$ Flux (TgCO$_2$ year$^{-1}$)')\n\n1238   -44.7589\nName: Wood+Crop unc (TgCO2), dtype: float64\n\n\nText(0, 0.5, 'CO$_2$ Flux (TgCO$_2$ year$^{-1}$)')\n\n\n\n\n\n\n\n\n\nUncertainty is an important consideration when analyzing the flux estimates provided by Byrne et al. (2023).\nEach flux estimate is provided with an error estimate representing the standard deviation, and assuming the errors are well prepresented by a normal distribution. This probability dirtribution provided by this uncertainty can be visualized below. We can further quantify the\n\n\n# Select NCE, NBE or dC_loss\nquantity = 'dC_loss'\n\n# Value for comparison\ncomparison_value = 1000 # TgCO2/year\n\n\nMIP_mean = country_data_mean[experiment+' '+quantity+' (TgCO2)'].item()\nMIP_std = country_data_mean[experiment+' '+quantity+' unc (TgCO2)'].item()\n\n# Perform t-test\nt_value = abs(MIP_mean - comparison_value)/(MIP_std / np.sqrt(11))\ncrtical_value = 2.23 # use p=0.05 significance\nif t_value &gt; crtical_value:\n    ttest = 'statistically different'\nif t_value &lt; crtical_value:\n    ttest = 'not statistically\\ndifferent'\n\n# Make plot\nxbounds = abs(MIP_mean)+MIP_std*4\nif abs(crtical_value) &gt; xbounds:\n    xbounds = abs(crtical_value)\nx_axis = np.arange(-1.*xbounds, xbounds, 1) \nplt.plot(x_axis, norm.pdf(x_axis, MIP_mean, MIP_std)) \nax = plt.gca()\nymin, ymax = ax.get_ylim()\nxmin, xmax = ax.get_xlim()\nplt.plot([0,0],[ymin,ymax*1.2],'k:',linewidth=0.5)\nplt.plot([xmin,xmax],[0,0],'k:',linewidth=0.5)\nplt.plot([comparison_value,comparison_value],[ymin,ymax*1.2],'k')\nplt.text(comparison_value+(xmax-xmin)*0.01,ymax*0.96,'value = '+str(comparison_value),ha='left',va='top')\nplt.text(comparison_value+(xmax-xmin)*0.01,ymax*0.9,ttest,ha='left',va='top')\nplt.ylim([ymin,ymax*1.2])\nplt.xlim([xmin,xmax])\nplt.plot(MIP_mean,ymax*1.03,'ko')\nplt.plot([MIP_mean-MIP_std,\n         MIP_mean+MIP_std],\n         [ymax*1.03,ymax*1.03],'k')\nplt.plot([MIP_mean-MIP_std,\n         MIP_mean-MIP_std],\n         [ymax*1.005,ymax*1.055],'k')\nplt.plot([MIP_mean+MIP_std,\n         MIP_mean+MIP_std],\n         [ymax*1.005,ymax*1.055],'k')\nplt.text(MIP_mean,ymax*1.115,\n         str(round(MIP_mean))+' $\\pm$ '+\n         str(round(MIP_std))+' TgCO$_2$',ha='center')\nplt.title(country_name+' '+year+' '+quantity+'')\nplt.yticks([])\nplt.ylabel('Probability')\nplt.xlabel(quantity+' (TgCO$_2$ year$^{-1}$)')\n\nText(0.5, 0, 'dC_loss (TgCO$_2$ year$^{-1}$)')\n\n\n\n\n\n\n\n\n\nFinally, we will examine two metrics that are useful for understanding the confidence in the top-down results:\n\nZ-statistic: metric of agreement in NCE estimates across the experiments that assimilate different CO2 datasets. Experiments are considered significantly different if the magnitude exceeds 1.96\nFractional Uncertainty Reduction (FUR): metric of how strongly the assimilated CO2 data on reduce NCE uncertainties. Values range from 0 to 1, with 0 meaning zero error reduction and 1 meaning complete error reduction\n\nHere we will add a plot of the Z-statistic for each year, and add the FUR value for the country.\n\n# Make plot\nfig, ax1 = plt.subplots(1, 1, figsize=(6, 4))\nax1.plot(country_data['Year'],country_data['Z-statistic'],label=experiment)\nax1.legend(loc='upper right')\nax1.set_ylabel('Z-statistic')\nax1.set_xlabel('Year')\nax1.set_title(country_name)\nymin, ymax = ax1.get_ylim()\nmax_abs_y = max(abs(ymin), abs(ymax))\nax1.set_ylim([-3, 3])\nxmin, xmax = ax1.get_xlim()\nax1.plot([xmin,xmax],[0,0],'k',linewidth=0.5)\nax1.plot([xmin,xmax],[-1.96,-1.96],'k--',linewidth=0.5)\nax1.plot([xmin,xmax],[1.96,1.96],'k--',linewidth=0.5)\nax1.set_xlim([xmin, xmax])\nax1.text(xmin+0.12,2.6,'Fractional error reduction: '+str(country_data['FUR '+experiment].iloc[1]))\n\nText(-0.18000000000000005, 2.6, 'Fractional error reduction: 0.91')"
  },
  {
    "objectID": "user_data_notebooks/oco2-mip-National-co2budget.html#approach",
    "href": "user_data_notebooks/oco2-mip-National-co2budget.html#approach",
    "title": "OCO-2 MIP National Top-Down CO2 Budgets",
    "section": "",
    "text": "Read in National CO2 Budgets using Pandas\nSub-select the data structure using Pandas\nVisualize the CO2 budgets for a country\nInvestigate uncertainties and metrics for understanding the dataset"
  },
  {
    "objectID": "user_data_notebooks/oco2-mip-National-co2budget.html#about-the-data",
    "href": "user_data_notebooks/oco2-mip-National-co2budget.html#about-the-data",
    "title": "OCO-2 MIP National Top-Down CO2 Budgets",
    "section": "",
    "text": "This tutorial shows how to explore and plot the dataset reported in:\nByrne et al.: National CO2 budgets (2015–2020) inferred from atmospheric CO2 observations in support of the global stocktake, Earth Syst. Sci. Data, 15, 963–1004, https://doi.org/10.5194/essd-15-963-2023, 2023."
  },
  {
    "objectID": "user_data_notebooks/oco2-mip-National-co2budget.html#import-required-modules",
    "href": "user_data_notebooks/oco2-mip-National-co2budget.html#import-required-modules",
    "title": "OCO-2 MIP National Top-Down CO2 Budgets",
    "section": "",
    "text": "First we will need to import the relevant python modules:\n\nimport pandas as pd # for manipulating csv dataset\nimport numpy as np\nimport matplotlib.pyplot as plt # make plots\nfrom scipy.stats import norm # We will use this for understanding significance"
  },
  {
    "objectID": "user_data_notebooks/oco2-mip-National-co2budget.html#read-the-co2-national-budget-dataset",
    "href": "user_data_notebooks/oco2-mip-National-co2budget.html#read-the-co2-national-budget-dataset",
    "title": "OCO-2 MIP National Top-Down CO2 Budgets",
    "section": "",
    "text": "Now we will read in the csv dataset from https://ceos.org/gst/carbon-dioxide.html\n\nurl ='https://ceos.org/gst/files/pilot_topdown_CO2_Budget_countries_v1.csv'\ndf_all = pd.read_csv(url, skiprows=52)"
  },
  {
    "objectID": "user_data_notebooks/oco2-mip-National-co2budget.html#sub-select-a-single-top-down-dataset-experiment",
    "href": "user_data_notebooks/oco2-mip-National-co2budget.html#sub-select-a-single-top-down-dataset-experiment",
    "title": "OCO-2 MIP National Top-Down CO2 Budgets",
    "section": "",
    "text": "To simplify the analysis, let’s subselect the results for a single experiment. The experiments are: - IS: estimates fluxes from in situ CO2 measurements - LNLG: estimates fluxes from OCO-2 land CO2 data - LNLGIS: combines in situ and OCO-2 land CO2 data - LNLGOGIS: combines in situ and OCO-2 land and ocean CO2 data\nWe would like to use the experiment that uses the most high-quality CO2 data. There are some concerns about small residual biases in OCO-2 ocean data (Byrne et al., 2023), so let’s use the LNLGIS experiment.\n\n# Choose one experiment from the list ['IS', 'LNLG', 'LNLGIS', 'LNLGOGIS']\nexperiment = 'LNLGIS'\n\n# Subset of columns for a given experiment\nif experiment == 'IS':\n    df = df_all.drop(df_all.columns[[4,5,6,7,8,9,12,13,14,15,16,17,20,21,22,23,24,25,34,35,36]], axis=1)\nif experiment == 'LNLG':\n    df = df_all.drop(df_all.columns[[2,3,6,7,8,9,10,11,14,15,16,17,18,19,22,23,24,25,33,35,36]], axis=1)\nif experiment == 'LNLGIS':\n    df = df_all.drop(df_all.columns[[2,3,4,5,8,9,10,11,12,13,16,17,18,19,20,21,24,25,33,34,36]], axis=1)\nif experiment == 'LNLGOGIS':\n    df = df_all.drop(df_all.columns[[2,3,4,5,6,7,10,11,12,13,14,15,18,19,20,21,22,23,33,34,35]], axis=1)\n\n# We can now look at the colums of data\ndf.head()\n\n\n\n\n\n\n\n\n\nAlpha 3 Code\nYear\nLNLGIS dC_loss (TgCO2)\nLNLGIS dC_loss unc (TgCO2)\nLNLGIS NBE (TgCO2)\nLNLGIS NBE unc (TgCO2)\nLNLGIS NCE (TgCO2)\nLNLGIS NCE unc (TgCO2)\nRivers (TgCO2)\nRiver unc (TgCO2)\nWood+Crop (TgCO2)\nWood+Crop unc (TgCO2)\nFF (TgCO2)\nFF unc (TgCO2)\nZ-statistic\nFUR LNLGIS\n\n\n\n\n0\nAFG\n2015\n39.3407\n153.746\n40.9643\n153.746\n60.3537\n153.744\n-2.43286\n1.69832\n4.05648\n1.21694\n19.3894\n0.797698\n0.37\n0.19\n\n\n1\nAFG\n2016\n50.6167\n175.454\n52.5114\n175.454\n73.0333\n175.452\n-2.16185\n2.24033\n4.05648\n1.21694\n20.5220\n0.678080\n0.31\n0.19\n\n\n2\nAFG\n2017\n54.5096\n179.794\n56.4726\n179.794\n77.5355\n179.793\n-2.09349\n2.37705\n4.05648\n1.21694\n21.0629\n0.695856\n0.47\n0.19\n\n\n3\nAFG\n2018\n116.4260\n243.057\n118.4610\n243.057\n143.9580\n243.056\n-2.02199\n2.52005\n4.05648\n1.21694\n25.4974\n0.695856\n0.39\n0.19\n\n\n4\nAFG\n2019\n64.0162\n181.516\n66.0388\n181.516\n93.8974\n181.514\n-2.03383\n2.49637\n4.05648\n1.21694\n27.8585\n0.797698\n0.49\n0.19"
  },
  {
    "objectID": "user_data_notebooks/oco2-mip-National-co2budget.html#sub-select-a-single-country",
    "href": "user_data_notebooks/oco2-mip-National-co2budget.html#sub-select-a-single-country",
    "title": "OCO-2 MIP National Top-Down CO2 Budgets",
    "section": "",
    "text": "Let’s further filter the dataset to look at a specific country. Choose a country by entering the alpha code in the country_name variable below\n\n# Choose a country\ncountry_name = 'USA' \n\n# We can sub-select the data for the country\ncountry_data = df[df['Alpha 3 Code'] == country_name]\n\n# Now we can look at the data for a specific experiment and country\ncountry_data.head()\n\n\n\n\n\n\n\n\n\nAlpha 3 Code\nYear\nLNLGIS dC_loss (TgCO2)\nLNLGIS dC_loss unc (TgCO2)\nLNLGIS NBE (TgCO2)\nLNLGIS NBE unc (TgCO2)\nLNLGIS NCE (TgCO2)\nLNLGIS NCE unc (TgCO2)\nRivers (TgCO2)\nRiver unc (TgCO2)\nWood+Crop (TgCO2)\nWood+Crop unc (TgCO2)\nFF (TgCO2)\nFF unc (TgCO2)\nZ-statistic\nFUR LNLGIS\n\n\n\n\n1232\nUSA\n2015\n-1031.83\n721.213\n-1346.46\n721.213\n4017.31\n713.897\n-165.430\n71.7453\n-149.196\n-44.7589\n5363.77\n102.4670\n-0.81\n0.91\n\n\n1233\nUSA\n2016\n-1419.92\n399.738\n-1743.80\n399.738\n3529.45\n387.079\n-174.684\n53.2375\n-149.196\n-44.7589\n5273.24\n99.8012\n0.04\n0.91\n\n\n1234\nUSA\n2017\n-1375.12\n1034.010\n-1696.63\n1034.010\n3515.14\n1029.250\n-172.308\n57.9894\n-149.196\n-44.7589\n5211.76\n99.0981\n0.67\n0.91\n\n\n1235\nUSA\n2018\n-1018.89\n784.463\n-1333.83\n784.463\n4036.65\n778.179\n-165.747\n71.1117\n-149.196\n-44.7589\n5370.48\n99.0981\n-0.20\n0.91\n\n\n1236\nUSA\n2019\n-1161.41\n718.054\n-1504.61\n718.054\n3728.95\n710.705\n-194.005\n14.5948\n-149.196\n-44.7589\n5233.56\n102.4670\n-0.38\n0.91\n\n\n\n\n\n\n\n\n#This dataset contains fluxes over a five year period, 2015-2020.\nLet’s look at a plot of the annual net terrestrial carbon stock loss (ΔCloss) for each year.\n\n# Make plot\nfig, ax1 = plt.subplots(1, 1, figsize=(6, 4))\nax1.errorbar(country_data['Year'],country_data[experiment+' dC_loss (TgCO2)'],\n                    yerr=country_data[experiment+' dC_loss unc (TgCO2)'],label=experiment,capsize=10)\nax1.legend(loc='upper right')\nax1.set_ylabel('$\\Delta$C$_\\mathrm{loss}$ (TgCO$_2$ year$^{-1}$)')\nax1.set_xlabel('Year')\nax1.set_title('$\\Delta$C$_\\mathrm{loss}$ for '+country_name)\nymin, ymax = ax1.get_ylim()\nmax_abs_y = max(abs(ymin), abs(ymax))\nax1.set_ylim([-max_abs_y, max_abs_y])\nxmin, xmax = ax1.get_xlim()\nax1.plot([xmin,xmax],[0,0],'k',linewidth=0.5)\nax1.set_xlim([xmin, xmax])\n\n\n\n\n\n\n\n\nNext, we can look at the full carbon budget for a given year.\nThe code below creates a plot similar to Fig 13 of Byrne et al. (2023). Each of the bars on the left side of the dashed vertical line (Fossil fuel emissions, lateral C transport by rivers, lateral C transport in crop and wood products, and the net terrestrial carbon stock loss combined to give the net carbon exchange (net surface-atmosphere CO2 flux) shown on the right.\n\n# Pick a specifc year (or mean year)\nyear='mean'\n\n# Make plot\ncountry_data_mean = country_data[country_data['Year'] == year]\na=country_data_mean['Wood+Crop (TgCO2)']\nb=country_data_mean['Wood+Crop unc (TgCO2)']\nprint(b)\n#\nplt.bar(1, country_data_mean['FF (TgCO2)'], yerr=country_data_mean['FF unc (TgCO2)'], label='FF', alpha=0.5)\nplt.bar(2, country_data_mean['Rivers (TgCO2)'], yerr=country_data_mean['River unc (TgCO2)'], label='Rivers', alpha=0.5)\nplt.bar(3, country_data_mean['Wood+Crop (TgCO2)'], yerr=abs(country_data_mean['Wood+Crop unc (TgCO2)']), label='WoodCrop', alpha=0.5)\nplt.bar(4, country_data_mean[experiment+' dC_loss (TgCO2)'], yerr=country_data_mean['LNLGIS dC_loss unc (TgCO2)'], label='dC', alpha=0.5)\nplt.bar(6, country_data_mean[experiment+' NCE (TgCO2)'], yerr=country_data_mean['LNLGIS NCE unc (TgCO2)'], label='NCE', alpha=0.5)\nax = plt.gca()\nymin, ymax = ax.get_ylim()\nplt.plot([5,5],[ymin,ymax],'k:')\nxmin, xmax = ax.get_xlim()\nplt.plot([xmin,xmax],[0,0],'k',linewidth=0.5)\nplt.xlim([xmin,xmax])\nplt.ylim([ymin,ymax])\n#\nplt.xticks([1,2,3,4,6], ['Fossil\\nFuels','Rivers','Wood+\\nCrops','$\\mathrm{\\Delta C _{loss}}$','NCE'])\nplt.title(country_name+' '+year)\nplt.ylabel('CO$_2$ Flux (TgCO$_2$ year$^{-1}$)')\n\n1238   -44.7589\nName: Wood+Crop unc (TgCO2), dtype: float64\n\n\nText(0, 0.5, 'CO$_2$ Flux (TgCO$_2$ year$^{-1}$)')\n\n\n\n\n\n\n\n\n\nUncertainty is an important consideration when analyzing the flux estimates provided by Byrne et al. (2023).\nEach flux estimate is provided with an error estimate representing the standard deviation, and assuming the errors are well prepresented by a normal distribution. This probability dirtribution provided by this uncertainty can be visualized below. We can further quantify the\n\n\n# Select NCE, NBE or dC_loss\nquantity = 'dC_loss'\n\n# Value for comparison\ncomparison_value = 1000 # TgCO2/year\n\n\nMIP_mean = country_data_mean[experiment+' '+quantity+' (TgCO2)'].item()\nMIP_std = country_data_mean[experiment+' '+quantity+' unc (TgCO2)'].item()\n\n# Perform t-test\nt_value = abs(MIP_mean - comparison_value)/(MIP_std / np.sqrt(11))\ncrtical_value = 2.23 # use p=0.05 significance\nif t_value &gt; crtical_value:\n    ttest = 'statistically different'\nif t_value &lt; crtical_value:\n    ttest = 'not statistically\\ndifferent'\n\n# Make plot\nxbounds = abs(MIP_mean)+MIP_std*4\nif abs(crtical_value) &gt; xbounds:\n    xbounds = abs(crtical_value)\nx_axis = np.arange(-1.*xbounds, xbounds, 1) \nplt.plot(x_axis, norm.pdf(x_axis, MIP_mean, MIP_std)) \nax = plt.gca()\nymin, ymax = ax.get_ylim()\nxmin, xmax = ax.get_xlim()\nplt.plot([0,0],[ymin,ymax*1.2],'k:',linewidth=0.5)\nplt.plot([xmin,xmax],[0,0],'k:',linewidth=0.5)\nplt.plot([comparison_value,comparison_value],[ymin,ymax*1.2],'k')\nplt.text(comparison_value+(xmax-xmin)*0.01,ymax*0.96,'value = '+str(comparison_value),ha='left',va='top')\nplt.text(comparison_value+(xmax-xmin)*0.01,ymax*0.9,ttest,ha='left',va='top')\nplt.ylim([ymin,ymax*1.2])\nplt.xlim([xmin,xmax])\nplt.plot(MIP_mean,ymax*1.03,'ko')\nplt.plot([MIP_mean-MIP_std,\n         MIP_mean+MIP_std],\n         [ymax*1.03,ymax*1.03],'k')\nplt.plot([MIP_mean-MIP_std,\n         MIP_mean-MIP_std],\n         [ymax*1.005,ymax*1.055],'k')\nplt.plot([MIP_mean+MIP_std,\n         MIP_mean+MIP_std],\n         [ymax*1.005,ymax*1.055],'k')\nplt.text(MIP_mean,ymax*1.115,\n         str(round(MIP_mean))+' $\\pm$ '+\n         str(round(MIP_std))+' TgCO$_2$',ha='center')\nplt.title(country_name+' '+year+' '+quantity+'')\nplt.yticks([])\nplt.ylabel('Probability')\nplt.xlabel(quantity+' (TgCO$_2$ year$^{-1}$)')\n\nText(0.5, 0, 'dC_loss (TgCO$_2$ year$^{-1}$)')\n\n\n\n\n\n\n\n\n\nFinally, we will examine two metrics that are useful for understanding the confidence in the top-down results:\n\nZ-statistic: metric of agreement in NCE estimates across the experiments that assimilate different CO2 datasets. Experiments are considered significantly different if the magnitude exceeds 1.96\nFractional Uncertainty Reduction (FUR): metric of how strongly the assimilated CO2 data on reduce NCE uncertainties. Values range from 0 to 1, with 0 meaning zero error reduction and 1 meaning complete error reduction\n\nHere we will add a plot of the Z-statistic for each year, and add the FUR value for the country.\n\n# Make plot\nfig, ax1 = plt.subplots(1, 1, figsize=(6, 4))\nax1.plot(country_data['Year'],country_data['Z-statistic'],label=experiment)\nax1.legend(loc='upper right')\nax1.set_ylabel('Z-statistic')\nax1.set_xlabel('Year')\nax1.set_title(country_name)\nymin, ymax = ax1.get_ylim()\nmax_abs_y = max(abs(ymin), abs(ymax))\nax1.set_ylim([-3, 3])\nxmin, xmax = ax1.get_xlim()\nax1.plot([xmin,xmax],[0,0],'k',linewidth=0.5)\nax1.plot([xmin,xmax],[-1.96,-1.96],'k--',linewidth=0.5)\nax1.plot([xmin,xmax],[1.96,1.96],'k--',linewidth=0.5)\nax1.set_xlim([xmin, xmax])\nax1.text(xmin+0.12,2.6,'Fractional error reduction: '+str(country_data['FUR '+experiment].iloc[1]))\n\nText(-0.18000000000000005, 2.6, 'Fractional error reduction: 0.91')"
  },
  {
    "objectID": "user_data_notebooks/casagfed-carbonflux-monthgrid-v3_User_Notebook.html",
    "href": "user_data_notebooks/casagfed-carbonflux-monthgrid-v3_User_Notebook.html",
    "title": "CASA-GFED3 Land Carbon Flux",
    "section": "",
    "text": "Identify available dates and temporal frequency of observations for a given collection using the GHGC API /stac endpoint. The collection processed in this notebook is the Land-Atmosphere Carbon Flux data product.\nPass the STAC item into the raster API /stac/tilejson.jsonendpoint.\nUsing folium.plugins.DualMap, visualize two tiles (side-by-side), allowing time point comparison.\nAfter the visualization, perform zonal statistics for a given polygon.",
    "crumbs": [
      "Data Usage Notebooks",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "CASA-GFED3 Land Carbon Flux"
    ]
  },
  {
    "objectID": "user_data_notebooks/casagfed-carbonflux-monthgrid-v3_User_Notebook.html#approach",
    "href": "user_data_notebooks/casagfed-carbonflux-monthgrid-v3_User_Notebook.html#approach",
    "title": "CASA-GFED3 Land Carbon Flux",
    "section": "",
    "text": "Identify available dates and temporal frequency of observations for a given collection using the GHGC API /stac endpoint. The collection processed in this notebook is the Land-Atmosphere Carbon Flux data product.\nPass the STAC item into the raster API /stac/tilejson.jsonendpoint.\nUsing folium.plugins.DualMap, visualize two tiles (side-by-side), allowing time point comparison.\nAfter the visualization, perform zonal statistics for a given polygon.",
    "crumbs": [
      "Data Usage Notebooks",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "CASA-GFED3 Land Carbon Flux"
    ]
  },
  {
    "objectID": "user_data_notebooks/casagfed-carbonflux-monthgrid-v3_User_Notebook.html#about-the-data",
    "href": "user_data_notebooks/casagfed-carbonflux-monthgrid-v3_User_Notebook.html#about-the-data",
    "title": "CASA-GFED3 Land Carbon Flux",
    "section": "About the Data",
    "text": "About the Data\nThis dataset presents a variety of carbon flux parameters derived from the Carnegie-Ames-Stanford-Approach – Global Fire Emissions Database version 3 (CASA-GFED3) model. The model’s input data includes air temperature, precipitation, incident solar radiation, a soil classification map, and a number of satellite derived products. All model calculations are driven by analyzed meteorological data from NASA’s Modern-Era Retrospective analysis for Research and Application, Version 2 (MERRA-2). The resulting product provides monthly, global data at 0.5 degree resolution from January 2003 through December 2017. It includes the following carbon flux variables expressed in units of kilograms of carbon per square meter per month (kg Carbon m²/mon) from the following sources: net primary production (NPP), net ecosystem exchange (NEE), heterotrophic respiration (Rh), wildfire emissions (FIRE), and fuel wood burning emissions (FUEL). This product and earlier versions of MERRA-driven CASA-GFED carbon fluxes have been used in a number of atmospheric CO₂ transport studies, and through the support of NASA’s Carbon Monitoring System (CMS), it helps characterize, quantify, understand and predict the evolution of global carbon sources and sinks.",
    "crumbs": [
      "Data Usage Notebooks",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "CASA-GFED3 Land Carbon Flux"
    ]
  },
  {
    "objectID": "user_data_notebooks/casagfed-carbonflux-monthgrid-v3_User_Notebook.html#querying-the-stac-api",
    "href": "user_data_notebooks/casagfed-carbonflux-monthgrid-v3_User_Notebook.html#querying-the-stac-api",
    "title": "CASA-GFED3 Land Carbon Flux",
    "section": "Querying the STAC API",
    "text": "Querying the STAC API\nPlease run the next cell to import the required libraries.\n\nimport requests\nimport folium\nimport folium.plugins\nfrom folium import Map, TileLayer \nfrom pystac_client import Client \nimport branca \nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n\n# Provide STAC and RASTER API endpoints\nSTAC_API_URL = \"http://ghg.center/api/stac\"\nRASTER_API_URL = \"https://ghg.center/api/raster\"\n\n# Please use the collection name similar to the one used in the STAC collection.\n# Name of the collection for CASA GFED Land-Atmosphere Carbon Flux monthly emissions. \ncollection_name = \"casagfed-carbonflux-monthgrid-v3\"\n\n\n# Fetch the collection from STAC collections using the appropriate endpoint\n# the 'requests' library allows a HTTP request possible\ncollection = requests.get(f\"{STAC_API_URL}/collections/{collection_name}\").json()\ncollection\n\nExamining the contents of our collection under the temporal variable, we see that the data is available from January 2003 to December 2017. By looking at the dashboard:time density, we observe that the periodic frequency of these observations is monthly.\n\n# Create a function that would search for the above data collection in the STAC API\ndef get_item_count(collection_id):\n    count = 0\n    items_url = f\"{STAC_API_URL}/collections/{collection_id}/items\"\n\n    while True:\n        response = requests.get(items_url)\n\n        if not response.ok:\n            print(\"error getting items\")\n            exit()\n\n        stac = response.json()\n        count += int(stac[\"context\"].get(\"returned\", 0))\n        next = [link for link in stac[\"links\"] if link[\"rel\"] == \"next\"]\n\n        if not next:\n            break\n        items_url = next[0][\"href\"]\n\n    return count\n\n\n# Apply the above function and check the total number of items available within the collection\nnumber_of_items = get_item_count(collection_name)\nitems = requests.get(f\"{STAC_API_URL}/collections/{collection_name}/items?limit={number_of_items}\").json()[\"features\"]\nprint(f\"Found {len(items)} items\")\n\n\n# Examine the first item in the collection\nitems[0]",
    "crumbs": [
      "Data Usage Notebooks",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "CASA-GFED3 Land Carbon Flux"
    ]
  },
  {
    "objectID": "user_data_notebooks/casagfed-carbonflux-monthgrid-v3_User_Notebook.html#exploring-changes-in-carbon-flux-levels-using-the-raster-api",
    "href": "user_data_notebooks/casagfed-carbonflux-monthgrid-v3_User_Notebook.html#exploring-changes-in-carbon-flux-levels-using-the-raster-api",
    "title": "CASA-GFED3 Land Carbon Flux",
    "section": "Exploring Changes in Carbon Flux Levels Using the Raster API",
    "text": "Exploring Changes in Carbon Flux Levels Using the Raster API\nWe will explore changes in the land atmosphere Carbon flux Heterotrophic Respiration and examine their impacts over time. We’ll then visualize the outputs on a map using folium.\n\n# To access the year value from each item more easily, this will let us query more explicitly by year and month (e.g., 2020-02)\nitems = {item[\"properties\"][\"start_datetime\"][:7]: item for item in items} \n# rh = Heterotrophic Respiration\nasset_name = \"rh\"\n\nBelow, we are entering the minimum and maximum values to provide our upper and lower bounds in rescale_values.\n\nrescale_values = {\"max\":items[list(items.keys())[0]][\"assets\"][asset_name][\"raster:bands\"][0][\"histogram\"][\"max\"], \"min\":items[list(items.keys())[0]][\"assets\"][asset_name][\"raster:bands\"][0][\"histogram\"][\"min\"]}\n\nNow, we will pass the item id, collection name, and rescaling_factor to the Raster API endpoint. We will do this twice, once for December 2003 and again for December 2017, so that we can visualize each event independently.\n\ncolor_map = \"purd\" # please refer to matplotlib library if you'd prefer choosing a different color ramp.\n# For more information on Colormaps in Matplotlib, please visit https://matplotlib.org/stable/users/explain/colors/colormaps.html\n\n# To change the year and month of the observed parameter, you can modify the \"items['YYYY-MM']\" statement\n# For example, you can change the current statement \"items['2003-12']\" to \"items['2016-10']\" \ndecember_2003_tile = requests.get(\n    f\"{RASTER_API_URL}/stac/tilejson.json?collection={items['2003-12']['collection']}&item={items['2003-12']['id']}\"\n    f\"&assets={asset_name}\"\n    f\"&color_formula=gamma+r+1.05&colormap_name={color_map}\"\n    f\"&rescale={rescale_values['min']},{rescale_values['max']}\", \n).json()\ndecember_2003_tile\n\n\n# Now we apply the same process used in the previous step for the December 2017 tile\ndecember_2017_tile = requests.get(\n    f\"{RASTER_API_URL}/stac/tilejson.json?collection={items['2017-12']['collection']}&item={items['2017-12']['id']}\"\n    f\"&assets={asset_name}\"\n    f\"&color_formula=gamma+r+1.05&colormap_name={color_map}\"\n    f\"&rescale={rescale_values['min']},{rescale_values['max']}\", \n).json()\ndecember_2017_tile",
    "crumbs": [
      "Data Usage Notebooks",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "CASA-GFED3 Land Carbon Flux"
    ]
  },
  {
    "objectID": "user_data_notebooks/casagfed-carbonflux-monthgrid-v3_User_Notebook.html#visualizing-land-atmosphere-carbon-flux-heterotrophic-respiration",
    "href": "user_data_notebooks/casagfed-carbonflux-monthgrid-v3_User_Notebook.html#visualizing-land-atmosphere-carbon-flux-heterotrophic-respiration",
    "title": "CASA-GFED3 Land Carbon Flux",
    "section": "Visualizing Land-Atmosphere Carbon Flux (Heterotrophic Respiration)",
    "text": "Visualizing Land-Atmosphere Carbon Flux (Heterotrophic Respiration)\n\n# For this study we are going to compare the RH level in 2003 and 2017 over the State of Texas \n# To change the location, you can simply insert the latitude and longitude of the area of your interest in the \"location=(LAT, LONG)\" statement\n# For example, you can change the current statement \"location=(31.9, -99.9)\" to \"location=(34, -118)\" to monitor the RH level in California instead of Texas\n\n# Set initial zoom and center of map for CO₂ Layer\n# 'folium.plugins' allows mapping side-by-side\nmap_ = folium.plugins.DualMap(location=(31.9, -99.9), zoom_start=6)\n\n# The TileLayer library helps in manipulating and displaying raster layers on a map\n# December 2003\nmap_layer_2003 = TileLayer(\n    tiles=december_2003_tile[\"tiles\"][0],\n    attr=\"GHG\",\n    opacity=0.8,\n    name=\"December 2003 RH Level\",\n    overlay= True,\n    legendEnabled = True\n)\nmap_layer_2003.add_to(map_.m1)\n\n\n# December 2017\nmap_layer_2017 = TileLayer(\n    tiles=december_2017_tile[\"tiles\"][0],\n    attr=\"GHG\",\n    opacity=0.8,\n    name=\"December 2017 RH Level\",\n    overlay= True,\n    legendEnabled = True\n)\nmap_layer_2017.add_to(map_.m2)\n\n\n# Display data markers (titles) on both maps\nfolium.Marker((40, 5.0), tooltip=\"both\").add_to(map_)\nfolium.LayerControl(collapsed=False).add_to(map_)\n\n\n# Add a legend to the dual map using the 'branca' library. \n# Note: the inserted legend is representing the minimum and maximum values for both tiles.\ncolormap = branca.colormap.linear.PuRd_09.scale(0, 0.3) # minimum value = 0, maximum value = 0.3 (kg Carbon/m2/month)\ncolormap = colormap.to_step(index=[0, 0.07, 0.15, 0.22, 0.3])\ncolormap.caption = 'Rh Values (kg Carbon/m2/month)'\n\ncolormap.add_to(map_.m1)\n\n\n# Visualizing the map\nmap_",
    "crumbs": [
      "Data Usage Notebooks",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "CASA-GFED3 Land Carbon Flux"
    ]
  },
  {
    "objectID": "user_data_notebooks/casagfed-carbonflux-monthgrid-v3_User_Notebook.html#visualizing-the-data-as-a-time-series",
    "href": "user_data_notebooks/casagfed-carbonflux-monthgrid-v3_User_Notebook.html#visualizing-the-data-as-a-time-series",
    "title": "CASA-GFED3 Land Carbon Flux",
    "section": "Visualizing the Data as a Time Series",
    "text": "Visualizing the Data as a Time Series\nWe can now explore the Heterotrophic Respiration time series (January 2003 -December 2017) available for the Dallas, Texas area. We can plot the data set using the code below:\n\nfig = plt.figure(figsize=(20, 10)) #determine the width and height of the plot using the 'matplotlib' library\n\nplt.plot(\n    df[\"date\"],\n    df[\"max\"],\n    color=\"purple\",\n    linestyle=\"-\",\n    linewidth=0.5,\n    label=\"Max monthly Carbon emissions\",\n)\n\nplt.legend()\nplt.xlabel(\"Years\")\nplt.ylabel(\"kg Carbon/m2/month\")\nplt.title(\"Heterotrophic Respiration Values for Dallas, Texas (2003-2017)\")\n\n\n# Now let's examine the Rh level for the 3rd item in the collection for Dallas, Texas area\n# Keep in mind that a list starts from 0, 1, 2,... therefore items[2] is referring to the third item in the list/collection\nprint(items[2][\"properties\"][\"start_datetime\"]) #print the start Date Time of the third granule in the collection!\n\n\n# Fetch the third granule in the collection and set the color scheme and rescale values. \noctober_tile = requests.get(\n    f\"{RASTER_API_URL}/stac/tilejson.json?collection={items[2]['collection']}&item={items[2]['id']}\"\n    f\"&assets={asset_name}\"\n    f\"&color_formula=gamma+r+1.05&colormap_name={color_map}\"\n    f\"&rescale={rescale_values['min']},{rescale_values['max']}\",\n).json()\noctober_tile\n\n\n# Map the Rh level for the Dallas, Texas area for the October, 2017 timeframe\naoi_map_bbox = Map(\n    tiles=\"OpenStreetMap\",\n    location=[\n        32.8, # latitude\n        -96.79, # longitude\n    ],\n    zoom_start=9,\n)\n\nmap_layer = TileLayer(\n    tiles=october_tile[\"tiles\"][0],\n    attr=\"GHG\", opacity = 0.7, name=\"October 2017 RH Level\", overlay= True, legendEnabled = True\n)\n\nmap_layer.add_to(aoi_map_bbox)\n\n# Display data marker (title) on the map\nfolium.Marker((40, 5.9), tooltip=\"both\").add_to(aoi_map_bbox)\nfolium.LayerControl(collapsed=False).add_to(aoi_map_bbox)\n\n# Add a legend\ncolormap = branca.colormap.linear.PuRd_09.scale(0, 0.3) # minimum value = 0, maximum value = 0.3 (kg Carbon/m2/month)\ncolormap = colormap.to_step(index=[0, 0.07, 0.15, 0.22, 0.3])\ncolormap.caption = 'Rh Values (kg Carbon/m2/month)'\n\ncolormap.add_to(aoi_map_bbox)\n\naoi_map_bbox",
    "crumbs": [
      "Data Usage Notebooks",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "CASA-GFED3 Land Carbon Flux"
    ]
  },
  {
    "objectID": "user_data_notebooks/casagfed-carbonflux-monthgrid-v3_User_Notebook.html#summary",
    "href": "user_data_notebooks/casagfed-carbonflux-monthgrid-v3_User_Notebook.html#summary",
    "title": "CASA-GFED3 Land Carbon Flux",
    "section": "Summary",
    "text": "Summary\nIn this notebook we have successfully completed the following steps for the STAC collection for CASA GFED Land-Atmosphere Carbon Flux data: 1. Install and import the necessary libraries 2. Fetch the collection from STAC collections using the appropriate endpoints 3. Count the number of existing granules within the collection 4. Map and compare the Heterotrophic Respiration (Rh) levels over the Dallas, Texas area for two distinctive years 5. Create a table that displays the minimum, maximum, and sum of the Rh values for a specified region 6. Generate a time-series graph of the Rh values for a specified region\nIf you have any questions regarding this user notebook, please contact us using the feedback form.",
    "crumbs": [
      "Data Usage Notebooks",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "CASA-GFED3 Land Carbon Flux"
    ]
  },
  {
    "objectID": "user_data_notebooks/lpjwsl-wetlandch4-grid-v1_User_Notebook.html",
    "href": "user_data_notebooks/lpjwsl-wetlandch4-grid-v1_User_Notebook.html",
    "title": "Wetland Methane Emissions, LPJ-wsl Model",
    "section": "",
    "text": "Identify available dates and temporal frequency of observations for the given collection using the GHGC API /stac endpoint. The collection processed in this notebook is the Wetland Methane Emissions, LPJ-wsl Model data product.\nPass the STAC item into the raster API /stac/tilejson.jsonendpoint.\nUsing folium.plugins.DualMap, visualize two tiles (side-by-side), allowing time point comparison.\nAfter the visualization, perform zonal statistics for a given polygon.",
    "crumbs": [
      "Data Usage Notebooks",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "Wetland Methane Emissions, LPJ-wsl Model"
    ]
  },
  {
    "objectID": "user_data_notebooks/lpjwsl-wetlandch4-grid-v1_User_Notebook.html#approach",
    "href": "user_data_notebooks/lpjwsl-wetlandch4-grid-v1_User_Notebook.html#approach",
    "title": "Wetland Methane Emissions, LPJ-wsl Model",
    "section": "",
    "text": "Identify available dates and temporal frequency of observations for the given collection using the GHGC API /stac endpoint. The collection processed in this notebook is the Wetland Methane Emissions, LPJ-wsl Model data product.\nPass the STAC item into the raster API /stac/tilejson.jsonendpoint.\nUsing folium.plugins.DualMap, visualize two tiles (side-by-side), allowing time point comparison.\nAfter the visualization, perform zonal statistics for a given polygon.",
    "crumbs": [
      "Data Usage Notebooks",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "Wetland Methane Emissions, LPJ-wsl Model"
    ]
  },
  {
    "objectID": "user_data_notebooks/lpjwsl-wetlandch4-grid-v1_User_Notebook.html#about-the-data",
    "href": "user_data_notebooks/lpjwsl-wetlandch4-grid-v1_User_Notebook.html#about-the-data",
    "title": "Wetland Methane Emissions, LPJ-wsl Model",
    "section": "About the Data",
    "text": "About the Data\nMethane (CH₄) emissions from wetlands are estimated to be the largest natural source of methane in the global CH₄ budget, contributing to roughly one third of the total of natural and anthropogenic emissions. Wetland CH₄ is produced by microbes breaking down organic matter in the oxygen deprived environment of inundated soils. Due to limited data availability, the details of the role of wetland CH₄ emissions has thus far been underrepresented. Using the Wald Schnee und Landschaft version (LPJ-wsl) of the Lund-Potsdam-Jena Dynamic Global Vegetation Model (LPJ-DGVM) global CH₄ emissions from wetlands are estimated at 0.5 x 0.5 degree resolution by simulating wetland extent and using characteristics of these inundated areas, such as soil moisture, temperature, and carbon content, to estimate CH₄ quantities emitted into the atmosphere. Highlighted areas displayed in this dataset show concentrated methane sources from tropical and high latitude ecosystems. The LPJ-wsl Wetland Methane Emissions data product presented here consists of global daily and monthly model estimates of terrestrial wetland CH₄ emissions from 1980 - 2021. These data are regularly used in conjunction with NASA’s Goddard Earth Observing System (GEOS) model to simulate the impact of wetlands and other methane sources on atmospheric methane concentrations, to compare against satellite and airborne data, and to improve understanding and prediction of wetland emissions.",
    "crumbs": [
      "Data Usage Notebooks",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "Wetland Methane Emissions, LPJ-wsl Model"
    ]
  },
  {
    "objectID": "user_data_notebooks/lpjwsl-wetlandch4-grid-v1_User_Notebook.html#querying-the-stac-api",
    "href": "user_data_notebooks/lpjwsl-wetlandch4-grid-v1_User_Notebook.html#querying-the-stac-api",
    "title": "Wetland Methane Emissions, LPJ-wsl Model",
    "section": "Querying the STAC API",
    "text": "Querying the STAC API\n\nimport requests\nfrom folium import Map, TileLayer\nfrom pystac_client import Client\n\n\n# Provide STAC and RASTER API endpoints\nSTAC_API_URL = \"http://ghg.center/api/stac\"\nRASTER_API_URL = \"https://ghg.center/api/raster\"\n\n# Please use the collection name similar to the one used in STAC collection.\n\n# Name of the collection for wetland methane monthly emissions. \ncollection_name = \"lpjwsl-wetlandch4-monthgrid-v1\"\n\n\n# Fetching the collection from STAC collections using appropriate endpoint.\ncollection = requests.get(f\"{STAC_API_URL}/collections/{collection_name}\").json()\ncollection\n\nExamining the contents of our collection under summaries, we see that the data is available from January 1980 to December 2021. By looking at dashboard: time density, we can see that these observations are collected monthly.\n\ndef get_item_count(collection_id):\n    count = 0\n    items_url = f\"{STAC_API_URL}/collections/{collection_id}/items\"\n\n    while True:\n        response = requests.get(items_url)\n\n        if not response.ok:\n            print(\"error getting items\")\n            exit()\n\n        stac = response.json()\n        count += int(stac[\"context\"].get(\"returned\", 0))\n        next = [link for link in stac[\"links\"] if link[\"rel\"] == \"next\"]\n\n        if not next:\n            break\n        items_url = next[0][\"href\"]\n\n    return count\n\n\n# Check total number of items available\nnumber_of_items = get_item_count(collection_name)\nitems = requests.get(f\"{STAC_API_URL}/collections/{collection_name}/items?limit={number_of_items}\").json()[\"features\"]\nprint(f\"Found {len(items)} items\")\n\n\n# Examining the first item in the collection\nitems[0]\n\nBelow, we enter minimum and maximum values to provide our upper and lower bounds in rescale_values.\n\nrescale_values = {'max': 0.2, 'min': 0.0}",
    "crumbs": [
      "Data Usage Notebooks",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "Wetland Methane Emissions, LPJ-wsl Model"
    ]
  },
  {
    "objectID": "user_data_notebooks/lpjwsl-wetlandch4-grid-v1_User_Notebook.html#exploring-changes-in-methane-ch4-emission-levels-using-the-raster-api",
    "href": "user_data_notebooks/lpjwsl-wetlandch4-grid-v1_User_Notebook.html#exploring-changes-in-methane-ch4-emission-levels-using-the-raster-api",
    "title": "Wetland Methane Emissions, LPJ-wsl Model",
    "section": "Exploring Changes in Methane (CH4) Emission Levels Using the Raster API",
    "text": "Exploring Changes in Methane (CH4) Emission Levels Using the Raster API\nIn this notebook, we will explore the temporal impacts of methane emissions. We will visualize the outputs on a map using folium.\n\n# To access the year value from each item more easily, this will let us query more explicity by year and month (e.g., 2020-02)\nitems = {item[\"properties\"][\"datetime\"][:7]: item for item in items} \n\nNow, we will pass the item id, collection name, and rescaling_factor to the Raster API endpoint. We will do this twice, once for December 2001 and again for December 2021, so we can visualize each event independently.\n\ncolor_map = \"magma\" # select the color ramp from matplotlib library.\ndecember_2001_tile = requests.get(\n    f\"{RASTER_API_URL}/stac/tilejson.json?collection={items['2001-12']['collection']}&item={items['2001-12']['id']}\"\n    \"&assets=ch4-wetlands-emissions\"\n    f\"&color_formula=gamma+r+1.05&colormap_name={color_map}\"\n    f\"&rescale={rescale_values['min']},{rescale_values['max']}\", \n).json()\ndecember_2001_tile\n\n\ndecember_2021_tile = requests.get(\n    f\"{RASTER_API_URL}/stac/tilejson.json?collection={items['2021-12']['collection']}&item={items['2021-12']['id']}\"\n    \"&assets=ch4-wetlands-emissions\"\n    f\"&color_formula=gamma+r+1.05&colormap_name={color_map}\"\n    f\"&rescale={rescale_values['min']},{rescale_values['max']}\", \n).json()\ndecember_2021_tile",
    "crumbs": [
      "Data Usage Notebooks",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "Wetland Methane Emissions, LPJ-wsl Model"
    ]
  },
  {
    "objectID": "user_data_notebooks/lpjwsl-wetlandch4-grid-v1_User_Notebook.html#visualizing-ch₄-emissions",
    "href": "user_data_notebooks/lpjwsl-wetlandch4-grid-v1_User_Notebook.html#visualizing-ch₄-emissions",
    "title": "Wetland Methane Emissions, LPJ-wsl Model",
    "section": "Visualizing CH₄ Emissions",
    "text": "Visualizing CH₄ Emissions\n\n# We will import folium to map and folium.plugins to allow side-by-side mapping\nimport folium\nimport folium.plugins\n\n# Set initial zoom and center of map for CH₄ Layer\n# Centre of map [latitude,longitude]\nmap_ = folium.plugins.DualMap(location=(34, -118), zoom_start=6)\n\n# December 2001\nmap_layer_2001 = TileLayer(\n    tiles=december_2001_tile[\"tiles\"][0],\n    attr=\"GHG\",\n    opacity=0.5,\n)\nmap_layer_2001.add_to(map_.m1)\n\n# December 2021\nmap_layer_2021 = TileLayer(\n    tiles=december_2021_tile[\"tiles\"][0],\n    attr=\"GHG\",\n    opacity=0.5,\n)\nmap_layer_2021.add_to(map_.m2)\n\n# visualising the map\nmap_",
    "crumbs": [
      "Data Usage Notebooks",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "Wetland Methane Emissions, LPJ-wsl Model"
    ]
  },
  {
    "objectID": "user_data_notebooks/lpjwsl-wetlandch4-grid-v1_User_Notebook.html#visualizing-the-data-as-a-time-series",
    "href": "user_data_notebooks/lpjwsl-wetlandch4-grid-v1_User_Notebook.html#visualizing-the-data-as-a-time-series",
    "title": "Wetland Methane Emissions, LPJ-wsl Model",
    "section": "Visualizing the Data as a Time Series",
    "text": "Visualizing the Data as a Time Series\nWe can now explore the wetland methane emissions time series (January 1980 – December 2021) available for the Dallas, Texas area of the U.S. We can plot the data set using the code below:\n\nimport matplotlib.pyplot as plt\n\nfig = plt.figure(figsize=(20, 10))\n\n\nplt.plot(\n    df[\"date\"],\n    df[\"max\"],\n    color=\"red\",\n    linestyle=\"-\",\n    linewidth=0.5,\n    label=\"Max monthly CH₄ emissions\",\n)\n\nplt.legend()\nplt.xlabel(\"Years\")\nplt.ylabel(\"CH4 emissions g/m2\")\nplt.title(\"CH4 emission Values for Texas, Dallas (1980-2021)\")\n\n\nprint(items[2][\"properties\"][\"datetime\"])\n\n\noctober_tile = requests.get(\n    f\"{RASTER_API_URL}/stac/tilejson.json?collection={items[2]['collection']}&item={items[2]['id']}\"\n    \"&assets=ch4-wetlands-emissions\"\n    f\"&color_formula=gamma+r+1.05&colormap_name={color_map}\"\n    f\"&rescale={rescale_values['min']},{rescale_values['max']}\",\n).json()\noctober_tile\n\n\n# Use bbox initial zoom and map\n# Set up a map located w/in event bounds\nimport folium\n\naoi_map_bbox = Map(\n    tiles=\"OpenStreetMap\",\n    location=[\n        30,-100\n    ],\n    zoom_start=8,\n)\n\nmap_layer = TileLayer(\n    tiles=october_tile[\"tiles\"][0],\n    attr=\"GHG\", opacity = 0.5\n)\n\nmap_layer.add_to(aoi_map_bbox)\n\naoi_map_bbox",
    "crumbs": [
      "Data Usage Notebooks",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "Wetland Methane Emissions, LPJ-wsl Model"
    ]
  },
  {
    "objectID": "user_data_notebooks/lpjwsl-wetlandch4-grid-v1_User_Notebook.html#summary",
    "href": "user_data_notebooks/lpjwsl-wetlandch4-grid-v1_User_Notebook.html#summary",
    "title": "Wetland Methane Emissions, LPJ-wsl Model",
    "section": "Summary",
    "text": "Summary\nIn this notebook, we have successfully explored, analyzed, and visualized the STAC collection for wetland methane emissions.",
    "crumbs": [
      "Data Usage Notebooks",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "Wetland Methane Emissions, LPJ-wsl Model"
    ]
  },
  {
    "objectID": "user_data_notebooks/odiac-ffco2-monthgrid-v2022_User_Notebook.html",
    "href": "user_data_notebooks/odiac-ffco2-monthgrid-v2022_User_Notebook.html",
    "title": "ODIAC Fossil Fuel CO₂ Emissions",
    "section": "",
    "text": "Identify available dates and temporal frequency of observations for the given collection using the GHGC API /stac endpoint. Collection processed in this notebook is ODIAC CO₂ emissions version 2022.\nPass the STAC item into raster API /stac/tilejson.json endpoint\nWe’ll visualize two tiles (side-by-side) allowing for comparison of each of the time points using folium.plugins.DualMap\nAfter the visualization, we’ll perform zonal statistics for a given polygon.",
    "crumbs": [
      "Data Usage Notebooks",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "ODIAC Fossil Fuel CO₂ Emissions"
    ]
  },
  {
    "objectID": "user_data_notebooks/odiac-ffco2-monthgrid-v2022_User_Notebook.html#approach",
    "href": "user_data_notebooks/odiac-ffco2-monthgrid-v2022_User_Notebook.html#approach",
    "title": "ODIAC Fossil Fuel CO₂ Emissions",
    "section": "",
    "text": "Identify available dates and temporal frequency of observations for the given collection using the GHGC API /stac endpoint. Collection processed in this notebook is ODIAC CO₂ emissions version 2022.\nPass the STAC item into raster API /stac/tilejson.json endpoint\nWe’ll visualize two tiles (side-by-side) allowing for comparison of each of the time points using folium.plugins.DualMap\nAfter the visualization, we’ll perform zonal statistics for a given polygon.",
    "crumbs": [
      "Data Usage Notebooks",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "ODIAC Fossil Fuel CO₂ Emissions"
    ]
  },
  {
    "objectID": "user_data_notebooks/odiac-ffco2-monthgrid-v2022_User_Notebook.html#about-the-data",
    "href": "user_data_notebooks/odiac-ffco2-monthgrid-v2022_User_Notebook.html#about-the-data",
    "title": "ODIAC Fossil Fuel CO₂ Emissions",
    "section": "About the Data",
    "text": "About the Data\nThe Open-Data Inventory for Anthropogenic Carbon dioxide (ODIAC) is a high-spatial resolution global emission data product of CO₂ emissions from fossil fuel combustion (Oda and Maksyutov, 2011). ODIAC pioneered the combined use of space-based nighttime light data and individual power plant emission/location profiles to estimate the global spatial extent of fossil fuel CO₂ emissions. With the innovative emission modeling approach, ODIAC achieved the fine picture of global fossil fuel CO₂ emissions at a 1x1km.",
    "crumbs": [
      "Data Usage Notebooks",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "ODIAC Fossil Fuel CO₂ Emissions"
    ]
  },
  {
    "objectID": "user_data_notebooks/odiac-ffco2-monthgrid-v2022_User_Notebook.html#querying-the-stac-api",
    "href": "user_data_notebooks/odiac-ffco2-monthgrid-v2022_User_Notebook.html#querying-the-stac-api",
    "title": "ODIAC Fossil Fuel CO₂ Emissions",
    "section": "Querying the STAC API",
    "text": "Querying the STAC API\n\nimport requests\nfrom folium import Map, TileLayer\nfrom pystac_client import Client\n\n\n# Provide STAC and RASTER API endpoints\nSTAC_API_URL = \"http://ghg.center/api/stac\"\nRASTER_API_URL = \"https://ghg.center/api/raster\"\n\n#Please use the collection name similar to the one used in STAC collection.\n# Name of the collection for ODIAC dataset. \ncollection_name = \"odiac-ffco2-monthgrid-v2022\"\n\n\n# Fetching the collection from STAC collections using appropriate endpoint.\ncollection = requests.get(f\"{STAC_API_URL}/collections/{collection_name}\").json()\ncollection\n\nExamining the contents of our collection under summaries we see that the data is available from January 2000 to December 2021. By looking at the dashboard:time density we observe that the periodic frequency of these observations is monthly.\n\ndef get_item_count(collection_id):\n    count = 0\n    items_url = f\"{STAC_API_URL}/collections/{collection_id}/items\"\n\n    while True:\n        response = requests.get(items_url)\n\n        if not response.ok:\n            print(\"error getting items\")\n            exit()\n\n        stac = response.json()\n        count += int(stac[\"context\"].get(\"returned\", 0))\n        next = [link for link in stac[\"links\"] if link[\"rel\"] == \"next\"]\n\n        if not next:\n            break\n        items_url = next[0][\"href\"]\n\n    return count\n\n\n# Check total number of items available\nnumber_of_items = get_item_count(collection_name)\nitems = requests.get(f\"{STAC_API_URL}/collections/{collection_name}/items?limit={number_of_items}\").json()[\"features\"]\nprint(f\"Found {len(items)} items\")\n\n\nitems[0]\n\nThis makes sense as there are 22 years between 2000 - 2021, with 12 months per year, meaning 264 records in total.\nBelow, we are entering the minimum and maximum values to provide our upper and lower bounds in rescale_values.",
    "crumbs": [
      "Data Usage Notebooks",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "ODIAC Fossil Fuel CO₂ Emissions"
    ]
  },
  {
    "objectID": "user_data_notebooks/odiac-ffco2-monthgrid-v2022_User_Notebook.html#exploring-changes-in-carbon-dioxide-co₂-levels-using-the-raster-api",
    "href": "user_data_notebooks/odiac-ffco2-monthgrid-v2022_User_Notebook.html#exploring-changes-in-carbon-dioxide-co₂-levels-using-the-raster-api",
    "title": "ODIAC Fossil Fuel CO₂ Emissions",
    "section": "Exploring Changes in Carbon Dioxide (CO₂) levels using the Raster API",
    "text": "Exploring Changes in Carbon Dioxide (CO₂) levels using the Raster API\nWe will explore changes in fossil fuel emissions in urban egions. In this notebook, we’ll explore the impacts of these emissions and explore these changes over time. We’ll then visualize the outputs on a map using folium.\n\n# to access the year value from each item more easily, this will let us query more explicity by year and month (e.g., 2020-02)\nitems = {item[\"properties\"][\"start_datetime\"][:7]: item for item in items} \nasset_name = \"co2-emissions\"\n\n\nrescale_values = {\"max\":items[list(items.keys())[0]][\"assets\"][asset_name][\"raster:bands\"][0][\"histogram\"][\"max\"], \"min\":items[list(items.keys())[0]][\"assets\"][asset_name][\"raster:bands\"][0][\"histogram\"][\"min\"]}\n\nNow we will pass the item id, collection name, and rescaling_factor to the Raster API endpoint. We will do this twice, once for January 2020 and again for January 2000, so that we can visualize each event independently.\n\ncolor_map = \"rainbow\" # please select the color ramp from matplotlib library.\njanuary_2020_tile = requests.get(\n    f\"{RASTER_API_URL}/stac/tilejson.json?collection={items['2020-01']['collection']}&item={items['2020-01']['id']}\"\n    f\"&assets={asset_name}\"\n    f\"&color_formula=gamma+r+1.05&colormap_name={color_map}\"\n    f\"&rescale={rescale_values['min']},{rescale_values['max']}\", \n).json()\njanuary_2020_tile\n\n\njanuary_2000_tile = requests.get(\n    f\"{RASTER_API_URL}/stac/tilejson.json?collection={items['2000-01']['collection']}&item={items['2000-01']['id']}\"\n    f\"&assets={asset_name}\"\n    f\"&color_formula=gamma+r+1.05&colormap_name={color_map}\"\n    f\"&rescale={rescale_values['min']},{rescale_values['max']}\", \n).json()\njanuary_2000_tile",
    "crumbs": [
      "Data Usage Notebooks",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "ODIAC Fossil Fuel CO₂ Emissions"
    ]
  },
  {
    "objectID": "user_data_notebooks/odiac-ffco2-monthgrid-v2022_User_Notebook.html#visualizing-co₂-emissions",
    "href": "user_data_notebooks/odiac-ffco2-monthgrid-v2022_User_Notebook.html#visualizing-co₂-emissions",
    "title": "ODIAC Fossil Fuel CO₂ Emissions",
    "section": "Visualizing CO₂ emissions",
    "text": "Visualizing CO₂ emissions\n\n# We'll import folium to map and folium.plugins to allow mapping side-by-side\nimport folium\nimport folium.plugins\n\n# Set initial zoom and center of map for CO₂ Layer\nmap_ = folium.plugins.DualMap(location=(34, -118), zoom_start=6)\n\n# December 2001\nmap_layer_2020 = TileLayer(\n    tiles=january_2020_tile[\"tiles\"][0],\n    attr=\"GHG\",\n    opacity=0.8,\n)\nmap_layer_2020.add_to(map_.m1)\n\n# December 2021\nmap_layer_2000 = TileLayer(\n    tiles=january_2000_tile[\"tiles\"][0],\n    attr=\"GHG\",\n    opacity=0.8,\n)\nmap_layer_2000.add_to(map_.m2)\n\n# visualising the map\nmap_",
    "crumbs": [
      "Data Usage Notebooks",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "ODIAC Fossil Fuel CO₂ Emissions"
    ]
  },
  {
    "objectID": "user_data_notebooks/odiac-ffco2-monthgrid-v2022_User_Notebook.html#section",
    "href": "user_data_notebooks/odiac-ffco2-monthgrid-v2022_User_Notebook.html#section",
    "title": "ODIAC Fossil Fuel CO₂ Emissions",
    "section": "",
    "text": "# Texas, USA\ntexas_aoi = {\n    \"type\": \"Feature\",\n    \"properties\": {},\n    \"geometry\": {\n        \"coordinates\": [\n            [\n                # [13.686159004559698, -21.700046934333145],\n                # [13.686159004559698, -23.241974326585833],\n                # [14.753560168039911, -23.241974326585833],\n                # [14.753560168039911, -21.700046934333145],\n                # [13.686159004559698, -21.700046934333145],\n                [-95, 29],\n                [-95, 33],\n                [-104, 33],\n                [-104,29],\n                [-95, 29]\n            ]\n        ],\n        \"type\": \"Polygon\",\n    },\n}\n\n\n# We'll plug in the coordinates for a location\n# central to the study area and a reasonable zoom level\n\nimport folium\n\naoi_map = Map(\n    tiles=\"OpenStreetMap\",\n    location=[\n        30,-100\n    ],\n    zoom_start=6,\n)\n\nfolium.GeoJson(texas_aoi, name=\"Texas, USA\").add_to(aoi_map)\naoi_map\n\n\n# Check total number of items available\nitems = requests.get(\n    f\"{STAC_API_URL}/collections/{collection_name}/items?limit=300\"\n).json()[\"features\"]\nprint(f\"Found {len(items)} items\")\n\n\n# Explore one item to see what it contains\nitems[0]\n\n\n# the bounding box should be passed to the geojson param as a geojson Feature or FeatureCollection\ndef generate_stats(item, geojson):\n    result = requests.post(\n        f\"{RASTER_API_URL}/cog/statistics\",\n        params={\"url\": item[\"assets\"][asset_name][\"href\"]},\n        json=geojson,\n    ).json()\n    return {\n        **result[\"properties\"],\n        \"start_datetime\": item[\"properties\"][\"start_datetime\"][:7],\n    }\n\nWith the function above we can generate the statistics for the AOI.\n\n%%time\nstats = [generate_stats(item, texas_aoi) for item in items]\n\n\nstats[0]\n\n\nimport pandas as pd\n\n\ndef clean_stats(stats_json) -&gt; pd.DataFrame:\n    df = pd.json_normalize(stats_json)\n    df.columns = [col.replace(\"statistics.b1.\", \"\") for col in df.columns]\n    df[\"date\"] = pd.to_datetime(df[\"start_datetime\"])\n    return df\n\n\ndf = clean_stats(stats)\ndf.head(5)",
    "crumbs": [
      "Data Usage Notebooks",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "ODIAC Fossil Fuel CO₂ Emissions"
    ]
  },
  {
    "objectID": "user_data_notebooks/odiac-ffco2-monthgrid-v2022_User_Notebook.html#visualizing-the-data-as-a-time-series",
    "href": "user_data_notebooks/odiac-ffco2-monthgrid-v2022_User_Notebook.html#visualizing-the-data-as-a-time-series",
    "title": "ODIAC Fossil Fuel CO₂ Emissions",
    "section": "Visualizing the Data as a Time Series",
    "text": "Visualizing the Data as a Time Series\nWe can now explore the ODIAC fossil fuel emission time series available (January 2000 -December 2021) for the Texas, Dallas area of USA. We can plot the data set using the code below:\n\nimport matplotlib.pyplot as plt\n\nfig = plt.figure(figsize=(20, 10))\n\n\nplt.plot(\n    df[\"date\"],\n    df[\"max\"],\n    color=\"red\",\n    linestyle=\"-\",\n    linewidth=0.5,\n    label=\"Max monthly CO₂ emissions\",\n)\n\nplt.legend()\nplt.xlabel(\"Years\")\nplt.ylabel(\"CO2 emissions gC/m2/d\")\nplt.title(\"CO2 emission Values for Texas, Dallas (2000-2021)\")\n\n\nprint(items[2][\"properties\"][\"start_datetime\"])\n\n\noctober_tile = requests.get(\n    f\"{RASTER_API_URL}/stac/tilejson.json?collection={items[2]['collection']}&item={items[2]['id']}\"\n    f\"&assets={asset_name}\"\n    f\"&color_formula=gamma+r+1.05&colormap_name={color_map}\"\n    f\"&rescale={rescale_values['min']},{rescale_values['max']}\",\n).json()\noctober_tile\n\n\n# Use bbox initial zoom and map\n# Set up a map located w/in event bounds\nimport folium\n\naoi_map_bbox = Map(\n    tiles=\"OpenStreetMap\",\n    location=[\n        30,-100\n    ],\n    zoom_start=8,\n)\n\nmap_layer = TileLayer(\n    tiles=october_tile[\"tiles\"][0],\n    attr=\"GHG\", opacity = 0.5\n)\n\nmap_layer.add_to(aoi_map_bbox)\n\naoi_map_bbox",
    "crumbs": [
      "Data Usage Notebooks",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "ODIAC Fossil Fuel CO₂ Emissions"
    ]
  },
  {
    "objectID": "user_data_notebooks/odiac-ffco2-monthgrid-v2022_User_Notebook.html#summary",
    "href": "user_data_notebooks/odiac-ffco2-monthgrid-v2022_User_Notebook.html#summary",
    "title": "ODIAC Fossil Fuel CO₂ Emissions",
    "section": "Summary",
    "text": "Summary\nIn this notebook we have successfully explored, analysed and visualized STAC collecetion for ODIAC C02 fossisl fuel emission (2022).",
    "crumbs": [
      "Data Usage Notebooks",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "ODIAC Fossil Fuel CO₂ Emissions"
    ]
  },
  {
    "objectID": "user_data_notebooks/oco2geos-co2-daygrid-v10r_User_Notebook.html",
    "href": "user_data_notebooks/oco2geos-co2-daygrid-v10r_User_Notebook.html",
    "title": "OCO-2 GEOS Column CO₂ Concentrations",
    "section": "",
    "text": "Identify available dates and temporal frequency of observations for the given collection using the GHGC API /stac endpoint. The collection processed in this notebook is the OCO-2 GEOS Column CO₂ Concentrations data product.\nPass the STAC item into the raster API /stac/tilejson.json endpoint.\nUsing folium.plugins.DualMap, visualize two tiles (side-by-side), allowing time point comparison.\nAfter the visualization, perform zonal statistics for a given polygon.",
    "crumbs": [
      "Data Usage Notebooks",
      "Greenhouse Gas Concentrations",
      "OCO-2 GEOS Column CO₂ Concentrations"
    ]
  },
  {
    "objectID": "user_data_notebooks/oco2geos-co2-daygrid-v10r_User_Notebook.html#approach",
    "href": "user_data_notebooks/oco2geos-co2-daygrid-v10r_User_Notebook.html#approach",
    "title": "OCO-2 GEOS Column CO₂ Concentrations",
    "section": "",
    "text": "Identify available dates and temporal frequency of observations for the given collection using the GHGC API /stac endpoint. The collection processed in this notebook is the OCO-2 GEOS Column CO₂ Concentrations data product.\nPass the STAC item into the raster API /stac/tilejson.json endpoint.\nUsing folium.plugins.DualMap, visualize two tiles (side-by-side), allowing time point comparison.\nAfter the visualization, perform zonal statistics for a given polygon.",
    "crumbs": [
      "Data Usage Notebooks",
      "Greenhouse Gas Concentrations",
      "OCO-2 GEOS Column CO₂ Concentrations"
    ]
  },
  {
    "objectID": "user_data_notebooks/oco2geos-co2-daygrid-v10r_User_Notebook.html#about-the-data",
    "href": "user_data_notebooks/oco2geos-co2-daygrid-v10r_User_Notebook.html#about-the-data",
    "title": "OCO-2 GEOS Column CO₂ Concentrations",
    "section": "About the Data",
    "text": "About the Data\nIn July 2014, NASA successfully launched the first dedicated Earth remote sensing satellite to study atmospheric carbon dioxide (CO₂) from space. The Orbiting Carbon Observatory-2 (OCO-2) is an exploratory science mission designed to collect space-based global measurements of atmospheric CO₂ with the precision, resolution, and coverage needed to characterize sources and sinks (fluxes) on regional scales (≥1000 km). This dataset provides global gridded, daily column-averaged carbon dioxide (XCO₂) concentrations from January 1, 2015 - February 28, 2022. The data are derived from OCO-2 observations that were input to the Goddard Earth Observing System (GEOS) Constituent Data Assimilation System (CoDAS), a modeling and data assimilation system maintained by NASA’s Global Modeling and Assimilation Office (GMAO). Concentrations are measured in moles of carbon dioxide per mole of dry air (mol CO₂/mol dry) at a spatial resolution of 0.5° x 0.625°. Data assimilation synthesizes simulations and observations, adjusting modeled atmospheric constituents like CO₂ to reflect observed values. With the support of NASA’s Carbon Monitoring System (CMS) Program and the OCO Science Team, this dataset was produced as part of the OCO-2 mission which provides the highest quality space-based XCO₂ retrievals to date.",
    "crumbs": [
      "Data Usage Notebooks",
      "Greenhouse Gas Concentrations",
      "OCO-2 GEOS Column CO₂ Concentrations"
    ]
  },
  {
    "objectID": "user_data_notebooks/oco2geos-co2-daygrid-v10r_User_Notebook.html#querying-the-stac-api",
    "href": "user_data_notebooks/oco2geos-co2-daygrid-v10r_User_Notebook.html#querying-the-stac-api",
    "title": "OCO-2 GEOS Column CO₂ Concentrations",
    "section": "Querying the STAC API",
    "text": "Querying the STAC API\n\n# Provide STAC and RASTER API endpoints\nSTAC_API_URL = \"http://ghg.center/api/stac\"\nRASTER_API_URL = \"https://ghg.center/api/raster\"\n\n# Please use the collection name similar to the one used in STAC collection.\n# Name of the collection for OCO-2 GEOS Column CO₂ Concentrations. \ncollection_name = \"oco2geos-co2-daygrid-v10r\"\n\n\n# Fetching the collection from STAC collections using appropriate endpoint.\ncollection = requests.get(f\"{STAC_API_URL}/collections/{collection_name}\").json()\ncollection\n\nExamining the contents of our collection under the temporal variable, we see that the data is available from January 2015 to February 2022. By looking at the dashboard:time density, we can see that these observations are collected daily.\n\ndef get_item_count(collection_id):\n    count = 0\n    items_url = f\"{STAC_API_URL}/collections/{collection_id}/items\"\n\n    while True:\n        response = requests.get(items_url)\n\n        if not response.ok:\n            print(\"error getting items\")\n            exit()\n\n        stac = response.json()\n        count += int(stac[\"context\"].get(\"returned\", 0))\n        next = [link for link in stac[\"links\"] if link[\"rel\"] == \"next\"]\n\n        if not next:\n            break\n        items_url = next[0][\"href\"]\n\n    return count\n\n\n# Check total number of items available\nnumber_of_items = get_item_count(collection_name)\nitems = requests.get(f\"{STAC_API_URL}/collections/{collection_name}/items?limit={number_of_items}\").json()[\"features\"]\nprint(f\"Found {len(items)} items\")\n\n\n# Examining the first item in the collection\nitems[0]\n\nBelow, we enter minimum and maximum values to provide our upper and lower bounds in rescale_values.",
    "crumbs": [
      "Data Usage Notebooks",
      "Greenhouse Gas Concentrations",
      "OCO-2 GEOS Column CO₂ Concentrations"
    ]
  },
  {
    "objectID": "user_data_notebooks/oco2geos-co2-daygrid-v10r_User_Notebook.html#exploring-changes-in-column-averaged-xco₂-concentrations-levels-using-the-raster-api",
    "href": "user_data_notebooks/oco2geos-co2-daygrid-v10r_User_Notebook.html#exploring-changes-in-column-averaged-xco₂-concentrations-levels-using-the-raster-api",
    "title": "OCO-2 GEOS Column CO₂ Concentrations",
    "section": "Exploring Changes in Column-Averaged XCO₂ Concentrations Levels Using the Raster API",
    "text": "Exploring Changes in Column-Averaged XCO₂ Concentrations Levels Using the Raster API\nIn this notebook, we will explore the temporal impacts of CO₂ emissions. We will visualize the outputs on a map using folium.\n\n# To access the year value from each item more easily, this will let us query more explicitly by year and month (e.g., 2020-02)\nitems = {item[\"properties\"][\"datetime\"]: item for item in items} \nasset_name = \"xco2\" #fossil fuel\n\n\n# Fetching the min and max values for a specific item\nrescale_values = {\"max\":items[list(items.keys())[0]][\"assets\"][asset_name][\"raster:bands\"][0][\"histogram\"][\"max\"], \"min\":items[list(items.keys())[0]][\"assets\"][asset_name][\"raster:bands\"][0][\"histogram\"][\"min\"]}\n\nNow, we will pass the item id, collection name, and rescaling_factor to the Raster API endpoint. We will do this twice, once for 2022-02-08 and again for 2022-01-27, so that we can visualize each event independently.\n\ncolor_map = \"magma\"\noco2_1 = requests.get(\n    f\"{RASTER_API_URL}/stac/tilejson.json?collection={items[list(items.keys())[0]]['collection']}&item={items[list(items.keys())[0]]['id']}\"\n    f\"&assets={asset_name}\"\n    f\"&color_formula=gamma+r+1.05&colormap_name={color_map}\"\n    f\"&rescale={rescale_values['min']},{rescale_values['max']}\", \n).json()\noco2_1\n\n\noco2_2 = requests.get(\n    f\"{RASTER_API_URL}/stac/tilejson.json?collection={items[list(items.keys())[1]]['collection']}&item={items[list(items.keys())[1]]['id']}\"\n    f\"&assets={asset_name}\"\n    f\"&color_formula=gamma+r+1.05&colormap_name={color_map}\"\n    f\"&rescale={rescale_values['min']},{rescale_values['max']}\", \n).json()\noco2_2",
    "crumbs": [
      "Data Usage Notebooks",
      "Greenhouse Gas Concentrations",
      "OCO-2 GEOS Column CO₂ Concentrations"
    ]
  },
  {
    "objectID": "user_data_notebooks/oco2geos-co2-daygrid-v10r_User_Notebook.html#visualizing-daily-column-averaged-xco₂-concentrations",
    "href": "user_data_notebooks/oco2geos-co2-daygrid-v10r_User_Notebook.html#visualizing-daily-column-averaged-xco₂-concentrations",
    "title": "OCO-2 GEOS Column CO₂ Concentrations",
    "section": "Visualizing Daily Column-Averaged XCO₂ Concentrations",
    "text": "Visualizing Daily Column-Averaged XCO₂ Concentrations\n\n# We will import folium to map and folium.plugins to allow mapping side-by-side\nimport folium\nimport folium.plugins\n\n# Set initial zoom and center of map for XCO₂ Layer\n# Centre of map [latitude,longitude]\nmap_ = folium.plugins.DualMap(location=(34, -118), zoom_start=6)\n\n\nmap_layer_2020 = TileLayer(\n    tiles=oco2_1[\"tiles\"][0],\n    attr=\"GHG\",\n    opacity=0.5,\n)\nmap_layer_2020.add_to(map_.m1)\n\nmap_layer_2019 = TileLayer(\n    tiles=oco2_2[\"tiles\"][0],\n    attr=\"GHG\",\n    opacity=0.5,\n)\nmap_layer_2019.add_to(map_.m2)\n\n# visualising the map\nmap_",
    "crumbs": [
      "Data Usage Notebooks",
      "Greenhouse Gas Concentrations",
      "OCO-2 GEOS Column CO₂ Concentrations"
    ]
  },
  {
    "objectID": "user_data_notebooks/oco2geos-co2-daygrid-v10r_User_Notebook.html#visualizing-the-data-as-a-time-series",
    "href": "user_data_notebooks/oco2geos-co2-daygrid-v10r_User_Notebook.html#visualizing-the-data-as-a-time-series",
    "title": "OCO-2 GEOS Column CO₂ Concentrations",
    "section": "Visualizing the Data as a Time Series",
    "text": "Visualizing the Data as a Time Series\nWe can now explore the XCO₂ concentrations time series (January 1, 2015 - February 28, 2022) available for the Dallas, Texas area of the U.S. We can plot the data set using the code below:\n\nimport matplotlib.pyplot as plt\n\nfig = plt.figure(figsize=(20, 10))\n\n\nplt.plot(\n    df[\"datetime\"],\n    df[\"max\"],\n    color=\"red\",\n    linestyle=\"-\",\n    linewidth=0.5,\n    label=\"CO₂ concentrations\",\n)\n\nplt.legend()\nplt.xlabel(\"Years\")\nplt.ylabel(\"CO2 concentrations ppm\")\nplt.title(\"CO₂ concentrations Values for Texas, Dallas (Jan 2015- Feb 2022)\")\n\n\nprint(items[2][\"properties\"][\"datetime\"])\n\n\noco2_3 = requests.get(\n    f\"{RASTER_API_URL}/stac/tilejson.json?collection={items[2]['collection']}&item={items[2]['id']}\"\n    f\"&assets={asset_name}\"\n    f\"&color_formula=gamma+r+1.05&colormap_name={color_map}\"\n    f\"&rescale={rescale_values['min']},{rescale_values['max']}\",\n).json()\noco2_3\n\n\n# Use bbox initial zoom and map\n# Set up a map located w/in event bounds\nimport folium\n\naoi_map_bbox = Map(\n    tiles=\"OpenStreetMap\",\n    location=[\n        30,-100\n    ],\n    zoom_start=6.8,\n)\n\nmap_layer = TileLayer(\n    tiles=oco2_3[\"tiles\"][0],\n    attr=\"GHG\", opacity = 0.7\n)\n\nmap_layer.add_to(aoi_map_bbox)\n\naoi_map_bbox",
    "crumbs": [
      "Data Usage Notebooks",
      "Greenhouse Gas Concentrations",
      "OCO-2 GEOS Column CO₂ Concentrations"
    ]
  },
  {
    "objectID": "user_data_notebooks/oco2geos-co2-daygrid-v10r_User_Notebook.html#summary",
    "href": "user_data_notebooks/oco2geos-co2-daygrid-v10r_User_Notebook.html#summary",
    "title": "OCO-2 GEOS Column CO₂ Concentrations",
    "section": "Summary",
    "text": "Summary\nIn this notebook, we have successfully explored, analyzed, and visualized the STAC collection for OCO-2 GEOS Column CO₂ Concentrations.",
    "crumbs": [
      "Data Usage Notebooks",
      "Greenhouse Gas Concentrations",
      "OCO-2 GEOS Column CO₂ Concentrations"
    ]
  },
  {
    "objectID": "cog_transformation/epa-ch4emission-monthgrid-v2.html",
    "href": "cog_transformation/epa-ch4emission-monthgrid-v2.html",
    "title": "Gridded Anthropogenic Methane Emissions Inventory",
    "section": "",
    "text": "This script was used to transform the Gridded Anthropogenic Methane Emissions Inventory dataset from netCDF to Cloud Optimized GeoTIFF (COG) format for display in the Greenhouse Gas (GHG) Center.\n\nimport os\nimport xarray\nimport re\nimport pandas as pd\nimport json\nimport tempfile\nimport boto3\nfrom datetime import datetime\nfrom dateutil.relativedelta import relativedelta\n\n\nsession = boto3.session.Session()\ns3_client = session.client(\"s3\")\nbucket_name = (\n    \"ghgc-data-store-dev\"  # S3 bucket where the COGs are stored after transformation\n)\nFOLDER_NAME = \"epa_emissions/monthly_scale\"\ns3_folder_name = \"epa-emissions-monthly-scale-factors\"\n\nfiles_processed = pd.DataFrame(\n    columns=[\"file_name\", \"COGs_created\"]\n)  # A dataframe to keep track of the files that we have transformed into COGs\n\n# Reading the raw netCDF files from local machine\nfor name in os.listdir(FOLDER_NAME):\n    xds = xarray.open_dataset(f\"{FOLDER_NAME}/{name}\", engine=\"netcdf4\")\n    xds = xds.assign_coords(lon=(((xds.lon + 180) % 360) - 180)).sortby(\"lon\")\n    variable = [var for var in xds.data_vars]\n    filename = name.split(\"/ \")[-1]\n    filename_elements = re.split(\"[_ .]\", filename)\n    start_time = datetime(int(filename_elements[-2]), 1, 1)\n\n    for time_increment in range(0, len(xds.time)):\n        for var in variable:\n            filename = name.split(\"/ \")[-1]\n            filename_elements = re.split(\"[_ .]\", filename)\n            data = getattr(xds.isel(time=time_increment), var)\n            data = data.isel(lat=slice(None, None, -1))\n            data.rio.set_spatial_dims(\"lon\", \"lat\", inplace=True)\n            data.rio.write_crs(\"epsg:4326\", inplace=True)\n            date = start_time + relativedelta(months=+time_increment)\n\n            # # insert date of generated COG into filename\n            filename_elements.pop()\n            filename_elements[-1] = date.strftime(\"%Y%m\")\n            filename_elements.insert(2, var)\n            cog_filename = \"_\".join(filename_elements)\n            # # add extension\n            cog_filename = f\"{cog_filename}.tif\"\n\n            with tempfile.NamedTemporaryFile() as temp_file:\n                data.rio.to_raster(\n                    temp_file.name,\n                    driver=\"COG\",\n                )\n                s3_client.upload_file(\n                    Filename=temp_file.name,\n                    Bucket=bucket_name,\n                    Key=f\"{s3_folder_name}/{cog_filename}\",\n                )\n\n            files_processed = files_processed._append(\n                {\"file_name\": name, \"COGs_created\": cog_filename},\n                ignore_index=True,\n            )\n\n            print(f\"Generated and saved COG: {cog_filename}\")\n\n# Generate the json file with the metadata that is present in the netCDF files.\nwith tempfile.NamedTemporaryFile(mode=\"w+\") as fp:\n    json.dump(xds.attrs, fp)\n    json.dump({\"data_dimensions\": dict(xds.dims)}, fp)\n    json.dump({\"data_variables\": list(xds.data_vars)}, fp)\n    fp.flush()\n\n    s3_client.upload_file(\n        Filename=fp.name,\n        Bucket=bucket_name,\n        Key=f\"{s3_folder_name}/metadata.json\",\n    )\n\n# creating the csv file with the names of files transformed.\nfiles_processed.to_csv(\n    f\"s3://{bucket_name}/{s3_folder_name}/files_converted.csv\",\n)\nprint(\"Done generating COGs\")\n\n\n\n\n Back to top"
  },
  {
    "objectID": "cog_transformation/lpjwsl-wetlandch4-daygrid-v1.html",
    "href": "cog_transformation/lpjwsl-wetlandch4-daygrid-v1.html",
    "title": "Wetland Methane Emissions, LPJ-wsl Model",
    "section": "",
    "text": "This script was used to transform the Wetland Methane Emissions, LPJ-wsl Model dataset from netCDF to Cloud Optimized GeoTIFF (COG) format for display in the Greenhouse Gas (GHG) Center.\n\nimport os\nimport xarray\nimport re\nimport pandas as pd\nimport json\nimport tempfile\nimport boto3\nfrom datetime import datetime, timedelta\n\n\nsession = boto3.session.Session()\ns3_client = session.client(\"s3\")\nbucket_name = (\n    \"ghgc-data-store-dev\"  # S3 bucket where the COGs are stored after transformation\n)\nFOLDER_NAME = \"NASA_GSFC_ch4_wetlands_daily\"\ndirectory = \"ch4_wetlands_daily\"\n\nfiles_processed = pd.DataFrame(\n    columns=[\"file_name\", \"COGs_created\"]\n)  # A dataframe to keep track of the files that we have transformed into COGs\n\n# Reading the raw netCDF files from local machine\nfor name in os.listdir(directory):\n    xds = xarray.open_dataset(\n        f\"{directory}/{name}\", engine=\"netcdf4\", decode_times=False\n    )\n    xds = xds.assign_coords(longitude=(((xds.longitude + 180) % 360) - 180)).sortby(\n        \"longitude\"\n    )\n    variable = [var for var in xds.data_vars]\n    filename = name.split(\"/ \")[-1]\n    filename_elements = re.split(\"[_ .]\", filename)\n    start_time = datetime(int(filename_elements[-2]), 1, 1)\n\n    for time_increment in range(0, len(xds.time)):\n        for var in variable:\n            filename = name.split(\"/ \")[-1]\n            filename_elements = re.split(\"[_ .]\", filename)\n            data = getattr(xds.isel(time=time_increment), var)\n            data = data.isel(latitude=slice(None, None, -1))\n            data = data * 1000\n            data.rio.set_spatial_dims(\"longitude\", \"latitude\", inplace=True)\n            data.rio.write_crs(\"epsg:4326\", inplace=True)\n            date = start_time + timedelta(hours=data.time.item(0))\n\n            # # insert date of generated COG into filename\n            filename_elements.pop()\n            filename_elements[-1] = date.strftime(\"%Y%m%d\")\n            filename_elements.insert(2, var)\n            cog_filename = \"_\".join(filename_elements)\n            # # add extension\n            cog_filename = f\"{cog_filename}.tif\"\n\n            with tempfile.NamedTemporaryFile() as temp_file:\n                data.rio.to_raster(\n                    temp_file.name,\n                    driver=\"COG\",\n                )\n                s3_client.upload_file(\n                    Filename=temp_file.name,\n                    Bucket=bucket_name,\n                    Key=f\"{FOLDER_NAME}/{cog_filename}\",\n                )\n\n            files_processed = files_processed._append(\n                {\"file_name\": name, \"COGs_created\": cog_filename},\n                ignore_index=True,\n            )\n\n            print(f\"Generated and saved COG: {cog_filename}\")\n\n# Generate the json file with the metadata that is present in the netCDF files.\nwith tempfile.NamedTemporaryFile(mode=\"w+\") as fp:\n    json.dump(xds.attrs, fp)\n    json.dump({\"data_dimensions\": dict(xds.dims)}, fp)\n    json.dump({\"data_variables\": list(xds.data_vars)}, fp)\n    fp.flush()\n\n    s3_client.upload_file(\n        Filename=fp.name,\n        Bucket=bucket_name,\n        Key=f\"{FOLDER_NAME}/metadata.json\",\n    )\n\n# creating the csv file with the names of files transformed.\nfiles_processed.to_csv(\n    f\"s3://{bucket_name}/{FOLDER_NAME}/files_converted.csv\",\n)\nprint(\"Done generating COGs\")\n\n\n\n\n Back to top"
  },
  {
    "objectID": "cog_transformation/casagfed-carbonflux-monthgrid-v3.html",
    "href": "cog_transformation/casagfed-carbonflux-monthgrid-v3.html",
    "title": "CASA-GFED3 Land Carbon Flux",
    "section": "",
    "text": "Code used to transform CASA-GFED3 Land Carbon Flux data from netcdf to Cloud Optimized Geotiff.\n\nimport os\nimport xarray\nimport re\nimport pandas as pd\nimport json\nimport tempfile\nimport boto3\n\n\nsession = boto3.session.Session()\ns3_client = session.client(\"s3\")\nbucket_name = \"ghgc-data-store-dev\"\ndate_fmt = \"%Y%m\"\n\nfiles_processed = pd.DataFrame(columns=[\"file_name\", \"COGs_created\"])\nfor name in os.listdir(\"geoscarb\"):\n    xds = xarray.open_dataset(\n        f\"geoscarb/{name}\",\n        engine=\"netcdf4\",\n    )\n    xds = xds.assign_coords(\n        longitude=(((xds.longitude + 180) % 360) - 180)\n    ).sortby(\"longitude\")\n    variable = [var for var in xds.data_vars]\n\n    for time_increment in range(0, len(xds.time)):\n        for var in variable[:-1]:\n            filename = name.split(\"/ \")[-1]\n            filename_elements = re.split(\"[_ .]\", filename)\n            data = getattr(xds.isel(time=time_increment), var)\n            data = data.isel(latitude=slice(None, None, -1))\n            data.rio.set_spatial_dims(\"longitude\", \"latitude\", inplace=True)\n            data.rio.write_crs(\"epsg:4326\", inplace=True)\n\n            date = data.time.dt.strftime(date_fmt).item(0)\n            # # insert date of generated COG into filename\n            filename_elements.pop()\n            filename_elements[-1] = date\n            filename_elements.insert(2, var)\n            cog_filename = \"_\".join(filename_elements)\n            # # add extension\n            cog_filename = f\"{cog_filename}.tif\"\n\n            with tempfile.NamedTemporaryFile() as temp_file:\n                data.rio.to_raster(\n                    temp_file.name,\n                    driver=\"COG\",\n                )\n                s3_client.upload_file(\n                    Filename=temp_file.name,\n                    Bucket=bucket_name,\n                    Key=f\"GEOS-Carbs/{cog_filename}\",\n                )\n\n            files_processed = files_processed._append(\n                {\"file_name\": name, \"COGs_created\": cog_filename},\n                ignore_index=True,\n            )\n\n            print(f\"Generated and saved COG: {cog_filename}\")\n\nwith tempfile.NamedTemporaryFile(mode=\"w+\") as fp:\n    json.dump(xds.attrs, fp)\n    json.dump({\"data_dimensions\": dict(xds.dims)}, fp)\n    json.dump({\"data_variables\": list(xds.data_vars)}, fp)\n    fp.flush()\n\n    s3_client.upload_file(\n        Filename=fp.name,\n        Bucket=bucket_name,\n        Key=\"GEOS-Carbs/metadata.json\",\n    )\nfiles_processed.to_csv(\n    f\"s3://{bucket_name}/GEOS-Carbs/files_converted.csv\",\n)\nprint(\"Done generating COGs\")\n\n\n\n\n Back to top"
  },
  {
    "objectID": "cog_transformation/epa-ch4emission-grid-v2express.html",
    "href": "cog_transformation/epa-ch4emission-grid-v2express.html",
    "title": "U.S. Gridded Anthropogenic Methane Emissions Inventory",
    "section": "",
    "text": "This script was used to transform the Gridded Anthropogenic Methane Emissions Inventory monthly dataset from netCDF to Cloud Optimized GeoTIFF (COG) format for display in the Greenhouse Gas (GHG) Center.\n\nimport os\nimport xarray\nimport re\nimport pandas as pd\nimport json\nimport tempfile\nimport boto3\nfrom datetime import datetime\nimport numpy as np\n\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\nTrue\n\n\n\n# session = boto3.session.Session()\nsession = boto3.Session(\n    aws_access_key_id=os.environ.get(\"AWS_ACCESS_KEY_ID\"),\n    aws_secret_access_key=os.environ.get(\"AWS_SECRET_ACCESS_KEY\"),\n    aws_session_token=os.environ.get(\"AWS_SESSION_TOKEN\"),\n)\ns3_client = session.client(\"s3\")\nbucket_name = \"ghgc-data-store-dev\" # S3 bucket where the COGs are stored after transformation\nFOLDER_NAME = \"../data/epa_emissions_express_extension\"\ns3_folder_name = \"epa_express_extension_Mg_km2_yr\"\n# raw gridded data [molec/cm2/s] * 1/6.022x10^23 [molec/mol] * 16.04x10^-6 [ Mg/mol] * 366 [days/yr] * 1x10^10 [cm2/km2]\n\nfiles_processed = pd.DataFrame(columns=[\"file_name\", \"COGs_created\"])   # A dataframe to keep track of the files that we have transformed into COGs\n\n# Reading the raw netCDF files from local machine\nfor name in os.listdir(FOLDER_NAME):\n    xds = xarray.open_dataset(f\"{FOLDER_NAME}/{name}\", engine=\"netcdf4\")\n    xds = xds.assign_coords(lon=(((xds.lon + 180) % 360) - 180)).sortby(\"lon\")\n    variable = [var for var in xds.data_vars]\n    filename = name.split(\"/ \")[-1]\n    filename_elements = re.split(\"[_ .]\", filename)\n    start_time = datetime(int(filename_elements[-2]), 1, 1)\n\n    for time_increment in range(0, len(xds.time)):\n        for var in variable:\n            filename = name.split(\"/ \")[-1]\n            filename_elements = re.split(\"[_ .]\", filename)\n            data = getattr(xds.isel(time=time_increment), var)\n            data.values[data.values==0] = np.nan\n            data = data*((1/(6.022*pow(10,23)))*(16.04*pow(10,-6))*366*pow(10,10)*86400)\n            data = data.fillna(-9999)\n            data = data.isel(lat=slice(None, None, -1))\n            data.rio.set_spatial_dims(\"lon\", \"lat\", inplace=True)\n            data.rio.write_crs(\"epsg:4326\", inplace=True)\n\n            # # insert date of generated COG into filename\n            filename_elements.pop()\n            filename_elements[-1] = start_time.strftime(\"%Y\")\n            filename_elements.insert(2, var)\n            cog_filename = \"_\".join(filename_elements)\n            # # add extension\n            cog_filename = f\"{cog_filename}.tif\"\n\n            with tempfile.NamedTemporaryFile() as temp_file:\n                data.rio.to_raster(\n                    temp_file.name,\n                    driver=\"COG\",\n                )\n                s3_client.upload_file(\n                    Filename=temp_file.name,\n                    Bucket=bucket_name,\n                    Key=f\"{s3_folder_name}/{cog_filename}\",\n                )\n\n            files_processed = files_processed._append(\n                {\"file_name\": name, \"COGs_created\": cog_filename},\n                ignore_index=True,\n            )\n\n            print(f\"Generated and saved COG: {cog_filename}\")\n\n# Generate the json file with the metadata that is present in the netCDF files.\nwith tempfile.NamedTemporaryFile(mode=\"w+\") as fp:\n    json.dump(xds.attrs, fp)\n    json.dump({\"data_dimensions\": dict(xds.dims)}, fp)\n    json.dump({\"data_variables\": list(xds.data_vars)}, fp)\n    fp.flush()\n\n    s3_client.upload_file(\n        Filename=fp.name,\n        Bucket=bucket_name,\n        Key=f\"{s3_folder_name}/metadata.json\",\n    )\n\n# creating the csv file with the names of files transformed.\nfiles_processed.to_csv(\n    f\"s3://{bucket_name}/{s3_folder_name}/files_converted.csv\",\n)\nprint(\"Done generating COGs\")\n\nGenerated and saved COG: Express_Extension_emi_ch4_1A_Combustion_Mobile_Gridded_GHGI_Methane_v2_2015.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1A_Combustion_Stationary_Gridded_GHGI_Methane_v2_2015.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B1a_Abandoned_Coal_Gridded_GHGI_Methane_v2_2015.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B1a_Surface_Coal_Gridded_GHGI_Methane_v2_2015.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B1a_Underground_Coal_Gridded_GHGI_Methane_v2_2015.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2a_Petroleum_Systems_Exploration_Gridded_GHGI_Methane_v2_2015.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2a_Petroleum_Systems_Production_Gridded_GHGI_Methane_v2_2015.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2a_Petroleum_Systems_Refining_Gridded_GHGI_Methane_v2_2015.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2a_Petroleum_Systems_Transport_Gridded_GHGI_Methane_v2_2015.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2ab_Abandoned_Oil_Gas_Gridded_GHGI_Methane_v2_2015.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_Distribution_Gridded_GHGI_Methane_v2_2015.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_Exploration_Gridded_GHGI_Methane_v2_2015.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_Processing_Gridded_GHGI_Methane_v2_2015.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_Production_Gridded_GHGI_Methane_v2_2015.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_TransmissionStorage_Gridded_GHGI_Methane_v2_2015.tif\nGenerated and saved COG: Express_Extension_emi_ch4_2B8_Industry_Petrochemical_Gridded_GHGI_Methane_v2_2015.tif\nGenerated and saved COG: Express_Extension_emi_ch4_2C2_Industry_Ferroalloy_Gridded_GHGI_Methane_v2_2015.tif\nGenerated and saved COG: Express_Extension_emi_ch4_3A_Enteric_Fermentation_Gridded_GHGI_Methane_v2_2015.tif\nGenerated and saved COG: Express_Extension_emi_ch4_3B_Manure_Management_Gridded_GHGI_Methane_v2_2015.tif\nGenerated and saved COG: Express_Extension_emi_ch4_3C_Rice_Cultivation_Gridded_GHGI_Methane_v2_2015.tif\nGenerated and saved COG: Express_Extension_emi_ch4_3F_Field_Burning_Gridded_GHGI_Methane_v2_2015.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5A1_Landfills_Industrial_Gridded_GHGI_Methane_v2_2015.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5A1_Landfills_MSW_Gridded_GHGI_Methane_v2_2015.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5B1_Composting_Gridded_GHGI_Methane_v2_2015.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5D_Wastewater_Treatment_Domestic_Gridded_GHGI_Methane_v2_2015.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5D_Wastewater_Treatment_Industrial_Gridded_GHGI_Methane_v2_2015.tif\nGenerated and saved COG: Express_Extension_emi_ch4_Supp_1B2b_PostMeter_Gridded_GHGI_Methane_v2_2015.tif\nGenerated and saved COG: Express_Extension_grid_cell_area_Gridded_GHGI_Methane_v2_2015.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1A_Combustion_Mobile_Gridded_GHGI_Methane_v2_2020.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1A_Combustion_Stationary_Gridded_GHGI_Methane_v2_2020.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B1a_Abandoned_Coal_Gridded_GHGI_Methane_v2_2020.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B1a_Surface_Coal_Gridded_GHGI_Methane_v2_2020.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B1a_Underground_Coal_Gridded_GHGI_Methane_v2_2020.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2a_Petroleum_Systems_Exploration_Gridded_GHGI_Methane_v2_2020.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2a_Petroleum_Systems_Production_Gridded_GHGI_Methane_v2_2020.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2a_Petroleum_Systems_Refining_Gridded_GHGI_Methane_v2_2020.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2a_Petroleum_Systems_Transport_Gridded_GHGI_Methane_v2_2020.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2ab_Abandoned_Oil_Gas_Gridded_GHGI_Methane_v2_2020.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_Distribution_Gridded_GHGI_Methane_v2_2020.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_Exploration_Gridded_GHGI_Methane_v2_2020.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_Processing_Gridded_GHGI_Methane_v2_2020.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_Production_Gridded_GHGI_Methane_v2_2020.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_TransmissionStorage_Gridded_GHGI_Methane_v2_2020.tif\nGenerated and saved COG: Express_Extension_emi_ch4_2B8_Industry_Petrochemical_Gridded_GHGI_Methane_v2_2020.tif\nGenerated and saved COG: Express_Extension_emi_ch4_2C2_Industry_Ferroalloy_Gridded_GHGI_Methane_v2_2020.tif\nGenerated and saved COG: Express_Extension_emi_ch4_3A_Enteric_Fermentation_Gridded_GHGI_Methane_v2_2020.tif\nGenerated and saved COG: Express_Extension_emi_ch4_3B_Manure_Management_Gridded_GHGI_Methane_v2_2020.tif\nGenerated and saved COG: Express_Extension_emi_ch4_3C_Rice_Cultivation_Gridded_GHGI_Methane_v2_2020.tif\nGenerated and saved COG: Express_Extension_emi_ch4_3F_Field_Burning_Gridded_GHGI_Methane_v2_2020.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5A1_Landfills_Industrial_Gridded_GHGI_Methane_v2_2020.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5A1_Landfills_MSW_Gridded_GHGI_Methane_v2_2020.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5B1_Composting_Gridded_GHGI_Methane_v2_2020.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5D_Wastewater_Treatment_Domestic_Gridded_GHGI_Methane_v2_2020.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5D_Wastewater_Treatment_Industrial_Gridded_GHGI_Methane_v2_2020.tif\nGenerated and saved COG: Express_Extension_emi_ch4_Supp_1B2b_PostMeter_Gridded_GHGI_Methane_v2_2020.tif\nGenerated and saved COG: Express_Extension_grid_cell_area_Gridded_GHGI_Methane_v2_2020.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1A_Combustion_Mobile_Gridded_GHGI_Methane_v2_2014.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1A_Combustion_Stationary_Gridded_GHGI_Methane_v2_2014.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B1a_Abandoned_Coal_Gridded_GHGI_Methane_v2_2014.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B1a_Surface_Coal_Gridded_GHGI_Methane_v2_2014.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B1a_Underground_Coal_Gridded_GHGI_Methane_v2_2014.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2a_Petroleum_Systems_Exploration_Gridded_GHGI_Methane_v2_2014.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2a_Petroleum_Systems_Production_Gridded_GHGI_Methane_v2_2014.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2a_Petroleum_Systems_Refining_Gridded_GHGI_Methane_v2_2014.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2a_Petroleum_Systems_Transport_Gridded_GHGI_Methane_v2_2014.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2ab_Abandoned_Oil_Gas_Gridded_GHGI_Methane_v2_2014.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_Distribution_Gridded_GHGI_Methane_v2_2014.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_Exploration_Gridded_GHGI_Methane_v2_2014.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_Processing_Gridded_GHGI_Methane_v2_2014.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_Production_Gridded_GHGI_Methane_v2_2014.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_TransmissionStorage_Gridded_GHGI_Methane_v2_2014.tif\nGenerated and saved COG: Express_Extension_emi_ch4_2B8_Industry_Petrochemical_Gridded_GHGI_Methane_v2_2014.tif\nGenerated and saved COG: Express_Extension_emi_ch4_2C2_Industry_Ferroalloy_Gridded_GHGI_Methane_v2_2014.tif\nGenerated and saved COG: Express_Extension_emi_ch4_3A_Enteric_Fermentation_Gridded_GHGI_Methane_v2_2014.tif\nGenerated and saved COG: Express_Extension_emi_ch4_3B_Manure_Management_Gridded_GHGI_Methane_v2_2014.tif\nGenerated and saved COG: Express_Extension_emi_ch4_3C_Rice_Cultivation_Gridded_GHGI_Methane_v2_2014.tif\nGenerated and saved COG: Express_Extension_emi_ch4_3F_Field_Burning_Gridded_GHGI_Methane_v2_2014.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5A1_Landfills_Industrial_Gridded_GHGI_Methane_v2_2014.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5A1_Landfills_MSW_Gridded_GHGI_Methane_v2_2014.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5B1_Composting_Gridded_GHGI_Methane_v2_2014.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5D_Wastewater_Treatment_Domestic_Gridded_GHGI_Methane_v2_2014.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5D_Wastewater_Treatment_Industrial_Gridded_GHGI_Methane_v2_2014.tif\nGenerated and saved COG: Express_Extension_emi_ch4_Supp_1B2b_PostMeter_Gridded_GHGI_Methane_v2_2014.tif\nGenerated and saved COG: Express_Extension_grid_cell_area_Gridded_GHGI_Methane_v2_2014.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1A_Combustion_Mobile_Gridded_GHGI_Methane_v2_2013.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1A_Combustion_Stationary_Gridded_GHGI_Methane_v2_2013.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B1a_Abandoned_Coal_Gridded_GHGI_Methane_v2_2013.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B1a_Surface_Coal_Gridded_GHGI_Methane_v2_2013.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B1a_Underground_Coal_Gridded_GHGI_Methane_v2_2013.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2a_Petroleum_Systems_Exploration_Gridded_GHGI_Methane_v2_2013.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2a_Petroleum_Systems_Production_Gridded_GHGI_Methane_v2_2013.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2a_Petroleum_Systems_Refining_Gridded_GHGI_Methane_v2_2013.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2a_Petroleum_Systems_Transport_Gridded_GHGI_Methane_v2_2013.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2ab_Abandoned_Oil_Gas_Gridded_GHGI_Methane_v2_2013.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_Distribution_Gridded_GHGI_Methane_v2_2013.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_Exploration_Gridded_GHGI_Methane_v2_2013.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_Processing_Gridded_GHGI_Methane_v2_2013.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_Production_Gridded_GHGI_Methane_v2_2013.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_TransmissionStorage_Gridded_GHGI_Methane_v2_2013.tif\nGenerated and saved COG: Express_Extension_emi_ch4_2B8_Industry_Petrochemical_Gridded_GHGI_Methane_v2_2013.tif\nGenerated and saved COG: Express_Extension_emi_ch4_2C2_Industry_Ferroalloy_Gridded_GHGI_Methane_v2_2013.tif\nGenerated and saved COG: Express_Extension_emi_ch4_3A_Enteric_Fermentation_Gridded_GHGI_Methane_v2_2013.tif\nGenerated and saved COG: Express_Extension_emi_ch4_3B_Manure_Management_Gridded_GHGI_Methane_v2_2013.tif\nGenerated and saved COG: Express_Extension_emi_ch4_3C_Rice_Cultivation_Gridded_GHGI_Methane_v2_2013.tif\nGenerated and saved COG: Express_Extension_emi_ch4_3F_Field_Burning_Gridded_GHGI_Methane_v2_2013.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5A1_Landfills_Industrial_Gridded_GHGI_Methane_v2_2013.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5A1_Landfills_MSW_Gridded_GHGI_Methane_v2_2013.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5B1_Composting_Gridded_GHGI_Methane_v2_2013.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5D_Wastewater_Treatment_Domestic_Gridded_GHGI_Methane_v2_2013.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5D_Wastewater_Treatment_Industrial_Gridded_GHGI_Methane_v2_2013.tif\nGenerated and saved COG: Express_Extension_emi_ch4_Supp_1B2b_PostMeter_Gridded_GHGI_Methane_v2_2013.tif\nGenerated and saved COG: Express_Extension_grid_cell_area_Gridded_GHGI_Methane_v2_2013.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1A_Combustion_Mobile_Gridded_GHGI_Methane_v2_2017.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1A_Combustion_Stationary_Gridded_GHGI_Methane_v2_2017.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B1a_Abandoned_Coal_Gridded_GHGI_Methane_v2_2017.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B1a_Surface_Coal_Gridded_GHGI_Methane_v2_2017.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B1a_Underground_Coal_Gridded_GHGI_Methane_v2_2017.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2a_Petroleum_Systems_Exploration_Gridded_GHGI_Methane_v2_2017.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2a_Petroleum_Systems_Production_Gridded_GHGI_Methane_v2_2017.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2a_Petroleum_Systems_Refining_Gridded_GHGI_Methane_v2_2017.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2a_Petroleum_Systems_Transport_Gridded_GHGI_Methane_v2_2017.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2ab_Abandoned_Oil_Gas_Gridded_GHGI_Methane_v2_2017.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_Distribution_Gridded_GHGI_Methane_v2_2017.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_Exploration_Gridded_GHGI_Methane_v2_2017.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_Processing_Gridded_GHGI_Methane_v2_2017.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_Production_Gridded_GHGI_Methane_v2_2017.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_TransmissionStorage_Gridded_GHGI_Methane_v2_2017.tif\nGenerated and saved COG: Express_Extension_emi_ch4_2B8_Industry_Petrochemical_Gridded_GHGI_Methane_v2_2017.tif\nGenerated and saved COG: Express_Extension_emi_ch4_2C2_Industry_Ferroalloy_Gridded_GHGI_Methane_v2_2017.tif\nGenerated and saved COG: Express_Extension_emi_ch4_3A_Enteric_Fermentation_Gridded_GHGI_Methane_v2_2017.tif\nGenerated and saved COG: Express_Extension_emi_ch4_3B_Manure_Management_Gridded_GHGI_Methane_v2_2017.tif\nGenerated and saved COG: Express_Extension_emi_ch4_3C_Rice_Cultivation_Gridded_GHGI_Methane_v2_2017.tif\nGenerated and saved COG: Express_Extension_emi_ch4_3F_Field_Burning_Gridded_GHGI_Methane_v2_2017.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5A1_Landfills_Industrial_Gridded_GHGI_Methane_v2_2017.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5A1_Landfills_MSW_Gridded_GHGI_Methane_v2_2017.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5B1_Composting_Gridded_GHGI_Methane_v2_2017.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5D_Wastewater_Treatment_Domestic_Gridded_GHGI_Methane_v2_2017.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5D_Wastewater_Treatment_Industrial_Gridded_GHGI_Methane_v2_2017.tif\nGenerated and saved COG: Express_Extension_emi_ch4_Supp_1B2b_PostMeter_Gridded_GHGI_Methane_v2_2017.tif\nGenerated and saved COG: Express_Extension_grid_cell_area_Gridded_GHGI_Methane_v2_2017.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1A_Combustion_Mobile_Gridded_GHGI_Methane_v2_2016.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1A_Combustion_Stationary_Gridded_GHGI_Methane_v2_2016.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B1a_Abandoned_Coal_Gridded_GHGI_Methane_v2_2016.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B1a_Surface_Coal_Gridded_GHGI_Methane_v2_2016.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B1a_Underground_Coal_Gridded_GHGI_Methane_v2_2016.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2a_Petroleum_Systems_Exploration_Gridded_GHGI_Methane_v2_2016.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2a_Petroleum_Systems_Production_Gridded_GHGI_Methane_v2_2016.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2a_Petroleum_Systems_Refining_Gridded_GHGI_Methane_v2_2016.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2a_Petroleum_Systems_Transport_Gridded_GHGI_Methane_v2_2016.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2ab_Abandoned_Oil_Gas_Gridded_GHGI_Methane_v2_2016.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_Distribution_Gridded_GHGI_Methane_v2_2016.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_Exploration_Gridded_GHGI_Methane_v2_2016.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_Processing_Gridded_GHGI_Methane_v2_2016.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_Production_Gridded_GHGI_Methane_v2_2016.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_TransmissionStorage_Gridded_GHGI_Methane_v2_2016.tif\nGenerated and saved COG: Express_Extension_emi_ch4_2B8_Industry_Petrochemical_Gridded_GHGI_Methane_v2_2016.tif\nGenerated and saved COG: Express_Extension_emi_ch4_2C2_Industry_Ferroalloy_Gridded_GHGI_Methane_v2_2016.tif\nGenerated and saved COG: Express_Extension_emi_ch4_3A_Enteric_Fermentation_Gridded_GHGI_Methane_v2_2016.tif\nGenerated and saved COG: Express_Extension_emi_ch4_3B_Manure_Management_Gridded_GHGI_Methane_v2_2016.tif\nGenerated and saved COG: Express_Extension_emi_ch4_3C_Rice_Cultivation_Gridded_GHGI_Methane_v2_2016.tif\nGenerated and saved COG: Express_Extension_emi_ch4_3F_Field_Burning_Gridded_GHGI_Methane_v2_2016.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5A1_Landfills_Industrial_Gridded_GHGI_Methane_v2_2016.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5A1_Landfills_MSW_Gridded_GHGI_Methane_v2_2016.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5B1_Composting_Gridded_GHGI_Methane_v2_2016.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5D_Wastewater_Treatment_Domestic_Gridded_GHGI_Methane_v2_2016.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5D_Wastewater_Treatment_Industrial_Gridded_GHGI_Methane_v2_2016.tif\nGenerated and saved COG: Express_Extension_emi_ch4_Supp_1B2b_PostMeter_Gridded_GHGI_Methane_v2_2016.tif\nGenerated and saved COG: Express_Extension_grid_cell_area_Gridded_GHGI_Methane_v2_2016.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1A_Combustion_Mobile_Gridded_GHGI_Methane_v2_2012.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1A_Combustion_Stationary_Gridded_GHGI_Methane_v2_2012.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B1a_Abandoned_Coal_Gridded_GHGI_Methane_v2_2012.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B1a_Surface_Coal_Gridded_GHGI_Methane_v2_2012.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B1a_Underground_Coal_Gridded_GHGI_Methane_v2_2012.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2a_Petroleum_Systems_Exploration_Gridded_GHGI_Methane_v2_2012.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2a_Petroleum_Systems_Production_Gridded_GHGI_Methane_v2_2012.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2a_Petroleum_Systems_Refining_Gridded_GHGI_Methane_v2_2012.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2a_Petroleum_Systems_Transport_Gridded_GHGI_Methane_v2_2012.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2ab_Abandoned_Oil_Gas_Gridded_GHGI_Methane_v2_2012.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_Distribution_Gridded_GHGI_Methane_v2_2012.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_Exploration_Gridded_GHGI_Methane_v2_2012.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_Processing_Gridded_GHGI_Methane_v2_2012.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_Production_Gridded_GHGI_Methane_v2_2012.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_TransmissionStorage_Gridded_GHGI_Methane_v2_2012.tif\nGenerated and saved COG: Express_Extension_emi_ch4_2B8_Industry_Petrochemical_Gridded_GHGI_Methane_v2_2012.tif\nGenerated and saved COG: Express_Extension_emi_ch4_2C2_Industry_Ferroalloy_Gridded_GHGI_Methane_v2_2012.tif\nGenerated and saved COG: Express_Extension_emi_ch4_3A_Enteric_Fermentation_Gridded_GHGI_Methane_v2_2012.tif\nGenerated and saved COG: Express_Extension_emi_ch4_3B_Manure_Management_Gridded_GHGI_Methane_v2_2012.tif\nGenerated and saved COG: Express_Extension_emi_ch4_3C_Rice_Cultivation_Gridded_GHGI_Methane_v2_2012.tif\nGenerated and saved COG: Express_Extension_emi_ch4_3F_Field_Burning_Gridded_GHGI_Methane_v2_2012.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5A1_Landfills_Industrial_Gridded_GHGI_Methane_v2_2012.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5A1_Landfills_MSW_Gridded_GHGI_Methane_v2_2012.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5B1_Composting_Gridded_GHGI_Methane_v2_2012.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5D_Wastewater_Treatment_Domestic_Gridded_GHGI_Methane_v2_2012.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5D_Wastewater_Treatment_Industrial_Gridded_GHGI_Methane_v2_2012.tif\nGenerated and saved COG: Express_Extension_emi_ch4_Supp_1B2b_PostMeter_Gridded_GHGI_Methane_v2_2012.tif\nGenerated and saved COG: Express_Extension_grid_cell_area_Gridded_GHGI_Methane_v2_2012.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1A_Combustion_Mobile_Gridded_GHGI_Methane_v2_2019.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1A_Combustion_Stationary_Gridded_GHGI_Methane_v2_2019.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B1a_Abandoned_Coal_Gridded_GHGI_Methane_v2_2019.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B1a_Surface_Coal_Gridded_GHGI_Methane_v2_2019.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B1a_Underground_Coal_Gridded_GHGI_Methane_v2_2019.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2a_Petroleum_Systems_Exploration_Gridded_GHGI_Methane_v2_2019.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2a_Petroleum_Systems_Production_Gridded_GHGI_Methane_v2_2019.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2a_Petroleum_Systems_Refining_Gridded_GHGI_Methane_v2_2019.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2a_Petroleum_Systems_Transport_Gridded_GHGI_Methane_v2_2019.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2ab_Abandoned_Oil_Gas_Gridded_GHGI_Methane_v2_2019.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_Distribution_Gridded_GHGI_Methane_v2_2019.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_Exploration_Gridded_GHGI_Methane_v2_2019.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_Processing_Gridded_GHGI_Methane_v2_2019.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_Production_Gridded_GHGI_Methane_v2_2019.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_TransmissionStorage_Gridded_GHGI_Methane_v2_2019.tif\nGenerated and saved COG: Express_Extension_emi_ch4_2B8_Industry_Petrochemical_Gridded_GHGI_Methane_v2_2019.tif\nGenerated and saved COG: Express_Extension_emi_ch4_2C2_Industry_Ferroalloy_Gridded_GHGI_Methane_v2_2019.tif\nGenerated and saved COG: Express_Extension_emi_ch4_3A_Enteric_Fermentation_Gridded_GHGI_Methane_v2_2019.tif\nGenerated and saved COG: Express_Extension_emi_ch4_3B_Manure_Management_Gridded_GHGI_Methane_v2_2019.tif\nGenerated and saved COG: Express_Extension_emi_ch4_3C_Rice_Cultivation_Gridded_GHGI_Methane_v2_2019.tif\nGenerated and saved COG: Express_Extension_emi_ch4_3F_Field_Burning_Gridded_GHGI_Methane_v2_2019.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5A1_Landfills_Industrial_Gridded_GHGI_Methane_v2_2019.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5A1_Landfills_MSW_Gridded_GHGI_Methane_v2_2019.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5B1_Composting_Gridded_GHGI_Methane_v2_2019.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5D_Wastewater_Treatment_Domestic_Gridded_GHGI_Methane_v2_2019.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5D_Wastewater_Treatment_Industrial_Gridded_GHGI_Methane_v2_2019.tif\nGenerated and saved COG: Express_Extension_emi_ch4_Supp_1B2b_PostMeter_Gridded_GHGI_Methane_v2_2019.tif\nGenerated and saved COG: Express_Extension_grid_cell_area_Gridded_GHGI_Methane_v2_2019.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1A_Combustion_Mobile_Gridded_GHGI_Methane_v2_2018.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1A_Combustion_Stationary_Gridded_GHGI_Methane_v2_2018.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B1a_Abandoned_Coal_Gridded_GHGI_Methane_v2_2018.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B1a_Surface_Coal_Gridded_GHGI_Methane_v2_2018.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B1a_Underground_Coal_Gridded_GHGI_Methane_v2_2018.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2a_Petroleum_Systems_Exploration_Gridded_GHGI_Methane_v2_2018.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2a_Petroleum_Systems_Production_Gridded_GHGI_Methane_v2_2018.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2a_Petroleum_Systems_Refining_Gridded_GHGI_Methane_v2_2018.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2a_Petroleum_Systems_Transport_Gridded_GHGI_Methane_v2_2018.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2ab_Abandoned_Oil_Gas_Gridded_GHGI_Methane_v2_2018.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_Distribution_Gridded_GHGI_Methane_v2_2018.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_Exploration_Gridded_GHGI_Methane_v2_2018.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_Processing_Gridded_GHGI_Methane_v2_2018.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_Production_Gridded_GHGI_Methane_v2_2018.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_TransmissionStorage_Gridded_GHGI_Methane_v2_2018.tif\nGenerated and saved COG: Express_Extension_emi_ch4_2B8_Industry_Petrochemical_Gridded_GHGI_Methane_v2_2018.tif\nGenerated and saved COG: Express_Extension_emi_ch4_2C2_Industry_Ferroalloy_Gridded_GHGI_Methane_v2_2018.tif\nGenerated and saved COG: Express_Extension_emi_ch4_3A_Enteric_Fermentation_Gridded_GHGI_Methane_v2_2018.tif\nGenerated and saved COG: Express_Extension_emi_ch4_3B_Manure_Management_Gridded_GHGI_Methane_v2_2018.tif\nGenerated and saved COG: Express_Extension_emi_ch4_3C_Rice_Cultivation_Gridded_GHGI_Methane_v2_2018.tif\nGenerated and saved COG: Express_Extension_emi_ch4_3F_Field_Burning_Gridded_GHGI_Methane_v2_2018.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5A1_Landfills_Industrial_Gridded_GHGI_Methane_v2_2018.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5A1_Landfills_MSW_Gridded_GHGI_Methane_v2_2018.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5B1_Composting_Gridded_GHGI_Methane_v2_2018.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5D_Wastewater_Treatment_Domestic_Gridded_GHGI_Methane_v2_2018.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5D_Wastewater_Treatment_Industrial_Gridded_GHGI_Methane_v2_2018.tif\nGenerated and saved COG: Express_Extension_emi_ch4_Supp_1B2b_PostMeter_Gridded_GHGI_Methane_v2_2018.tif\nGenerated and saved COG: Express_Extension_grid_cell_area_Gridded_GHGI_Methane_v2_2018.tif\nDone generating COGs\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "cog_transformation/sedac-popdensity-yeargrid5yr-v4.11.html",
    "href": "cog_transformation/sedac-popdensity-yeargrid5yr-v4.11.html",
    "title": "SEDAC Gridded World Population Data",
    "section": "",
    "text": "This script was used to transform SEDAC Gridded World Population Data from netCDF to Cloud Optimized GeoTIFF (COG) format for display in the Greenhouse Gas (GHG) Center.\n\nimport os\nimport xarray\nimport re\nimport pandas as pd\n\nimport tempfile\nimport boto3\n\n\nsession = boto3.session.Session()\ns3_client = session.client(\"s3\")\nbucket_name = (\n    \"ghgc-data-store-dev\"  # S3 bucket where the COGs are stored after transformation\n)\n\nfold_names = os.listdir(\"gpw\")\n\nfiles_processed = pd.DataFrame(\n    columns=[\"file_name\", \"COGs_created\"]\n)  # A dataframe to keep track of the files that we have transformed into COGs\n\n# Reading the raw netCDF files from local machine\nfor fol_ in fold_names:\n    for name in os.listdir(f\"gpw/{fol_}\"):\n        if name.endswith(\".tif\"):\n            xds = xarray.open_dataarray(f\"gpw/{fol_}/{name}\")\n\n            filename = name.split(\"/ \")[-1]\n            filename_elements = re.split(\"[_ .]\", filename)\n            # # insert date of generated COG into filename\n            filename_elements.pop()\n            filename_elements.append(filename_elements[-3])\n\n            xds.rio.set_spatial_dims(\"x\", \"y\", inplace=True)\n            xds.rio.write_crs(\"epsg:4326\", inplace=True)\n\n            cog_filename = \"_\".join(filename_elements)\n            # # add extension\n            cog_filename = f\"{cog_filename}.tif\"\n\n            with tempfile.NamedTemporaryFile() as temp_file:\n                xds.rio.to_raster(temp_file.name, driver=\"COG\")\n                s3_client.upload_file(\n                    Filename=temp_file.name,\n                    Bucket=bucket_name,\n                    Key=f\"gridded_population_cog/{cog_filename}\",\n                )\n\n            files_processed = files_processed._append(\n                {\"file_name\": name, \"COGs_created\": cog_filename},\n                ignore_index=True,\n            )\n\n            print(f\"Generated and saved COG: {cog_filename}\")\n\n\n# creating the csv file with the names of files transformed.\nfiles_processed.to_csv(\n    f\"s3://{bucket_name}/gridded_population_cog/files_converted.csv\",\n)\nprint(\"Done generating COGs\")\n\n\n\n\n Back to top"
  },
  {
    "objectID": "cog_transformation/oco2-mip-co2budget-yeargrid-v1.html",
    "href": "cog_transformation/oco2-mip-co2budget-yeargrid-v1.html",
    "title": "OCO-2 MIP Top-Down CO₂ Budgets",
    "section": "",
    "text": "This script was used to transform the OCO-2 MIP Top-Down CO₂ Budgets dataset from netCDF to Cloud Optimized GeoTIFF (COG) format for display in the Greenhouse Gas (GHG) Center.\n\nimport os\nimport xarray\nimport re\nimport pandas as pd\nimport json\nimport tempfile\nimport boto3\nimport rasterio\nfrom datetime import datetime\nfrom dateutil.relativedelta import relativedelta\n\n\nsession = boto3.session.Session()\ns3_client = session.client(\"s3\")\nbucket_name = \"ghgc-data-store-dev\" # S3 bucket where the COGs are to be stored\nyear_ = datetime(2015, 1, 1)    # Initialize the starting date time of the dataset.\n\nCOG_PROFILE = {\"driver\": \"COG\", \"compress\": \"DEFLATE\"}\n\n# Reading the raw netCDF files from local machine\nfiles_processed = pd.DataFrame(columns=[\"file_name\", \"COGs_created\"])   # A dataframe to keep track of the files that are converted into COGs\nfor name in os.listdir(\"new_data\"):\n    ds = xarray.open_dataset(\n        f\"new_data/{name}\",\n        engine=\"netcdf4\",\n    )\n    ds = ds.rename({\"latitude\": \"lat\", \"longitude\": \"lon\"})\n    # assign coords from dimensions\n    ds = ds.assign_coords(lon=(((ds.lon + 180) % 360) - 180)).sortby(\"lon\")\n    ds = ds.assign_coords(lat=list(ds.lat))\n\n    variable = [var for var in ds.data_vars]\n\n    for time_increment in range(0, len(ds.year)):\n        for var in variable[2:]:\n            filename = name.split(\"/ \")[-1]\n            filename_elements = re.split(\"[_ .]\", filename)\n            try:\n                data = ds[var].sel(year=time_increment)\n                date = year_ + relativedelta(years=+time_increment)\n                filename_elements[-1] = date.strftime(\"%Y\")\n                # # insert date of generated COG into filename\n                filename_elements.insert(2, var)\n                cog_filename = \"_\".join(filename_elements)\n                # # add extension\n                cog_filename = f\"{cog_filename}.tif\"\n            except KeyError:\n                data = ds[var]\n                date = year_ + relativedelta(years=+(len(ds.year) - 1))\n                filename_elements.pop()\n                filename_elements.append(year_.strftime(\"%Y\"))\n                filename_elements.append(date.strftime(\"%Y\"))\n                filename_elements.insert(2, var)\n                cog_filename = \"_\".join(filename_elements)\n                # # add extension\n                cog_filename = f\"{cog_filename}.tif\"\n\n            data = data.reindex(lat=list(reversed(data.lat)))\n\n            data.rio.set_spatial_dims(\"lon\", \"lat\")\n            data.rio.write_crs(\"epsg:4326\", inplace=True)\n\n            # generate COG\n            COG_PROFILE = {\"driver\": \"COG\", \"compress\": \"DEFLATE\"}\n            with tempfile.NamedTemporaryFile() as temp_file:\n                data.rio.to_raster(temp_file.name, **COG_PROFILE)\n                s3_client.upload_file(\n                    Filename=temp_file.name,\n                    Bucket=bucket_name,\n                    Key=f\"ceos_co2_flux/{cog_filename}\",\n                )\n\n            files_processed = files_processed._append(\n                {\"file_name\": name, \"COGs_created\": cog_filename},\n                ignore_index=True,\n            )\n\n            print(f\"Generated and saved COG: {cog_filename}\")\n\n# creating the csv file with the names of files transformed.\nfiles_processed.to_csv(\n    f\"s3://{bucket_name}/ceos_co2_flux/files_converted.csv\",\n)\nprint(\"Done generating COGs\")\n\n\n\n\n Back to top"
  },
  {
    "objectID": "cog_transformation/eccodarwin-co2flux-monthgrid-v5.html",
    "href": "cog_transformation/eccodarwin-co2flux-monthgrid-v5.html",
    "title": "Air-Sea CO₂ Flux, ECCO-Darwin Model v5",
    "section": "",
    "text": "This script was used to transform the Air-Sea CO₂ Flux, ECCO-Darwin Mode dataset from netCDF to Cloud Optimized GeoTIFF (COG) format for display in the Greenhouse Gas (GHG) Center.\n\nimport os\nimport xarray\nimport re\nimport pandas as pd\nimport json\nimport tempfile\nimport boto3\nimport rasterio\nfrom datetime import datetime\nfrom dateutil.relativedelta import relativedelta\n\n\nsession = boto3.session.Session()\ns3_client = session.client(\"s3\")\n\nbucket_name = (\n    \"ghgc-data-store-dev\"  # S3 bucket where the COGs are stored after transformation\n)\nFOLDER_NAME = \"ecco-darwin\"\ns3_fol_name = \"ecco_darwin\"\n\n# Reading the raw netCDF files from local machine\nfiles_processed = pd.DataFrame(\n    columns=[\"file_name\", \"COGs_created\"]\n)  # A dataframe to keep track of the files that we have transformed into COGs\nfor name in os.listdir(FOLDER_NAME):\n    xds = xarray.open_dataset(\n        f\"{FOLDER_NAME}/{name}\",\n        engine=\"netcdf4\",\n    )\n    xds = xds.rename({\"y\": \"latitude\", \"x\": \"longitude\"})\n    xds = xds.assign_coords(longitude=((xds.longitude / 1440) * 360) - 180).sortby(\n        \"longitude\"\n    )\n    xds = xds.assign_coords(latitude=((xds.latitude / 721) * 180) - 90).sortby(\n        \"latitude\"\n    )\n\n    variable = [var for var in xds.data_vars]\n\n    for time_increment in xds.time.values:\n        for var in variable[2:]:\n            filename = name.split(\"/ \")[-1]\n            filename_elements = re.split(\"[_ .]\", filename)\n            data = xds[var]\n\n            data = data.reindex(latitude=list(reversed(data.latitude)))\n            data.rio.set_spatial_dims(\"longitude\", \"latitude\", inplace=True)\n            data.rio.write_crs(\"epsg:4326\", inplace=True)\n\n            # generate COG\n            COG_PROFILE = {\"driver\": \"COG\", \"compress\": \"DEFLATE\"}\n\n            filename_elements.pop()\n            filename_elements[-1] = filename_elements[-2] + filename_elements[-1]\n            filename_elements.pop(-2)\n            # # insert date of generated COG into filename\n            cog_filename = \"_\".join(filename_elements)\n            # # add extension\n            cog_filename = f\"{cog_filename}.tif\"\n\n            with tempfile.NamedTemporaryFile() as temp_file:\n                data.rio.to_raster(temp_file.name, **COG_PROFILE)\n                s3_client.upload_file(\n                    Filename=temp_file.name,\n                    Bucket=bucket_name,\n                    Key=f\"{s3_fol_name}/{cog_filename}\",\n                )\n\n            files_processed = files_processed._append(\n                {\"file_name\": name, \"COGs_created\": cog_filename},\n                ignore_index=True,\n            )\n            del data\n\n            print(f\"Generated and saved COG: {cog_filename}\")\n\n# Generate the json file with the metadata that is present in the netCDF files.\nwith tempfile.NamedTemporaryFile(mode=\"w+\") as fp:\n    json.dump(xds.attrs, fp)\n    json.dump({\"data_dimensions\": dict(xds.dims)}, fp)\n    json.dump({\"data_variables\": list(xds.data_vars)}, fp)\n    fp.flush()\n\n    s3_client.upload_file(\n        Filename=fp.name,\n        Bucket=bucket_name,\n        Key=\"s3_fol_name/metadata.json\",\n    )\n\n# A csv file to store the names of all the files converted.\nfiles_processed.to_csv(\n    f\"s3://{bucket_name}/{s3_fol_name}/files_converted.csv\",\n)\nprint(\"Done generating COGs\")\n\n\n\n\n Back to top"
  },
  {
    "objectID": "datausage.html",
    "href": "datausage.html",
    "title": "U.S. Greenhouse Gas Center: Data Usage Notebooks",
    "section": "",
    "text": "Welcome to the U.S. Greenhouse Gas (GHG) Center data usage notebooks, your gateway to exploring and analyzing curated datasets on greenhouse gas emissions. Our cloud-based system offers seamless access to GHG curated datasets. Dive into the data with our data usage Jupyter notebooks, which demonstrate how to explore, access, visualize, and conduct basic data analysis for each GHG Center dataset in a code notebook environment. The data usage notebooks are grouped topically. The dataset product type (model output, satellite observation, etc.) is noted next to the notebook name. Click on a notebook to learn more about the dataset and to view the data usage code.\nJoin us in our mission to make data-driven environmental solutions. Explore, analyze, and make a difference with the US GHG Center.\nView the US GHG Center Data Catalog",
    "crumbs": [
      "Data Usage Notebooks"
    ]
  },
  {
    "objectID": "datausage.html#gridded-anthropogenic-greenhouse-gas-emissions",
    "href": "datausage.html#gridded-anthropogenic-greenhouse-gas-emissions",
    "title": "U.S. Greenhouse Gas Center: Data Usage Notebooks",
    "section": "Gridded Anthropogenic Greenhouse Gas Emissions",
    "text": "Gridded Anthropogenic Greenhouse Gas Emissions\n\nOCO-2 MIP Top-Down CO₂ Budgets - Model Output Beginner Level, Intermediate Level, Advance Level\nODIAC Fossil Fuel CO₂ Emissions - Model Output\nTM5-4DVar Isotopic CH₄ Inverse Fluxes - Model Output, Ground Measurements\nU.S. Gridded Anthropogenic Methane Emissions Inventory - Gridded Inventory",
    "crumbs": [
      "Data Usage Notebooks"
    ]
  },
  {
    "objectID": "datausage.html#natural-greenhouse-gas-emissions-and-sinks",
    "href": "datausage.html#natural-greenhouse-gas-emissions-and-sinks",
    "title": "U.S. Greenhouse Gas Center: Data Usage Notebooks",
    "section": "Natural Greenhouse Gas Emissions and Sinks",
    "text": "Natural Greenhouse Gas Emissions and Sinks\n\nAir-Sea CO₂ Flux, ECCO-Darwin Model v5 - Model Output\nCASA-GFED3 Land Carbon Flux - Model Output\nGOSAT-based Top-down Total and Natural Methane Emissions - Model Output\nOCO-2 MIP Top-Down CO₂ Budgets - Model Output\nTM5-4DVar Isotopic CH₄ Inverse Fluxes - Model Output, Ground Measurements\nWetland Methane Emissions from the LPJ-wsl model - Model Output",
    "crumbs": [
      "Data Usage Notebooks"
    ]
  },
  {
    "objectID": "datausage.html#large-emissions-events",
    "href": "datausage.html#large-emissions-events",
    "title": "U.S. Greenhouse Gas Center: Data Usage Notebooks",
    "section": "Large Emissions Events",
    "text": "Large Emissions Events\n\nEMIT Methane Point Source Plume Complexes - Satellite Observations",
    "crumbs": [
      "Data Usage Notebooks"
    ]
  },
  {
    "objectID": "datausage.html#greenhouse-gas-concentrations",
    "href": "datausage.html#greenhouse-gas-concentrations",
    "title": "U.S. Greenhouse Gas Center: Data Usage Notebooks",
    "section": "Greenhouse Gas Concentrations",
    "text": "Greenhouse Gas Concentrations\n\nAtmospheric Carbon Dioxide Concentrations from NOAA Global Monitoring Laboratory - Ground Measurements\nAtmospheric Methane Concentrations from NOAA Global Monitoring Laboratory - Ground Measurements\nOCO-2 GEOS Column CO₂ Concentrations - Model Output, Satellite Observations",
    "crumbs": [
      "Data Usage Notebooks"
    ]
  },
  {
    "objectID": "datausage.html#socioeconomic",
    "href": "datausage.html#socioeconomic",
    "title": "U.S. Greenhouse Gas Center: Data Usage Notebooks",
    "section": "Socioeconomic",
    "text": "Socioeconomic\n\nSEDAC Gridded World Population Density - Model Output",
    "crumbs": [
      "Data Usage Notebooks"
    ]
  },
  {
    "objectID": "datausage.html#contact",
    "href": "datausage.html#contact",
    "title": "U.S. Greenhouse Gas Center: Data Usage Notebooks",
    "section": "Contact",
    "text": "Contact\nFor technical help or general questions, please contact the support team using the feedback form.",
    "crumbs": [
      "Data Usage Notebooks"
    ]
  },
  {
    "objectID": "workflow.html",
    "href": "workflow.html",
    "title": "U.S. Greenhouse Gas Center: Data Workflow Documentation",
    "section": "",
    "text": "Welcome to the U.S. Greenhouse Gas (GHG) Center data workflow documentation. View data flow diagrams to discover the journey of each dataset from acquisition into the US GHG Center.\nData flow diagrams are grouped topically and labeled by dataset name. The dataset product type (model output, satellite observation, etc.) is noted next to each. Click on a dataset name to view the data flow diagram for that dataset, which summarizes the process followed to bring the dataset into the US GHG Center.\nJoin us in our mission to make data-driven environmental solutions accessible. Explore, analyze, and make a difference with the US GHG Center.\nBack to Data Catalog",
    "crumbs": [
      "Data Workflow Documentation"
    ]
  },
  {
    "objectID": "workflow.html#gridded-anthropogenic-greenhouse-gas-emissions",
    "href": "workflow.html#gridded-anthropogenic-greenhouse-gas-emissions",
    "title": "U.S. Greenhouse Gas Center: Data Workflow Documentation",
    "section": "Gridded Anthropogenic Greenhouse Gas Emissions",
    "text": "Gridded Anthropogenic Greenhouse Gas Emissions\n\nOCO-2 MIP Top-Down CO₂ Budgets - Model Output\nODIAC Fossil Fuel CO₂ Emissions - Model Output\nTM5-4DVar Isotopic CH₄ Inverse Fluxes - Model Output, Ground Measurements\nU.S. Gridded Anthropogenic Methane Emissions Inventory - Gridded Inventory",
    "crumbs": [
      "Data Workflow Documentation"
    ]
  },
  {
    "objectID": "workflow.html#natural-greenhouse-gas-sources-emissions-and-sinks",
    "href": "workflow.html#natural-greenhouse-gas-sources-emissions-and-sinks",
    "title": "U.S. Greenhouse Gas Center: Data Workflow Documentation",
    "section": "Natural Greenhouse Gas Sources Emissions and Sinks",
    "text": "Natural Greenhouse Gas Sources Emissions and Sinks\n\nAir-Sea CO₂ Flux, ECCO-Darwin Model v5 - Model Output\nCASA-GFED3 Land Carbon Flux - Model Output\nGOSAT-based Top-down Total and Natural Methane Emissions - Model Output\nOCO-2 MIP Top-Down CO₂ Budgets - Model Output\nTM5-4DVar Isotopic CH₄ Inverse Fluxes - Model Output, Ground Measurements\nWetland Methane Emissions from the LPJ-wsl model - Model Output",
    "crumbs": [
      "Data Workflow Documentation"
    ]
  },
  {
    "objectID": "workflow.html#large-emissions-events",
    "href": "workflow.html#large-emissions-events",
    "title": "U.S. Greenhouse Gas Center: Data Workflow Documentation",
    "section": "Large Emissions Events",
    "text": "Large Emissions Events\n\nEMIT Methane Point Source Plume Complexes - Satellite Observations",
    "crumbs": [
      "Data Workflow Documentation"
    ]
  },
  {
    "objectID": "workflow.html#greenhouse-gas-concentrations",
    "href": "workflow.html#greenhouse-gas-concentrations",
    "title": "U.S. Greenhouse Gas Center: Data Workflow Documentation",
    "section": "Greenhouse Gas Concentrations",
    "text": "Greenhouse Gas Concentrations\n\nAtmospheric Carbon Dioxide Concentrations from NOAA Global Monitoring Laboratory - Ground Measurements\nAtmospheric Carbon Dioxide Concentrations from NOAA Global Monitoring Laboratory - Ground Measurements\nOCO-2 GEOS Column CO₂ Concentrations - Model Output, Satellite Observations",
    "crumbs": [
      "Data Workflow Documentation"
    ]
  },
  {
    "objectID": "workflow.html#socioeconomic",
    "href": "workflow.html#socioeconomic",
    "title": "U.S. Greenhouse Gas Center: Data Workflow Documentation",
    "section": "Socioeconomic",
    "text": "Socioeconomic\n\nSEDAC Gridded World Population Density - Model Output",
    "crumbs": [
      "Data Workflow Documentation"
    ]
  },
  {
    "objectID": "workflow.html#contact",
    "href": "workflow.html#contact",
    "title": "U.S. Greenhouse Gas Center: Data Workflow Documentation",
    "section": "Contact",
    "text": "Contact\nFor technical help or general questions, please contact the support team using the feedback form.",
    "crumbs": [
      "Data Workflow Documentation"
    ]
  },
  {
    "objectID": "processing_and_verification_reports/epa-ch4emission-grid-v2express_Processing and Verification Report.html",
    "href": "processing_and_verification_reports/epa-ch4emission-grid-v2express_Processing and Verification Report.html",
    "title": "Gridded Anthropogenic Methane Emissions Inventory",
    "section": "",
    "text": "This browser does not support PDFs. Please download the PDF to view it: Download PDF.\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Processing and Verification Reports",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "Gridded Anthropogenic Methane Emissions Inventory"
    ]
  },
  {
    "objectID": "processing_and_verification_reports/sedac-popdensity-yeargrid5yr-v4.11_Processing and Verification Report.html",
    "href": "processing_and_verification_reports/sedac-popdensity-yeargrid5yr-v4.11_Processing and Verification Report.html",
    "title": "SEDAC Gridded World Population Density",
    "section": "",
    "text": "This browser does not support PDFs. Please download the PDF to view it: Download PDF.\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Processing and Verification Reports",
      "Socioeconomic",
      "SEDAC Gridded World Population Density"
    ]
  },
  {
    "objectID": "processing_and_verification_reports/lpjwsl-wetlandch4-grid-v1_Processing and Verification Report.html",
    "href": "processing_and_verification_reports/lpjwsl-wetlandch4-grid-v1_Processing and Verification Report.html",
    "title": "Wetland Methane Emissions, LPJ-wsl Model",
    "section": "",
    "text": "This browser does not support PDFs. Please download the PDF to view it: Download PDF.\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Processing and Verification Reports",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "Wetland Methane Emissions, LPJ-wsl Model"
    ]
  },
  {
    "objectID": "processing_and_verification_reports/odiac-ffco2-monthgrid-v2022_Processing and Verification Report.html",
    "href": "processing_and_verification_reports/odiac-ffco2-monthgrid-v2022_Processing and Verification Report.html",
    "title": "ODIAC Fossil Fuel CO₂ Emissions",
    "section": "",
    "text": "This browser does not support PDFs. Please download the PDF to view it: Download PDF.\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Processing and Verification Reports",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "ODIAC Fossil Fuel CO₂ Emissions"
    ]
  },
  {
    "objectID": "processing_and_verification_reports/gosat-based-ch4budget-yeargrid-v1_Processing and Verification Report.html",
    "href": "processing_and_verification_reports/gosat-based-ch4budget-yeargrid-v1_Processing and Verification Report.html",
    "title": "GOSAT-based Top-down Total and Natural Methane Emissions",
    "section": "",
    "text": "This browser does not support PDFs. Please download the PDF to view it: Download PDF.\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Processing and Verification Reports",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "GOSAT-based Top-down Total and Natural Methane Emissions"
    ]
  },
  {
    "objectID": "advanceduser.html",
    "href": "advanceduser.html",
    "title": "U.S. Greenhouse Gas Center: Advanced User Notebook",
    "section": "",
    "text": "Welcome to the U.S. Greenhouse Gas (GHG) Center: Advanced User Notebook, your gateway to exploring and analyzing curated datasets on greenhouse gas emissions. Our cloud-based system offers seamless access to Greenhouse Gas curated datasets. Dive into the data with our data usage Jupyter notebooks, designed for efficient exploration, visualization, and analysis. Whether you are focused on specific focus areas or product types, our dataset usage notebooks provide invaluable insights to drive informed decision-making.\nJoin us in our mission to make data-driven environmental solutions. Explore, analyze, and make a difference with the US GHG Center.\nView the US GHG Center Data Catalogg"
  },
  {
    "objectID": "advanceduser.html#gridded-anthropogenic-greenhouse-gas-emissions",
    "href": "advanceduser.html#gridded-anthropogenic-greenhouse-gas-emissions",
    "title": "U.S. Greenhouse Gas Center: Advanced User Notebook",
    "section": "Gridded Anthropogenic Greenhouse Gas Emissions",
    "text": "Gridded Anthropogenic Greenhouse Gas Emissions\n\nOCO-2 MIP Top-Down National CO₂ Budgets - Model Output"
  },
  {
    "objectID": "advanceduser.html#natural-greenhouse-gas-emissions-and-sinks",
    "href": "advanceduser.html#natural-greenhouse-gas-emissions-and-sinks",
    "title": "U.S. Greenhouse Gas Center: Advanced User Notebook",
    "section": "Natural Greenhouse Gas Emissions and Sinks",
    "text": "Natural Greenhouse Gas Emissions and Sinks"
  },
  {
    "objectID": "advanceduser.html#large-emissions-events",
    "href": "advanceduser.html#large-emissions-events",
    "title": "U.S. Greenhouse Gas Center: Advanced User Notebook",
    "section": "Large Emissions Events",
    "text": "Large Emissions Events"
  },
  {
    "objectID": "advanceduser.html#greenhouse-gas-concentrations",
    "href": "advanceduser.html#greenhouse-gas-concentrations",
    "title": "U.S. Greenhouse Gas Center: Advanced User Notebook",
    "section": "Greenhouse Gas Concentrations",
    "text": "Greenhouse Gas Concentrations"
  },
  {
    "objectID": "advanceduser.html#socioeconomic",
    "href": "advanceduser.html#socioeconomic",
    "title": "U.S. Greenhouse Gas Center: Advanced User Notebook",
    "section": "Socioeconomic",
    "text": "Socioeconomic"
  },
  {
    "objectID": "advanceduser.html#contact",
    "href": "advanceduser.html#contact",
    "title": "U.S. Greenhouse Gas Center: Advanced User Notebook",
    "section": "Contact",
    "text": "Contact\nFor technical help or general questions, please contact the support team using the feedback form."
  },
  {
    "objectID": "services/jupyterhub.html",
    "href": "services/jupyterhub.html",
    "title": "JupyterHub",
    "section": "",
    "text": "The US GHG Center promotes the use of JupyterHub environments for interactive data science. JupyterHub enables you to analyze massive archives of Earth science data in the cloud in an interactive environment that alleviates the complexities of managing compute resources (virtual machines, roles and permissions, etc).\nUsers affiliated with the US GHG Center can get access to a dedicated JupyterHub service, provided in collaboration with 2i2c: hub.ghg.center. Please find instructions for requesting access below.\nIf you are a scientist affiliated with other NASA projects such as VEDA, EIS, and MAAP, you can also keep using the resources provided by these projects. Through the use of open-source technology, we make sure our services are interoperable and exchangeable.",
    "crumbs": [
      "User Services",
      "JupyterHub"
    ]
  },
  {
    "objectID": "services/jupyterhub.html#to-get-us-ghg-center-jupyterhub-access",
    "href": "services/jupyterhub.html#to-get-us-ghg-center-jupyterhub-access",
    "title": "JupyterHub",
    "section": "To Get US GHG Center JupyterHub access:",
    "text": "To Get US GHG Center JupyterHub access:\nThe US GHG Center notebook environment is available to authorized users on an as-need basis. If you are a user affiliated with the US GHG Center, you can gain access by using our Hub Access Request form.\n\nMake sure you have a GitHub Account. Take note of your GitHub username.\nFill out the request form and provide needed information.\nWatch your email for notification of authorization and the invite to join the US GHG Center Hub Access GitHub Team.\nOnce you accept the invitation, you can go to hub.ghg.center and login using your GitHub credentials.",
    "crumbs": [
      "User Services",
      "JupyterHub"
    ]
  },
  {
    "objectID": "services/jupyterhub.html#to-access-user-notebooks",
    "href": "services/jupyterhub.html#to-access-user-notebooks",
    "title": "JupyterHub",
    "section": "To access User Notebooks",
    "text": "To access User Notebooks\nThis site provides Jupyter notebooks showing how to load and analyze Earth data in the interactive cloud computing environment.\nFurther instructions are included in each notebook.\nIf you have any questions, please use the feedback form to contact the US GHG Center user support team.",
    "crumbs": [
      "User Services",
      "JupyterHub"
    ]
  }
]