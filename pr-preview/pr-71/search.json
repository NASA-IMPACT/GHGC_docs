[
  {
    "objectID": "datausage.html",
    "href": "datausage.html",
    "title": "U.S. Greenhouse Gas Center: Data Usage Notebooks",
    "section": "",
    "text": "Welcome to the U.S. Greenhouse Gas (GHG) Center data usage notebooks, your gateway to exploring and analyzing curated datasets on greenhouse gas emissions. Our cloud-based system offers seamless access to GHG curated datasets. Dive into the data with our data usage Jupyter notebooks, which demonstrate how to explore, access, visualize, and conduct basic data analysis for each GHG Center dataset in a code notebook environment. The data usage notebooks are grouped topically. Click on a notebook to learn more about the dataset and to view the data usage code.\nJoin us in our mission to make data-driven environmental solutions. Explore, analyze, and make a difference with the US GHG Center.\nView the US GHG Center Data Catalog",
    "crumbs": [
      "Data Usage Notebooks"
    ]
  },
  {
    "objectID": "datausage.html#gridded-anthropogenic-greenhouse-gas-emissions",
    "href": "datausage.html#gridded-anthropogenic-greenhouse-gas-emissions",
    "title": "U.S. Greenhouse Gas Center: Data Usage Notebooks",
    "section": "Gridded Anthropogenic Greenhouse Gas Emissions",
    "text": "Gridded Anthropogenic Greenhouse Gas Emissions\n\nOCO-2 MIP Top-Down CO₂ Budgets\n\nBeginner level notebook to access, visualize, explore statistics, and create a time series of the OCO-2 MIP Top-Down CO₂ Budgets dataset.\nIntermediate level notebook to read and visualizeNational CO₂ Budgets using OCO-2 MIP Top-Down CO₂ Budget country total data. This notebook utilizes the country totals available at ceos.org/gst/carbon-dioxide, which compliment the global 1° x 1° gridded CO₂ Budget data featured in the US GHG Center.\n\nODIAC Fossil Fuel CO₂ Emissions\n\nBeginner level notebook to access, visualize, explore statistics, and create a time series of the ODIAC Fossil Fuel CO₂ Emissions dataset.\n\nTM5-4DVar Isotopic CH₄ Inverse Fluxes\n\nBeginner level notebook to access, visualize, explore statistics, and create a time series of the TM5-4DVar Isotopic CH₄ Inverse Fluxes dataset.\n\nU.S. Gridded Anthropogenic Methane Emissions Inventory\n\nBeginner level notebook to access, visualize, explore statistics, and create a time series of the U.S. Gridded Anthropogenic Methane Emissions Inventory dataset.",
    "crumbs": [
      "Data Usage Notebooks"
    ]
  },
  {
    "objectID": "datausage.html#natural-greenhouse-gas-emissions-and-sinks",
    "href": "datausage.html#natural-greenhouse-gas-emissions-and-sinks",
    "title": "U.S. Greenhouse Gas Center: Data Usage Notebooks",
    "section": "Natural Greenhouse Gas Emissions and Sinks",
    "text": "Natural Greenhouse Gas Emissions and Sinks\n\nAir-Sea CO₂ Flux, ECCO-Darwin Model v5\n\nBeginner level notebook to access, visualize, explore statistics, and create a time series of the Air-Sea CO₂ Flux, ECCO-Darwin Model v5 dataset.\n\nMiCASA Land Carbon Flux\n\nBeginner level notebook to access, visualize, explore statistics, and create a time series of the MiCASA Land Carbon Flux dataset.\n\nGOSAT-based Top-down Total and Natural Methane Emissions\n\nBeginner level notebook to access, visualize, explore statistics, and create a time series of the GOSAT-based Top-down Total and Natural Methane Emissions dataset.\n\nOCO-2 MIP Top-Down CO₂ Budgets\n\nBeginner level notebook to access, visualize, explore statistics, and create a time series of the OCO-2 MIP Top-Down CO₂ Budgets dataset.\nIntermediate level notebook to read and visualizeNational CO₂ Budgets using OCO-2 MIP Top-Down CO₂ Budget country total data. This notebook utilizes the country totals available at ceos.org/gst/carbon-dioxide, which compliment the global 1° x 1° gridded CO₂ Budget data featured in the US GHG Center.\n\nTM5-4DVar Isotopic CH₄ Inverse Fluxes\n\nBeginner level notebook to access, visualize, explore statistics, and create a time series of the TM5-4DVar Isotopic CH₄ Inverse Fluxes dataset.\n\nWetland Methane Emissions, LPJ-EOSIM model\n\nBeginner level notebook to access, visualize, explore statistics, and create a time series of the Wetland Methane Emissions, LPJ-EOSIM model dataset.",
    "crumbs": [
      "Data Usage Notebooks"
    ]
  },
  {
    "objectID": "datausage.html#large-emissions-events",
    "href": "datausage.html#large-emissions-events",
    "title": "U.S. Greenhouse Gas Center: Data Usage Notebooks",
    "section": "Large Emissions Events",
    "text": "Large Emissions Events\n\nEMIT Methane Point Source Plume Complexes\n\nBeginner level notebook to access, visualize, explore statistics, and create a time series of the EMIT Methane Point Source Plume Complexes dataset.",
    "crumbs": [
      "Data Usage Notebooks"
    ]
  },
  {
    "objectID": "datausage.html#greenhouse-gas-concentrations",
    "href": "datausage.html#greenhouse-gas-concentrations",
    "title": "U.S. Greenhouse Gas Center: Data Usage Notebooks",
    "section": "Greenhouse Gas Concentrations",
    "text": "Greenhouse Gas Concentrations\n\nAtmospheric Carbon Dioxide Concentrations from NOAA Global Monitoring Laboratory\n\nBeginner level notebook to access, visualize, explore statistics, and create a time series of the Atmospheric Carbon Dioxide Concentrations from NOAA Global Monitoring Laboratory dataset.\n\nAtmospheric Methane Concentrations from NOAA Global Monitoring Laboratory\n\nBeginner level notebook to access, visualize, explore statistics, and create a time series of the Atmospheric Methane Concentrations from NOAA Global Monitoring Laboratory dataset.\n\nOCO-2 GEOS Column CO₂ Concentrations\n\nBeginner level notebook to access, visualize, explore statistics, and create a time series of the OCO-2 GEOS Column CO₂ Concentrations dataset.",
    "crumbs": [
      "Data Usage Notebooks"
    ]
  },
  {
    "objectID": "datausage.html#socioeconomic",
    "href": "datausage.html#socioeconomic",
    "title": "U.S. Greenhouse Gas Center: Data Usage Notebooks",
    "section": "Socioeconomic",
    "text": "Socioeconomic\n\nSEDAC Gridded World Population Density\n\nBeginner level notebook to access, visualize, explore statistics, and create a time series of the SEDAC Gridded World Population Density dataset.",
    "crumbs": [
      "Data Usage Notebooks"
    ]
  },
  {
    "objectID": "datausage.html#contact",
    "href": "datausage.html#contact",
    "title": "U.S. Greenhouse Gas Center: Data Usage Notebooks",
    "section": "Contact",
    "text": "Contact\nFor technical help or general questions, please contact the support team using the feedback form.",
    "crumbs": [
      "Data Usage Notebooks"
    ]
  },
  {
    "objectID": "workflow.html",
    "href": "workflow.html",
    "title": "U.S. Greenhouse Gas Center: Data Flow Diagrams",
    "section": "",
    "text": "Welcome to the homepage for U.S. Greenhouse Gas (GHG) Center data workflow diagrams. Use these diagrams to discover the journey of each dataset from acquisition to integration in the US GHG Center.\nData flow diagrams are grouped topically and labeled by dataset name. Click on a dataset name to view the data flow diagram for that dataset, which summarizes the process followed to bring the dataset into the US GHG Center.\nJoin us in our mission to make data-driven environmental solutions accessible. Explore, analyze, and make a difference with the US GHG Center.\nView the US GHG Center Data Catalog",
    "crumbs": [
      "Data Workflow Documentation"
    ]
  },
  {
    "objectID": "workflow.html#gridded-anthropogenic-greenhouse-gas-emissions",
    "href": "workflow.html#gridded-anthropogenic-greenhouse-gas-emissions",
    "title": "U.S. Greenhouse Gas Center: Data Flow Diagrams",
    "section": "Gridded Anthropogenic Greenhouse Gas Emissions",
    "text": "Gridded Anthropogenic Greenhouse Gas Emissions\n\nOCO-2 MIP Top-Down CO₂ Budgets Data Flow Diagram\nODIAC Fossil Fuel CO₂ Emissions Data Flow Diagram\nTM5-4DVar Isotopic CH₄ Inverse Fluxes Data Flow Diagram\nU.S. Gridded Anthropogenic Methane Emissions Inventory Data Flow Diagram",
    "crumbs": [
      "Data Workflow Documentation"
    ]
  },
  {
    "objectID": "workflow.html#natural-greenhouse-gas-sources-emissions-and-sinks",
    "href": "workflow.html#natural-greenhouse-gas-sources-emissions-and-sinks",
    "title": "U.S. Greenhouse Gas Center: Data Flow Diagrams",
    "section": "Natural Greenhouse Gas Sources Emissions and Sinks",
    "text": "Natural Greenhouse Gas Sources Emissions and Sinks\n\nAir-Sea CO₂ Flux, ECCO-Darwin Model v5 Data Flow Diagram\nMiCASA Land Carbon Flux Data Flow Diagram\nGOSAT-based Top-down Total and Natural Methane Emissions Data Flow Diagram\nOCO-2 MIP Top-Down CO₂ Budgets Data Flow Diagram\nTM5-4DVar Isotopic CH₄ Inverse Fluxes Data Flow Diagram\nWetland Methane Emissions, LPJ-EOSIM Model Data Flow Diagram",
    "crumbs": [
      "Data Workflow Documentation"
    ]
  },
  {
    "objectID": "workflow.html#large-emissions-events",
    "href": "workflow.html#large-emissions-events",
    "title": "U.S. Greenhouse Gas Center: Data Flow Diagrams",
    "section": "Large Emissions Events",
    "text": "Large Emissions Events\n\nEMIT Methane Point Source Plume Complexes Data Flow Diagram",
    "crumbs": [
      "Data Workflow Documentation"
    ]
  },
  {
    "objectID": "workflow.html#greenhouse-gas-concentrations",
    "href": "workflow.html#greenhouse-gas-concentrations",
    "title": "U.S. Greenhouse Gas Center: Data Flow Diagrams",
    "section": "Greenhouse Gas Concentrations",
    "text": "Greenhouse Gas Concentrations\n\nAtmospheric Carbon Dioxide Concentrations from NOAA Global Monitoring Laboratory Data Flow Diagram\nAtmospheric Carbon Dioxide Concentrations from NOAA Global Monitoring Laboratory Data Flow Diagram\nOCO-2 GEOS Column CO₂ Concentrations Data Flow Diagram, Satellite Observations",
    "crumbs": [
      "Data Workflow Documentation"
    ]
  },
  {
    "objectID": "workflow.html#socioeconomic",
    "href": "workflow.html#socioeconomic",
    "title": "U.S. Greenhouse Gas Center: Data Flow Diagrams",
    "section": "Socioeconomic",
    "text": "Socioeconomic\n\nSEDAC Gridded World Population Density Data Flow Diagram",
    "crumbs": [
      "Data Workflow Documentation"
    ]
  },
  {
    "objectID": "workflow.html#contact",
    "href": "workflow.html#contact",
    "title": "U.S. Greenhouse Gas Center: Data Flow Diagrams",
    "section": "Contact",
    "text": "Contact\nFor technical help or general questions, please contact the support team using the feedback form.",
    "crumbs": [
      "Data Workflow Documentation"
    ]
  },
  {
    "objectID": "datatransformation.html",
    "href": "datatransformation.html",
    "title": "U.S. Greenhouse Gas Center: Data Transformation Notebooks",
    "section": "",
    "text": "Welcome to the U.S. Greenhouse Gas (GHG) Center data transformation notebooks, where we harness the power of Cloud Optimized Geotiffs (COGs) to offer a dynamic, cloud-based platform for exploring and analyzing greenhouse gas datasets. Dive into our curated collection of GHG datasets, all optimized for seamless accessibility and analysis.\nDiscover the journey of each dataset from its original format to COGs through the below Jupyter notebooks. The transformation examples are grouped topically. The dataset product type (model output, satellite observation, etc.) is noted next to the notebook name. Click on a notebook to learn more about the dataset and to view the transformation code.\nJoin us in our mission to make data-driven environmental solutions. Explore, analyze, and make a difference with the US GHG Center.\nView the US GHG Center Data Catalog",
    "crumbs": [
      "Data Transformation Notebooks"
    ]
  },
  {
    "objectID": "datatransformation.html#gridded-anthropogenic-greenhouse-gas-emissions",
    "href": "datatransformation.html#gridded-anthropogenic-greenhouse-gas-emissions",
    "title": "U.S. Greenhouse Gas Center: Data Transformation Notebooks",
    "section": "Gridded Anthropogenic Greenhouse Gas Emissions",
    "text": "Gridded Anthropogenic Greenhouse Gas Emissions\n\nOCO-2 MIP Top-Down CO₂ Budgets\nODIAC Fossil Fuel CO₂ Emissions\nTM5-4DVar Isotopic CH₄ Inverse Fluxes\nU.S. Gridded Anthropogenic Methane Emissions Inventory",
    "crumbs": [
      "Data Transformation Notebooks"
    ]
  },
  {
    "objectID": "datatransformation.html#natural-greenhouse-gas-emissions-and-sinks",
    "href": "datatransformation.html#natural-greenhouse-gas-emissions-and-sinks",
    "title": "U.S. Greenhouse Gas Center: Data Transformation Notebooks",
    "section": "Natural Greenhouse Gas Emissions and Sinks",
    "text": "Natural Greenhouse Gas Emissions and Sinks\n\nAir-Sea CO₂ Flux, ECCO-Darwin Model v5\nGOSAT-based Top-down Total and Natural Methane Emissions\nOCO-2 MIP Top-Down CO₂ Budgets\nTM5-4DVar Isotopic CH₄ Inverse Fluxes",
    "crumbs": [
      "Data Transformation Notebooks"
    ]
  },
  {
    "objectID": "datatransformation.html#large-emissions-events",
    "href": "datatransformation.html#large-emissions-events",
    "title": "U.S. Greenhouse Gas Center: Data Transformation Notebooks",
    "section": "Large Emissions Events",
    "text": "Large Emissions Events\n\nEMIT Methane Point Source Plume Complexes",
    "crumbs": [
      "Data Transformation Notebooks"
    ]
  },
  {
    "objectID": "datatransformation.html#greenhouse-gas-concentrations",
    "href": "datatransformation.html#greenhouse-gas-concentrations",
    "title": "U.S. Greenhouse Gas Center: Data Transformation Notebooks",
    "section": "Greenhouse Gas Concentrations",
    "text": "Greenhouse Gas Concentrations\n\nOCO-2 GEOS Column CO₂ Concentrations",
    "crumbs": [
      "Data Transformation Notebooks"
    ]
  },
  {
    "objectID": "datatransformation.html#socioeconomic",
    "href": "datatransformation.html#socioeconomic",
    "title": "U.S. Greenhouse Gas Center: Data Transformation Notebooks",
    "section": "Socioeconomic",
    "text": "Socioeconomic\n\nSEDAC Gridded World Population Density",
    "crumbs": [
      "Data Transformation Notebooks"
    ]
  },
  {
    "objectID": "datatransformation.html#contact",
    "href": "datatransformation.html#contact",
    "title": "U.S. Greenhouse Gas Center: Data Transformation Notebooks",
    "section": "Contact",
    "text": "Contact\nFor technical help or general questions, please contact the support team using the feedback form.",
    "crumbs": [
      "Data Transformation Notebooks"
    ]
  },
  {
    "objectID": "cog_transformation/epa-ch4emission-monthgrid-v2.html",
    "href": "cog_transformation/epa-ch4emission-monthgrid-v2.html",
    "title": "Gridded Anthropogenic Methane Emissions Inventory",
    "section": "",
    "text": "This script was used to transform the Gridded Anthropogenic Methane Emissions Inventory dataset from netCDF to Cloud Optimized GeoTIFF (COG) format for display in the Greenhouse Gas (GHG) Center.\n\nimport os\nimport xarray\nimport re\nimport pandas as pd\nimport json\nimport tempfile\nimport boto3\nfrom datetime import datetime\nfrom dateutil.relativedelta import relativedelta\n\n\nsession = boto3.session.Session()\ns3_client = session.client(\"s3\")\nbucket_name = (\n    \"ghgc-data-store-dev\"  # S3 bucket where the COGs are stored after transformation\n)\nFOLDER_NAME = \"epa_emissions/monthly_scale\"\ns3_folder_name = \"epa-emissions-monthly-scale-factors\"\n\nfiles_processed = pd.DataFrame(\n    columns=[\"file_name\", \"COGs_created\"]\n)  # A dataframe to keep track of the files that we have transformed into COGs\n\n# Reading the raw netCDF files from local machine\nfor name in os.listdir(FOLDER_NAME):\n    xds = xarray.open_dataset(f\"{FOLDER_NAME}/{name}\", engine=\"netcdf4\")\n    xds = xds.assign_coords(lon=(((xds.lon + 180) % 360) - 180)).sortby(\"lon\")\n    variable = [var for var in xds.data_vars]\n    filename = name.split(\"/ \")[-1]\n    filename_elements = re.split(\"[_ .]\", filename)\n    start_time = datetime(int(filename_elements[-2]), 1, 1)\n\n    for time_increment in range(0, len(xds.time)):\n        for var in variable:\n            filename = name.split(\"/ \")[-1]\n            filename_elements = re.split(\"[_ .]\", filename)\n            data = getattr(xds.isel(time=time_increment), var)\n            data = data.isel(lat=slice(None, None, -1))\n            data.rio.set_spatial_dims(\"lon\", \"lat\", inplace=True)\n            data.rio.write_crs(\"epsg:4326\", inplace=True)\n            date = start_time + relativedelta(months=+time_increment)\n\n            # # insert date of generated COG into filename\n            filename_elements.pop()\n            filename_elements[-1] = date.strftime(\"%Y%m\")\n            filename_elements.insert(2, var)\n            cog_filename = \"_\".join(filename_elements)\n            # # add extension\n            cog_filename = f\"{cog_filename}.tif\"\n\n            with tempfile.NamedTemporaryFile() as temp_file:\n                data.rio.to_raster(\n                    temp_file.name,\n                    driver=\"COG\",\n                )\n                s3_client.upload_file(\n                    Filename=temp_file.name,\n                    Bucket=bucket_name,\n                    Key=f\"{s3_folder_name}/{cog_filename}\",\n                )\n\n            files_processed = files_processed._append(\n                {\"file_name\": name, \"COGs_created\": cog_filename},\n                ignore_index=True,\n            )\n\n            print(f\"Generated and saved COG: {cog_filename}\")\n\n# Generate the json file with the metadata that is present in the netCDF files.\nwith tempfile.NamedTemporaryFile(mode=\"w+\") as fp:\n    json.dump(xds.attrs, fp)\n    json.dump({\"data_dimensions\": dict(xds.dims)}, fp)\n    json.dump({\"data_variables\": list(xds.data_vars)}, fp)\n    fp.flush()\n\n    s3_client.upload_file(\n        Filename=fp.name,\n        Bucket=bucket_name,\n        Key=f\"{s3_folder_name}/metadata.json\",\n    )\n\n# creating the csv file with the names of files transformed.\nfiles_processed.to_csv(\n    f\"s3://{bucket_name}/{s3_folder_name}/files_converted.csv\",\n)\nprint(\"Done generating COGs\")\n\n\n\n\n Back to top"
  },
  {
    "objectID": "cog_transformation/oco2-mip-co2budget-yeargrid-v1.html",
    "href": "cog_transformation/oco2-mip-co2budget-yeargrid-v1.html",
    "title": "OCO-2 MIP Top-Down CO₂ Budgets",
    "section": "",
    "text": "This script was used to transform the OCO-2 MIP Top-Down CO₂ Budgets dataset from netCDF to Cloud Optimized GeoTIFF (COG) format for display in the Greenhouse Gas (GHG) Center.\n\nimport os\nimport xarray\nimport re\nimport pandas as pd\nimport json\nimport tempfile\nimport boto3\nimport rasterio\nfrom datetime import datetime\nfrom dateutil.relativedelta import relativedelta\n\n\nsession = boto3.session.Session()\ns3_client = session.client(\"s3\")\nbucket_name = \"ghgc-data-store-dev\" # S3 bucket where the COGs are to be stored\nyear_ = datetime(2015, 1, 1)    # Initialize the starting date time of the dataset.\n\nCOG_PROFILE = {\"driver\": \"COG\", \"compress\": \"DEFLATE\"}\n\n# Reading the raw netCDF files from local machine\nfiles_processed = pd.DataFrame(columns=[\"file_name\", \"COGs_created\"])   # A dataframe to keep track of the files that are converted into COGs\nfor name in os.listdir(\"new_data\"):\n    ds = xarray.open_dataset(\n        f\"new_data/{name}\",\n        engine=\"netcdf4\",\n    )\n    ds = ds.rename({\"latitude\": \"lat\", \"longitude\": \"lon\"})\n    # assign coords from dimensions\n    ds = ds.assign_coords(lon=(((ds.lon + 180) % 360) - 180)).sortby(\"lon\")\n    ds = ds.assign_coords(lat=list(ds.lat))\n\n    variable = [var for var in ds.data_vars]\n\n    for time_increment in range(0, len(ds.year)):\n        for var in variable[2:]:\n            filename = name.split(\"/ \")[-1]\n            filename_elements = re.split(\"[_ .]\", filename)\n            try:\n                data = ds[var].sel(year=time_increment)\n                date = year_ + relativedelta(years=+time_increment)\n                filename_elements[-1] = date.strftime(\"%Y\")\n                # # insert date of generated COG into filename\n                filename_elements.insert(2, var)\n                cog_filename = \"_\".join(filename_elements)\n                # # add extension\n                cog_filename = f\"{cog_filename}.tif\"\n            except KeyError:\n                data = ds[var]\n                date = year_ + relativedelta(years=+(len(ds.year) - 1))\n                filename_elements.pop()\n                filename_elements.append(year_.strftime(\"%Y\"))\n                filename_elements.append(date.strftime(\"%Y\"))\n                filename_elements.insert(2, var)\n                cog_filename = \"_\".join(filename_elements)\n                # # add extension\n                cog_filename = f\"{cog_filename}.tif\"\n\n            data = data.reindex(lat=list(reversed(data.lat)))\n\n            data.rio.set_spatial_dims(\"lon\", \"lat\")\n            data.rio.write_crs(\"epsg:4326\", inplace=True)\n\n            # generate COG\n            COG_PROFILE = {\"driver\": \"COG\", \"compress\": \"DEFLATE\"}\n            with tempfile.NamedTemporaryFile() as temp_file:\n                data.rio.to_raster(temp_file.name, **COG_PROFILE)\n                s3_client.upload_file(\n                    Filename=temp_file.name,\n                    Bucket=bucket_name,\n                    Key=f\"ceos_co2_flux/{cog_filename}\",\n                )\n\n            files_processed = files_processed._append(\n                {\"file_name\": name, \"COGs_created\": cog_filename},\n                ignore_index=True,\n            )\n\n            print(f\"Generated and saved COG: {cog_filename}\")\n\n# creating the csv file with the names of files transformed.\nfiles_processed.to_csv(\n    f\"s3://{bucket_name}/ceos_co2_flux/files_converted.csv\",\n)\nprint(\"Done generating COGs\")\n\n\n\n\n Back to top",
    "crumbs": [
      "Data Transformation Notebooks",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "OCO-2 MIP Top-Down CO₂ Budgets"
    ]
  },
  {
    "objectID": "cog_transformation/odiac-ffco2-monthgrid-v2022.html",
    "href": "cog_transformation/odiac-ffco2-monthgrid-v2022.html",
    "title": "ODIAC Fossil Fuel CO₂ Emissions",
    "section": "",
    "text": "This script was used to transform the ODIAC Fossil Fuel CO₂ Emissions dataset from GeoTIFF to Cloud Optimized GeoTIFF (COG) format for display in the Greenhouse Gas (GHG) Center.\n\nimport os\nimport xarray\nimport re\nimport pandas as pd\n\nimport tempfile\nimport boto3\n\n\nsession = boto3.session.Session()\ns3_client = session.client(\"s3\")\nbucket_name = \"ghgc-data-store-dev\" # S3 bucket where the COGs are stored after transformation\n\nfold_names = os.listdir(\"ODIAC\")\n\nfiles_processed = pd.DataFrame(columns=[\"file_name\", \"COGs_created\"])   # A dataframe to keep track of the files that we have transformed into COGs\n\n# Reading the raw netCDF files from local machine\nfor fol_ in fold_names:\n    for name in os.listdir(f\"ODIAC/{fol_}\"):\n        xds = xarray.open_dataarray(f\"ODIAC/{fol_}/{name}\")\n\n        filename = name.split(\"/ \")[-1]\n        filename_elements = re.split(\"[_ .]\", filename)\n        # # insert date of generated COG into filename\n        filename_elements.pop()\n        filename_elements[-1] = fol_ + filename_elements[-1][-2:]\n\n        xds.rio.set_spatial_dims(\"x\", \"y\", inplace=True)\n        xds.rio.write_nodata(-9999, inplace=True)\n        xds.rio.write_crs(\"epsg:4326\", inplace=True)\n\n        cog_filename = \"_\".join(filename_elements)\n        # # add extension\n        cog_filename = f\"{cog_filename}.tif\"\n\n        with tempfile.NamedTemporaryFile() as temp_file:\n            xds.rio.to_raster(\n                temp_file.name,\n                driver=\"COG\",\n            )\n            s3_client.upload_file(\n                Filename=temp_file.name,\n                Bucket=bucket_name,\n                Key=f\"ODIAC_geotiffs_COGs/{cog_filename}\",\n            )\n\n        files_processed = files_processed._append(\n            {\"file_name\": name, \"COGs_created\": cog_filename},\n            ignore_index=True,\n        )\n\n        print(f\"Generated and saved COG: {cog_filename}\")\n\n\n# creating the csv file with the names of files transformed.\nfiles_processed.to_csv(\n    f\"s3://{bucket_name}/ODIAC_COGs/files_converted.csv\",\n)\nprint(\"Done generating COGs\")\n\n\n\n\n Back to top",
    "crumbs": [
      "Data Transformation Notebooks",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "ODIAC Fossil Fuel CO₂ Emissions"
    ]
  },
  {
    "objectID": "cog_transformation/casagfed-carbonflux-monthgrid-v3.html",
    "href": "cog_transformation/casagfed-carbonflux-monthgrid-v3.html",
    "title": "CASA-GFED3 Land Carbon Flux",
    "section": "",
    "text": "Code used to transform CASA-GFED3 Land Carbon Flux data from netcdf to Cloud Optimized Geotiff.\n\nimport os\nimport xarray\nimport re\nimport pandas as pd\nimport json\nimport tempfile\nimport boto3\n\n\nsession = boto3.session.Session()\ns3_client = session.client(\"s3\")\nbucket_name = \"ghgc-data-store-dev\"\ndate_fmt = \"%Y%m\"\n\nfiles_processed = pd.DataFrame(columns=[\"file_name\", \"COGs_created\"])\nfor name in os.listdir(\"geoscarb\"):\n    xds = xarray.open_dataset(\n        f\"geoscarb/{name}\",\n        engine=\"netcdf4\",\n    )\n    xds = xds.assign_coords(\n        longitude=(((xds.longitude + 180) % 360) - 180)\n    ).sortby(\"longitude\")\n    variable = [var for var in xds.data_vars]\n\n    for time_increment in range(0, len(xds.time)):\n        for var in variable[:-1]:\n            filename = name.split(\"/ \")[-1]\n            filename_elements = re.split(\"[_ .]\", filename)\n            data = getattr(xds.isel(time=time_increment), var)\n            data = data.isel(latitude=slice(None, None, -1))\n            data.rio.set_spatial_dims(\"longitude\", \"latitude\", inplace=True)\n            data.rio.write_crs(\"epsg:4326\", inplace=True)\n\n            date = data.time.dt.strftime(date_fmt).item(0)\n            # # insert date of generated COG into filename\n            filename_elements.pop()\n            filename_elements[-1] = date\n            filename_elements.insert(2, var)\n            cog_filename = \"_\".join(filename_elements)\n            # # add extension\n            cog_filename = f\"{cog_filename}.tif\"\n\n            with tempfile.NamedTemporaryFile() as temp_file:\n                data.rio.to_raster(\n                    temp_file.name,\n                    driver=\"COG\",\n                )\n                s3_client.upload_file(\n                    Filename=temp_file.name,\n                    Bucket=bucket_name,\n                    Key=f\"GEOS-Carbs/{cog_filename}\",\n                )\n\n            files_processed = files_processed._append(\n                {\"file_name\": name, \"COGs_created\": cog_filename},\n                ignore_index=True,\n            )\n\n            print(f\"Generated and saved COG: {cog_filename}\")\n\nwith tempfile.NamedTemporaryFile(mode=\"w+\") as fp:\n    json.dump(xds.attrs, fp)\n    json.dump({\"data_dimensions\": dict(xds.dims)}, fp)\n    json.dump({\"data_variables\": list(xds.data_vars)}, fp)\n    fp.flush()\n\n    s3_client.upload_file(\n        Filename=fp.name,\n        Bucket=bucket_name,\n        Key=\"GEOS-Carbs/metadata.json\",\n    )\nfiles_processed.to_csv(\n    f\"s3://{bucket_name}/GEOS-Carbs/files_converted.csv\",\n)\nprint(\"Done generating COGs\")\n\n\n\n\n Back to top"
  },
  {
    "objectID": "cog_transformation/epa-ch4emission-grid-v2express_layers_update.html",
    "href": "cog_transformation/epa-ch4emission-grid-v2express_layers_update.html",
    "title": "Gridded Anthropogenic Methane Emissions Inventory",
    "section": "",
    "text": "This script was used to add concatenated layers and transform Gridded Anthropogenic Methane Emissions Inventory dataset from netCDF to Cloud Optimized GeoTIFF (COG) format for display in the Greenhouse Gas (GHG) Center.\n\nimport os\nimport xarray\nimport re\nimport pandas as pd\nimport json\nimport tempfile\nimport boto3\nfrom datetime import datetime\nimport numpy as np\n\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\nTrue\n\n\n\n# session = boto3.session.Session()\nsession = boto3.Session(\n    aws_access_key_id=os.environ.get(\"AWS_ACCESS_KEY_ID\"),\n    aws_secret_access_key=os.environ.get(\"AWS_SECRET_ACCESS_KEY\"),\n    aws_session_token=os.environ.get(\"AWS_SESSION_TOKEN\"),\n)\ns3_client = session.client(\"s3\")\nbucket_name = (\n    \"ghgc-data-store-dev\"  # S3 bucket where the COGs are stored after transformation\n)\nFOLDER_NAME = \"../data/epa_emissions_express_extension\"\ns3_folder_name = \"epa_express_extension_Mg_km2_yr\"\n\nfiles_processed = pd.DataFrame(\n    columns=[\"file_name\", \"COGs_created\"]\n)  # A dataframe to keep track of the files that we have transformed into COGs\n\n# Reading the raw netCDF files from local machine\nfor name in os.listdir(FOLDER_NAME):\n    xds = xarray.open_dataset(f\"{FOLDER_NAME}/{name}\", engine=\"netcdf4\")\n    xds = xds.assign_coords(lon=(((xds.lon + 180) % 360) - 180)).sortby(\"lon\")\n    variable = [var for var in xds.data_vars]\n    new_variables = {\n        \"all-variables\": variable[:-1],\n        \"agriculture\": variable[17:21],\n        \"natural-gas-systems\": variable[10:15] + [variable[26]],\n        \"petroleum-systems\": variable[5:9],\n        \"waste\": variable[21:26],\n        \"coal-mines\": variable[2:5],\n        \"other\": variable[:2] + [variable[9]] + variable[15:17],\n    }\n    filename = name.split(\"/ \")[-1]\n    filename_elements = re.split(\"[_ .]\", filename)\n    start_time = datetime(int(filename_elements[-2]), 1, 1)\n\n    for time_increment in range(0, len(xds.time)):\n        for key, value in new_variables.items():\n            data = np.zeros(dtype=np.float32, shape=(len(xds.lat), len(xds.lon)))\n            filename = name.split(\"/ \")[-1]\n            filename_elements = re.split(\"[_ .]\", filename)\n            for var in value:\n                data = data + getattr(xds.isel(time=time_increment), var)\n            # data = np.round(data / pow(10, 9), 2)\n            data.values[data.values==0] = np.nan\n            data = data*((1/(6.022*pow(10,23)))*(16.04*pow(10,-6))*366*pow(10,10)*86400)\n            data = data.fillna(-9999)\n            data = data.isel(lat=slice(None, None, -1))\n            data.rio.set_spatial_dims(\"lon\", \"lat\", inplace=True)\n            data.rio.write_crs(\"epsg:4326\", inplace=True)\n\n            # # insert date of generated COG into filename\n            filename_elements.pop()\n            filename_elements[-1] = start_time.strftime(\"%Y\")\n            filename_elements.insert(2, key)\n            cog_filename = \"_\".join(filename_elements)\n            # # add extension\n            cog_filename = f\"{cog_filename}.tif\"\n\n            with tempfile.NamedTemporaryFile() as temp_file:\n                data.rio.to_raster(\n                    temp_file.name,\n                    driver=\"COG\",\n                )\n                s3_client.upload_file(\n                    Filename=temp_file.name,\n                    Bucket=bucket_name,\n                    Key=f\"{s3_folder_name}/{cog_filename}\",\n                )\n\n                files_processed = files_processed._append(\n                    {\"file_name\": name, \"COGs_created\": cog_filename},\n                    ignore_index=True,\n                )\n\n                print(f\"Generated and saved COG: {cog_filename}\")\nprint(\"Done generating COGs\")\n\nTraceback (most recent call last):\n  File \"_pydevd_bundle/pydevd_cython.pyx\", line 1078, in _pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\n  File \"_pydevd_bundle/pydevd_cython.pyx\", line 297, in _pydevd_bundle.pydevd_cython.PyDBFrame.do_wait_suspend\n  File \"/Users/vgaur/miniconda3/envs/cmip6/lib/python3.9/site-packages/debugpy/_vendored/pydevd/pydevd.py\", line 1976, in do_wait_suspend\n    keep_suspended = self._do_wait_suspend(thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\n  File \"/Users/vgaur/miniconda3/envs/cmip6/lib/python3.9/site-packages/debugpy/_vendored/pydevd/pydevd.py\", line 2011, in _do_wait_suspend\n    time.sleep(0.01)\nKeyboardInterrupt\n\n\nKeyboardInterrupt: \n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "cog_transformation/gosat-based-ch4budget-yeargrid-v1.html",
    "href": "cog_transformation/gosat-based-ch4budget-yeargrid-v1.html",
    "title": "GOSAT-based Top-down Methane Budgets",
    "section": "",
    "text": "This script was used to transform the GOSAT-based Top-down Methane Budgets dataset from netCDF to Cloud Optimized GeoTIFF (COG) format for display in the Greenhouse Gas (GHG) Center.\n\nimport os\nimport xarray\nimport re\nimport pandas as pd\nimport json\nimport tempfile\nimport boto3\nimport rasterio\nfrom datetime import datetime\nfrom dateutil.relativedelta import relativedelta\n\n\nsession = boto3.session.Session()\ns3_client = session.client(\"s3\")\nbucket_name = (\n    \"ghgc-data-store-dev\"  # S3 bucket where the COGs are stored after transformation\n)\nyear_ = datetime(2019, 1, 1)\nfolder_name = \"new_data/CH4-inverse-flux\"\n\nCOG_PROFILE = {\"driver\": \"COG\", \"compress\": \"DEFLATE\"}\n\nfiles_processed = pd.DataFrame(\n    columns=[\"file_name\", \"COGs_created\"]\n)  # A dataframe to keep track of the files that we have transformed into COGs\n\n# Reading the raw netCDF files from local machine\nfor name in os.listdir(folder_name):\n    ds = xarray.open_dataset(\n        f\"{folder_name}/{name}\",\n        engine=\"netcdf4\",\n    )\n\n    ds = ds.rename({\"dimy\": \"lat\", \"dimx\": \"lon\"})\n    # assign coords from dimensions\n    ds = ds.assign_coords(lon=(((ds.lon + 180) % 360) - 180)).sortby(\"lon\")\n    ds = ds.assign_coords(lat=((ds.lat / 180) * 180) - 90).sortby(\"lat\")\n\n    variable = [var for var in ds.data_vars]\n\n    for var in variable[2:]:\n        filename = name.split(\"/ \")[-1]\n        filename_elements = re.split(\"[_ .]\", filename)\n        data = ds[var]\n        filename_elements.pop()\n        filename_elements.insert(2, var)\n        cog_filename = \"_\".join(filename_elements)\n        # # add extension\n        cog_filename = f\"{cog_filename}.tif\"\n\n        data = data.reindex(lat=list(reversed(data.lat)))\n\n        data.rio.set_spatial_dims(\"lon\", \"lat\")\n        data.rio.write_crs(\"epsg:4326\", inplace=True)\n\n        # generate COG\n        COG_PROFILE = {\"driver\": \"COG\", \"compress\": \"DEFLATE\"}\n\n        with tempfile.NamedTemporaryFile() as temp_file:\n            data.rio.to_raster(temp_file.name, **COG_PROFILE)\n            s3_client.upload_file(\n                Filename=temp_file.name,\n                Bucket=bucket_name,\n                Key=f\"ch4_inverse_flux/{cog_filename}\",\n            )\n\n        files_processed = files_processed._append(\n            {\"file_name\": name, \"COGs_created\": cog_filename},\n            ignore_index=True,\n        )\n\n        print(f\"Generated and saved COG: {cog_filename}\")\n\n# Generate the json file with the metadata that is present in the netCDF files.\nwith tempfile.NamedTemporaryFile(mode=\"w+\") as fp:\n    json.dump(ds.attrs, fp)\n    json.dump({\"data_dimensions\": dict(ds.dims)}, fp)\n    json.dump({\"data_variables\": list(ds.data_vars)}, fp)\n    fp.flush()\n\n    s3_client.upload_file(\n        Filename=fp.name,\n        Bucket=bucket_name,\n        Key=\"ch4_inverse_flux/metadata.json\",\n    )\n\n# creating the csv file with the names of files transformed.\nfiles_processed.to_csv(\n    f\"s3://{bucket_name}/ch4_inverse_flux/files_converted.csv\",\n)\nprint(\"Done generating COGs\")\n\n\n\n\n Back to top",
    "crumbs": [
      "Data Transformation Notebooks",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "GOSAT-based Top-down Methane Budgets"
    ]
  },
  {
    "objectID": "cog_transformation/tm54dvar-ch4flux-monthgrid-v1.html",
    "href": "cog_transformation/tm54dvar-ch4flux-monthgrid-v1.html",
    "title": "TM5-4DVar Isotopic CH₄ Inverse Fluxes",
    "section": "",
    "text": "This script was used to transform the TM5-4DVar Isotopic CH₄ Inverse Fluxes dataset from netCDF to Cloud Optimized GeoTIFF (COG) format for display in the Greenhouse Gas (GHG) Center.\n\nimport os\nimport xarray\nimport re\nimport pandas as pd\nimport json\nimport tempfile\nimport boto3\nfrom datetime import datetime\n\n\nsession = boto3.session.Session()\ns3_client = session.client(\"s3\")\nbucket_name = (\n    \"ghgc-data-store-dev\"  # S3 bucket where the COGs are stored after transformation\n)\nFOLDER_NAME = \"tm5-ch4-inverse-flux\"\n\nfiles_processed = pd.DataFrame(\n    columns=[\"file_name\", \"COGs_created\"]\n)  # A dataframe to keep track of the files that we have transformed into COGs\n\n# Reading the raw netCDF files from local machine\nfor name in os.listdir(FOLDER_NAME):\n    xds = xarray.open_dataset(f\"{FOLDER_NAME}/{name}\", engine=\"netcdf4\")\n    xds = xds.rename({\"latitude\": \"lat\", \"longitude\": \"lon\"})\n    xds = xds.assign_coords(lon=(((xds.lon + 180) % 360) - 180)).sortby(\"lon\")\n    variable = [var for var in xds.data_vars if \"global\" not in var]\n\n    for time_increment in range(0, len(xds.months)):\n        filename = name.split(\"/ \")[-1]\n        filename_elements = re.split(\"[_ .]\", filename)\n        start_time = datetime(int(filename_elements[-2]), time_increment + 1, 1)\n        for var in variable:\n            data = getattr(xds.isel(months=time_increment), var)\n            data = data.isel(lat=slice(None, None, -1))\n            data.rio.set_spatial_dims(\"lon\", \"lat\", inplace=True)\n            data.rio.write_crs(\"epsg:4326\", inplace=True)\n\n            # # insert date of generated COG into filename\n            filename_elements.pop()\n            filename_elements[-1] = start_time.strftime(\"%Y%m\")\n            filename_elements.insert(2, var)\n            cog_filename = \"_\".join(filename_elements)\n            # # add extension\n            cog_filename = f\"{cog_filename}.tif\"\n\n            with tempfile.NamedTemporaryFile() as temp_file:\n                data.rio.to_raster(\n                    temp_file.name,\n                    driver=\"COG\",\n                )\n                s3_client.upload_file(\n                    Filename=temp_file.name,\n                    Bucket=bucket_name,\n                    Key=f\"{FOLDER_NAME}/{cog_filename}\",\n                )\n\n            files_processed = files_processed._append(\n                {\"file_name\": name, \"COGs_created\": cog_filename},\n                ignore_index=True,\n            )\n\n            print(f\"Generated and saved COG: {cog_filename}\")\n\n# Generate the json file with the metadata that is present in the netCDF files.\nwith tempfile.NamedTemporaryFile(mode=\"w+\") as fp:\n    json.dump(xds.attrs, fp)\n    json.dump({\"data_dimensions\": dict(xds.dims)}, fp)\n    json.dump({\"data_variables\": list(xds.data_vars)}, fp)\n    fp.flush()\n\n    s3_client.upload_file(\n        Filename=fp.name,\n        Bucket=bucket_name,\n        Key=f\"{FOLDER_NAME}/metadata.json\",\n    )\n\n# creating the csv file with the names of files transformed.\nfiles_processed.to_csv(\n    f\"s3://{bucket_name}/{FOLDER_NAME}/files_converted.csv\",\n)\nprint(\"Done generating COGs\")\n\n\n\n\n Back to top",
    "crumbs": [
      "Data Transformation Notebooks",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "TM5-4DVar Isotopic CH₄ Inverse Fluxes"
    ]
  },
  {
    "objectID": "processing_and_verification_reports/tm54dvar-ch4flux-monthgrid-v1_Processing and Verification Report.html",
    "href": "processing_and_verification_reports/tm54dvar-ch4flux-monthgrid-v1_Processing and Verification Report.html",
    "title": "TM5-4DVar Isotopic CH₄ Inverse Fluxes",
    "section": "",
    "text": "This browser does not support PDFs. Please download the PDF to view it: Download PDF.\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Processing and Verification Reports",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "TM5-4DVar Isotopic CH₄ Inverse Fluxes"
    ]
  },
  {
    "objectID": "processing_and_verification_reports/oco2geos-co2-daygrid-v10r_Processing and Verification Report.html",
    "href": "processing_and_verification_reports/oco2geos-co2-daygrid-v10r_Processing and Verification Report.html",
    "title": "OCO-2 GEOS Column CO₂ Concentrations",
    "section": "",
    "text": "This browser does not support PDFs. Please download the PDF to view it: Download PDF.\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Processing and Verification Reports",
      "Greenhouse Gas Concentrations",
      "OCO-2 GEOS Column CO₂ Concentrations"
    ]
  },
  {
    "objectID": "processing_and_verification_reports/lpjwsl-wetlandch4-grid-v1_Processing and Verification Report.html",
    "href": "processing_and_verification_reports/lpjwsl-wetlandch4-grid-v1_Processing and Verification Report.html",
    "title": "Wetland Methane Emissions, LPJ-wsl Model",
    "section": "",
    "text": "This browser does not support PDFs. Please download the PDF to view it: Download PDF.\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Processing and Verification Reports",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "Wetland Methane Emissions, LPJ-wsl Model"
    ]
  },
  {
    "objectID": "processing_and_verification_reports/odiac-ffco2-monthgrid-v2022_Processing and Verification Report.html",
    "href": "processing_and_verification_reports/odiac-ffco2-monthgrid-v2022_Processing and Verification Report.html",
    "title": "ODIAC Fossil Fuel CO₂ Emissions",
    "section": "",
    "text": "This browser does not support PDFs. Please download the PDF to view it: Download PDF.\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Processing and Verification Reports",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "ODIAC Fossil Fuel CO₂ Emissions"
    ]
  },
  {
    "objectID": "processing_and_verification_reports/epa-ch4emission-grid-v2express_Processing and Verification Report.html",
    "href": "processing_and_verification_reports/epa-ch4emission-grid-v2express_Processing and Verification Report.html",
    "title": "Gridded Anthropogenic Methane Emissions Inventory",
    "section": "",
    "text": "This browser does not support PDFs. Please download the PDF to view it: Download PDF.\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Processing and Verification Reports",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "Gridded Anthropogenic Methane Emissions Inventory"
    ]
  },
  {
    "objectID": "processing_and_verification_reports/eccodarwin-co2flux-monthgrid-v5_Processing and Verification Report.html",
    "href": "processing_and_verification_reports/eccodarwin-co2flux-monthgrid-v5_Processing and Verification Report.html",
    "title": "Air-Sea CO₂ Flux, ECCO-Darwin Model v5",
    "section": "",
    "text": "This browser does not support PDFs. Please download the PDF to view it: Download PDF.\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Processing and Verification Reports",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "Air-Sea CO₂ Flux, ECCO-Darwin Model v5"
    ]
  },
  {
    "objectID": "services/apis.html",
    "href": "services/apis.html",
    "title": "APIs",
    "section": "",
    "text": "Please note: while some of our services are already very mature, the US GHG Center platform is currently in the beta phase and will undergo many changes in coming months.",
    "crumbs": [
      "User Services",
      "APIs"
    ]
  },
  {
    "objectID": "services/apis.html#open-source",
    "href": "services/apis.html#open-source",
    "title": "APIs",
    "section": "Open Source",
    "text": "Open Source\nMost of the US GHG Center APIs are hosted out of a single project (ghgc-backend) that combines multiple standalone services.",
    "crumbs": [
      "User Services",
      "APIs"
    ]
  },
  {
    "objectID": "user_data_notebooks/oco2-mip-National-co2budget.html",
    "href": "user_data_notebooks/oco2-mip-National-co2budget.html",
    "title": "OCO-2 MIP National Top-Down CO2 Budgets",
    "section": "",
    "text": "Read in National CO2 Budgets using Pandas\nSub-select the data structure using Pandas\nVisualize the CO2 budgets for a country\nInvestigate uncertainties and metrics for understanding the dataset\n\n\n\n\nThis tutorial shows how to explore and plot the dataset reported in:\nByrne et al.: National CO2 budgets (2015–2020) inferred from atmospheric CO2 observations in support of the global stocktake, Earth Syst. Sci. Data, 15, 963–1004, https://doi.org/10.5194/essd-15-963-2023, 2023.\n\n\n\nFirst we will need to import the relevant python modules:\n\nimport pandas as pd # for manipulating csv dataset\nimport numpy as np\nimport matplotlib.pyplot as plt # make plots\nfrom scipy.stats import norm # We will use this for understanding significance\n\n\n\n\nNow we will read in the csv dataset from https://ceos.org/gst/carbon-dioxide.html\n\nurl ='https://ceos.org/gst/files/pilot_topdown_CO2_Budget_countries_v1.csv'\ndf_all = pd.read_csv(url, skiprows=52)\n\n\n\n\nTo simplify the analysis, let’s subselect the results for a single experiment. The experiments are: - IS: estimates fluxes from in situ CO2 measurements - LNLG: estimates fluxes from OCO-2 land CO2 data - LNLGIS: combines in situ and OCO-2 land CO2 data - LNLGOGIS: combines in situ and OCO-2 land and ocean CO2 data\nWe would like to use the experiment that uses the most high-quality CO2 data. There are some concerns about small residual biases in OCO-2 ocean data (Byrne et al., 2023), so let’s use the LNLGIS experiment.\n\n# Choose one experiment from the list ['IS', 'LNLG', 'LNLGIS', 'LNLGOGIS']\nexperiment = 'LNLGIS'\n\n# Subset of columns for a given experiment\nif experiment == 'IS':\n    df = df_all.drop(df_all.columns[[4,5,6,7,8,9,12,13,14,15,16,17,20,21,22,23,24,25,34,35,36]], axis=1)\nif experiment == 'LNLG':\n    df = df_all.drop(df_all.columns[[2,3,6,7,8,9,10,11,14,15,16,17,18,19,22,23,24,25,33,35,36]], axis=1)\nif experiment == 'LNLGIS':\n    df = df_all.drop(df_all.columns[[2,3,4,5,8,9,10,11,12,13,16,17,18,19,20,21,24,25,33,34,36]], axis=1)\nif experiment == 'LNLGOGIS':\n    df = df_all.drop(df_all.columns[[2,3,4,5,6,7,10,11,12,13,14,15,18,19,20,21,22,23,33,34,35]], axis=1)\n\n# We can now look at the colums of data\ndf.head()\n\n\n\n\n\n\n\n\n\nAlpha 3 Code\nYear\nLNLGIS dC_loss (TgCO2)\nLNLGIS dC_loss unc (TgCO2)\nLNLGIS NBE (TgCO2)\nLNLGIS NBE unc (TgCO2)\nLNLGIS NCE (TgCO2)\nLNLGIS NCE unc (TgCO2)\nRivers (TgCO2)\nRiver unc (TgCO2)\nWood+Crop (TgCO2)\nWood+Crop unc (TgCO2)\nFF (TgCO2)\nFF unc (TgCO2)\nZ-statistic\nFUR LNLGIS\n\n\n\n\n0\nAFG\n2015\n39.3407\n153.746\n40.9643\n153.746\n60.3537\n153.744\n-2.43286\n1.69832\n4.05648\n1.21694\n19.3894\n0.797698\n0.37\n0.19\n\n\n1\nAFG\n2016\n50.6167\n175.454\n52.5114\n175.454\n73.0333\n175.452\n-2.16185\n2.24033\n4.05648\n1.21694\n20.5220\n0.678080\n0.31\n0.19\n\n\n2\nAFG\n2017\n54.5096\n179.794\n56.4726\n179.794\n77.5355\n179.793\n-2.09349\n2.37705\n4.05648\n1.21694\n21.0629\n0.695856\n0.47\n0.19\n\n\n3\nAFG\n2018\n116.4260\n243.057\n118.4610\n243.057\n143.9580\n243.056\n-2.02199\n2.52005\n4.05648\n1.21694\n25.4974\n0.695856\n0.39\n0.19\n\n\n4\nAFG\n2019\n64.0162\n181.516\n66.0388\n181.516\n93.8974\n181.514\n-2.03383\n2.49637\n4.05648\n1.21694\n27.8585\n0.797698\n0.49\n0.19\n\n\n\n\n\n\n\n\n\n\n\nLet’s further filter the dataset to look at a specific country. Choose a country by entering the alpha code in the country_name variable below\n\n# Choose a country\ncountry_name = 'USA' \n\n# We can sub-select the data for the country\ncountry_data = df[df['Alpha 3 Code'] == country_name]\n\n# Now we can look at the data for a specific experiment and country\ncountry_data.head()\n\n\n\n\n\n\n\n\n\nAlpha 3 Code\nYear\nLNLGIS dC_loss (TgCO2)\nLNLGIS dC_loss unc (TgCO2)\nLNLGIS NBE (TgCO2)\nLNLGIS NBE unc (TgCO2)\nLNLGIS NCE (TgCO2)\nLNLGIS NCE unc (TgCO2)\nRivers (TgCO2)\nRiver unc (TgCO2)\nWood+Crop (TgCO2)\nWood+Crop unc (TgCO2)\nFF (TgCO2)\nFF unc (TgCO2)\nZ-statistic\nFUR LNLGIS\n\n\n\n\n1232\nUSA\n2015\n-1031.83\n721.213\n-1346.46\n721.213\n4017.31\n713.897\n-165.430\n71.7453\n-149.196\n-44.7589\n5363.77\n102.4670\n-0.81\n0.91\n\n\n1233\nUSA\n2016\n-1419.92\n399.738\n-1743.80\n399.738\n3529.45\n387.079\n-174.684\n53.2375\n-149.196\n-44.7589\n5273.24\n99.8012\n0.04\n0.91\n\n\n1234\nUSA\n2017\n-1375.12\n1034.010\n-1696.63\n1034.010\n3515.14\n1029.250\n-172.308\n57.9894\n-149.196\n-44.7589\n5211.76\n99.0981\n0.67\n0.91\n\n\n1235\nUSA\n2018\n-1018.89\n784.463\n-1333.83\n784.463\n4036.65\n778.179\n-165.747\n71.1117\n-149.196\n-44.7589\n5370.48\n99.0981\n-0.20\n0.91\n\n\n1236\nUSA\n2019\n-1161.41\n718.054\n-1504.61\n718.054\n3728.95\n710.705\n-194.005\n14.5948\n-149.196\n-44.7589\n5233.56\n102.4670\n-0.38\n0.91\n\n\n\n\n\n\n\n\n#This dataset contains fluxes over a five year period, 2015-2020.\nLet’s look at a plot of the annual net terrestrial carbon stock loss (ΔCloss) for each year.\n\n# Make plot\nfig, ax1 = plt.subplots(1, 1, figsize=(6, 4))\nax1.errorbar(country_data['Year'],country_data[experiment+' dC_loss (TgCO2)'],\n                    yerr=country_data[experiment+' dC_loss unc (TgCO2)'],label=experiment,capsize=10)\nax1.legend(loc='upper right')\nax1.set_ylabel('$\\Delta$C$_\\mathrm{loss}$ (TgCO$_2$ year$^{-1}$)')\nax1.set_xlabel('Year')\nax1.set_title('$\\Delta$C$_\\mathrm{loss}$ for '+country_name)\nymin, ymax = ax1.get_ylim()\nmax_abs_y = max(abs(ymin), abs(ymax))\nax1.set_ylim([-max_abs_y, max_abs_y])\nxmin, xmax = ax1.get_xlim()\nax1.plot([xmin,xmax],[0,0],'k',linewidth=0.5)\nax1.set_xlim([xmin, xmax])\n\n\n\n\n\n\n\n\nNext, we can look at the full carbon budget for a given year.\nThe code below creates a plot similar to Fig 13 of Byrne et al. (2023). Each of the bars on the left side of the dashed vertical line (Fossil fuel emissions, lateral C transport by rivers, lateral C transport in crop and wood products, and the net terrestrial carbon stock loss combined to give the net carbon exchange (net surface-atmosphere CO2 flux) shown on the right.\n\n# Pick a specifc year (or mean year)\nyear='mean'\n\n# Make plot\ncountry_data_mean = country_data[country_data['Year'] == year]\na=country_data_mean['Wood+Crop (TgCO2)']\nb=country_data_mean['Wood+Crop unc (TgCO2)']\nprint(b)\n#\nplt.bar(1, country_data_mean['FF (TgCO2)'], yerr=country_data_mean['FF unc (TgCO2)'], label='FF', alpha=0.5)\nplt.bar(2, country_data_mean['Rivers (TgCO2)'], yerr=country_data_mean['River unc (TgCO2)'], label='Rivers', alpha=0.5)\nplt.bar(3, country_data_mean['Wood+Crop (TgCO2)'], yerr=abs(country_data_mean['Wood+Crop unc (TgCO2)']), label='WoodCrop', alpha=0.5)\nplt.bar(4, country_data_mean[experiment+' dC_loss (TgCO2)'], yerr=country_data_mean['LNLGIS dC_loss unc (TgCO2)'], label='dC', alpha=0.5)\nplt.bar(6, country_data_mean[experiment+' NCE (TgCO2)'], yerr=country_data_mean['LNLGIS NCE unc (TgCO2)'], label='NCE', alpha=0.5)\nax = plt.gca()\nymin, ymax = ax.get_ylim()\nplt.plot([5,5],[ymin,ymax],'k:')\nxmin, xmax = ax.get_xlim()\nplt.plot([xmin,xmax],[0,0],'k',linewidth=0.5)\nplt.xlim([xmin,xmax])\nplt.ylim([ymin,ymax])\n#\nplt.xticks([1,2,3,4,6], ['Fossil\\nFuels','Rivers','Wood+\\nCrops','$\\mathrm{\\Delta C _{loss}}$','NCE'])\nplt.title(country_name+' '+year)\nplt.ylabel('CO$_2$ Flux (TgCO$_2$ year$^{-1}$)')\n\n1238   -44.7589\nName: Wood+Crop unc (TgCO2), dtype: float64\n\n\nText(0, 0.5, 'CO$_2$ Flux (TgCO$_2$ year$^{-1}$)')\n\n\n\n\n\n\n\n\n\nUncertainty is an important consideration when analyzing the flux estimates provided by Byrne et al. (2023).\nEach flux estimate is provided with an error estimate representing the standard deviation, and assuming the errors are well prepresented by a normal distribution. This probability dirtribution provided by this uncertainty can be visualized below. We can further quantify the\n\n\n# Select NCE, NBE or dC_loss\nquantity = 'dC_loss'\n\n# Value for comparison\ncomparison_value = 1000 # TgCO2/year\n\n\nMIP_mean = country_data_mean[experiment+' '+quantity+' (TgCO2)'].item()\nMIP_std = country_data_mean[experiment+' '+quantity+' unc (TgCO2)'].item()\n\n# Perform t-test\nt_value = abs(MIP_mean - comparison_value)/(MIP_std / np.sqrt(11))\ncrtical_value = 2.23 # use p=0.05 significance\nif t_value &gt; crtical_value:\n    ttest = 'statistically different'\nif t_value &lt; crtical_value:\n    ttest = 'not statistically\\ndifferent'\n\n# Make plot\nxbounds = abs(MIP_mean)+MIP_std*4\nif abs(crtical_value) &gt; xbounds:\n    xbounds = abs(crtical_value)\nx_axis = np.arange(-1.*xbounds, xbounds, 1) \nplt.plot(x_axis, norm.pdf(x_axis, MIP_mean, MIP_std)) \nax = plt.gca()\nymin, ymax = ax.get_ylim()\nxmin, xmax = ax.get_xlim()\nplt.plot([0,0],[ymin,ymax*1.2],'k:',linewidth=0.5)\nplt.plot([xmin,xmax],[0,0],'k:',linewidth=0.5)\nplt.plot([comparison_value,comparison_value],[ymin,ymax*1.2],'k')\nplt.text(comparison_value+(xmax-xmin)*0.01,ymax*0.96,'value = '+str(comparison_value),ha='left',va='top')\nplt.text(comparison_value+(xmax-xmin)*0.01,ymax*0.9,ttest,ha='left',va='top')\nplt.ylim([ymin,ymax*1.2])\nplt.xlim([xmin,xmax])\nplt.plot(MIP_mean,ymax*1.03,'ko')\nplt.plot([MIP_mean-MIP_std,\n         MIP_mean+MIP_std],\n         [ymax*1.03,ymax*1.03],'k')\nplt.plot([MIP_mean-MIP_std,\n         MIP_mean-MIP_std],\n         [ymax*1.005,ymax*1.055],'k')\nplt.plot([MIP_mean+MIP_std,\n         MIP_mean+MIP_std],\n         [ymax*1.005,ymax*1.055],'k')\nplt.text(MIP_mean,ymax*1.115,\n         str(round(MIP_mean))+' $\\pm$ '+\n         str(round(MIP_std))+' TgCO$_2$',ha='center')\nplt.title(country_name+' '+year+' '+quantity+'')\nplt.yticks([])\nplt.ylabel('Probability')\nplt.xlabel(quantity+' (TgCO$_2$ year$^{-1}$)')\n\nText(0.5, 0, 'dC_loss (TgCO$_2$ year$^{-1}$)')\n\n\n\n\n\n\n\n\n\nFinally, we will examine two metrics that are useful for understanding the confidence in the top-down results:\n\nZ-statistic: metric of agreement in NCE estimates across the experiments that assimilate different CO2 datasets. Experiments are considered significantly different if the magnitude exceeds 1.96\nFractional Uncertainty Reduction (FUR): metric of how strongly the assimilated CO2 data on reduce NCE uncertainties. Values range from 0 to 1, with 0 meaning zero error reduction and 1 meaning complete error reduction\n\nHere we will add a plot of the Z-statistic for each year, and add the FUR value for the country.\n\n# Make plot\nfig, ax1 = plt.subplots(1, 1, figsize=(6, 4))\nax1.plot(country_data['Year'],country_data['Z-statistic'],label=experiment)\nax1.legend(loc='upper right')\nax1.set_ylabel('Z-statistic')\nax1.set_xlabel('Year')\nax1.set_title(country_name)\nymin, ymax = ax1.get_ylim()\nmax_abs_y = max(abs(ymin), abs(ymax))\nax1.set_ylim([-3, 3])\nxmin, xmax = ax1.get_xlim()\nax1.plot([xmin,xmax],[0,0],'k',linewidth=0.5)\nax1.plot([xmin,xmax],[-1.96,-1.96],'k--',linewidth=0.5)\nax1.plot([xmin,xmax],[1.96,1.96],'k--',linewidth=0.5)\nax1.set_xlim([xmin, xmax])\nax1.text(xmin+0.12,2.6,'Fractional error reduction: '+str(country_data['FUR '+experiment].iloc[1]))\n\nText(-0.18000000000000005, 2.6, 'Fractional error reduction: 0.91')"
  },
  {
    "objectID": "user_data_notebooks/oco2-mip-National-co2budget.html#approach",
    "href": "user_data_notebooks/oco2-mip-National-co2budget.html#approach",
    "title": "OCO-2 MIP National Top-Down CO2 Budgets",
    "section": "",
    "text": "Read in National CO2 Budgets using Pandas\nSub-select the data structure using Pandas\nVisualize the CO2 budgets for a country\nInvestigate uncertainties and metrics for understanding the dataset"
  },
  {
    "objectID": "user_data_notebooks/oco2-mip-National-co2budget.html#about-the-data",
    "href": "user_data_notebooks/oco2-mip-National-co2budget.html#about-the-data",
    "title": "OCO-2 MIP National Top-Down CO2 Budgets",
    "section": "",
    "text": "This tutorial shows how to explore and plot the dataset reported in:\nByrne et al.: National CO2 budgets (2015–2020) inferred from atmospheric CO2 observations in support of the global stocktake, Earth Syst. Sci. Data, 15, 963–1004, https://doi.org/10.5194/essd-15-963-2023, 2023."
  },
  {
    "objectID": "user_data_notebooks/oco2-mip-National-co2budget.html#import-required-modules",
    "href": "user_data_notebooks/oco2-mip-National-co2budget.html#import-required-modules",
    "title": "OCO-2 MIP National Top-Down CO2 Budgets",
    "section": "",
    "text": "First we will need to import the relevant python modules:\n\nimport pandas as pd # for manipulating csv dataset\nimport numpy as np\nimport matplotlib.pyplot as plt # make plots\nfrom scipy.stats import norm # We will use this for understanding significance"
  },
  {
    "objectID": "user_data_notebooks/oco2-mip-National-co2budget.html#read-the-co2-national-budget-dataset",
    "href": "user_data_notebooks/oco2-mip-National-co2budget.html#read-the-co2-national-budget-dataset",
    "title": "OCO-2 MIP National Top-Down CO2 Budgets",
    "section": "",
    "text": "Now we will read in the csv dataset from https://ceos.org/gst/carbon-dioxide.html\n\nurl ='https://ceos.org/gst/files/pilot_topdown_CO2_Budget_countries_v1.csv'\ndf_all = pd.read_csv(url, skiprows=52)"
  },
  {
    "objectID": "user_data_notebooks/oco2-mip-National-co2budget.html#sub-select-a-single-top-down-dataset-experiment",
    "href": "user_data_notebooks/oco2-mip-National-co2budget.html#sub-select-a-single-top-down-dataset-experiment",
    "title": "OCO-2 MIP National Top-Down CO2 Budgets",
    "section": "",
    "text": "To simplify the analysis, let’s subselect the results for a single experiment. The experiments are: - IS: estimates fluxes from in situ CO2 measurements - LNLG: estimates fluxes from OCO-2 land CO2 data - LNLGIS: combines in situ and OCO-2 land CO2 data - LNLGOGIS: combines in situ and OCO-2 land and ocean CO2 data\nWe would like to use the experiment that uses the most high-quality CO2 data. There are some concerns about small residual biases in OCO-2 ocean data (Byrne et al., 2023), so let’s use the LNLGIS experiment.\n\n# Choose one experiment from the list ['IS', 'LNLG', 'LNLGIS', 'LNLGOGIS']\nexperiment = 'LNLGIS'\n\n# Subset of columns for a given experiment\nif experiment == 'IS':\n    df = df_all.drop(df_all.columns[[4,5,6,7,8,9,12,13,14,15,16,17,20,21,22,23,24,25,34,35,36]], axis=1)\nif experiment == 'LNLG':\n    df = df_all.drop(df_all.columns[[2,3,6,7,8,9,10,11,14,15,16,17,18,19,22,23,24,25,33,35,36]], axis=1)\nif experiment == 'LNLGIS':\n    df = df_all.drop(df_all.columns[[2,3,4,5,8,9,10,11,12,13,16,17,18,19,20,21,24,25,33,34,36]], axis=1)\nif experiment == 'LNLGOGIS':\n    df = df_all.drop(df_all.columns[[2,3,4,5,6,7,10,11,12,13,14,15,18,19,20,21,22,23,33,34,35]], axis=1)\n\n# We can now look at the colums of data\ndf.head()\n\n\n\n\n\n\n\n\n\nAlpha 3 Code\nYear\nLNLGIS dC_loss (TgCO2)\nLNLGIS dC_loss unc (TgCO2)\nLNLGIS NBE (TgCO2)\nLNLGIS NBE unc (TgCO2)\nLNLGIS NCE (TgCO2)\nLNLGIS NCE unc (TgCO2)\nRivers (TgCO2)\nRiver unc (TgCO2)\nWood+Crop (TgCO2)\nWood+Crop unc (TgCO2)\nFF (TgCO2)\nFF unc (TgCO2)\nZ-statistic\nFUR LNLGIS\n\n\n\n\n0\nAFG\n2015\n39.3407\n153.746\n40.9643\n153.746\n60.3537\n153.744\n-2.43286\n1.69832\n4.05648\n1.21694\n19.3894\n0.797698\n0.37\n0.19\n\n\n1\nAFG\n2016\n50.6167\n175.454\n52.5114\n175.454\n73.0333\n175.452\n-2.16185\n2.24033\n4.05648\n1.21694\n20.5220\n0.678080\n0.31\n0.19\n\n\n2\nAFG\n2017\n54.5096\n179.794\n56.4726\n179.794\n77.5355\n179.793\n-2.09349\n2.37705\n4.05648\n1.21694\n21.0629\n0.695856\n0.47\n0.19\n\n\n3\nAFG\n2018\n116.4260\n243.057\n118.4610\n243.057\n143.9580\n243.056\n-2.02199\n2.52005\n4.05648\n1.21694\n25.4974\n0.695856\n0.39\n0.19\n\n\n4\nAFG\n2019\n64.0162\n181.516\n66.0388\n181.516\n93.8974\n181.514\n-2.03383\n2.49637\n4.05648\n1.21694\n27.8585\n0.797698\n0.49\n0.19"
  },
  {
    "objectID": "user_data_notebooks/oco2-mip-National-co2budget.html#sub-select-a-single-country",
    "href": "user_data_notebooks/oco2-mip-National-co2budget.html#sub-select-a-single-country",
    "title": "OCO-2 MIP National Top-Down CO2 Budgets",
    "section": "",
    "text": "Let’s further filter the dataset to look at a specific country. Choose a country by entering the alpha code in the country_name variable below\n\n# Choose a country\ncountry_name = 'USA' \n\n# We can sub-select the data for the country\ncountry_data = df[df['Alpha 3 Code'] == country_name]\n\n# Now we can look at the data for a specific experiment and country\ncountry_data.head()\n\n\n\n\n\n\n\n\n\nAlpha 3 Code\nYear\nLNLGIS dC_loss (TgCO2)\nLNLGIS dC_loss unc (TgCO2)\nLNLGIS NBE (TgCO2)\nLNLGIS NBE unc (TgCO2)\nLNLGIS NCE (TgCO2)\nLNLGIS NCE unc (TgCO2)\nRivers (TgCO2)\nRiver unc (TgCO2)\nWood+Crop (TgCO2)\nWood+Crop unc (TgCO2)\nFF (TgCO2)\nFF unc (TgCO2)\nZ-statistic\nFUR LNLGIS\n\n\n\n\n1232\nUSA\n2015\n-1031.83\n721.213\n-1346.46\n721.213\n4017.31\n713.897\n-165.430\n71.7453\n-149.196\n-44.7589\n5363.77\n102.4670\n-0.81\n0.91\n\n\n1233\nUSA\n2016\n-1419.92\n399.738\n-1743.80\n399.738\n3529.45\n387.079\n-174.684\n53.2375\n-149.196\n-44.7589\n5273.24\n99.8012\n0.04\n0.91\n\n\n1234\nUSA\n2017\n-1375.12\n1034.010\n-1696.63\n1034.010\n3515.14\n1029.250\n-172.308\n57.9894\n-149.196\n-44.7589\n5211.76\n99.0981\n0.67\n0.91\n\n\n1235\nUSA\n2018\n-1018.89\n784.463\n-1333.83\n784.463\n4036.65\n778.179\n-165.747\n71.1117\n-149.196\n-44.7589\n5370.48\n99.0981\n-0.20\n0.91\n\n\n1236\nUSA\n2019\n-1161.41\n718.054\n-1504.61\n718.054\n3728.95\n710.705\n-194.005\n14.5948\n-149.196\n-44.7589\n5233.56\n102.4670\n-0.38\n0.91\n\n\n\n\n\n\n\n\n#This dataset contains fluxes over a five year period, 2015-2020.\nLet’s look at a plot of the annual net terrestrial carbon stock loss (ΔCloss) for each year.\n\n# Make plot\nfig, ax1 = plt.subplots(1, 1, figsize=(6, 4))\nax1.errorbar(country_data['Year'],country_data[experiment+' dC_loss (TgCO2)'],\n                    yerr=country_data[experiment+' dC_loss unc (TgCO2)'],label=experiment,capsize=10)\nax1.legend(loc='upper right')\nax1.set_ylabel('$\\Delta$C$_\\mathrm{loss}$ (TgCO$_2$ year$^{-1}$)')\nax1.set_xlabel('Year')\nax1.set_title('$\\Delta$C$_\\mathrm{loss}$ for '+country_name)\nymin, ymax = ax1.get_ylim()\nmax_abs_y = max(abs(ymin), abs(ymax))\nax1.set_ylim([-max_abs_y, max_abs_y])\nxmin, xmax = ax1.get_xlim()\nax1.plot([xmin,xmax],[0,0],'k',linewidth=0.5)\nax1.set_xlim([xmin, xmax])\n\n\n\n\n\n\n\n\nNext, we can look at the full carbon budget for a given year.\nThe code below creates a plot similar to Fig 13 of Byrne et al. (2023). Each of the bars on the left side of the dashed vertical line (Fossil fuel emissions, lateral C transport by rivers, lateral C transport in crop and wood products, and the net terrestrial carbon stock loss combined to give the net carbon exchange (net surface-atmosphere CO2 flux) shown on the right.\n\n# Pick a specifc year (or mean year)\nyear='mean'\n\n# Make plot\ncountry_data_mean = country_data[country_data['Year'] == year]\na=country_data_mean['Wood+Crop (TgCO2)']\nb=country_data_mean['Wood+Crop unc (TgCO2)']\nprint(b)\n#\nplt.bar(1, country_data_mean['FF (TgCO2)'], yerr=country_data_mean['FF unc (TgCO2)'], label='FF', alpha=0.5)\nplt.bar(2, country_data_mean['Rivers (TgCO2)'], yerr=country_data_mean['River unc (TgCO2)'], label='Rivers', alpha=0.5)\nplt.bar(3, country_data_mean['Wood+Crop (TgCO2)'], yerr=abs(country_data_mean['Wood+Crop unc (TgCO2)']), label='WoodCrop', alpha=0.5)\nplt.bar(4, country_data_mean[experiment+' dC_loss (TgCO2)'], yerr=country_data_mean['LNLGIS dC_loss unc (TgCO2)'], label='dC', alpha=0.5)\nplt.bar(6, country_data_mean[experiment+' NCE (TgCO2)'], yerr=country_data_mean['LNLGIS NCE unc (TgCO2)'], label='NCE', alpha=0.5)\nax = plt.gca()\nymin, ymax = ax.get_ylim()\nplt.plot([5,5],[ymin,ymax],'k:')\nxmin, xmax = ax.get_xlim()\nplt.plot([xmin,xmax],[0,0],'k',linewidth=0.5)\nplt.xlim([xmin,xmax])\nplt.ylim([ymin,ymax])\n#\nplt.xticks([1,2,3,4,6], ['Fossil\\nFuels','Rivers','Wood+\\nCrops','$\\mathrm{\\Delta C _{loss}}$','NCE'])\nplt.title(country_name+' '+year)\nplt.ylabel('CO$_2$ Flux (TgCO$_2$ year$^{-1}$)')\n\n1238   -44.7589\nName: Wood+Crop unc (TgCO2), dtype: float64\n\n\nText(0, 0.5, 'CO$_2$ Flux (TgCO$_2$ year$^{-1}$)')\n\n\n\n\n\n\n\n\n\nUncertainty is an important consideration when analyzing the flux estimates provided by Byrne et al. (2023).\nEach flux estimate is provided with an error estimate representing the standard deviation, and assuming the errors are well prepresented by a normal distribution. This probability dirtribution provided by this uncertainty can be visualized below. We can further quantify the\n\n\n# Select NCE, NBE or dC_loss\nquantity = 'dC_loss'\n\n# Value for comparison\ncomparison_value = 1000 # TgCO2/year\n\n\nMIP_mean = country_data_mean[experiment+' '+quantity+' (TgCO2)'].item()\nMIP_std = country_data_mean[experiment+' '+quantity+' unc (TgCO2)'].item()\n\n# Perform t-test\nt_value = abs(MIP_mean - comparison_value)/(MIP_std / np.sqrt(11))\ncrtical_value = 2.23 # use p=0.05 significance\nif t_value &gt; crtical_value:\n    ttest = 'statistically different'\nif t_value &lt; crtical_value:\n    ttest = 'not statistically\\ndifferent'\n\n# Make plot\nxbounds = abs(MIP_mean)+MIP_std*4\nif abs(crtical_value) &gt; xbounds:\n    xbounds = abs(crtical_value)\nx_axis = np.arange(-1.*xbounds, xbounds, 1) \nplt.plot(x_axis, norm.pdf(x_axis, MIP_mean, MIP_std)) \nax = plt.gca()\nymin, ymax = ax.get_ylim()\nxmin, xmax = ax.get_xlim()\nplt.plot([0,0],[ymin,ymax*1.2],'k:',linewidth=0.5)\nplt.plot([xmin,xmax],[0,0],'k:',linewidth=0.5)\nplt.plot([comparison_value,comparison_value],[ymin,ymax*1.2],'k')\nplt.text(comparison_value+(xmax-xmin)*0.01,ymax*0.96,'value = '+str(comparison_value),ha='left',va='top')\nplt.text(comparison_value+(xmax-xmin)*0.01,ymax*0.9,ttest,ha='left',va='top')\nplt.ylim([ymin,ymax*1.2])\nplt.xlim([xmin,xmax])\nplt.plot(MIP_mean,ymax*1.03,'ko')\nplt.plot([MIP_mean-MIP_std,\n         MIP_mean+MIP_std],\n         [ymax*1.03,ymax*1.03],'k')\nplt.plot([MIP_mean-MIP_std,\n         MIP_mean-MIP_std],\n         [ymax*1.005,ymax*1.055],'k')\nplt.plot([MIP_mean+MIP_std,\n         MIP_mean+MIP_std],\n         [ymax*1.005,ymax*1.055],'k')\nplt.text(MIP_mean,ymax*1.115,\n         str(round(MIP_mean))+' $\\pm$ '+\n         str(round(MIP_std))+' TgCO$_2$',ha='center')\nplt.title(country_name+' '+year+' '+quantity+'')\nplt.yticks([])\nplt.ylabel('Probability')\nplt.xlabel(quantity+' (TgCO$_2$ year$^{-1}$)')\n\nText(0.5, 0, 'dC_loss (TgCO$_2$ year$^{-1}$)')\n\n\n\n\n\n\n\n\n\nFinally, we will examine two metrics that are useful for understanding the confidence in the top-down results:\n\nZ-statistic: metric of agreement in NCE estimates across the experiments that assimilate different CO2 datasets. Experiments are considered significantly different if the magnitude exceeds 1.96\nFractional Uncertainty Reduction (FUR): metric of how strongly the assimilated CO2 data on reduce NCE uncertainties. Values range from 0 to 1, with 0 meaning zero error reduction and 1 meaning complete error reduction\n\nHere we will add a plot of the Z-statistic for each year, and add the FUR value for the country.\n\n# Make plot\nfig, ax1 = plt.subplots(1, 1, figsize=(6, 4))\nax1.plot(country_data['Year'],country_data['Z-statistic'],label=experiment)\nax1.legend(loc='upper right')\nax1.set_ylabel('Z-statistic')\nax1.set_xlabel('Year')\nax1.set_title(country_name)\nymin, ymax = ax1.get_ylim()\nmax_abs_y = max(abs(ymin), abs(ymax))\nax1.set_ylim([-3, 3])\nxmin, xmax = ax1.get_xlim()\nax1.plot([xmin,xmax],[0,0],'k',linewidth=0.5)\nax1.plot([xmin,xmax],[-1.96,-1.96],'k--',linewidth=0.5)\nax1.plot([xmin,xmax],[1.96,1.96],'k--',linewidth=0.5)\nax1.set_xlim([xmin, xmax])\nax1.text(xmin+0.12,2.6,'Fractional error reduction: '+str(country_data['FUR '+experiment].iloc[1]))\n\nText(-0.18000000000000005, 2.6, 'Fractional error reduction: 0.91')"
  },
  {
    "objectID": "user_data_notebooks/noaa-insitu_User_Notebook.html",
    "href": "user_data_notebooks/noaa-insitu_User_Notebook.html",
    "title": "Atmospheric Carbon Dioxide Concentrations from NOAA Global Monitoring Laboratory",
    "section": "",
    "text": "Identify available dates and temporal frequency of observations for the given data. The collection processed in this notebook is the Atmospheric Carbon Dioxide Concentrations from NOAA Global Monitoring Laboratory.\nVisualize the time series data",
    "crumbs": [
      "Data Usage Notebooks",
      "Greenhouse Gas Concentrations",
      "Atmospheric Carbon Dioxide Concentrations from NOAA Global Monitoring Laboratory"
    ]
  },
  {
    "objectID": "user_data_notebooks/noaa-insitu_User_Notebook.html#approach",
    "href": "user_data_notebooks/noaa-insitu_User_Notebook.html#approach",
    "title": "Atmospheric Carbon Dioxide Concentrations from NOAA Global Monitoring Laboratory",
    "section": "",
    "text": "Identify available dates and temporal frequency of observations for the given data. The collection processed in this notebook is the Atmospheric Carbon Dioxide Concentrations from NOAA Global Monitoring Laboratory.\nVisualize the time series data",
    "crumbs": [
      "Data Usage Notebooks",
      "Greenhouse Gas Concentrations",
      "Atmospheric Carbon Dioxide Concentrations from NOAA Global Monitoring Laboratory"
    ]
  },
  {
    "objectID": "user_data_notebooks/noaa-insitu_User_Notebook.html#about-the-data",
    "href": "user_data_notebooks/noaa-insitu_User_Notebook.html#about-the-data",
    "title": "Atmospheric Carbon Dioxide Concentrations from NOAA Global Monitoring Laboratory",
    "section": "About the Data",
    "text": "About the Data\nThe Global Greenhouse Gas Reference Network (GGGRN) for the Carbon Cycle and Greenhouse Gases (CCGG) Group is part of NOAA’S Global Monitoring Laboratory (GML) in Boulder, CO. The Reference Network measures the atmospheric distribution and trends of the three main long-term drivers of climate change, carbon dioxide (CO₂), methane (CH₄), and nitrous oxide (N2O), as well as carbon monoxide (CO) and many other trace gases which help interpretation of the main GHGs. The Reference Network measurement program includes continuous in-situ measurements at 4 baseline observatories (global background sites) and 8 tall towers, as well as flask-air samples collected by volunteers at over 50 additional regional background sites and from small aircraft conducting regular vertical profiles. The air samples are returned to GML for analysis where measurements of about 55 trace gases are done. NOAA’s GGGRN maintains the World Meteorological Organization international calibration scales for CO₂, CH₄, CO, N2O, and SF6 in air. The measurements from the GGGRN serve as a comparison with measurements made by many other international laboratories, and with regional studies. They are widely used in modeling studies that infer space-time patterns of emissions and removals of greenhouse gases that are optimally consistent with the atmospheric observations, given wind patterns. These data serve as an early warning for climate “surprises”. The measurements are also helpful for the ongoing evaluation of remote sensing technologies.",
    "crumbs": [
      "Data Usage Notebooks",
      "Greenhouse Gas Concentrations",
      "Atmospheric Carbon Dioxide Concentrations from NOAA Global Monitoring Laboratory"
    ]
  },
  {
    "objectID": "user_data_notebooks/noaa-insitu_User_Notebook.html#installing-the-required-libraries",
    "href": "user_data_notebooks/noaa-insitu_User_Notebook.html#installing-the-required-libraries",
    "title": "Atmospheric Carbon Dioxide Concentrations from NOAA Global Monitoring Laboratory",
    "section": "Installing the required libraries",
    "text": "Installing the required libraries\nPlease run the cell below to install the libraries required to run this notebook.\n\n%pip install matplotlib\n%pip install pandas\n%pip install requests\n\nRequirement already satisfied: matplotlib in /Users/vgaur/miniconda3/envs/cmip6/lib/python3.9/site-packages (3.7.1)\nRequirement already satisfied: pyparsing&gt;=2.3.1 in /Users/vgaur/miniconda3/envs/cmip6/lib/python3.9/site-packages (from matplotlib) (3.0.9)\nRequirement already satisfied: importlib-resources&gt;=3.2.0 in /Users/vgaur/miniconda3/envs/cmip6/lib/python3.9/site-packages (from matplotlib) (5.12.0)\nRequirement already satisfied: kiwisolver&gt;=1.0.1 in /Users/vgaur/miniconda3/envs/cmip6/lib/python3.9/site-packages (from matplotlib) (1.4.4)\nRequirement already satisfied: contourpy&gt;=1.0.1 in /Users/vgaur/miniconda3/envs/cmip6/lib/python3.9/site-packages (from matplotlib) (1.0.5)\nRequirement already satisfied: numpy&gt;=1.20 in /Users/vgaur/miniconda3/envs/cmip6/lib/python3.9/site-packages (from matplotlib) (1.24.3)\nRequirement already satisfied: fonttools&gt;=4.22.0 in /Users/vgaur/miniconda3/envs/cmip6/lib/python3.9/site-packages (from matplotlib) (4.25.0)\nRequirement already satisfied: python-dateutil&gt;=2.7 in /Users/vgaur/miniconda3/envs/cmip6/lib/python3.9/site-packages (from matplotlib) (2.8.2)\nRequirement already satisfied: pillow&gt;=6.2.0 in /Users/vgaur/miniconda3/envs/cmip6/lib/python3.9/site-packages (from matplotlib) (9.5.0)\nRequirement already satisfied: packaging&gt;=20.0 in /Users/vgaur/miniconda3/envs/cmip6/lib/python3.9/site-packages (from matplotlib) (23.1)\nRequirement already satisfied: cycler&gt;=0.10 in /Users/vgaur/miniconda3/envs/cmip6/lib/python3.9/site-packages (from matplotlib) (0.11.0)\nRequirement already satisfied: zipp&gt;=3.1.0 in /Users/vgaur/miniconda3/envs/cmip6/lib/python3.9/site-packages (from importlib-resources&gt;=3.2.0-&gt;matplotlib) (3.15.0)\nRequirement already satisfied: six&gt;=1.5 in /Users/vgaur/miniconda3/envs/cmip6/lib/python3.9/site-packages (from python-dateutil&gt;=2.7-&gt;matplotlib) (1.16.0)\nNote: you may need to restart the kernel to use updated packages.\nRequirement already satisfied: pandas in /Users/vgaur/miniconda3/envs/cmip6/lib/python3.9/site-packages (2.0.3)\nRequirement already satisfied: numpy&gt;=1.20.3 in /Users/vgaur/miniconda3/envs/cmip6/lib/python3.9/site-packages (from pandas) (1.24.3)\nRequirement already satisfied: tzdata&gt;=2022.1 in /Users/vgaur/miniconda3/envs/cmip6/lib/python3.9/site-packages (from pandas) (2023.3)\nRequirement already satisfied: pytz&gt;=2020.1 in /Users/vgaur/miniconda3/envs/cmip6/lib/python3.9/site-packages (from pandas) (2023.3)\nRequirement already satisfied: python-dateutil&gt;=2.8.2 in /Users/vgaur/miniconda3/envs/cmip6/lib/python3.9/site-packages (from pandas) (2.8.2)\nRequirement already satisfied: six&gt;=1.5 in /Users/vgaur/miniconda3/envs/cmip6/lib/python3.9/site-packages (from python-dateutil&gt;=2.8.2-&gt;pandas) (1.16.0)\nNote: you may need to restart the kernel to use updated packages.\nRequirement already satisfied: requests in /Users/vgaur/miniconda3/envs/cmip6/lib/python3.9/site-packages (2.31.0)\nRequirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in /Users/vgaur/miniconda3/envs/cmip6/lib/python3.9/site-packages (from requests) (1.26.16)\nRequirement already satisfied: idna&lt;4,&gt;=2.5 in /Users/vgaur/miniconda3/envs/cmip6/lib/python3.9/site-packages (from requests) (3.4)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /Users/vgaur/miniconda3/envs/cmip6/lib/python3.9/site-packages (from requests) (2023.7.22)\nRequirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /Users/vgaur/miniconda3/envs/cmip6/lib/python3.9/site-packages (from requests) (3.1.0)\nNote: you may need to restart the kernel to use updated packages.\n\n\n\nImporting required libraries\n\nimport numpy as np\nimport pandas as pd\nfrom glob import glob\nfrom io import StringIO\nimport matplotlib.pyplot as plt\nimport requests",
    "crumbs": [
      "Data Usage Notebooks",
      "Greenhouse Gas Concentrations",
      "Atmospheric Carbon Dioxide Concentrations from NOAA Global Monitoring Laboratory"
    ]
  },
  {
    "objectID": "user_data_notebooks/noaa-insitu_User_Notebook.html#reading-the-noaa-data-from-github-repo",
    "href": "user_data_notebooks/noaa-insitu_User_Notebook.html#reading-the-noaa-data-from-github-repo",
    "title": "Atmospheric Carbon Dioxide Concentrations from NOAA Global Monitoring Laboratory",
    "section": "Reading the NOAA data from GitHub repo",
    "text": "Reading the NOAA data from GitHub repo\n\ngithub_repo_owner = \"NASA-IMPACT\"\ngithub_repo_name = \"noaa-viz\"\nfolder_path_ch4, folder_path_co2 = \"flask/ch4\", \"flask/c02\"\ncombined_df_co2, combined_df_ch4 = pd.DataFrame(), pd.DataFrame()\n\n\n# Function to fetch and append a file from GitHub\ndef append_github_file(file_url):\n    response = requests.get(file_url)\n    response.raise_for_status()\n    return response.text\n\n# Get the list of CH4 files in the specified directory using GitHub API\ngithub_api_url = f\"https://api.github.com/repos/{github_repo_owner}/{github_repo_name}/contents/{folder_path_ch4}\"\nresponse = requests.get(github_api_url)\nresponse.raise_for_status()\nfile_list_ch4 = response.json()\n\n# Get the list of CO2 files in the specified directory using GitHub API\ngithub_api_url = f\"https://api.github.com/repos/{github_repo_owner}/{github_repo_name}/contents/{folder_path_ch4}\"\nresponse = requests.get(github_api_url)\nresponse.raise_for_status()\nfile_list_co2 = response.json()",
    "crumbs": [
      "Data Usage Notebooks",
      "Greenhouse Gas Concentrations",
      "Atmospheric Carbon Dioxide Concentrations from NOAA Global Monitoring Laboratory"
    ]
  },
  {
    "objectID": "user_data_notebooks/noaa-insitu_User_Notebook.html#concatenating-the-ch4-data-into-a-single-dataframe",
    "href": "user_data_notebooks/noaa-insitu_User_Notebook.html#concatenating-the-ch4-data-into-a-single-dataframe",
    "title": "Atmospheric Carbon Dioxide Concentrations from NOAA Global Monitoring Laboratory",
    "section": "Concatenating the CH4 data into a single DataFrame",
    "text": "Concatenating the CH4 data into a single DataFrame\n\nfor file_info in file_list_ch4:\n    if file_info[\"name\"].endswith(\"txt\"):\n        file_content = append_github_file(file_info[\"download_url\"])\n        Lines = file_content.splitlines()\n        index = Lines.index(\"# VARIABLE ORDER\")+2\n        df = pd.read_csv(StringIO(\"\\n\".join(Lines[index:])), delim_whitespace=True)\n        combined_df_ch4 = pd.concat([combined_df_ch4, df], ignore_index=True)",
    "crumbs": [
      "Data Usage Notebooks",
      "Greenhouse Gas Concentrations",
      "Atmospheric Carbon Dioxide Concentrations from NOAA Global Monitoring Laboratory"
    ]
  },
  {
    "objectID": "user_data_notebooks/noaa-insitu_User_Notebook.html#concatenating-the-co2-data-into-a-single-dataframe",
    "href": "user_data_notebooks/noaa-insitu_User_Notebook.html#concatenating-the-co2-data-into-a-single-dataframe",
    "title": "Atmospheric Carbon Dioxide Concentrations from NOAA Global Monitoring Laboratory",
    "section": "Concatenating the CO2 data into a single DataFrame",
    "text": "Concatenating the CO2 data into a single DataFrame\n\nfor file_info in file_list_co2:\n    if file_info[\"name\"].endswith(\"txt\"):\n        file_content = append_github_file(file_info[\"download_url\"])\n        Lines = file_content.splitlines()\n        index = Lines.index(\"# VARIABLE ORDER\")+2\n        df = pd.read_csv(StringIO(\"\\n\".join(Lines[index:])), delim_whitespace=True)\n        combined_df_co2 = pd.concat([combined_df_co2, df], ignore_index=True)",
    "crumbs": [
      "Data Usage Notebooks",
      "Greenhouse Gas Concentrations",
      "Atmospheric Carbon Dioxide Concentrations from NOAA Global Monitoring Laboratory"
    ]
  },
  {
    "objectID": "user_data_notebooks/noaa-insitu_User_Notebook.html#visualizing-the-noaa-data-for-ch4-and-co2",
    "href": "user_data_notebooks/noaa-insitu_User_Notebook.html#visualizing-the-noaa-data-for-ch4-and-co2",
    "title": "Atmospheric Carbon Dioxide Concentrations from NOAA Global Monitoring Laboratory",
    "section": "Visualizing the NOAA data for CH4 and CO2",
    "text": "Visualizing the NOAA data for CH4 and CO2\n\nsite_to_filter = 'ABP'\nfiltered_df = combined_df_co2[combined_df_co2['site_code'] == site_to_filter]\n\nfiltered_df['datetime'] = pd.to_datetime(filtered_df['datetime'])\n\n# Set the \"Date\" column as the index\nfiltered_df.set_index('datetime', inplace=True)\n\n# Create a time series plot for 'Data' and 'Value'\nplt.figure(figsize=(12, 6))\nplt.plot(filtered_df.index, filtered_df['value'], label='Carbon Dioxide(CO2) Concentration (ppm)')\nplt.xlabel(\"Observed Date/Time\")\nplt.ylabel(\"Carbon Dioxide(CO2) Concentration (ppm)\")\nplt.title(f\"Observed Co2 Concentration {site_to_filter}\")\nplt.legend()\nplt.grid(True)\n# plt.show()\n\n/var/folders/7b/5rrvrjx51l54jchgs0tqps0c0000gn/T/ipykernel_70808/2606016741.py:4: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  filtered_df['datetime'] = pd.to_datetime(filtered_df['datetime'])\n\n\n\n\n\n\n\n\n\n\nsite_to_filter = 'ABP'\nfiltered_df = combined_df_ch4[combined_df_ch4['site_code'] == site_to_filter]\nfiltered_df['datetime'] = pd.to_datetime(filtered_df['datetime'])\n\n# Set the \"Date\" column as the index\nfiltered_df.set_index('datetime', inplace=True)\n\n# Create a time series plot for 'Data' and 'Value'\nplt.figure(figsize=(12, 6))\nplt.plot(filtered_df.index, filtered_df['value'], label='Methane Ch4 Concentration (ppb)')\nplt.xlabel(\"Observation Date/Time\")\nplt.ylabel(\"Methane Ch4 Concentration (ppb)\")\nplt.title(f\"Observed CH4 Concentration {site_to_filter}\")\nplt.legend()\nplt.grid(True)\nplt.show()\n\n/var/folders/7b/5rrvrjx51l54jchgs0tqps0c0000gn/T/ipykernel_70808/1635934907.py:3: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  filtered_df['datetime'] = pd.to_datetime(filtered_df['datetime'])",
    "crumbs": [
      "Data Usage Notebooks",
      "Greenhouse Gas Concentrations",
      "Atmospheric Carbon Dioxide Concentrations from NOAA Global Monitoring Laboratory"
    ]
  },
  {
    "objectID": "user_data_notebooks/noaa-insitu_User_Notebook.html#summary",
    "href": "user_data_notebooks/noaa-insitu_User_Notebook.html#summary",
    "title": "Atmospheric Carbon Dioxide Concentrations from NOAA Global Monitoring Laboratory",
    "section": "Summary",
    "text": "Summary\nIn this notebook we have successfully visualized the data for Atmospheric Carbon Dioxide Concentrations from NOAA Global Monitoring Laboratory.",
    "crumbs": [
      "Data Usage Notebooks",
      "Greenhouse Gas Concentrations",
      "Atmospheric Carbon Dioxide Concentrations from NOAA Global Monitoring Laboratory"
    ]
  },
  {
    "objectID": "user_data_notebooks/micasa-carbonflux-daygrid-v1_User_Notebook.html",
    "href": "user_data_notebooks/micasa-carbonflux-daygrid-v1_User_Notebook.html",
    "title": "MiCASA Land Carbon Flux",
    "section": "",
    "text": "Identify available dates and temporal frequency of observations for a given collection using the GHGC API /stac endpoint. The collection processed in this notebook is the Land-Atmosphere Carbon Flux data product\nPass the STAC item into the raster API /stac/tilejson.json endpoint\nUsing folium.plugins.DualMap, visualize two tiles (side-by-side), allowing time point comparison\nAfter the visualization, perform zonal statistics for a given polygon",
    "crumbs": [
      "Data Usage Notebooks",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "MiCASA Land Carbon Flux"
    ]
  },
  {
    "objectID": "user_data_notebooks/micasa-carbonflux-daygrid-v1_User_Notebook.html#approach",
    "href": "user_data_notebooks/micasa-carbonflux-daygrid-v1_User_Notebook.html#approach",
    "title": "MiCASA Land Carbon Flux",
    "section": "",
    "text": "Identify available dates and temporal frequency of observations for a given collection using the GHGC API /stac endpoint. The collection processed in this notebook is the Land-Atmosphere Carbon Flux data product\nPass the STAC item into the raster API /stac/tilejson.json endpoint\nUsing folium.plugins.DualMap, visualize two tiles (side-by-side), allowing time point comparison\nAfter the visualization, perform zonal statistics for a given polygon",
    "crumbs": [
      "Data Usage Notebooks",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "MiCASA Land Carbon Flux"
    ]
  },
  {
    "objectID": "user_data_notebooks/micasa-carbonflux-daygrid-v1_User_Notebook.html#about-the-data",
    "href": "user_data_notebooks/micasa-carbonflux-daygrid-v1_User_Notebook.html#about-the-data",
    "title": "MiCASA Land Carbon Flux",
    "section": "About the Data",
    "text": "About the Data\nThis dataset presents a variety of carbon flux parameters derived from the Más Informada Carnegie-Ames-Stanford-Approach (MiCASA) model. The model’s input data includes air temperature, precipitation, incident solar radiation, a soil classification map, and several satellite derived products. All model calculations are driven by analyzed meteorological data from NASA’s Modern-Era Retrospective analysis for Research and Application, Version 2 (MERRA-2). The resulting product provides global, daily data at 0.1 degree resolution from January 2001 through December 2023. It includes carbon flux variables expressed in units of kilograms of carbon per square meter per day (kg Carbon/m²/day) from net primary production (NPP), heterotrophic respiration (Rh), wildfire emissions (FIRE), fuel wood burning emissions (FUEL), net ecosystem exchange (NEE), and net biosphere exchange (NBE). The latter two are derived from the first four (see Scientific Details below). MiCASA is an extensive revision of the CASA – Global Fire Emissions Database, version 3 (CASA-GFED3) product. CASA-GFED3 and earlier versions of MERRA-driven CASA-GFED carbon fluxes have been used in several atmospheric carbon dioxide (CO₂) transport studies, serve as a community standard for priors of flux inversion systems, and through the support of NASA’s Carbon Monitoring System (CMS), help characterize, quantify, understand and predict the evolution of global carbon sources and sinks.\nFor more information regarding this dataset, please visit the U.S. Greenhouse Gas Center.",
    "crumbs": [
      "Data Usage Notebooks",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "MiCASA Land Carbon Flux"
    ]
  },
  {
    "objectID": "user_data_notebooks/micasa-carbonflux-daygrid-v1_User_Notebook.html#query-the-stac-api",
    "href": "user_data_notebooks/micasa-carbonflux-daygrid-v1_User_Notebook.html#query-the-stac-api",
    "title": "MiCASA Land Carbon Flux",
    "section": "Query the STAC API",
    "text": "Query the STAC API\nFirst, we are going to import the required libraries. Once imported, they allow better executing a query in the GHG Center Spatio Temporal Asset Catalog (STAC) Application Programming Interface (API) where the granules for this collection are stored.\n\n# Import the following libraries\nimport requests\nimport folium\nimport folium.plugins\nfrom folium import Map, TileLayer\nfrom pystac_client import Client\nimport branca\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n\n# Provide the STAC and RASTER API endpoints\n# The endpoint is referring to a location within the API that executes a request on a data collection nesting on the server.\n\n# The STAC API is a catalog of all the existing data collections that are stored in the GHG Center.\nSTAC_API_URL = \"http://dev.ghg.center/ghgcenter/api/stac\"\n\n# The RASTER API is used to fetch collections for visualization\nRASTER_API_URL = \"https://dev.ghg.center/ghgcenter/api/raster\"\n\n# The collection name is used to fetch the dataset from the STAC API. First, we define the collection name as a variable\n# Name of the collection for MiCASA Land Carbon Flux\ncollection_name = \"micasa-carbonflux-daygrid-v1\"\n\n# Next, we need to specify the asset name for this collection\n# The asset name is referring to the raster band containing the pixel values for the parameter of interest\n# For the case of the MiCASA Land Carbon Flux collection, the parameter of interest is “rh”\n# rh = Heterotrophic Respiration\nasset_name = \"rh\"\n\n\n# Fetch the collection from the STAC API using the appropriate endpoint\n# The 'requests' library allows a HTTP request possible\ncollection = requests.get(f\"{STAC_API_URL}/collections/{collection_name}\").json()\n\n# Print the properties of the collection to the console\ncollection\n\n{'id': 'micasa-carbonflux-daygrid-v1',\n 'type': 'Collection',\n 'links': [{'rel': 'items',\n   'type': 'application/geo+json',\n   'href': 'https://dev.ghg.center/ghgcenter/api/stac/collections/micasa-carbonflux-daygrid-v1/items'},\n  {'rel': 'parent',\n   'type': 'application/json',\n   'href': 'https://dev.ghg.center/ghgcenter/api/stac/'},\n  {'rel': 'root',\n   'type': 'application/json',\n   'href': 'https://dev.ghg.center/ghgcenter/api/stac/'},\n  {'rel': 'self',\n   'type': 'application/json',\n   'href': 'https://dev.ghg.center/ghgcenter/api/stac/collections/micasa-carbonflux-daygrid-v1'}],\n 'title': 'MiCASA Land Carbon Flux v1',\n 'assets': None,\n 'extent': {'spatial': {'bbox': [[-180, -90, 180, 90]]},\n  'temporal': {'interval': [['2001-01-01T00:00:00+00:00',\n     '2001-03-08T00:00:00+00:00']]}},\n 'license': 'CC0 1.0',\n 'keywords': None,\n 'providers': [{'url': None,\n   'name': 'NASA',\n   'roles': None,\n   'description': None}],\n 'summaries': {'datetime': ['2001-01-01T00:00:00Z', '2001-03-08T00:00:00Z']},\n 'description': \"This product provides estimated daily average Net Ecosystem Exchange (NEE), which is the net carbon flux to the atmosphere. It also provides the estimated amount of carbon flux to the atmosphere from Net Primary Production (NPP), heterotrophic respiration (Rh), wildfire emissions (FIRE), and wood fuel emissions (FUEL) derived from the Más Informada Carnegie-Ames-Stanford-Approach (MiCASA) model. All model calculations are driven by analyzed meteorological data from NASA's Modern-Era Retrospective analysis for Research and Application, Version 2 (MERRA-2). The resulting product provides global, daily data at 0.1 degree resolution starting from January 2001. The carbon flux variables are expressed in units of kilograms of carbon per square meter per day. MiCASA is an extensive revision of the CASA – Global Fire Emissions Database, version 3 (CASA-GFED3) product. CASA-GFED3 and earlier versions of MERRA-driven CASA-GFED carbon fluxes have been used in several atmospheric carbon dioxide (CO2) transport studies, serve as a community standard for priors of flux inversion systems, and through the support of NASA's Carbon Monitoring System (CMS), help characterize, quantify, understand and predict the evolution of global carbon sources and sinks.\",\n 'item_assets': {'rh': {'type': 'image/tiff; application=geotiff; profile=cloud-optimized',\n   'roles': ['data', 'layer'],\n   'title': 'Heterotrophic respiration (Rh), MiCASA Model v1',\n   'description': 'Heterotrophic respiration (carbon flux from the soil to the atmosphere) in units of kilograms of carbon per square meter per day'},\n  'nee': {'type': 'image/tiff; application=geotiff; profile=cloud-optimized',\n   'roles': ['data', 'layer'],\n   'title': 'Net Ecosystem Exchange (NEE), MiCASA Model v1',\n   'description': 'Net Ecosystem Exchange (net carbon flux to the atmosphere) in units of kilograms of carbon per square meter per day'},\n  'npp': {'type': 'image/tiff; application=geotiff; profile=cloud-optimized',\n   'roles': ['data', 'layer'],\n   'title': 'Net Primary Production (NPP)',\n   'description': 'Net Primary Production (carbon available from plants) in units of kilograms of carbon per square meter per day'},\n  'fire': {'type': 'image/tiff; application=geotiff; profile=cloud-optimized',\n   'roles': ['data', 'layer'],\n   'title': 'Fire emissions (FIRE), MiCASA Model v1',\n   'description': 'Fire emissions (flux of carbon to the atmosphere from wildfires) in units of kilograms of carbon per square meter per day'},\n  'fuel': {'type': 'image/tiff; application=geotiff; profile=cloud-optimized',\n   'roles': ['data', 'layer'],\n   'title': 'Wood fuel emissions (FUEL), MiCASA Model v1',\n   'description': 'Wood fuel emissions (flux of carbon to the atmosphere from wood burned for fuel) in units of kilograms of carbon per square meter per day'}},\n 'stac_version': '1.0.0',\n 'stac_extensions': None,\n 'dashboard:is_periodic': True,\n 'dashboard:time_density': 'day'}\n\n\nExamining the contents of our collection under the temporal variable, we see that the data is available from January 2003 to December 2017. By looking at the dashboard:time density, we observe that the periodic frequency of these observations is monthly.\n\n# Create a function that would search for a data collection in the US GHG Center STAC API\n\n# First, we need to define the function\n# The name of the function = \"get_item_count\"\n# The argument that will be passed through the defined function = \"collection_id\"\ndef get_item_count(collection_id):\n   \n    # Set a counter for the number of items existing in the collection\n    count = 0\n\n    # Define the path to retrieve the granules (items) of the collection of interest (MiCASA Land Carbon Flux) in the STAC API\n    items_url = f\"{STAC_API_URL}/collections/{collection_id}/items\"\n\n    # Run a while loop to make HTTP requests until there are no more URLs associated with the collection in the STAC API\n    while True:\n\n        # Retrieve information about the granules by sending a \"get\" request to the STAC API using the defined collection path\n        response = requests.get(items_url)\n\n        # If the items do not exist, print an error message and quit the loop\n        if not response.ok:\n            print(\"error getting items\")\n            exit()\n\n        # Return the results of the HTTP response as JSON\n        stac = response.json()\n       \n        # Increase the \"count\" by the number of items (granules) returned in the response\n        count += int(stac[\"context\"].get(\"returned\", 0))\n\n        # Retrieve information about the next URL associated with the collection (MiCASA Land Carbon Flux) in the STAC API (if applicable)\n        next = [link for link in stac[\"links\"] if link[\"rel\"] == \"next\"]\n\n        # Exit the loop if there are no other URLs\n        if not next:\n            break\n       \n        # Ensure the information gathered by other STAC API links associated with the collection are added to the original path\n        # \"href\" is the identifier for each of the tiles stored in the STAC API\n        items_url = next[0][\"href\"]\n        temp = items_url.split('/')\n        temp.insert(3, 'ghgcenter')\n        temp.insert(4, 'api')\n        temp.insert(5, 'stac')\n        items_url = '/'.join(temp)\n\n    # Return the information about the total number of granules found associated with the collection (MiCASA Land Carbon Flux)\n    return count\n\n\n# Apply the function created above \"get_item_count\" to the data collection\nnumber_of_items = get_item_count(collection_name)\n\n# Get the information about the number of granules found in the collection\nitems = requests.get(f\"{STAC_API_URL}/collections/{collection_name}/items?limit=600\").json()[\"features\"]\n\n# Print the total number of items (granules) found\nprint(f\"Found {len(items)} items\")\n\nFound 67 items\n\n\n\n# Examine the first item in the collection\n# Keep in mind that a list starts from 0, 1, 2... therefore items[0] is referring to the first item in the list/collection\nitems[0]\n\n{'id': 'micasa-carbonflux-daygrid-v1-20010308',\n 'bbox': [-180.0, -90.0, 179.99999999999994, 90.0],\n 'type': 'Feature',\n 'links': [{'rel': 'collection',\n   'type': 'application/json',\n   'href': 'https://dev.ghg.center/ghgcenter/api/stac/collections/micasa-carbonflux-daygrid-v1'},\n  {'rel': 'parent',\n   'type': 'application/json',\n   'href': 'https://dev.ghg.center/ghgcenter/api/stac/collections/micasa-carbonflux-daygrid-v1'},\n  {'rel': 'root',\n   'type': 'application/json',\n   'href': 'https://dev.ghg.center/ghgcenter/api/stac/'},\n  {'rel': 'self',\n   'type': 'application/geo+json',\n   'href': 'https://dev.ghg.center/ghgcenter/api/stac/collections/micasa-carbonflux-daygrid-v1/items/micasa-carbonflux-daygrid-v1-20010308'}],\n 'assets': {'rh': {'href': 's3://ghgc-data-store-dev/MiCASA/v1/daily/2001/03/MiCASAv1_flux_Rh_x3600_y1800_daily_20010308.tif',\n   'type': 'image/tiff; application=geotiff; profile=cloud-optimized',\n   'roles': ['data', 'layer'],\n   'title': 'Heterotrophic respiration (Rh), MiCASA Model v1',\n   'proj:bbox': [-180.0, -90.0, 179.99999999999994, 90.0],\n   'proj:epsg': 4326.0,\n   'proj:shape': [1800.0, 3600.0],\n   'description': 'Heterotrophic respiration (carbon flux from the soil to the atmosphere) in units of kilograms of carbon per square meter per day',\n   'raster:bands': [{'unit': 'g C m-2 day-1',\n     'scale': 1.0,\n     'nodata': 9.969209968386869e+36,\n     'offset': 0.0,\n     'sampling': 'area',\n     'data_type': 'float32',\n     'histogram': {'max': 5.658170223236084,\n      'min': -0.28565365076065063,\n      'count': 11.0,\n      'buckets': [454672.0,\n       41472.0,\n       9375.0,\n       6748.0,\n       3266.0,\n       2075.0,\n       2882.0,\n       3069.0,\n       683.0,\n       46.0]},\n     'statistics': {'mean': 0.17458568513393402,\n      'stddev': 0.5804132581892004,\n      'maximum': 5.658170223236084,\n      'minimum': -0.28565365076065063,\n      'valid_percent': 100.0}}],\n   'proj:geometry': {'type': 'Polygon',\n    'coordinates': [[[-180.0, -90.0],\n      [179.99999999999994, -90.0],\n      [179.99999999999994, 90.0],\n      [-180.0, 90.0],\n      [-180.0, -90.0]]]},\n   'proj:projjson': {'id': {'code': 4326.0, 'authority': 'EPSG'},\n    'name': 'WGS 84',\n    'type': 'GeographicCRS',\n    'datum': {'name': 'World Geodetic System 1984',\n     'type': 'GeodeticReferenceFrame',\n     'ellipsoid': {'name': 'WGS 84',\n      'semi_major_axis': 6378137.0,\n      'inverse_flattening': 298.257223563}},\n    '$schema': 'https://proj.org/schemas/v0.4/projjson.schema.json',\n    'coordinate_system': {'axis': [{'name': 'Geodetic latitude',\n       'unit': 'degree',\n       'direction': 'north',\n       'abbreviation': 'Lat'},\n      {'name': 'Geodetic longitude',\n       'unit': 'degree',\n       'direction': 'east',\n       'abbreviation': 'Lon'}],\n     'subtype': 'ellipsoidal'}},\n   'proj:transform': [0.09999999999999999,\n    0.0,\n    -180.0,\n    0.0,\n    -0.1,\n    90.0,\n    0.0,\n    0.0,\n    1.0]},\n  'nee': {'href': 's3://ghgc-data-store-dev/MiCASA/v1/daily/2001/03/MiCASAv1_flux_NEE_x3600_y1800_daily_20010308.tif',\n   'type': 'image/tiff; application=geotiff; profile=cloud-optimized',\n   'roles': ['data', 'layer'],\n   'title': 'Net Ecosystem Exchange (NEE), MiCASA Model v1',\n   'proj:bbox': [-180.0, -90.0, 179.99999999999994, 90.0],\n   'proj:epsg': 4326.0,\n   'proj:shape': [1800.0, 3600.0],\n   'description': 'Net Ecosystem Exchange (net carbon flux to the atmosphere) in units of kilograms of carbon per square meter per day',\n   'raster:bands': [{'unit': 'g C m-2 day-1',\n     'scale': 1.0,\n     'nodata': 9.969209968386869e+36,\n     'offset': 0.0,\n     'sampling': 'area',\n     'data_type': 'float32',\n     'histogram': {'max': 3.4336233139038086,\n      'min': -3.14528751373291,\n      'count': 11.0,\n      'buckets': [313.0,\n       2416.0,\n       6520.0,\n       8375.0,\n       450955.0,\n       50578.0,\n       4605.0,\n       407.0,\n       100.0,\n       19.0]},\n     'statistics': {'mean': 0.0055059753358364105,\n      'stddev': 0.31420707462947906,\n      'maximum': 3.4336233139038086,\n      'minimum': -3.14528751373291,\n      'valid_percent': 100.0}}],\n   'proj:geometry': {'type': 'Polygon',\n    'coordinates': [[[-180.0, -90.0],\n      [179.99999999999994, -90.0],\n      [179.99999999999994, 90.0],\n      [-180.0, 90.0],\n      [-180.0, -90.0]]]},\n   'proj:projjson': {'id': {'code': 4326.0, 'authority': 'EPSG'},\n    'name': 'WGS 84',\n    'type': 'GeographicCRS',\n    'datum': {'name': 'World Geodetic System 1984',\n     'type': 'GeodeticReferenceFrame',\n     'ellipsoid': {'name': 'WGS 84',\n      'semi_major_axis': 6378137.0,\n      'inverse_flattening': 298.257223563}},\n    '$schema': 'https://proj.org/schemas/v0.4/projjson.schema.json',\n    'coordinate_system': {'axis': [{'name': 'Geodetic latitude',\n       'unit': 'degree',\n       'direction': 'north',\n       'abbreviation': 'Lat'},\n      {'name': 'Geodetic longitude',\n       'unit': 'degree',\n       'direction': 'east',\n       'abbreviation': 'Lon'}],\n     'subtype': 'ellipsoidal'}},\n   'proj:transform': [0.09999999999999999,\n    0.0,\n    -180.0,\n    0.0,\n    -0.1,\n    90.0,\n    0.0,\n    0.0,\n    1.0]},\n  'npp': {'href': 's3://ghgc-data-store-dev/MiCASA/v1/daily/2001/03/MiCASAv1_flux_NPP_x3600_y1800_daily_20010308.tif',\n   'type': 'image/tiff; application=geotiff; profile=cloud-optimized',\n   'roles': ['data', 'layer'],\n   'title': 'Net Primary Production (NPP)',\n   'proj:bbox': [-180.0, -90.0, 179.99999999999994, 90.0],\n   'proj:epsg': 4326.0,\n   'proj:shape': [1800.0, 3600.0],\n   'description': 'Net Primary Production (carbon available from plants) in units of kilograms of carbon per square meter per day',\n   'raster:bands': [{'unit': 'g C m-2 day-1',\n     'scale': 1.0,\n     'nodata': 9.969209968386869e+36,\n     'offset': 0.0,\n     'sampling': 'area',\n     'data_type': 'float32',\n     'histogram': {'max': 5.009734630584717,\n      'min': -0.3224586248397827,\n      'count': 11.0,\n      'buckets': [479749.0,\n       13577.0,\n       6278.0,\n       4079.0,\n       3117.0,\n       3368.0,\n       3781.0,\n       4031.0,\n       4799.0,\n       1509.0]},\n     'statistics': {'mean': 0.1690797060728073,\n      'stddev': 0.6919349796740665,\n      'maximum': 5.009734630584717,\n      'minimum': -0.3224586248397827,\n      'valid_percent': 100.0}}],\n   'proj:geometry': {'type': 'Polygon',\n    'coordinates': [[[-180.0, -90.0],\n      [179.99999999999994, -90.0],\n      [179.99999999999994, 90.0],\n      [-180.0, 90.0],\n      [-180.0, -90.0]]]},\n   'proj:projjson': {'id': {'code': 4326.0, 'authority': 'EPSG'},\n    'name': 'WGS 84',\n    'type': 'GeographicCRS',\n    'datum': {'name': 'World Geodetic System 1984',\n     'type': 'GeodeticReferenceFrame',\n     'ellipsoid': {'name': 'WGS 84',\n      'semi_major_axis': 6378137.0,\n      'inverse_flattening': 298.257223563}},\n    '$schema': 'https://proj.org/schemas/v0.4/projjson.schema.json',\n    'coordinate_system': {'axis': [{'name': 'Geodetic latitude',\n       'unit': 'degree',\n       'direction': 'north',\n       'abbreviation': 'Lat'},\n      {'name': 'Geodetic longitude',\n       'unit': 'degree',\n       'direction': 'east',\n       'abbreviation': 'Lon'}],\n     'subtype': 'ellipsoidal'}},\n   'proj:transform': [0.09999999999999999,\n    0.0,\n    -180.0,\n    0.0,\n    -0.1,\n    90.0,\n    0.0,\n    0.0,\n    1.0]},\n  'fire': {'href': 's3://ghgc-data-store-dev/MiCASA/v1/daily/2001/03/MiCASAv1_flux_FIRE_x3600_y1800_daily_20010308.tif',\n   'type': 'image/tiff; application=geotiff; profile=cloud-optimized',\n   'roles': ['data', 'layer'],\n   'title': 'Fire emissions (FIRE), MiCASA Model v1',\n   'proj:bbox': [-180.0, -90.0, 179.99999999999994, 90.0],\n   'proj:epsg': 4326.0,\n   'proj:shape': [1800.0, 3600.0],\n   'description': 'Fire emissions (flux of carbon to the atmosphere from wildfires) in units of kilograms of carbon per square meter per day',\n   'raster:bands': [{'unit': 'g C m-2 day-1',\n     'scale': 1.0,\n     'nodata': 9.969209968386869e+36,\n     'offset': 0.0,\n     'sampling': 'area',\n     'data_type': 'float32',\n     'histogram': {'max': 201.761962890625,\n      'min': -22.26268768310547,\n      'count': 11.0,\n      'buckets': [523043.0, 1226.0, 14.0, 2.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0]},\n     'statistics': {'mean': 0.005909561179578304,\n      'stddev': 0.4367886503035904,\n      'maximum': 201.761962890625,\n      'minimum': -22.26268768310547,\n      'valid_percent': 100.0}}],\n   'proj:geometry': {'type': 'Polygon',\n    'coordinates': [[[-180.0, -90.0],\n      [179.99999999999994, -90.0],\n      [179.99999999999994, 90.0],\n      [-180.0, 90.0],\n      [-180.0, -90.0]]]},\n   'proj:projjson': {'id': {'code': 4326.0, 'authority': 'EPSG'},\n    'name': 'WGS 84',\n    'type': 'GeographicCRS',\n    'datum': {'name': 'World Geodetic System 1984',\n     'type': 'GeodeticReferenceFrame',\n     'ellipsoid': {'name': 'WGS 84',\n      'semi_major_axis': 6378137.0,\n      'inverse_flattening': 298.257223563}},\n    '$schema': 'https://proj.org/schemas/v0.4/projjson.schema.json',\n    'coordinate_system': {'axis': [{'name': 'Geodetic latitude',\n       'unit': 'degree',\n       'direction': 'north',\n       'abbreviation': 'Lat'},\n      {'name': 'Geodetic longitude',\n       'unit': 'degree',\n       'direction': 'east',\n       'abbreviation': 'Lon'}],\n     'subtype': 'ellipsoidal'}},\n   'proj:transform': [0.09999999999999999,\n    0.0,\n    -180.0,\n    0.0,\n    -0.1,\n    90.0,\n    0.0,\n    0.0,\n    1.0]},\n  'fuel': {'href': 's3://ghgc-data-store-dev/MiCASA/v1/daily/2001/03/MiCASAv1_flux_FUEL_x3600_y1800_daily_20010308.tif',\n   'type': 'image/tiff; application=geotiff; profile=cloud-optimized',\n   'roles': ['data', 'layer'],\n   'title': 'Wood fuel emissions (FUEL), MiCASA Model v1',\n   'proj:bbox': [-180.0, -90.0, 179.99999999999994, 90.0],\n   'proj:epsg': 4326.0,\n   'proj:shape': [1800.0, 3600.0],\n   'description': 'Wood fuel emissions (flux of carbon to the atmosphere from wood burned for fuel) in units of kilograms of carbon per square meter per day',\n   'raster:bands': [{'unit': 'g C m-2 day-1',\n     'scale': 1.0,\n     'nodata': 9.969209968386869e+36,\n     'offset': 0.0,\n     'sampling': 'area',\n     'data_type': 'float32',\n     'histogram': {'max': 0.5715039372444153,\n      'min': -0.020534303039312363,\n      'count': 11.0,\n      'buckets': [518243.0,\n       4890.0,\n       792.0,\n       214.0,\n       83.0,\n       34.0,\n       17.0,\n       2.0,\n       9.0,\n       4.0]},\n     'statistics': {'mean': 0.0014613016974180937,\n      'stddev': 0.010342230703904214,\n      'maximum': 0.5715039372444153,\n      'minimum': -0.020534303039312363,\n      'valid_percent': 100.0}}],\n   'proj:geometry': {'type': 'Polygon',\n    'coordinates': [[[-180.0, -90.0],\n      [179.99999999999994, -90.0],\n      [179.99999999999994, 90.0],\n      [-180.0, 90.0],\n      [-180.0, -90.0]]]},\n   'proj:projjson': {'id': {'code': 4326.0, 'authority': 'EPSG'},\n    'name': 'WGS 84',\n    'type': 'GeographicCRS',\n    'datum': {'name': 'World Geodetic System 1984',\n     'type': 'GeodeticReferenceFrame',\n     'ellipsoid': {'name': 'WGS 84',\n      'semi_major_axis': 6378137.0,\n      'inverse_flattening': 298.257223563}},\n    '$schema': 'https://proj.org/schemas/v0.4/projjson.schema.json',\n    'coordinate_system': {'axis': [{'name': 'Geodetic latitude',\n       'unit': 'degree',\n       'direction': 'north',\n       'abbreviation': 'Lat'},\n      {'name': 'Geodetic longitude',\n       'unit': 'degree',\n       'direction': 'east',\n       'abbreviation': 'Lon'}],\n     'subtype': 'ellipsoidal'}},\n   'proj:transform': [0.09999999999999999,\n    0.0,\n    -180.0,\n    0.0,\n    -0.1,\n    90.0,\n    0.0,\n    0.0,\n    1.0]}},\n 'geometry': {'type': 'Polygon',\n  'coordinates': [[[-180, -90],\n    [179.99999999999994, -90],\n    [179.99999999999994, 90],\n    [-180, 90],\n    [-180, -90]]]},\n 'collection': 'micasa-carbonflux-daygrid-v1',\n 'properties': {'datetime': '2001-03-08T00:00:00+00:00'},\n 'stac_version': '1.0.0',\n 'stac_extensions': []}",
    "crumbs": [
      "Data Usage Notebooks",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "MiCASA Land Carbon Flux"
    ]
  },
  {
    "objectID": "user_data_notebooks/micasa-carbonflux-daygrid-v1_User_Notebook.html#explore-changes-in-carbon-flux-levels-using-the-raster-api",
    "href": "user_data_notebooks/micasa-carbonflux-daygrid-v1_User_Notebook.html#explore-changes-in-carbon-flux-levels-using-the-raster-api",
    "title": "MiCASA Land Carbon Flux",
    "section": "Explore Changes in Carbon Flux Levels Using the Raster API",
    "text": "Explore Changes in Carbon Flux Levels Using the Raster API\nWe will explore changes in the land atmosphere Carbon flux Heterotrophic Respiration and examine their impacts over time. We’ll then visualize the outputs on a map using folium.\n\n# Now we create a dictionary where the start datetime values for each granule is queried more explicitly by year and month (e.g., 2020-02)\nitems = {item[\"properties\"][\"datetime\"][:10]: item for item in items}\n\nBelow, we are entering the minimum and maximum values to provide our upper and lower bounds in the rescale_values.\n\n# Fetch the minimum and maximum values for rescaling\nrescale_values = {\"max\":items[list(items.keys())[0]][\"assets\"][asset_name][\"raster:bands\"][0][\"histogram\"][\"max\"], \"min\":items[list(items.keys())[0]][\"assets\"][asset_name][\"raster:bands\"][0][\"histogram\"][\"min\"]}\n\nNow, we will pass the item id, collection name, asset name, and the rescaling factor to the Raster API endpoint. This step is done twice, once for December 2003 and again for December 2017, so that we can visualize each event independently.\n\n# Choose a color for displaying the tiles\n# Please refer to matplotlib library if you'd prefer choosing a different color ramp.\n# For more information on Colormaps in Matplotlib, please visit https://matplotlib.org/stable/users/explain/colors/colormaps.html\ncolor_map = \"purd\"\n\n# Make a GET request to retrieve information for the date mentioned below\ndate1 = '2001-01-01'\ndate1_tile = requests.get(\n\n    # Pass the collection name, collection date, and its ID\n    # To change the year and month of the observed parameter, you can modify the \"items['YYYY-MM-DD']\" statement\n    f\"{RASTER_API_URL}/stac/tilejson.json?collection={items[date1]['collection']}&item={items[date1]['id']}\"\n\n    # Pass the asset name\n    f\"&assets={asset_name}\"\n\n    # Pass the color formula and colormap for custom visualization\n    f\"&color_formula=gamma+r+1.05&colormap_name={color_map}\"\n\n    # Pass the minimum and maximum values for rescaling\n    f\"&rescale={rescale_values['min']},{rescale_values['max']}\",\n\n# Return response in JSON format\n).json()\n\n# Print the properties of the retrieved granule to the console\ndate1_tile\n\n{'tilejson': '2.2.0',\n 'version': '1.0.0',\n 'scheme': 'xyz',\n 'tiles': ['https://dev.ghg.center/ghgcenter/api/raster/stac/tiles/WebMercatorQuad/{z}/{x}/{y}@1x?collection=micasa-carbonflux-daygrid-v1&item=micasa-carbonflux-daygrid-v1-20010101&assets=rh&color_formula=gamma+r+1.05&colormap_name=purd&rescale=-0.28565365076065063%2C5.658170223236084'],\n 'minzoom': 0,\n 'maxzoom': 24,\n 'bounds': [-180.0, -90.0, 179.99999999999994, 90.0],\n 'center': [-2.842170943040401e-14, 0.0, 0]}\n\n\n\n# Make a GET request to retrieve information for the date mentioned below\ndate2 = '2001-01-31'\ndate2_tile = requests.get(\n\n    # Pass the collection name, collection date, and its ID\n    # To change the year and month of the observed parameter, you can modify the \"items['YYYY-MM-DD']\" statement\n    f\"{RASTER_API_URL}/stac/tilejson.json?collection={items[date2]['collection']}&item={items[date2]['id']}\"\n\n    # Pass the asset name\n    f\"&assets={asset_name}\"\n\n    # Pass the color formula and colormap for custom visualization\n    f\"&color_formula=gamma+r+1.05&colormap_name={color_map}\"\n\n    # Pass the minimum and maximum values for rescaling\n    f\"&rescale={rescale_values['min']},{rescale_values['max']}\", \n\n# Return response in JSON format\n).json()\n\n# Print the properties of the retrieved granule to the console\ndate2_tile\n\n{'tilejson': '2.2.0',\n 'version': '1.0.0',\n 'scheme': 'xyz',\n 'tiles': ['https://dev.ghg.center/ghgcenter/api/raster/stac/tiles/WebMercatorQuad/{z}/{x}/{y}@1x?collection=micasa-carbonflux-daygrid-v1&item=micasa-carbonflux-daygrid-v1-20010131&assets=rh&color_formula=gamma+r+1.05&colormap_name=purd&rescale=-0.28565365076065063%2C5.658170223236084'],\n 'minzoom': 0,\n 'maxzoom': 24,\n 'bounds': [-180.0, -90.0, 179.99999999999994, 90.0],\n 'center': [-2.842170943040401e-14, 0.0, 0]}",
    "crumbs": [
      "Data Usage Notebooks",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "MiCASA Land Carbon Flux"
    ]
  },
  {
    "objectID": "user_data_notebooks/micasa-carbonflux-daygrid-v1_User_Notebook.html#visualize-land-atmosphere-carbon-flux-heterotrophic-respiration",
    "href": "user_data_notebooks/micasa-carbonflux-daygrid-v1_User_Notebook.html#visualize-land-atmosphere-carbon-flux-heterotrophic-respiration",
    "title": "MiCASA Land Carbon Flux",
    "section": "Visualize Land-Atmosphere Carbon Flux (Heterotrophic Respiration)",
    "text": "Visualize Land-Atmosphere Carbon Flux (Heterotrophic Respiration)\n\n# For this study we are going to compare the Rh level in 2003 and 2017 over the State of Texas \n# To change the location, you can simply insert the latitude and longitude of the area of your interest in the \"location=(LAT, LONG)\" statement\n# For example, you can change the current statement \"location=(31.9, -99.9)\" to \"location=(34, -118)\" to monitor the Rh level in California instead of Texas\n\n# Set initial zoom and center of map for CO₂ Layer\n# 'folium.plugins' allows mapping side-by-side\nmap_ = folium.plugins.DualMap(location=(31.9, -99.9), zoom_start=6)\n\n\n# Define the first map layer with Rh level for the tile fetched for date 1\n# The TileLayer library helps in manipulating and displaying raster layers on a map\nmap_layer_date1 = TileLayer(\n    tiles=date1_tile[\"tiles\"][0], # Path to retrieve the tile\n    attr=\"GHG\", # Set the attribution\n    opacity=0.8, # Adjust the transparency of the layer\n    name=f\"{date1} Rh Level\", # Title for the layer\n    overlay= True, # The layer can be overlaid on the map\n    legendEnabled = True # Enable displaying the legend on the map\n)\n\n# Add the first layer to the Dual Map\nmap_layer_date1.add_to(map_.m1)\n\n\n# Define the first map layer with Rh level for the tile fetched for date 2\nmap_layer_date2 = TileLayer(\n    tiles=date2_tile[\"tiles\"][0], # Path to retrieve the tile\n    attr=\"GHG\", # Set the attribution\n    opacity=0.8, # Adjust the transparency of the layer\n    name=f\"{date2} RH Level\", # Title for the layer\n    overlay= True, # The layer can be overlaid on the map\n    legendEnabled = True # Enable displaying the legend on the map\n)\n\n# Add the second layer to the Dual Map\nmap_layer_date2.add_to(map_.m2)\n\n# Display data markers (titles) on both maps\nfolium.Marker((40, 5.0), tooltip=\"both\").add_to(map_)\n\n# Add a layer control to switch between map layers\nfolium.LayerControl(collapsed=False).add_to(map_)\n\n# Add a legend to the dual map using the 'branca' library. \n# Note: the inserted legend is representing the minimum and maximum values for both tiles.\ncolormap = branca.colormap.linear.PuRd_09.scale(0, 0.3) # minimum value = 0, maximum value = 0.3 (kg Carbon/m2/daily)\n\n# Classify the colormap according to specified Rh values \ncolormap = colormap.to_step(index=[0, 0.07, 0.15, 0.22, 0.3])\n\n# Add the data unit as caption\ncolormap.caption = 'Rh Values (kg Carbon/m2/daily)'\n\n# Display the legend and caption on the map\ncolormap.add_to(map_.m1)\n\n# Visualize the Dual Map\nmap_\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook",
    "crumbs": [
      "Data Usage Notebooks",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "MiCASA Land Carbon Flux"
    ]
  },
  {
    "objectID": "user_data_notebooks/micasa-carbonflux-daygrid-v1_User_Notebook.html#generate-the-statistics-for-the-aoi",
    "href": "user_data_notebooks/micasa-carbonflux-daygrid-v1_User_Notebook.html#generate-the-statistics-for-the-aoi",
    "title": "MiCASA Land Carbon Flux",
    "section": "Generate the statistics for the AOI",
    "text": "Generate the statistics for the AOI\n\n%%time\n# %%time = Wall time (execution time) for running the code below\n\n# Generate statistics using the created function \"generate_stats\" within the bounding box defined by the \"texas_dallas_aoi\" polygon\nstats = [generate_stats(item, texas_dallas_aoi) for item in items]\n\n{'type': 'Feature', 'geometry': {'type': 'Polygon', 'coordinates': [[[-96.1, 32.28], [-96.1, 33.28], [-97.58, 33.28], [-97.58, 32.28], [-96.1, 32.28]]]}, 'properties': {'statistics': {'b1': {'min': 0.08267466723918915, 'max': 0.9830807447433472, 'mean': 0.6214667558670044, 'count': 150.0, 'sum': 93.22001647949219, 'std': 0.16884362462505265, 'median': 0.6536906361579895, 'majority': 0.08267466723918915, 'minority': 0.08267466723918915, 'unique': 150.0, 'histogram': [[2.0, 3.0, 3.0, 13.0, 19.0, 30.0, 40.0, 18.0, 16.0, 6.0], [0.08267466723918915, 0.17271527647972107, 0.2627558708190918, 0.3527964949607849, 0.44283708930015564, 0.5328776836395264, 0.6229183077812195, 0.7129589319229126, 0.8029995560646057, 0.893040120601654, 0.9830807447433472]], 'valid_percent': 100.0, 'masked_pixels': 0.0, 'valid_pixels': 150.0, 'percentile_2': 0.18744821846485138, 'percentile_98': 0.9215888977050781}}}}\n{'type': 'Feature', 'geometry': {'type': 'Polygon', 'coordinates': [[[-96.1, 32.28], [-96.1, 33.28], [-97.58, 33.28], [-97.58, 32.28], [-96.1, 32.28]]]}, 'properties': {'statistics': {'b1': {'min': 0.08297736942768097, 'max': 0.985992968082428, 'mean': 0.6233696341514587, 'count': 150.0, 'sum': 93.50544738769531, 'std': 0.16934459008590824, 'median': 0.6558751463890076, 'majority': 0.08297736942768097, 'minority': 0.08297736942768097, 'unique': 150.0, 'histogram': [[2.0, 3.0, 3.0, 13.0, 19.0, 30.0, 41.0, 17.0, 16.0, 6.0], [0.08297736942768097, 0.17327892780303955, 0.2635805010795593, 0.3538820445537567, 0.4441836178302765, 0.5344851613044739, 0.6247867345809937, 0.7150883078575134, 0.8053898215293884, 0.8956913948059082, 0.985992968082428]], 'valid_percent': 100.0, 'masked_pixels': 0.0, 'valid_pixels': 150.0, 'percentile_2': 0.18811501562595367, 'percentile_98': 0.9242913126945496}}}}\n{'type': 'Feature', 'geometry': {'type': 'Polygon', 'coordinates': [[[-96.1, 32.28], [-96.1, 33.28], [-97.58, 33.28], [-97.58, 32.28], [-96.1, 32.28]]]}, 'properties': {'statistics': {'b1': {'min': 0.08328218013048172, 'max': 0.9889252185821533, 'mean': 0.625286340713501, 'count': 150.0, 'sum': 93.79295349121094, 'std': 0.1698494899520313, 'median': 0.6580753326416016, 'majority': 0.08328218013048172, 'minority': 0.08328218013048172, 'unique': 150.0, 'histogram': [[2.0, 3.0, 3.0, 13.0, 19.0, 30.0, 41.0, 17.0, 16.0, 6.0], [0.08328218013048172, 0.17384648323059082, 0.2644107937812805, 0.3549751043319702, 0.4455393850803375, 0.5361037254333496, 0.6266679763793945, 0.7172322869300842, 0.8077965974807739, 0.8983609080314636, 0.9889252185821533]], 'valid_percent': 100.0, 'masked_pixels': 0.0, 'valid_pixels': 150.0, 'percentile_2': 0.1887865960597992, 'percentile_98': 0.9270120859146118}}}}\n{'type': 'Feature', 'geometry': {'type': 'Polygon', 'coordinates': [[[-96.1, 32.28], [-96.1, 33.28], [-97.58, 33.28], [-97.58, 32.28], [-96.1, 32.28]]]}, 'properties': {'statistics': {'b1': {'min': 0.083589106798172, 'max': 0.9918779134750366, 'mean': 0.6272168159484863, 'count': 150.0, 'sum': 94.08251953125, 'std': 0.17035835484375328, 'median': 0.6602911949157715, 'majority': 0.083589106798172, 'minority': 0.083589106798172, 'unique': 150.0, 'histogram': [[2.0, 3.0, 3.0, 13.0, 20.0, 29.0, 41.0, 17.0, 16.0, 6.0], [0.083589106798172, 0.17441798746585846, 0.2652468681335449, 0.3560757637023926, 0.44690462946891785, 0.5377334952354431, 0.6285623908042908, 0.7193912863731384, 0.8102201223373413, 0.901049017906189, 0.9918779134750366]], 'valid_percent': 100.0, 'masked_pixels': 0.0, 'valid_pixels': 150.0, 'percentile_2': 0.18946292996406555, 'percentile_98': 0.9297512769699097}}}}\n{'type': 'Feature', 'geometry': {'type': 'Polygon', 'coordinates': [[[-96.1, 32.28], [-96.1, 33.28], [-97.58, 33.28], [-97.58, 32.28], [-96.1, 32.28]]]}, 'properties': {'statistics': {'b1': {'min': 0.083898164331913, 'max': 0.9948509931564331, 'mean': 0.6291612386703491, 'count': 150.0, 'sum': 94.37418365478516, 'std': 0.1708712147420419, 'median': 0.662523090839386, 'majority': 0.083898164331913, 'minority': 0.083898164331913, 'unique': 150.0, 'histogram': [[2.0, 3.0, 3.0, 13.0, 20.0, 29.0, 41.0, 17.0, 16.0, 6.0], [0.083898164331913, 0.17499344050884247, 0.26608872413635254, 0.3571840226650238, 0.4482792913913727, 0.539374589920044, 0.6304698586463928, 0.7215651273727417, 0.8126604557037354, 0.9037557244300842, 0.9948509931564331]], 'valid_percent': 100.0, 'masked_pixels': 0.0, 'valid_pixels': 150.0, 'percentile_2': 0.1901441067457199, 'percentile_98': 0.9325092434883118}}}}\n{'type': 'Feature', 'geometry': {'type': 'Polygon', 'coordinates': [[[-96.1, 32.28], [-96.1, 33.28], [-97.58, 33.28], [-97.58, 32.28], [-96.1, 32.28]]]}, 'properties': {'statistics': {'b1': {'min': 0.0842093750834465, 'max': 0.9978445768356323, 'mean': 0.6311196088790894, 'count': 150.0, 'sum': 94.66793823242188, 'std': 0.1713881098590267, 'median': 0.6647709012031555, 'majority': 0.0842093750834465, 'minority': 0.0842093750834465, 'unique': 150.0, 'histogram': [[2.0, 3.0, 3.0, 13.0, 20.0, 29.0, 41.0, 17.0, 16.0, 6.0], [0.0842093750834465, 0.17557290196418762, 0.26693642139434814, 0.35829994082450867, 0.4496634602546692, 0.5410269498825073, 0.6323904991149902, 0.7237539887428284, 0.8151175379753113, 0.9064810276031494, 0.9978445768356323]], 'valid_percent': 100.0, 'masked_pixels': 0.0, 'valid_pixels': 150.0, 'percentile_2': 0.19083011150360107, 'percentile_98': 0.9352859854698181}}}}\n{'type': 'Feature', 'geometry': {'type': 'Polygon', 'coordinates': [[[-96.1, 32.28], [-96.1, 33.28], [-97.58, 33.28], [-97.58, 32.28], [-96.1, 32.28]]]}, 'properties': {'statistics': {'b1': {'min': 0.08452276885509491, 'max': 1.0008589029312134, 'mean': 0.6330921649932861, 'count': 150.0, 'sum': 94.96382141113281, 'std': 0.17190906338833103, 'median': 0.6670350432395935, 'majority': 0.08452276885509491, 'minority': 0.08452276885509491, 'unique': 150.0, 'histogram': [[2.0, 3.0, 3.0, 13.0, 20.0, 29.0, 41.0, 17.0, 16.0, 6.0], [0.08452276885509491, 0.17615638673305511, 0.2677899897098541, 0.35942360758781433, 0.45105722546577454, 0.5426908135414124, 0.6343244314193726, 0.7259580492973328, 0.817591667175293, 0.9092252850532532, 1.0008589029312134]], 'valid_percent': 100.0, 'masked_pixels': 0.0, 'valid_pixels': 150.0, 'percentile_2': 0.1915210485458374, 'percentile_98': 0.9380817413330078}}}}\n{'type': 'Feature', 'geometry': {'type': 'Polygon', 'coordinates': [[[-96.1, 32.28], [-96.1, 33.28], [-97.58, 33.28], [-97.58, 32.28], [-96.1, 32.28]]]}, 'properties': {'statistics': {'b1': {'min': 0.08483835309743881, 'max': 1.0038940906524658, 'mean': 0.6350789070129395, 'count': 150.0, 'sum': 95.26183319091797, 'std': 0.1724341519681625, 'median': 0.6692134141921997, 'majority': 0.08483835309743881, 'minority': 0.08483835309743881, 'unique': 150.0, 'histogram': [[2.0, 3.0, 3.0, 13.0, 20.0, 29.0, 41.0, 17.0, 16.0, 6.0], [0.08483835309743881, 0.17674392461776733, 0.26864948868751526, 0.36055508255958557, 0.4524606466293335, 0.5443662405014038, 0.6362717747688293, 0.7281773686408997, 0.82008296251297, 0.9119884967803955, 1.0038940906524658]], 'valid_percent': 100.0, 'masked_pixels': 0.0, 'valid_pixels': 150.0, 'percentile_2': 0.1922168731689453, 'percentile_98': 0.9408965110778809}}}}\n{'type': 'Feature', 'geometry': {'type': 'Polygon', 'coordinates': [[[-96.1, 32.28], [-96.1, 33.28], [-97.58, 33.28], [-97.58, 32.28], [-96.1, 32.28]]]}, 'properties': {'statistics': {'b1': {'min': 0.08213179558515549, 'max': 0.9745612144470215, 'mean': 0.5340356826782227, 'count': 150.0, 'sum': 80.10535430908203, 'std': 0.17560137976781046, 'median': 0.5202294588088989, 'majority': 0.08213179558515549, 'minority': 0.08213179558515549, 'unique': 150.0, 'histogram': [[3.0, 4.0, 14.0, 28.0, 28.0, 22.0, 25.0, 15.0, 8.0, 3.0], [0.08213179558515549, 0.17137473821640015, 0.2606176733970642, 0.34986060857772827, 0.4391035735607147, 0.5283464789390564, 0.6175894737243652, 0.7068324089050293, 0.7960753440856934, 0.8853182792663574, 0.9745612144470215]], 'valid_percent': 100.0, 'masked_pixels': 0.0, 'valid_pixels': 150.0, 'percentile_2': 0.14343418180942535, 'percentile_98': 0.8725346922874451}}}}\n{'type': 'Feature', 'geometry': {'type': 'Polygon', 'coordinates': [[[-96.1, 32.28], [-96.1, 33.28], [-97.58, 33.28], [-97.58, 32.28], [-96.1, 32.28]]]}, 'properties': {'statistics': {'b1': {'min': 0.08237312734127045, 'max': 0.9776971936225891, 'mean': 0.5355432033538818, 'count': 150.0, 'sum': 80.33148193359375, 'std': 0.17621934128655495, 'median': 0.5217724442481995, 'majority': 0.08237312734127045, 'minority': 0.08237312734127045, 'unique': 150.0, 'histogram': [[3.0, 4.0, 15.0, 27.0, 28.0, 22.0, 25.0, 15.0, 8.0, 3.0], [0.08237312734127045, 0.1719055324792862, 0.26143795251846313, 0.3509703576564789, 0.44050276279449463, 0.530035138130188, 0.6195675730705261, 0.7090999484062195, 0.7986323833465576, 0.888164758682251, 0.9776971936225891]], 'valid_percent': 100.0, 'masked_pixels': 0.0, 'valid_pixels': 150.0, 'percentile_2': 0.14382629096508026, 'percentile_98': 0.8752031326293945}}}}\n{'type': 'Feature', 'geometry': {'type': 'Polygon', 'coordinates': [[[-96.1, 32.28], [-96.1, 33.28], [-97.58, 33.28], [-97.58, 32.28], [-96.1, 32.28]]]}, 'properties': {'statistics': {'b1': {'min': 0.08261603116989136, 'max': 0.9808549284934998, 'mean': 0.5370603203773499, 'count': 150.0, 'sum': 80.55904388427734, 'std': 0.17684212670640373, 'median': 0.5233249664306641, 'majority': 0.08261603116989136, 'minority': 0.08261603116989136, 'unique': 150.0, 'histogram': [[3.0, 4.0, 15.0, 27.0, 28.0, 22.0, 25.0, 15.0, 8.0, 3.0], [0.08261603116989136, 0.17243991792201996, 0.26226380467414856, 0.35208770632743835, 0.44191157817840576, 0.5317354798316956, 0.6215593814849854, 0.7113832831382751, 0.8012071251869202, 0.89103102684021, 0.9808549284934998]], 'valid_percent': 100.0, 'masked_pixels': 0.0, 'valid_pixels': 150.0, 'percentile_2': 0.1442204862833023, 'percentile_98': 0.8778885006904602}}}}\n{'type': 'Feature', 'geometry': {'type': 'Polygon', 'coordinates': [[[-96.1, 32.28], [-96.1, 33.28], [-97.58, 33.28], [-97.58, 32.28], [-96.1, 32.28]]]}, 'properties': {'statistics': {'b1': {'min': 0.0828605443239212, 'max': 0.9840348362922668, 'mean': 0.5385870337486267, 'count': 150.0, 'sum': 80.78805541992188, 'std': 0.17746981118931643, 'median': 0.5248870849609375, 'majority': 0.0828605443239212, 'minority': 0.0828605443239212, 'unique': 150.0, 'histogram': [[3.0, 4.0, 15.0, 27.0, 28.0, 22.0, 25.0, 15.0, 8.0, 3.0], [0.0828605443239212, 0.1729779690504074, 0.2630954086780548, 0.353212833404541, 0.4433302581310272, 0.5334476828575134, 0.623565137386322, 0.7136825323104858, 0.8037999868392944, 0.8939173817634583, 0.9840348362922668]], 'valid_percent': 100.0, 'masked_pixels': 0.0, 'valid_pixels': 150.0, 'percentile_2': 0.1446167677640915, 'percentile_98': 0.8805912137031555}}}}\n{'type': 'Feature', 'geometry': {'type': 'Polygon', 'coordinates': [[[-96.1, 32.28], [-96.1, 33.28], [-97.58, 33.28], [-97.58, 32.28], [-96.1, 32.28]]]}, 'properties': {'statistics': {'b1': {'min': 0.08310665935277939, 'max': 0.9872369766235352, 'mean': 0.5401235222816467, 'count': 150.0, 'sum': 81.0185317993164, 'std': 0.17810238477126622, 'median': 0.5264588594436646, 'majority': 0.08310665935277939, 'minority': 0.08310665935277939, 'unique': 150.0, 'histogram': [[3.0, 4.0, 15.0, 27.0, 28.0, 22.0, 25.0, 15.0, 8.0, 3.0], [0.08310665935277939, 0.17351968586444855, 0.2639327347278595, 0.35434576869010925, 0.4447587728500366, 0.5351718068122864, 0.6255848407745361, 0.7159978747367859, 0.8064109086990356, 0.8968239426612854, 0.9872369766235352]], 'valid_percent': 100.0, 'masked_pixels': 0.0, 'valid_pixels': 150.0, 'percentile_2': 0.1450151950120926, 'percentile_98': 0.8833110928535461}}}}\n{'type': 'Feature', 'geometry': {'type': 'Polygon', 'coordinates': [[[-96.1, 32.28], [-96.1, 33.28], [-97.58, 33.28], [-97.58, 32.28], [-96.1, 32.28]]]}, 'properties': {'statistics': {'b1': {'min': 0.0833543986082077, 'max': 0.9904615879058838, 'mean': 0.5416699647903442, 'count': 150.0, 'sum': 81.25049591064453, 'std': 0.17873993101637706, 'median': 0.5280404686927795, 'majority': 0.0833543986082077, 'minority': 0.0833543986082077, 'unique': 150.0, 'histogram': [[3.0, 4.0, 15.0, 28.0, 27.0, 22.0, 25.0, 15.0, 8.0, 3.0], [0.0833543986082077, 0.17406511306762695, 0.2647758424282074, 0.35548654198646545, 0.4461972713470459, 0.536907970905304, 0.6276187300682068, 0.7183294296264648, 0.8090401291847229, 0.8997508883476257, 0.9904615879058838]], 'valid_percent': 100.0, 'masked_pixels': 0.0, 'valid_pixels': 150.0, 'percentile_2': 0.14541573822498322, 'percentile_98': 0.8860483169555664}}}}\n{'type': 'Feature', 'geometry': {'type': 'Polygon', 'coordinates': [[[-96.1, 32.28], [-96.1, 33.28], [-97.58, 33.28], [-97.58, 32.28], [-96.1, 32.28]]]}, 'properties': {'statistics': {'b1': {'min': 0.08360378444194794, 'max': 0.9937089681625366, 'mean': 0.5432262420654297, 'count': 150.0, 'sum': 81.48393249511719, 'std': 0.1793824903572406, 'median': 0.5296319127082825, 'majority': 0.08360378444194794, 'minority': 0.08360378444194794, 'unique': 150.0, 'histogram': [[3.0, 4.0, 15.0, 28.0, 27.0, 22.0, 25.0, 15.0, 8.0, 3.0], [0.08360378444194794, 0.1746143102645874, 0.2656248211860657, 0.35663533210754395, 0.4476458430290222, 0.5386563539505005, 0.6296669244766235, 0.7206774353981018, 0.8116879463195801, 0.9026984572410583, 0.9937089681625366]], 'valid_percent': 100.0, 'masked_pixels': 0.0, 'valid_pixels': 150.0, 'percentile_2': 0.14581841230392456, 'percentile_98': 0.8888033032417297}}}}\n{'type': 'Feature', 'geometry': {'type': 'Polygon', 'coordinates': [[[-96.1, 32.28], [-96.1, 33.28], [-97.58, 33.28], [-97.58, 32.28], [-96.1, 32.28]]]}, 'properties': {'statistics': {'b1': {'min': 0.0838548019528389, 'max': 0.9969790577888489, 'mean': 0.5447924733161926, 'count': 150.0, 'sum': 81.7188720703125, 'std': 0.1800300918863661, 'median': 0.5312331318855286, 'majority': 0.0838548019528389, 'minority': 0.0838548019528389, 'unique': 150.0, 'histogram': [[3.0, 4.0, 15.0, 29.0, 26.0, 22.0, 25.0, 15.0, 8.0, 3.0], [0.0838548019528389, 0.1751672327518463, 0.26647964119911194, 0.35779207944869995, 0.44910451769828796, 0.540416955947876, 0.6317293643951416, 0.7230417728424072, 0.8143541812896729, 0.9056666493415833, 0.9969790577888489]], 'valid_percent': 100.0, 'masked_pixels': 0.0, 'valid_pixels': 150.0, 'percentile_2': 0.146223247051239, 'percentile_98': 0.8915756940841675}}}}\n{'type': 'Feature', 'geometry': {'type': 'Polygon', 'coordinates': [[[-96.1, 32.28], [-96.1, 33.28], [-97.58, 33.28], [-97.58, 32.28], [-96.1, 32.28]]]}, 'properties': {'statistics': {'b1': {'min': 0.08410748094320297, 'max': 1.0002720355987549, 'mean': 0.5463687181472778, 'count': 150.0, 'sum': 81.95530700683594, 'std': 0.18068276385866283, 'median': 0.5328444242477417, 'majority': 0.08410748094320297, 'minority': 0.08410748094320297, 'unique': 150.0, 'histogram': [[3.0, 4.0, 15.0, 29.0, 27.0, 21.0, 26.0, 14.0, 8.0, 3.0], [0.08410748094320297, 0.17572394013404846, 0.26734039187431335, 0.35895684361457825, 0.45057329535484314, 0.5421897768974304, 0.6338062286376953, 0.7254226803779602, 0.8170391321182251, 0.90865558385849, 1.0002720355987549]], 'valid_percent': 100.0, 'masked_pixels': 0.0, 'valid_pixels': 150.0, 'percentile_2': 0.14663024246692657, 'percentile_98': 0.8943660855293274}}}}\n{'type': 'Feature', 'geometry': {'type': 'Polygon', 'coordinates': [[[-96.1, 32.28], [-96.1, 33.28], [-97.58, 33.28], [-97.58, 32.28], [-96.1, 32.28]]]}, 'properties': {'statistics': {'b1': {'min': 0.08436182886362076, 'max': 1.0035881996154785, 'mean': 0.5479550957679749, 'count': 150.0, 'sum': 82.19326782226562, 'std': 0.1813406158707585, 'median': 0.5344656705856323, 'majority': 0.08436182886362076, 'minority': 0.08436182886362076, 'unique': 150.0, 'histogram': [[3.0, 4.0, 15.0, 29.0, 27.0, 21.0, 26.0, 14.0, 8.0, 3.0], [0.08436182886362076, 0.17628446221351624, 0.2682071030139923, 0.3601297438144684, 0.45205238461494446, 0.5439749956130981, 0.6358976364135742, 0.7278202772140503, 0.8197429180145264, 0.9116655588150024, 1.0035881996154785]], 'valid_percent': 100.0, 'masked_pixels': 0.0, 'valid_pixels': 150.0, 'percentile_2': 0.14703941345214844, 'percentile_98': 0.8971741795539856}}}}\n{'type': 'Feature', 'geometry': {'type': 'Polygon', 'coordinates': [[[-96.1, 32.28], [-96.1, 33.28], [-97.58, 33.28], [-97.58, 32.28], [-96.1, 32.28]]]}, 'properties': {'statistics': {'b1': {'min': 0.08461787551641464, 'max': 1.006927728652954, 'mean': 0.5495516657829285, 'count': 150.0, 'sum': 82.43274688720703, 'std': 0.18200364292345064, 'median': 0.5360970497131348, 'majority': 0.08461787551641464, 'minority': 0.08461787551641464, 'unique': 150.0, 'histogram': [[3.0, 4.0, 15.0, 29.0, 27.0, 21.0, 26.0, 14.0, 8.0, 3.0], [0.08461787551641464, 0.1768488585948944, 0.2690798342227936, 0.36131083965301514, 0.4535418152809143, 0.5457727909088135, 0.6380037665367126, 0.7302348017692566, 0.8224657773971558, 0.9146967530250549, 1.006927728652954]], 'valid_percent': 100.0, 'masked_pixels': 0.0, 'valid_pixels': 150.0, 'percentile_2': 0.1474507749080658, 'percentile_98': 0.9000003337860107}}}}\n{'type': 'Feature', 'geometry': {'type': 'Polygon', 'coordinates': [[[-96.1, 32.28], [-96.1, 33.28], [-97.58, 33.28], [-97.58, 32.28], [-96.1, 32.28]]]}, 'properties': {'statistics': {'b1': {'min': 0.08487559854984283, 'max': 1.0102909803390503, 'mean': 0.5511584877967834, 'count': 150.0, 'sum': 82.67377471923828, 'std': 0.18267186004335811, 'median': 0.537738561630249, 'majority': 0.08487559854984283, 'minority': 0.08487559854984283, 'unique': 150.0, 'histogram': [[3.0, 4.0, 16.0, 28.0, 28.0, 20.0, 26.0, 14.0, 8.0, 3.0], [0.08487559854984283, 0.17741712927818298, 0.2699586749076843, 0.36250022053718567, 0.455041766166687, 0.547583281993866, 0.6401247978210449, 0.7326663732528687, 0.8252078890800476, 0.9177494645118713, 1.0102909803390503]], 'valid_percent': 100.0, 'masked_pixels': 0.0, 'valid_pixels': 150.0, 'percentile_2': 0.14786435663700104, 'percentile_98': 0.9028446078300476}}}}\n{'type': 'Feature', 'geometry': {'type': 'Polygon', 'coordinates': [[[-96.1, 32.28], [-96.1, 33.28], [-97.58, 33.28], [-97.58, 32.28], [-96.1, 32.28]]]}, 'properties': {'statistics': {'b1': {'min': 0.08513504266738892, 'max': 1.013677716255188, 'mean': 0.5527756214141846, 'count': 150.0, 'sum': 82.91634368896484, 'std': 0.18334538319030813, 'median': 0.5393903255462646, 'majority': 0.08513504266738892, 'minority': 0.08513504266738892, 'unique': 150.0, 'histogram': [[3.0, 4.0, 17.0, 27.0, 28.0, 20.0, 26.0, 14.0, 8.0, 3.0], [0.08513504266738892, 0.17798930406570435, 0.2708435654640198, 0.3636978566646576, 0.456552118062973, 0.5494064092636108, 0.6422606706619263, 0.7351149320602417, 0.8279691934585571, 0.9208234548568726, 1.013677716255188]], 'valid_percent': 100.0, 'masked_pixels': 0.0, 'valid_pixels': 150.0, 'percentile_2': 0.14828014373779297, 'percentile_98': 0.9057072997093201}}}}\n{'type': 'Feature', 'geometry': {'type': 'Polygon', 'coordinates': [[[-96.1, 32.28], [-96.1, 33.28], [-97.58, 33.28], [-97.58, 32.28], [-96.1, 32.28]]]}, 'properties': {'statistics': {'b1': {'min': 0.0853961929678917, 'max': 1.0170884132385254, 'mean': 0.5544031858444214, 'count': 150.0, 'sum': 83.16047668457031, 'std': 0.1840242148350343, 'median': 0.5410524010658264, 'majority': 0.0853961929678917, 'minority': 0.0853961929678917, 'unique': 150.0, 'histogram': [[3.0, 4.0, 17.0, 27.0, 28.0, 20.0, 26.0, 14.0, 8.0, 3.0], [0.0853961929678917, 0.17856541275978088, 0.2717346251010895, 0.36490386724472046, 0.45807307958602905, 0.5512422919273376, 0.6444115042686462, 0.7375807762145996, 0.8307499885559082, 0.9239192008972168, 1.0170884132385254]], 'valid_percent': 100.0, 'masked_pixels': 0.0, 'valid_pixels': 150.0, 'percentile_2': 0.14869816601276398, 'percentile_98': 0.9085883498191833}}}}\n{'type': 'Feature', 'geometry': {'type': 'Polygon', 'coordinates': [[[-96.1, 32.28], [-96.1, 33.28], [-97.58, 33.28], [-97.58, 32.28], [-96.1, 32.28]]]}, 'properties': {'statistics': {'b1': {'min': 0.08565908670425415, 'max': 1.020523190498352, 'mean': 0.5560411810874939, 'count': 150.0, 'sum': 83.40618133544922, 'std': 0.1847084174598808, 'median': 0.5427249073982239, 'majority': 0.08565908670425415, 'minority': 0.08565908670425415, 'unique': 150.0, 'histogram': [[3.0, 4.0, 17.0, 27.0, 28.0, 20.0, 26.0, 14.0, 8.0, 3.0], [0.08565908670425415, 0.17914550006389618, 0.2726319134235382, 0.36611831188201904, 0.45960474014282227, 0.5530911684036255, 0.6465775370597839, 0.7400639653205872, 0.8335503935813904, 0.9270367622375488, 1.020523190498352]], 'valid_percent': 100.0, 'masked_pixels': 0.0, 'valid_pixels': 150.0, 'percentile_2': 0.14911843836307526, 'percentile_98': 0.9114881753921509}}}}\n{'type': 'Feature', 'geometry': {'type': 'Polygon', 'coordinates': [[[-96.1, 32.28], [-96.1, 33.28], [-97.58, 33.28], [-97.58, 32.28], [-96.1, 32.28]]]}, 'properties': {'statistics': {'b1': {'min': 0.08592372387647629, 'max': 1.023982286453247, 'mean': 0.5576898455619812, 'count': 150.0, 'sum': 83.65347290039062, 'std': 0.18539802202143857, 'median': 0.544407844543457, 'majority': 0.08592372387647629, 'minority': 0.08592372387647629, 'unique': 150.0, 'histogram': [[3.0, 4.0, 17.0, 27.0, 28.0, 20.0, 26.0, 14.0, 8.0, 3.0], [0.08592372387647629, 0.17972958087921143, 0.27353543043136597, 0.3673412799835205, 0.46114715933799744, 0.5549529790878296, 0.6487588882446289, 0.7425647377967834, 0.836370587348938, 0.9301764369010925, 1.023982286453247]], 'valid_percent': 100.0, 'masked_pixels': 0.0, 'valid_pixels': 150.0, 'percentile_2': 0.1495409607887268, 'percentile_98': 0.9144063591957092}}}}\n{'type': 'Feature', 'geometry': {'type': 'Polygon', 'coordinates': [[[-96.1, 32.28], [-96.1, 33.28], [-97.58, 33.28], [-97.58, 32.28], [-96.1, 32.28]]]}, 'properties': {'statistics': {'b1': {'min': 0.0861901342868805, 'max': 1.0274657011032104, 'mean': 0.5593491792678833, 'count': 150.0, 'sum': 83.90237426757812, 'std': 0.18609310859486364, 'median': 0.5461013317108154, 'majority': 0.0861901342868805, 'minority': 0.0861901342868805, 'unique': 150.0, 'histogram': [[3.0, 4.0, 17.0, 27.0, 28.0, 20.0, 26.0, 15.0, 7.0, 3.0], [0.0861901342868805, 0.180317685008049, 0.27444523572921753, 0.36857280135154724, 0.46270036697387695, 0.5568279027938843, 0.6509554982185364, 0.7450830340385437, 0.839210569858551, 0.9333381652832031, 1.0274657011032104]], 'valid_percent': 100.0, 'masked_pixels': 0.0, 'valid_pixels': 150.0, 'percentile_2': 0.14996576309204102, 'percentile_98': 0.9173435568809509}}}}\n{'type': 'Feature', 'geometry': {'type': 'Polygon', 'coordinates': [[[-96.1, 32.28], [-96.1, 33.28], [-97.58, 33.28], [-97.58, 32.28], [-96.1, 32.28]]]}, 'properties': {'statistics': {'b1': {'min': 0.08645831048488617, 'max': 1.0309739112854004, 'mean': 0.5610190629959106, 'count': 150.0, 'sum': 84.15286254882812, 'std': 0.18679369575542332, 'median': 0.5478053689002991, 'majority': 0.08645831048488617, 'minority': 0.08645831048488617, 'unique': 150.0, 'histogram': [[3.0, 4.0, 17.0, 27.0, 28.0, 20.0, 26.0, 15.0, 7.0, 3.0], [0.08645831048488617, 0.1809098720550537, 0.27536141872406006, 0.3698129951953888, 0.46426454186439514, 0.5587161183357239, 0.6531676650047302, 0.7476192116737366, 0.8420708179473877, 0.936522364616394, 1.0309739112854004]], 'valid_percent': 100.0, 'masked_pixels': 0.0, 'valid_pixels': 150.0, 'percentile_2': 0.15039284527301788, 'percentile_98': 0.9202997088432312}}}}\n{'type': 'Feature', 'geometry': {'type': 'Polygon', 'coordinates': [[[-96.1, 32.28], [-96.1, 33.28], [-97.58, 33.28], [-97.58, 32.28], [-96.1, 32.28]]]}, 'properties': {'statistics': {'b1': {'min': 0.08672825992107391, 'max': 1.034506916999817, 'mean': 0.5626999139785767, 'count': 150.0, 'sum': 84.40498352050781, 'std': 0.18749985098832886, 'median': 0.5495202541351318, 'majority': 0.08672825992107391, 'minority': 0.08672825992107391, 'unique': 150.0, 'histogram': [[3.0, 4.0, 17.0, 27.0, 28.0, 21.0, 25.0, 15.0, 7.0, 3.0], [0.08672825992107391, 0.18150612711906433, 0.27628397941589355, 0.37106186151504517, 0.4658397138118744, 0.5606175661087036, 0.6553954482078552, 0.7501733303070068, 0.8449512124061584, 0.9397290349006653, 1.034506916999817]], 'valid_percent': 100.0, 'masked_pixels': 0.0, 'valid_pixels': 150.0, 'percentile_2': 0.1508222371339798, 'percentile_98': 0.9232748746871948}}}}\n{'type': 'Feature', 'geometry': {'type': 'Polygon', 'coordinates': [[[-96.1, 32.28], [-96.1, 33.28], [-97.58, 33.28], [-97.58, 32.28], [-96.1, 32.28]]]}, 'properties': {'statistics': {'b1': {'min': 0.08700002729892731, 'max': 1.0380650758743286, 'mean': 0.5643914341926575, 'count': 150.0, 'sum': 84.6587142944336, 'std': 0.188211610586144, 'median': 0.5510444045066833, 'majority': 0.08700002729892731, 'minority': 0.08700002729892731, 'unique': 150.0, 'histogram': [[3.0, 4.0, 17.0, 27.0, 28.0, 21.0, 25.0, 15.0, 7.0, 3.0], [0.08700002729892731, 0.18210652470588684, 0.27721303701400757, 0.3723195493221283, 0.467426061630249, 0.5625325441360474, 0.6576390266418457, 0.7527455687522888, 0.8478520512580872, 0.9429585933685303, 1.0380650758743286]], 'valid_percent': 100.0, 'masked_pixels': 0.0, 'valid_pixels': 150.0, 'percentile_2': 0.15125393867492676, 'percentile_98': 0.9262692332267761}}}}\n{'type': 'Feature', 'geometry': {'type': 'Polygon', 'coordinates': [[[-96.1, 32.28], [-96.1, 33.28], [-97.58, 33.28], [-97.58, 32.28], [-96.1, 32.28]]]}, 'properties': {'statistics': {'b1': {'min': 0.08727359026670456, 'max': 1.0416483879089355, 'mean': 0.5660940408706665, 'count': 150.0, 'sum': 84.91410827636719, 'std': 0.18892902951586768, 'median': 0.5528504252433777, 'majority': 0.08727359026670456, 'minority': 0.08727359026670456, 'unique': 150.0, 'histogram': [[3.0, 4.0, 17.0, 27.0, 29.0, 20.0, 25.0, 15.0, 7.0, 3.0], [0.08727359026670456, 0.18271106481552124, 0.2781485617160797, 0.3735860288143158, 0.4690234959125519, 0.5644609928131104, 0.6598984599113464, 0.7553359270095825, 0.8507734537124634, 0.9462109208106995, 1.0416483879089355]], 'valid_percent': 100.0, 'masked_pixels': 0.0, 'valid_pixels': 150.0, 'percentile_2': 0.15168797969818115, 'percentile_98': 0.929283082485199}}}}\n{'type': 'Feature', 'geometry': {'type': 'Polygon', 'coordinates': [[[-96.1, 32.28], [-96.1, 33.28], [-97.58, 33.28], [-97.58, 32.28], [-96.1, 32.28]]]}, 'properties': {'statistics': {'b1': {'min': 0.08754898607730865, 'max': 1.045257329940796, 'mean': 0.5678077340126038, 'count': 150.0, 'sum': 85.17115783691406, 'std': 0.18965216140941019, 'median': 0.5546693205833435, 'majority': 0.08754898607730865, 'minority': 0.08754898607730865, 'unique': 150.0, 'histogram': [[3.0, 4.0, 18.0, 26.0, 29.0, 20.0, 25.0, 15.0, 7.0, 3.0], [0.08754898607730865, 0.1833198219537735, 0.27909064292907715, 0.374861478805542, 0.47063231468200684, 0.5664031505584717, 0.6621739864349365, 0.7579448223114014, 0.8537156581878662, 0.949486494064331, 1.045257329940796]], 'valid_percent': 100.0, 'masked_pixels': 0.0, 'valid_pixels': 150.0, 'percentile_2': 0.1521243304014206, 'percentile_98': 0.9323163628578186}}}}\n{'type': 'Feature', 'geometry': {'type': 'Polygon', 'coordinates': [[[-96.1, 32.28], [-96.1, 33.28], [-97.58, 33.28], [-97.58, 32.28], [-96.1, 32.28]]]}, 'properties': {'statistics': {'b1': {'min': 0.08782622218132019, 'max': 1.0488916635513306, 'mean': 0.5695323944091797, 'count': 150.0, 'sum': 85.42985534667969, 'std': 0.19038103900518402, 'median': 0.5564876794815063, 'majority': 0.08782622218132019, 'minority': 0.08782622218132019, 'unique': 150.0, 'histogram': [[3.0, 4.0, 18.0, 26.0, 29.0, 20.0, 25.0, 15.0, 7.0, 3.0], [0.08782622218132019, 0.18393276631832123, 0.28003931045532227, 0.3761458396911621, 0.47225239872932434, 0.5683589577674866, 0.6644654870033264, 0.7605720162391663, 0.8566786050796509, 0.9527851343154907, 1.0488916635513306]], 'valid_percent': 100.0, 'masked_pixels': 0.0, 'valid_pixels': 150.0, 'percentile_2': 0.15256305038928986, 'percentile_98': 0.9353693127632141}}}}\n{'type': 'Feature', 'geometry': {'type': 'Polygon', 'coordinates': [[[-96.1, 32.28], [-96.1, 33.28], [-97.58, 33.28], [-97.58, 32.28], [-96.1, 32.28]]]}, 'properties': {'statistics': {'b1': {'min': 0.08810531347990036, 'max': 1.0525519847869873, 'mean': 0.5712683200836182, 'count': 150.0, 'sum': 85.69024658203125, 'std': 0.1911157427567373, 'median': 0.5582570433616638, 'majority': 0.08810531347990036, 'minority': 0.08810531347990036, 'unique': 150.0, 'histogram': [[3.0, 4.0, 18.0, 26.0, 29.0, 20.0, 25.0, 15.0, 7.0, 3.0], [0.08810531347990036, 0.1845499873161316, 0.2809946537017822, 0.37743932008743286, 0.4738839864730835, 0.5703286528587341, 0.6667733192443848, 0.7632179856300354, 0.859662652015686, 0.9561073184013367, 1.0525519847869873]], 'valid_percent': 100.0, 'masked_pixels': 0.0, 'valid_pixels': 150.0, 'percentile_2': 0.15300413966178894, 'percentile_98': 0.9384419918060303}}}}\n{'type': 'Feature', 'geometry': {'type': 'Polygon', 'coordinates': [[[-96.1, 32.28], [-96.1, 33.28], [-97.58, 33.28], [-97.58, 32.28], [-96.1, 32.28]]]}, 'properties': {'statistics': {'b1': {'min': 0.08838626742362976, 'max': 1.0562385320663452, 'mean': 0.5730155110359192, 'count': 150.0, 'sum': 85.95232391357422, 'std': 0.191856293108075, 'median': 0.5598471164703369, 'majority': 0.08838626742362976, 'minority': 0.08838626742362976, 'unique': 150.0, 'histogram': [[3.0, 4.0, 18.0, 26.0, 29.0, 20.0, 25.0, 15.0, 7.0, 3.0], [0.08838626742362976, 0.18517149984836578, 0.2819567322731018, 0.37874194979667664, 0.47552716732025146, 0.5723124146461487, 0.6690976023674011, 0.7658828496932983, 0.8626680970191956, 0.959453284740448, 1.0562385320663452]], 'valid_percent': 100.0, 'masked_pixels': 0.0, 'valid_pixels': 150.0, 'percentile_2': 0.15344762802124023, 'percentile_98': 0.9415346384048462}}}}\n{'type': 'Feature', 'geometry': {'type': 'Polygon', 'coordinates': [[[-96.1, 32.28], [-96.1, 33.28], [-97.58, 33.28], [-97.58, 32.28], [-96.1, 32.28]]]}, 'properties': {'statistics': {'b1': {'min': 0.08866909891366959, 'max': 1.0599510669708252, 'mean': 0.5747740864753723, 'count': 150.0, 'sum': 86.21611022949219, 'std': 0.1926027676830852, 'median': 0.5613285303115845, 'majority': 0.08866909891366959, 'minority': 0.08866909891366959, 'unique': 150.0, 'histogram': [[3.0, 4.0, 18.0, 26.0, 29.0, 20.0, 25.0, 15.0, 7.0, 3.0], [0.08866909891366959, 0.1857972890138626, 0.28292548656463623, 0.38005369901657104, 0.47718188166618347, 0.5743100643157959, 0.6714382767677307, 0.7685664892196655, 0.8656947016716003, 0.9628228545188904, 1.0599510669708252]], 'valid_percent': 100.0, 'masked_pixels': 0.0, 'valid_pixels': 150.0, 'percentile_2': 0.15389350056648254, 'percentile_98': 0.9446474313735962}}}}\n{'type': 'Feature', 'geometry': {'type': 'Polygon', 'coordinates': [[[-96.1, 32.28], [-96.1, 33.28], [-97.58, 33.28], [-97.58, 32.28], [-96.1, 32.28]]]}, 'properties': {'statistics': {'b1': {'min': 0.08895382285118103, 'max': 1.063690185546875, 'mean': 0.5765440464019775, 'count': 150.0, 'sum': 86.48160552978516, 'std': 0.1933552038340096, 'median': 0.5631556510925293, 'majority': 0.08895382285118103, 'minority': 0.08895382285118103, 'unique': 150.0, 'histogram': [[3.0, 4.0, 18.0, 26.0, 30.0, 19.0, 25.0, 15.0, 7.0, 3.0], [0.08895382285118103, 0.18642745912075043, 0.2839010953903198, 0.381374716758728, 0.4788483679294586, 0.5763220191001892, 0.6737956404685974, 0.7712692618370056, 0.8687429428100586, 0.9662165641784668, 1.063690185546875]], 'valid_percent': 100.0, 'masked_pixels': 0.0, 'valid_pixels': 150.0, 'percentile_2': 0.15434177219867706, 'percentile_98': 0.9477803111076355}}}}\n{'type': 'Feature', 'geometry': {'type': 'Polygon', 'coordinates': [[[-96.1, 32.28], [-96.1, 33.28], [-97.58, 33.28], [-97.58, 32.28], [-96.1, 32.28]]]}, 'properties': {'statistics': {'b1': {'min': 0.08924046903848648, 'max': 1.0674560070037842, 'mean': 0.5783255100250244, 'count': 150.0, 'sum': 86.74882507324219, 'std': 0.19411365697824084, 'median': 0.5652454495429993, 'majority': 0.08924046903848648, 'minority': 0.08924046903848648, 'unique': 150.0, 'histogram': [[3.0, 4.0, 18.0, 27.0, 29.0, 19.0, 25.0, 15.0, 7.0, 3.0], [0.08924046903848648, 0.18706202507019043, 0.284883588552475, 0.38270512223243713, 0.4805266857147217, 0.5783482193946838, 0.6761698126792908, 0.7739913463592529, 0.8718128800392151, 0.969634473323822, 1.0674560070037842]], 'valid_percent': 100.0, 'masked_pixels': 0.0, 'valid_pixels': 150.0, 'percentile_2': 0.15479248762130737, 'percentile_98': 0.950933575630188}}}}\n{'type': 'Feature', 'geometry': {'type': 'Polygon', 'coordinates': [[[-96.1, 32.28], [-96.1, 33.28], [-97.58, 33.28], [-97.58, 32.28], [-96.1, 32.28]]]}, 'properties': {'statistics': {'b1': {'min': 0.05157909169793129, 'max': 0.5966315269470215, 'mean': 0.3594294786453247, 'count': 150.0, 'sum': 53.91442108154297, 'std': 0.10194551706246059, 'median': 0.3716094493865967, 'majority': 0.05157909169793129, 'minority': 0.05157909169793129, 'unique': 150.0, 'histogram': [[2.0, 3.0, 8.0, 15.0, 25.0, 25.0, 37.0, 21.0, 9.0, 5.0], [0.05157909169793129, 0.10608433187007904, 0.1605895757675171, 0.21509481966495514, 0.2696000635623932, 0.32410532236099243, 0.3786105513572693, 0.43311581015586853, 0.4876210391521454, 0.5421262979507446, 0.5966315269470215]], 'valid_percent': 100.0, 'masked_pixels': 0.0, 'valid_pixels': 150.0, 'percentile_2': 0.10882321745157242, 'percentile_98': 0.5558964014053345}}}}\n{'type': 'Feature', 'geometry': {'type': 'Polygon', 'coordinates': [[[-96.1, 32.28], [-96.1, 33.28], [-97.58, 33.28], [-97.58, 32.28], [-96.1, 32.28]]]}, 'properties': {'statistics': {'b1': {'min': 0.051655568182468414, 'max': 0.5968155860900879, 'mean': 0.35960811376571655, 'count': 150.0, 'sum': 53.94121551513672, 'std': 0.10204233928606456, 'median': 0.3719262182712555, 'majority': 0.051655568182468414, 'minority': 0.051655568182468414, 'unique': 150.0, 'histogram': [[2.0, 3.0, 8.0, 15.0, 25.0, 25.0, 37.0, 21.0, 9.0, 5.0], [0.051655568182468414, 0.10617157071828842, 0.16068756580352783, 0.21520356833934784, 0.26971957087516785, 0.32423558831214905, 0.37875157594680786, 0.43326759338378906, 0.4877835810184479, 0.5422995686531067, 0.5968155860900879]], 'valid_percent': 100.0, 'masked_pixels': 0.0, 'valid_pixels': 150.0, 'percentile_2': 0.10889758169651031, 'percentile_98': 0.5558331608772278}}}}\n{'type': 'Feature', 'geometry': {'type': 'Polygon', 'coordinates': [[[-96.1, 32.28], [-96.1, 33.28], [-97.58, 33.28], [-97.58, 32.28], [-96.1, 32.28]]]}, 'properties': {'statistics': {'b1': {'min': 0.05173231661319733, 'max': 0.5969971418380737, 'mean': 0.359785258769989, 'count': 150.0, 'sum': 53.96778869628906, 'std': 0.10214025964331674, 'median': 0.3722434937953949, 'majority': 0.05173231661319733, 'minority': 0.05173231661319733, 'unique': 150.0, 'histogram': [[2.0, 3.0, 8.0, 15.0, 25.0, 24.0, 38.0, 21.0, 9.0, 5.0], [0.05173231661319733, 0.1062588021159172, 0.16078528761863708, 0.21531176567077637, 0.26983824372291565, 0.32436472177505493, 0.3788911998271942, 0.4334177076816559, 0.48794418573379517, 0.5424706339836121, 0.5969971418380737]], 'valid_percent': 100.0, 'masked_pixels': 0.0, 'valid_pixels': 150.0, 'percentile_2': 0.10897144675254822, 'percentile_98': 0.5557645559310913}}}}\n{'type': 'Feature', 'geometry': {'type': 'Polygon', 'coordinates': [[[-96.1, 32.28], [-96.1, 33.28], [-97.58, 33.28], [-97.58, 32.28], [-96.1, 32.28]]]}, 'properties': {'statistics': {'b1': {'min': 0.05180935189127922, 'max': 0.5971760749816895, 'mean': 0.3599608838558197, 'count': 150.0, 'sum': 53.99413299560547, 'std': 0.10223930230671288, 'median': 0.3725612759590149, 'majority': 0.05180935189127922, 'minority': 0.05180935189127922, 'unique': 150.0, 'histogram': [[2.0, 3.0, 8.0, 15.0, 24.0, 24.0, 39.0, 21.0, 9.0, 5.0], [0.05180935189127922, 0.1063460260629654, 0.16088269650936127, 0.21541936695575714, 0.2699560523033142, 0.3244927227497101, 0.37902939319610596, 0.43356606364250183, 0.4881027340888977, 0.5426393747329712, 0.5971760749816895]], 'valid_percent': 100.0, 'masked_pixels': 0.0, 'valid_pixels': 150.0, 'percentile_2': 0.10904483497142792, 'percentile_98': 0.5556904673576355}}}}\n{'type': 'Feature', 'geometry': {'type': 'Polygon', 'coordinates': [[[-96.1, 32.28], [-96.1, 33.28], [-97.58, 33.28], [-97.58, 32.28], [-96.1, 32.28]]]}, 'properties': {'statistics': {'b1': {'min': 0.0518866628408432, 'max': 0.5973522663116455, 'mean': 0.36013492941856384, 'count': 150.0, 'sum': 54.020240783691406, 'std': 0.10233945491747044, 'median': 0.37287941575050354, 'majority': 0.0518866628408432, 'minority': 0.0518866628408432, 'unique': 150.0, 'histogram': [[2.0, 3.0, 8.0, 15.0, 24.0, 24.0, 39.0, 21.0, 9.0, 5.0], [0.0518866628408432, 0.10643322020769119, 0.16097977757453918, 0.21552634239196777, 0.27007290720939636, 0.32461947202682495, 0.37916603684425354, 0.43371257185935974, 0.48825913667678833, 0.5428057312965393, 0.5973522663116455]], 'valid_percent': 100.0, 'masked_pixels': 0.0, 'valid_pixels': 150.0, 'percentile_2': 0.10911771655082703, 'percentile_98': 0.5556106567382812}}}}\n{'type': 'Feature', 'geometry': {'type': 'Polygon', 'coordinates': [[[-96.1, 32.28], [-96.1, 33.28], [-97.58, 33.28], [-97.58, 32.28], [-96.1, 32.28]]]}, 'properties': {'statistics': {'b1': {'min': 0.05196423828601837, 'max': 0.5975256562232971, 'mean': 0.36030736565589905, 'count': 150.0, 'sum': 54.046104431152344, 'std': 0.10244076422243784, 'median': 0.37319791316986084, 'majority': 0.05196423828601837, 'minority': 0.05196423828601837, 'unique': 150.0, 'histogram': [[2.0, 3.0, 8.0, 15.0, 24.0, 24.0, 39.0, 21.0, 9.0, 5.0], [0.05196423828601837, 0.10652037709951401, 0.16107651591300964, 0.21563266217708588, 0.2701888084411621, 0.32474493980407715, 0.3793011009693146, 0.4338572323322296, 0.48841336369514465, 0.5429695248603821, 0.5975256562232971]], 'valid_percent': 100.0, 'masked_pixels': 0.0, 'valid_pixels': 150.0, 'percentile_2': 0.10919009894132614, 'percentile_98': 0.555525004863739}}}}\n{'type': 'Feature', 'geometry': {'type': 'Polygon', 'coordinates': [[[-96.1, 32.28], [-96.1, 33.28], [-97.58, 33.28], [-97.58, 32.28], [-96.1, 32.28]]]}, 'properties': {'statistics': {'b1': {'min': 0.05204210802912712, 'max': 0.5976963043212891, 'mean': 0.36047816276550293, 'count': 150.0, 'sum': 54.07172393798828, 'std': 0.10254324495777441, 'median': 0.37351682782173157, 'majority': 0.05204210802912712, 'minority': 0.05204210802912712, 'unique': 150.0, 'histogram': [[2.0, 3.0, 8.0, 15.0, 23.0, 25.0, 39.0, 21.0, 9.0, 5.0], [0.05204210802912712, 0.10660752654075623, 0.16117294132709503, 0.21573837101459503, 0.27030378580093384, 0.32486921548843384, 0.37943461537361145, 0.43400004506111145, 0.48856547474861145, 0.5431308746337891, 0.5976963043212891]], 'valid_percent': 100.0, 'masked_pixels': 0.0, 'valid_pixels': 150.0, 'percentile_2': 0.10926199704408646, 'percentile_98': 0.5554335713386536}}}}\n{'type': 'Feature', 'geometry': {'type': 'Polygon', 'coordinates': [[[-96.1, 32.28], [-96.1, 33.28], [-97.58, 33.28], [-97.58, 32.28], [-96.1, 32.28]]]}, 'properties': {'statistics': {'b1': {'min': 0.05212024971842766, 'max': 0.5978642106056213, 'mean': 0.3606472909450531, 'count': 150.0, 'sum': 54.09709548950195, 'std': 0.10264693898022383, 'median': 0.3738361597061157, 'majority': 0.05212024971842766, 'minority': 0.05212024971842766, 'unique': 150.0, 'histogram': [[2.0, 3.0, 8.0, 16.0, 22.0, 25.0, 39.0, 21.0, 9.0, 5.0], [0.05212024971842766, 0.10669464617967606, 0.16126903891563416, 0.21584343910217285, 0.27041783928871155, 0.32499223947525024, 0.37956663966178894, 0.43414101004600525, 0.48871541023254395, 0.543289840221405, 0.5978642106056213]], 'valid_percent': 100.0, 'masked_pixels': 0.0, 'valid_pixels': 150.0, 'percentile_2': 0.10933336615562439, 'percentile_98': 0.5553362369537354}}}}\n{'type': 'Feature', 'geometry': {'type': 'Polygon', 'coordinates': [[[-96.1, 32.28], [-96.1, 33.28], [-97.58, 33.28], [-97.58, 32.28], [-96.1, 32.28]]]}, 'properties': {'statistics': {'b1': {'min': 0.05219866707921028, 'max': 0.5980291962623596, 'mean': 0.36081477999687195, 'count': 150.0, 'sum': 54.122215270996094, 'std': 0.10275185621225448, 'median': 0.37415581941604614, 'majority': 0.05219866707921028, 'minority': 0.05219866707921028, 'unique': 150.0, 'histogram': [[2.0, 3.0, 9.0, 15.0, 22.0, 25.0, 38.0, 22.0, 9.0, 5.0], [0.05219866707921028, 0.1067817211151123, 0.16136477887630463, 0.21594782173633575, 0.2705308794975281, 0.3251139223575592, 0.3796969950199127, 0.43428003787994385, 0.488863080739975, 0.5434461236000061, 0.5980291962623596]], 'valid_percent': 100.0, 'masked_pixels': 0.0, 'valid_pixels': 150.0, 'percentile_2': 0.10940421372652054, 'percentile_98': 0.5552330613136292}}}}\n{'type': 'Feature', 'geometry': {'type': 'Polygon', 'coordinates': [[[-96.1, 32.28], [-96.1, 33.28], [-97.58, 33.28], [-97.58, 32.28], [-96.1, 32.28]]]}, 'properties': {'statistics': {'b1': {'min': 0.05227737873792648, 'max': 0.5981912612915039, 'mean': 0.36098048090934753, 'count': 150.0, 'sum': 54.147071838378906, 'std': 0.10285804271023502, 'median': 0.3744759261608124, 'majority': 0.05227737873792648, 'minority': 0.05227737873792648, 'unique': 150.0, 'histogram': [[2.0, 3.0, 9.0, 15.0, 22.0, 25.0, 38.0, 21.0, 10.0, 5.0], [0.05227737873792648, 0.10686876624822617, 0.16146016120910645, 0.21605154871940613, 0.2706429362297058, 0.3252343237400055, 0.3798257112503052, 0.43441709876060486, 0.48900848627090454, 0.5435998439788818, 0.5981912612915039]], 'valid_percent': 100.0, 'masked_pixels': 0.0, 'valid_pixels': 150.0, 'percentile_2': 0.1094745397567749, 'percentile_98': 0.5551238059997559}}}}\n{'type': 'Feature', 'geometry': {'type': 'Polygon', 'coordinates': [[[-96.1, 32.28], [-96.1, 33.28], [-97.58, 33.28], [-97.58, 32.28], [-96.1, 32.28]]]}, 'properties': {'statistics': {'b1': {'min': 0.05235636606812477, 'max': 0.5983505249023438, 'mean': 0.361144483089447, 'count': 150.0, 'sum': 54.17167282104492, 'std': 0.10296552168223774, 'median': 0.37479639053344727, 'majority': 0.05235636606812477, 'minority': 0.05235636606812477, 'unique': 150.0, 'histogram': [[2.0, 3.0, 9.0, 15.0, 22.0, 25.0, 38.0, 21.0, 10.0, 5.0], [0.05235636606812477, 0.10695578157901764, 0.1615552008152008, 0.21615462005138397, 0.27075403928756714, 0.3253534436225891, 0.3799528479576111, 0.43455228209495544, 0.4891516864299774, 0.5437511205673218, 0.5983505249023438]], 'valid_percent': 100.0, 'masked_pixels': 0.0, 'valid_pixels': 150.0, 'percentile_2': 0.10954435169696808, 'percentile_98': 0.5550085306167603}}}}\n{'type': 'Feature', 'geometry': {'type': 'Polygon', 'coordinates': [[[-96.1, 32.28], [-96.1, 33.28], [-97.58, 33.28], [-97.58, 32.28], [-96.1, 32.28]]]}, 'properties': {'statistics': {'b1': {'min': 0.05243564024567604, 'max': 0.5985066890716553, 'mean': 0.36130666732788086, 'count': 150.0, 'sum': 54.19599914550781, 'std': 0.10307430263832042, 'median': 0.3751172125339508, 'majority': 0.05243564024567604, 'minority': 0.05243564024567604, 'unique': 150.0, 'histogram': [[2.0, 3.0, 9.0, 15.0, 22.0, 25.0, 38.0, 21.0, 10.0, 5.0], [0.05243564024567604, 0.10704274475574493, 0.16164985299110413, 0.21625696122646332, 0.2708640694618225, 0.3254711627960205, 0.3800782561302185, 0.4346853792667389, 0.4892924726009369, 0.5438995957374573, 0.5985066890716553]], 'valid_percent': 100.0, 'masked_pixels': 0.0, 'valid_pixels': 150.0, 'percentile_2': 0.10961362719535828, 'percentile_98': 0.5548869967460632}}}}\n{'type': 'Feature', 'geometry': {'type': 'Polygon', 'coordinates': [[[-96.1, 32.28], [-96.1, 33.28], [-97.58, 33.28], [-97.58, 32.28], [-96.1, 32.28]]]}, 'properties': {'statistics': {'b1': {'min': 0.05251520872116089, 'max': 0.5986599922180176, 'mean': 0.36146703362464905, 'count': 150.0, 'sum': 54.220054626464844, 'std': 0.1031844401284229, 'median': 0.375438392162323, 'majority': 0.05251520872116089, 'minority': 0.05251520872116089, 'unique': 150.0, 'histogram': [[2.0, 3.0, 9.0, 15.0, 21.0, 26.0, 38.0, 21.0, 10.0, 5.0], [0.05251520872116089, 0.10712968558073044, 0.1617441624403, 0.21635864675045013, 0.2709731161594391, 0.32558760046958923, 0.3802020847797394, 0.4348165690898895, 0.4894310235977173, 0.5440455079078674, 0.5986599922180176]], 'valid_percent': 100.0, 'masked_pixels': 0.0, 'valid_pixels': 150.0, 'percentile_2': 0.1096823513507843, 'percentile_98': 0.5547593832015991}}}}\n{'type': 'Feature', 'geometry': {'type': 'Polygon', 'coordinates': [[[-96.1, 32.28], [-96.1, 33.28], [-97.58, 33.28], [-97.58, 32.28], [-96.1, 32.28]]]}, 'properties': {'statistics': {'b1': {'min': 0.05259505286812782, 'max': 0.5988102555274963, 'mean': 0.3616255819797516, 'count': 150.0, 'sum': 54.24383544921875, 'std': 0.10329597489370619, 'median': 0.37575995922088623, 'majority': 0.05259505286812782, 'minority': 0.05259505286812782, 'unique': 150.0, 'histogram': [[2.0, 3.0, 9.0, 15.0, 21.0, 26.0, 38.0, 21.0, 10.0, 5.0], [0.05259505286812782, 0.10721657425165176, 0.161838099360466, 0.21645961701869965, 0.2710811197757721, 0.3257026672363281, 0.38032418489456177, 0.4349457025527954, 0.48956722021102905, 0.5441887378692627, 0.5988102555274963]], 'valid_percent': 100.0, 'masked_pixels': 0.0, 'valid_pixels': 150.0, 'percentile_2': 0.10975053161382675, 'percentile_98': 0.5546254515647888}}}}\n{'type': 'Feature', 'geometry': {'type': 'Polygon', 'coordinates': [[[-96.1, 32.28], [-96.1, 33.28], [-97.58, 33.28], [-97.58, 32.28], [-96.1, 32.28]]]}, 'properties': {'statistics': {'b1': {'min': 0.052675195038318634, 'max': 0.598957359790802, 'mean': 0.36178216338157654, 'count': 150.0, 'sum': 54.26732635498047, 'std': 0.10340891141916393, 'median': 0.3760818541049957, 'majority': 0.052675195038318634, 'minority': 0.052675195038318634, 'unique': 150.0, 'histogram': [[2.0, 3.0, 9.0, 15.0, 21.0, 26.0, 38.0, 21.0, 10.0, 5.0], [0.052675195038318634, 0.10730341076850891, 0.16193163394927979, 0.21655984222888947, 0.27118805050849915, 0.32581627368927, 0.3804444968700409, 0.43507272005081177, 0.48970091342926025, 0.5443291664123535, 0.598957359790802]], 'valid_percent': 100.0, 'masked_pixels': 0.0, 'valid_pixels': 150.0, 'percentile_2': 0.10981813818216324, 'percentile_98': 0.5544852018356323}}}}\n{'type': 'Feature', 'geometry': {'type': 'Polygon', 'coordinates': [[[-96.1, 32.28], [-96.1, 33.28], [-97.58, 33.28], [-97.58, 32.28], [-96.1, 32.28]]]}, 'properties': {'statistics': {'b1': {'min': 0.05275562405586243, 'max': 0.5991013646125793, 'mean': 0.3619368374347687, 'count': 150.0, 'sum': 54.29052734375, 'std': 0.10352328110218793, 'median': 0.3764040172100067, 'majority': 0.05275562405586243, 'minority': 0.05275562405586243, 'unique': 150.0, 'histogram': [[2.0, 3.0, 9.0, 15.0, 21.0, 26.0, 38.0, 21.0, 10.0, 5.0], [0.05275562405586243, 0.10739019513130188, 0.16202476620674133, 0.21665935218334198, 0.27129390835762024, 0.3259285092353821, 0.38056308031082153, 0.435197651386261, 0.48983222246170044, 0.5444667935371399, 0.5991013646125793]], 'valid_percent': 100.0, 'masked_pixels': 0.0, 'valid_pixels': 150.0, 'percentile_2': 0.10988516360521317, 'percentile_98': 0.5543383359909058}}}}\n{'type': 'Feature', 'geometry': {'type': 'Polygon', 'coordinates': [[[-96.1, 32.28], [-96.1, 33.28], [-97.58, 33.28], [-97.58, 32.28], [-96.1, 32.28]]]}, 'properties': {'statistics': {'b1': {'min': 0.0528363399207592, 'max': 0.599242091178894, 'mean': 0.3620895445346832, 'count': 150.0, 'sum': 54.31343078613281, 'std': 0.10363913311539451, 'median': 0.3767264485359192, 'majority': 0.0528363399207592, 'minority': 0.0528363399207592, 'unique': 150.0, 'histogram': [[2.0, 3.0, 9.0, 15.0, 21.0, 26.0, 38.0, 21.0, 10.0, 5.0], [0.0528363399207592, 0.10747691243886948, 0.16211749613285065, 0.21675807237625122, 0.2713986337184906, 0.32603922486305237, 0.38067978620529175, 0.4353203773498535, 0.4899609386920929, 0.5446015000343323, 0.599242091178894]], 'valid_percent': 100.0, 'masked_pixels': 0.0, 'valid_pixels': 150.0, 'percentile_2': 0.10995160788297653, 'percentile_98': 0.5541849136352539}}}}\n{'type': 'Feature', 'geometry': {'type': 'Polygon', 'coordinates': [[[-96.1, 32.28], [-96.1, 33.28], [-97.58, 33.28], [-97.58, 32.28], [-96.1, 32.28]]]}, 'properties': {'statistics': {'b1': {'min': 0.05291735380887985, 'max': 0.5993795990943909, 'mean': 0.3622402548789978, 'count': 150.0, 'sum': 54.336036682128906, 'std': 0.10375649839754911, 'median': 0.37704917788505554, 'majority': 0.05291735380887985, 'minority': 0.05291735380887985, 'unique': 150.0, 'histogram': [[2.0, 3.0, 9.0, 16.0, 20.0, 26.0, 38.0, 21.0, 10.0, 5.0], [0.05291735380887985, 0.1075635775923729, 0.16220980882644653, 0.21685603260993958, 0.2715022563934326, 0.32614848017692566, 0.3807947039604187, 0.43544092774391174, 0.4900871515274048, 0.5447333455085754, 0.5993795990943909]], 'valid_percent': 100.0, 'masked_pixels': 0.0, 'valid_pixels': 150.0, 'percentile_2': 0.11001747101545334, 'percentile_98': 0.5540248155593872}}}}\n{'type': 'Feature', 'geometry': {'type': 'Polygon', 'coordinates': [[[-96.1, 32.28], [-96.1, 33.28], [-97.58, 33.28], [-97.58, 32.28], [-96.1, 32.28]]]}, 'properties': {'statistics': {'b1': {'min': 0.052998658269643784, 'max': 0.5995140075683594, 'mean': 0.3623889088630676, 'count': 150.0, 'sum': 54.35833740234375, 'std': 0.10387541216522918, 'median': 0.3773723244667053, 'majority': 0.052998658269643784, 'minority': 0.052998658269643784, 'unique': 150.0, 'histogram': [[2.0, 3.0, 9.0, 16.0, 20.0, 26.0, 38.0, 21.0, 10.0, 5.0], [0.052998658269643784, 0.10765019059181213, 0.16230173408985138, 0.21695326268672943, 0.2716048061847687, 0.32625633478164673, 0.3809078633785248, 0.43555939197540283, 0.49021095037460327, 0.5448624491691589, 0.5995140075683594]], 'valid_percent': 100.0, 'masked_pixels': 0.0, 'valid_pixels': 150.0, 'percentile_2': 0.11008275300264359, 'percentile_98': 0.5538581609725952}}}}\n{'type': 'Feature', 'geometry': {'type': 'Polygon', 'coordinates': [[[-96.1, 32.28], [-96.1, 33.28], [-97.58, 33.28], [-97.58, 32.28], [-96.1, 32.28]]]}, 'properties': {'statistics': {'b1': {'min': 0.053080253303050995, 'max': 0.5996448993682861, 'mean': 0.3625355362892151, 'count': 150.0, 'sum': 54.38032913208008, 'std': 0.10399589597273924, 'median': 0.3776956796646118, 'majority': 0.053080253303050995, 'minority': 0.053080253303050995, 'unique': 150.0, 'histogram': [[2.0, 3.0, 9.0, 16.0, 20.0, 26.0, 38.0, 21.0, 10.0, 5.0], [0.053080253303050995, 0.10773672163486481, 0.16239318251609802, 0.21704964339733124, 0.27170610427856445, 0.32636258006095886, 0.38101905584335327, 0.4356755018234253, 0.4903319776058197, 0.5449884533882141, 0.5996448993682861]], 'valid_percent': 100.0, 'masked_pixels': 0.0, 'valid_pixels': 150.0, 'percentile_2': 0.1101474016904831, 'percentile_98': 0.5536845326423645}}}}\n{'type': 'Feature', 'geometry': {'type': 'Polygon', 'coordinates': [[[-96.1, 32.28], [-96.1, 33.28], [-97.58, 33.28], [-97.58, 32.28], [-96.1, 32.28]]]}, 'properties': {'statistics': {'b1': {'min': 0.05316215008497238, 'max': 0.5997725129127502, 'mean': 0.362680047750473, 'count': 150.0, 'sum': 54.402008056640625, 'std': 0.10411800698374098, 'median': 0.37801939249038696, 'majority': 0.05316215008497238, 'minority': 0.05316215008497238, 'unique': 150.0, 'histogram': [[2.0, 3.0, 9.0, 16.0, 20.0, 26.0, 38.0, 21.0, 10.0, 5.0], [0.05316215008497238, 0.10782318562269211, 0.16248422861099243, 0.21714526414871216, 0.2718062996864319, 0.3264673352241516, 0.38112837076187134, 0.43578940629959106, 0.4904504418373108, 0.5451114773750305, 0.5997725129127502]], 'valid_percent': 100.0, 'masked_pixels': 0.0, 'valid_pixels': 150.0, 'percentile_2': 0.11021147668361664, 'percentile_98': 0.5535041689872742}}}}\n{'type': 'Feature', 'geometry': {'type': 'Polygon', 'coordinates': [[[-96.1, 32.28], [-96.1, 33.28], [-97.58, 33.28], [-97.58, 32.28], [-96.1, 32.28]]]}, 'properties': {'statistics': {'b1': {'min': 0.053244348615407944, 'max': 0.5998967289924622, 'mean': 0.3628224730491638, 'count': 150.0, 'sum': 54.423370361328125, 'std': 0.10424176181545337, 'median': 0.3783433139324188, 'majority': 0.053244348615407944, 'minority': 0.053244348615407944, 'unique': 150.0, 'histogram': [[2.0, 3.0, 9.0, 16.0, 20.0, 26.0, 38.0, 21.0, 10.0, 5.0], [0.053244348615407944, 0.10790959000587463, 0.16257482767105103, 0.21724006533622742, 0.2719053030014038, 0.3265705406665802, 0.3812357783317566, 0.435901015996933, 0.4905662536621094, 0.5452314615249634, 0.5998967289924622]], 'valid_percent': 100.0, 'masked_pixels': 0.0, 'valid_pixels': 150.0, 'percentile_2': 0.11027494817972183, 'percentile_98': 0.5533167719841003}}}}\n{'type': 'Feature', 'geometry': {'type': 'Polygon', 'coordinates': [[[-96.1, 32.28], [-96.1, 33.28], [-97.58, 33.28], [-97.58, 32.28], [-96.1, 32.28]]]}, 'properties': {'statistics': {'b1': {'min': 0.05330297350883484, 'max': 0.6000174283981323, 'mean': 0.36296263337135315, 'count': 150.0, 'sum': 54.44439697265625, 'std': 0.10436720369968364, 'median': 0.3786674737930298, 'majority': 0.05330297350883484, 'minority': 0.05330297350883484, 'unique': 150.0, 'histogram': [[2.0, 3.0, 9.0, 16.0, 20.0, 26.0, 38.0, 21.0, 10.0, 5.0], [0.05330297350883484, 0.10797441750764847, 0.1626458615064621, 0.21731731295585632, 0.27198874950408936, 0.3266602158546448, 0.3813316524028778, 0.43600308895111084, 0.49067452549934387, 0.5453459620475769, 0.6000174283981323]], 'valid_percent': 100.0, 'masked_pixels': 0.0, 'valid_pixels': 150.0, 'percentile_2': 0.11033782362937927, 'percentile_98': 0.5531222820281982}}}}\n{'type': 'Feature', 'geometry': {'type': 'Polygon', 'coordinates': [[[-96.1, 32.28], [-96.1, 33.28], [-97.58, 33.28], [-97.58, 32.28], [-96.1, 32.28]]]}, 'properties': {'statistics': {'b1': {'min': 0.05336037650704384, 'max': 0.6001348495483398, 'mean': 0.36310067772865295, 'count': 150.0, 'sum': 54.46510314941406, 'std': 0.10449438003664056, 'median': 0.378991961479187, 'majority': 0.05336037650704384, 'minority': 0.05336037650704384, 'unique': 150.0, 'histogram': [[2.0, 3.0, 9.0, 17.0, 19.0, 26.0, 38.0, 21.0, 10.0, 5.0], [0.05336037650704384, 0.10803782194852829, 0.16271527111530304, 0.2173927128314972, 0.27207016944885254, 0.3267476260662079, 0.38142505288124084, 0.4361025094985962, 0.49077996611595154, 0.5454574227333069, 0.6001348495483398]], 'valid_percent': 100.0, 'masked_pixels': 0.0, 'valid_pixels': 150.0, 'percentile_2': 0.11040008813142776, 'percentile_98': 0.5529207587242126}}}}\n{'type': 'Feature', 'geometry': {'type': 'Polygon', 'coordinates': [[[-96.1, 32.28], [-96.1, 33.28], [-97.58, 33.28], [-97.58, 32.28], [-96.1, 32.28]]]}, 'properties': {'statistics': {'b1': {'min': 0.05341782793402672, 'max': 0.6002485752105713, 'mean': 0.3632364571094513, 'count': 150.0, 'sum': 54.485469818115234, 'std': 0.10462332900967777, 'median': 0.3793167471885681, 'majority': 0.05341782793402672, 'minority': 0.05341782793402672, 'unique': 150.0, 'histogram': [[2.0, 3.0, 9.0, 18.0, 18.0, 26.0, 38.0, 21.0, 10.0, 5.0], [0.05341782793402672, 0.10810090601444244, 0.16278398036956787, 0.2174670547246933, 0.2721501290798187, 0.32683318853378296, 0.3815162777900696, 0.4361993372440338, 0.49088242650032043, 0.5455654859542847, 0.6002485752105713]], 'valid_percent': 100.0, 'masked_pixels': 0.0, 'valid_pixels': 150.0, 'percentile_2': 0.11046174168586731, 'percentile_98': 0.5527121424674988}}}}\n{'type': 'Feature', 'geometry': {'type': 'Polygon', 'coordinates': [[[-96.1, 32.28], [-96.1, 33.28], [-97.58, 33.28], [-97.58, 32.28], [-96.1, 32.28]]]}, 'properties': {'statistics': {'b1': {'min': 0.05347532778978348, 'max': 0.600358784198761, 'mean': 0.36337003111839294, 'count': 150.0, 'sum': 54.5055046081543, 'std': 0.10475410630654797, 'median': 0.37964171171188354, 'majority': 0.05347532778978348, 'minority': 0.05347532778978348, 'unique': 150.0, 'histogram': [[2.0, 3.0, 9.0, 18.0, 18.0, 26.0, 37.0, 22.0, 10.0, 5.0], [0.05347532778978348, 0.10816366970539093, 0.16285201907157898, 0.21754036843776703, 0.2722287178039551, 0.32691705226898193, 0.3816053867340088, 0.43629375100135803, 0.4909820854663849, 0.5456704497337341, 0.600358784198761]], 'valid_percent': 100.0, 'masked_pixels': 0.0, 'valid_pixels': 150.0, 'percentile_2': 0.11052275449037552, 'percentile_98': 0.5524962544441223}}}}\n{'type': 'Feature', 'geometry': {'type': 'Polygon', 'coordinates': [[[-96.1, 32.28], [-96.1, 33.28], [-97.58, 33.28], [-97.58, 32.28], [-96.1, 32.28]]]}, 'properties': {'statistics': {'b1': {'min': 0.05353288725018501, 'max': 0.6004654169082642, 'mean': 0.36350128054618835, 'count': 150.0, 'sum': 54.52519226074219, 'std': 0.10488673616596475, 'median': 0.37996697425842285, 'majority': 0.05353288725018501, 'minority': 0.05353288725018501, 'unique': 150.0, 'histogram': [[2.0, 3.0, 9.0, 18.0, 18.0, 26.0, 37.0, 22.0, 10.0, 5.0], [0.05353288725018501, 0.10822614282369614, 0.16291938722133636, 0.2176126390695572, 0.2723059058189392, 0.32699915766716003, 0.38169240951538086, 0.4363856613636017, 0.4910789132118225, 0.545772135257721, 0.6004654169082642]], 'valid_percent': 100.0, 'masked_pixels': 0.0, 'valid_pixels': 150.0, 'percentile_2': 0.1105831041932106, 'percentile_98': 0.5522729158401489}}}}\n{'type': 'Feature', 'geometry': {'type': 'Polygon', 'coordinates': [[[-96.1, 32.28], [-96.1, 33.28], [-97.58, 33.28], [-97.58, 32.28], [-96.1, 32.28]]]}, 'properties': {'statistics': {'b1': {'min': 0.05359049513936043, 'max': 0.6005685329437256, 'mean': 0.3636302351951599, 'count': 150.0, 'sum': 54.54453659057617, 'std': 0.10502128694672565, 'median': 0.38029247522354126, 'majority': 0.05359049513936043, 'minority': 0.05359049513936043, 'unique': 150.0, 'histogram': [[2.0, 3.0, 9.0, 18.0, 18.0, 25.0, 38.0, 22.0, 10.0, 5.0], [0.05359049513936043, 0.10828829556703568, 0.16298609972000122, 0.21768391132354736, 0.2723817229270935, 0.32707950472831726, 0.3817773163318634, 0.43647512793540955, 0.4911729395389557, 0.5458707213401794, 0.6005685329437256]], 'valid_percent': 100.0, 'masked_pixels': 0.0, 'valid_pixels': 150.0, 'percentile_2': 0.11064280569553375, 'percentile_98': 0.5520422458648682}}}}\n{'type': 'Feature', 'geometry': {'type': 'Polygon', 'coordinates': [[[-96.1, 32.28], [-96.1, 33.28], [-97.58, 33.28], [-97.58, 32.28], [-96.1, 32.28]]]}, 'properties': {'statistics': {'b1': {'min': 0.05364815145730972, 'max': 0.6006680727005005, 'mean': 0.36375680565834045, 'count': 150.0, 'sum': 54.56352233886719, 'std': 0.105157813270308, 'median': 0.38061830401420593, 'majority': 0.05364815145730972, 'minority': 0.05364815145730972, 'unique': 150.0, 'histogram': [[2.0, 3.0, 9.0, 18.0, 18.0, 25.0, 38.0, 21.0, 11.0, 5.0], [0.05364815145730972, 0.10835014283657074, 0.16305214166641235, 0.21775412559509277, 0.2724561095237732, 0.327158123254776, 0.3818601071834564, 0.43656209111213684, 0.49126407504081726, 0.5459660887718201, 0.6006680727005005]], 'valid_percent': 100.0, 'masked_pixels': 0.0, 'valid_pixels': 150.0, 'percentile_2': 0.11070182174444199, 'percentile_98': 0.5518040657043457}}}}\n{'type': 'Feature', 'geometry': {'type': 'Polygon', 'coordinates': [[[-96.1, 32.28], [-96.1, 33.28], [-97.58, 33.28], [-97.58, 32.28], [-96.1, 32.28]]]}, 'properties': {'statistics': {'b1': {'min': 0.053705863654613495, 'max': 0.6007638573646545, 'mean': 0.36388105154037476, 'count': 150.0, 'sum': 54.582157135009766, 'std': 0.10529633398662833, 'median': 0.3809443712234497, 'majority': 0.053705863654613495, 'minority': 0.053705863654613495, 'unique': 150.0, 'histogram': [[2.0, 3.0, 9.0, 18.0, 18.0, 25.0, 38.0, 21.0, 11.0, 5.0], [0.053705863654613495, 0.10841166228055954, 0.16311746835708618, 0.21782326698303223, 0.27252906560897827, 0.3272348642349243, 0.38194066286087036, 0.4366464614868164, 0.49135226011276245, 0.5460580587387085, 0.6007638573646545]], 'valid_percent': 100.0, 'masked_pixels': 0.0, 'valid_pixels': 150.0, 'percentile_2': 0.11076019704341888, 'percentile_98': 0.5515584349632263}}}}\n{'type': 'Feature', 'geometry': {'type': 'Polygon', 'coordinates': [[[-96.1, 32.28], [-96.1, 33.28], [-97.58, 33.28], [-97.58, 32.28], [-96.1, 32.28]]]}, 'properties': {'statistics': {'b1': {'min': 0.05376362055540085, 'max': 0.6008560061454773, 'mean': 0.36400288343429565, 'count': 150.0, 'sum': 54.600433349609375, 'std': 0.10543692073199346, 'median': 0.3812706470489502, 'majority': 0.05376362055540085, 'minority': 0.05376362055540085, 'unique': 150.0, 'histogram': [[2.0, 3.0, 9.0, 18.0, 18.0, 25.0, 38.0, 21.0, 11.0, 5.0], [0.05376362055540085, 0.10847286134958267, 0.1631820946931839, 0.21789133548736572, 0.27260056138038635, 0.32730981707572937, 0.38201904296875, 0.436728298664093, 0.49143752455711365, 0.5461467504501343, 0.6008560061454773]], 'valid_percent': 100.0, 'masked_pixels': 0.0, 'valid_pixels': 150.0, 'percentile_2': 0.11081784963607788, 'percentile_98': 0.5513050556182861}}}}\nCPU times: user 1.52 s, sys: 224 ms, total: 1.75 s\nWall time: 36.1 s\n\n\n\n# Print the stats for the first item in the collection\nstats[0]\n\n{'statistics': {'b1': {'min': 0.08267466723918915,\n   'max': 0.9830807447433472,\n   'mean': 0.6214667558670044,\n   'count': 150.0,\n   'sum': 93.22001647949219,\n   'std': 0.16884362462505265,\n   'median': 0.6536906361579895,\n   'majority': 0.08267466723918915,\n   'minority': 0.08267466723918915,\n   'unique': 150.0,\n   'histogram': [[2.0, 3.0, 3.0, 13.0, 19.0, 30.0, 40.0, 18.0, 16.0, 6.0],\n    [0.08267466723918915,\n     0.17271527647972107,\n     0.2627558708190918,\n     0.3527964949607849,\n     0.44283708930015564,\n     0.5328776836395264,\n     0.6229183077812195,\n     0.7129589319229126,\n     0.8029995560646057,\n     0.893040120601654,\n     0.9830807447433472]],\n   'valid_percent': 100.0,\n   'masked_pixels': 0.0,\n   'valid_pixels': 150.0,\n   'percentile_2': 0.18744821846485138,\n   'percentile_98': 0.9215888977050781}},\n 'datetime': '2001-03-08'}\n\n\nCreate a function that goes through every single item in the collection and populates their properties - including the minimum, maximum, and sum of their values - in a table.\n\n# Create a function that converts statistics in JSON format into a pandas DataFrame\ndef clean_stats(stats_json) -&gt; pd.DataFrame:\n\n    # Normalize the JSON data\n    df = pd.json_normalize(stats_json)\n\n    # Replace the naming \"statistics.b1\" in the columns\n    df.columns = [col.replace(\"statistics.b1.\", \"\") for col in df.columns]\n\n    # Set the datetime format\n    df[\"date\"] = pd.to_datetime(df[\"datetime\"])\n\n    # Return the cleaned format\n    return df\n\n# Apply the generated function on the stats data\ndf = clean_stats(stats)\n\n# Display the stats for the first 5 granules in the collection in the table\n# Change the value in the parenthesis to show more or a smaller number of rows in the table\ndf.head(5)\n\n\n\n\n\n\n\n\n\ndatetime\nmin\nmax\nmean\ncount\nsum\nstd\nmedian\nmajority\nminority\nunique\nhistogram\nvalid_percent\nmasked_pixels\nvalid_pixels\npercentile_2\npercentile_98\ndate\n\n\n\n\n0\n2001-03-08\n0.082675\n0.983081\n0.621467\n150.0\n93.220016\n0.168844\n0.653691\n0.082675\n0.082675\n150.0\n[[2.0, 3.0, 3.0, 13.0, 19.0, 30.0, 40.0, 18.0,...\n100.0\n0.0\n150.0\n0.187448\n0.921589\n2001-03-08\n\n\n1\n2001-03-07\n0.082977\n0.985993\n0.623370\n150.0\n93.505447\n0.169345\n0.655875\n0.082977\n0.082977\n150.0\n[[2.0, 3.0, 3.0, 13.0, 19.0, 30.0, 41.0, 17.0,...\n100.0\n0.0\n150.0\n0.188115\n0.924291\n2001-03-07\n\n\n2\n2001-03-06\n0.083282\n0.988925\n0.625286\n150.0\n93.792953\n0.169849\n0.658075\n0.083282\n0.083282\n150.0\n[[2.0, 3.0, 3.0, 13.0, 19.0, 30.0, 41.0, 17.0,...\n100.0\n0.0\n150.0\n0.188787\n0.927012\n2001-03-06\n\n\n3\n2001-03-05\n0.083589\n0.991878\n0.627217\n150.0\n94.082520\n0.170358\n0.660291\n0.083589\n0.083589\n150.0\n[[2.0, 3.0, 3.0, 13.0, 20.0, 29.0, 41.0, 17.0,...\n100.0\n0.0\n150.0\n0.189463\n0.929751\n2001-03-05\n\n\n4\n2001-03-04\n0.083898\n0.994851\n0.629161\n150.0\n94.374184\n0.170871\n0.662523\n0.083898\n0.083898\n150.0\n[[2.0, 3.0, 3.0, 13.0, 20.0, 29.0, 41.0, 17.0,...\n100.0\n0.0\n150.0\n0.190144\n0.932509\n2001-03-04",
    "crumbs": [
      "Data Usage Notebooks",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "MiCASA Land Carbon Flux"
    ]
  },
  {
    "objectID": "user_data_notebooks/micasa-carbonflux-daygrid-v1_User_Notebook.html#visualize-the-data-as-a-time-series",
    "href": "user_data_notebooks/micasa-carbonflux-daygrid-v1_User_Notebook.html#visualize-the-data-as-a-time-series",
    "title": "MiCASA Land Carbon Flux",
    "section": "Visualize the Data as a Time Series",
    "text": "Visualize the Data as a Time Series\nWe can now explore the Heterotrophic Respiration time series (January 2003 -December 2017) available for the Dallas, Texas area. We can plot the data set using the code below:\n\n# Determine the width and height of the plot using the 'matplotlib' library\n# Figure size: 20 representing the width, 10 representing the height\nfig = plt.figure(figsize=(20, 10)) \n\n# Plot the time series analysis of the daily Heterotrophic Respiration changes in Dallas, Texas\nplt.plot(\n    df[\"date\"], # X-axis: date\n    df[\"max\"], # Y-axis: Rh value\n    color=\"purple\", # Line color\n    linestyle=\"-\", # Line style\n    linewidth=0.5, # Line width\n    label=\"RH Level\", # Legend label\n)\n\n# Display legend\nplt.legend()\n\n# Insert label for the X-axis\nplt.xlabel(\"Years\")\n\n# Insert label for the Y-axis\nplt.ylabel(\"kg Carbon/m2/day\")\n\n# Insert title for the plot\nplt.title(\"Heterotrophic Respiration Values for Dallas, Texas (January 2001 to March 2001)\")\n\nText(0.5, 1.0, 'Heterotrophic Respiration Values for Dallas, Texas (January 2001 to March 2001)')\n\n\n\n\n\n\n\n\n\nTo take a closer look at the daily Heterotrophic Respiration variability across this region, we are going to retrieve and display data collected during the October, 2017 observation.\n\n# The 2017-10 observation is the 3rd item in the list\n# Considering that a list starts with \"0\", we need to insert \"2\" in the \"items[2]\" statement\n# Print the start Date Time of the third granule in the collection\nprint(items[2][\"properties\"][\"datetime\"]) \n\n2001-03-06T00:00:00+00:00\n\n\n\n# A GET request is made for the October 2017 tile\noctober_tile = requests.get(\n\n    # Pass the collection name, the item number in the list, and its ID\n    f\"{RASTER_API_URL}/stac/tilejson.json?collection={items[2]['collection']}&item={items[2]['id']}\"\n\n    # Pass the asset name\n    f\"&assets={asset_name}\"\n\n    # Pass the color formula and colormap for custom visualization\n    f\"&color_formula=gamma+r+1.05&colormap_name={color_map}\"\n\n    # Pass the minimum and maximum values for rescaling\n    f\"&rescale={rescale_values['min']},{rescale_values['max']}\",\n\n# Return the response in JSON format\n).json()\n\n# Print the properties of the retrieved granule to the console \noctober_tile\n\n{'tilejson': '2.2.0',\n 'version': '1.0.0',\n 'scheme': 'xyz',\n 'tiles': ['https://dev.ghg.center/ghgcenter/api/raster/stac/tiles/WebMercatorQuad/{z}/{x}/{y}@1x?collection=micasa-carbonflux-daygrid-v1&item=micasa-carbonflux-daygrid-v1-20010306&assets=rh&color_formula=gamma+r+1.05&colormap_name=purd&rescale=-0.28565365076065063%2C5.658170223236084'],\n 'minzoom': 0,\n 'maxzoom': 24,\n 'bounds': [-180.0, -90.0, 179.99999999999994, 90.0],\n 'center': [-2.842170943040401e-14, 0.0, 0]}\n\n\n\n# Create a new map to display the Rh level for the Dallas, Texas area for the October, 2017 timeframe\naoi_map_bbox = Map(\n\n    # Base map is set to OpenStreetMap\n    tiles=\"OpenStreetMap\",\n\n    # Set the center of the map\n    location=[\n        32.8, # latitude\n        -96.79, # longitude\n    ],\n\n    # Set the zoom value\n    zoom_start=9,\n)\n\n# Define the map layer with the Rh level for October, 2017\nmap_layer = TileLayer(\n    tiles=october_tile[\"tiles\"][0], # Path to retrieve the tile\n\n    # Set the attribution, transparency, and the title along with enabling the visualization of the legend on the map \n    attr=\"GHG\", opacity = 0.7, name=\"October 2017 RH Level\", overlay= True, legendEnabled = True\n)\n\n# Add the layer to the map\nmap_layer.add_to(aoi_map_bbox)\n\n# Display data marker (title) on the map\nfolium.Marker((40, 5.9), tooltip=\"both\").add_to(aoi_map_bbox)\n\n# Add a layer control\nfolium.LayerControl(collapsed=False).add_to(aoi_map_bbox)\n\n# Add a legend using the 'branca' library\ncolormap = branca.colormap.linear.PuRd_09.scale(0, 0.3) # minimum value = 0, maximum value = 0.3 (kg Carbon/m2/daily)\n\n# Classify the colormap according to the specified Rh values\ncolormap = colormap.to_step(index=[0, 0.07, 0.15, 0.22, 0.3])\n\n# Add the data unit as caption\ncolormap.caption = 'Rh Values (kg Carbon/m2/daily)'\n\n# Display the legend and caption on the map\ncolormap.add_to(aoi_map_bbox)\n\n# Visualize the map\naoi_map_bbox\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook",
    "crumbs": [
      "Data Usage Notebooks",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "MiCASA Land Carbon Flux"
    ]
  },
  {
    "objectID": "user_data_notebooks/micasa-carbonflux-daygrid-v1_User_Notebook.html#summary",
    "href": "user_data_notebooks/micasa-carbonflux-daygrid-v1_User_Notebook.html#summary",
    "title": "MiCASA Land Carbon Flux",
    "section": "Summary",
    "text": "Summary\nIn this notebook we have successfully completed the following steps for the STAC collection for MiCASA Land Carbon Flux data: 1. Install and import the necessary libraries 2. Fetch the collection from STAC collections using the appropriate endpoints 3. Count the number of existing granules within the collection 4. Map and compare the Heterotrophic Respiration (Rh) levels over the Dallas, Texas area for two distinctive years 5. Create a table that displays the minimum, maximum, and sum of the Rh values for a specified region 6. Generate a time-series graph of the Rh values for a specified region\nIf you have any questions regarding this user notebook, please contact us using the feedback form.",
    "crumbs": [
      "Data Usage Notebooks",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "MiCASA Land Carbon Flux"
    ]
  },
  {
    "objectID": "user_data_notebooks/eccodarwin-co2flux-monthgrid-v5_User_Notebook.html",
    "href": "user_data_notebooks/eccodarwin-co2flux-monthgrid-v5_User_Notebook.html",
    "title": "Air-Sea CO₂ Flux, ECCO-Darwin Model v5",
    "section": "",
    "text": "Identify available dates and temporal frequency of observations for the given collection using the GHGC API /stac endpoint. The collection processed in this notebook is the Air-Sea CO₂ Flux, ECCO-Darwin Model v5 Data product.\nPass the STAC item into the raster API /stac/tilejson.json endpoint.\nUsing folium.plugins.DualMap, we will visualize two tiles (side-by-side), allowing us to compare time points.\nAfter the visualization, we will perform zonal statistics for a given polygon.",
    "crumbs": [
      "Data Usage Notebooks",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "Air-Sea CO₂ Flux, ECCO-Darwin Model v5"
    ]
  },
  {
    "objectID": "user_data_notebooks/eccodarwin-co2flux-monthgrid-v5_User_Notebook.html#approach",
    "href": "user_data_notebooks/eccodarwin-co2flux-monthgrid-v5_User_Notebook.html#approach",
    "title": "Air-Sea CO₂ Flux, ECCO-Darwin Model v5",
    "section": "",
    "text": "Identify available dates and temporal frequency of observations for the given collection using the GHGC API /stac endpoint. The collection processed in this notebook is the Air-Sea CO₂ Flux, ECCO-Darwin Model v5 Data product.\nPass the STAC item into the raster API /stac/tilejson.json endpoint.\nUsing folium.plugins.DualMap, we will visualize two tiles (side-by-side), allowing us to compare time points.\nAfter the visualization, we will perform zonal statistics for a given polygon.",
    "crumbs": [
      "Data Usage Notebooks",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "Air-Sea CO₂ Flux, ECCO-Darwin Model v5"
    ]
  },
  {
    "objectID": "user_data_notebooks/eccodarwin-co2flux-monthgrid-v5_User_Notebook.html#about-the-data",
    "href": "user_data_notebooks/eccodarwin-co2flux-monthgrid-v5_User_Notebook.html#about-the-data",
    "title": "Air-Sea CO₂ Flux, ECCO-Darwin Model v5",
    "section": "About the Data",
    "text": "About the Data\nThe ocean is a major sink for atmospheric carbon dioxide (CO2), largely due to the presence of phytoplankton that use the CO₂ to grow. Studies have shown that global ocean CO₂ uptake has increased over recent decades, however there is uncertainty in the various mechanisms that affect ocean CO₂ flux and storage and how the ocean carbon sink will respond to future climate change. Because CO₂ fluxes can vary significantly across space and time, combined with deficiencies in ocean and atmosphere CO₂ observations, there is a need for models that can thoroughly represent these processes. Ocean biogeochemical models (OBMs) have the ability to resolve the physical and biogeochemical mechanisms contributing to spatial and temporal variations in air-sea CO₂ fluxes but previous OBMs do not integrate observations to improve model accuracy and have not been able to operate on the seasonal and multi-decadal timescales needed to adequately characterize these processes. The ECCO-Darwin model is an OBM that assimilates Estimating the Circulation and Climate of the Ocean (ECCO) consortium ocean circulation estimates and biogeochemical processes from the Massachusetts Institute of Technology (MIT) Darwin Project. A pilot study using ECCO-Darwin was completed by Brix et al. (2015) however an improved version of the model was developed by Carroll et al. (2020) in which issues present in the first model were addressed using data assimilation and adjustments were made to initial conditions and biogeochemical parameters. The updated ECCO-Darwin model was compared with interpolation-based products to estimate surface ocean partial pressure (pCO2) and air-sea CO₂ flux. This dataset contains the gridded global, monthly mean air-sea CO₂ fluxes from version 5 of the ECCO-Darwin model. The data are available at ~1/3° horizontal resolution at the equator (~18 km at high latitudes) from January 2020 through December 2022.\nFor more information regarding this dataset, please visit the U.S. Greenhouse Gas Center.",
    "crumbs": [
      "Data Usage Notebooks",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "Air-Sea CO₂ Flux, ECCO-Darwin Model v5"
    ]
  },
  {
    "objectID": "user_data_notebooks/eccodarwin-co2flux-monthgrid-v5_User_Notebook.html#querying-the-stac-api",
    "href": "user_data_notebooks/eccodarwin-co2flux-monthgrid-v5_User_Notebook.html#querying-the-stac-api",
    "title": "Air-Sea CO₂ Flux, ECCO-Darwin Model v5",
    "section": "Querying the STAC API",
    "text": "Querying the STAC API\nFirst, we are going to import the required libraries. Once imported, they allow better executing a query in the GHG Center Spatio Temporal Asset Catalog (STAC) Application Programming Interface (API) where the granules for this collection are stored.\n\n# Import the following libraries\nimport requests\nimport folium\nimport folium.plugins\nfrom folium import Map, TileLayer \nfrom pystac_client import Client \nimport branca \nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n\n# Provide the STAC and RASTER API endpoints\n# The endpoint is referring to a location within the API that executes a request on a data collection nesting on the server.\n\n# The STAC API is a catalog of all the existing data collections that are stored in the GHG Center.\nSTAC_API_URL = \"http://ghg.center/api/stac\"\n\n# The RASTER API is used to fetch collections for visualization\nRASTER_API_URL = \"https://ghg.center/api/raster\"\n\n# The collection name is used to fetch the dataset from the STAC API. First, we define the collection name as a variable \n# Name of the collection for ECCO Darwin CO₂ flux monthly emissions\ncollection_name = \"eccodarwin-co2flux-monthgrid-v5\"\n\n\n# Fetch the collection from the STAC API using the appropriate endpoint\n# The 'requests' library allows a HTTP request possible\ncollection = requests.get(f\"{STAC_API_URL}/collections/{collection_name}\").json()\n\n# Print the properties of the collection to the console\ncollection\n\nExamining the contents of our collection under the temporal variable, we see that the data is available from January 2020 to December 2022. By looking at the dashboard:time density, we observe that the data is periodic with monthly time density.\n\n# Create a function that would search for a data collection in the US GHG Center STAC API\n\n# First, we need to define the function\n# The name of the function is \"get_item_count\" \n# The argument that will be passed to the defined function is \"collection_id\"\ndef get_item_count(collection_id):\n\n    # Set a counter for the number of items existing in the collection \n    count = 0 \n\n    # Define the path to retrieve the granules (items) of the collection of interest (Air-Sea CO2 Flux ECCO-Darwin model) in the STAC API\n    items_url = f\"{STAC_API_URL}/collections/{collection_id}/items\" \n\n    # Run a while loop to make HTTP requests until there are no more URLs associated with the collection (Air-Sea CO2 Flux ECCO-Darwin model) in the STAC API\n    while True:\n\n        # Retrieve information about the granules by sending a \"get\" request to the STAC API using the defined collection path \n        response = requests.get(items_url) \n\n        # If the items do not exist, print an error message and quit the loop\n        if not response.ok:\n            print(\"error getting items\")\n            exit()\n\n        # Return the results of the HTTP response as JSON\n        stac = response.json()\n\n        # Increase the \"count\" by the number of items (granules) returned in the response\n        count += int(stac[\"context\"].get(\"returned\", 0))\n\n        # Retrieve information about the next URL associated with the collection (Air-Sea CO2 Flux ECCO-Darwin model) in the STAC API (if applicable)\n        next = [link for link in stac[\"links\"] if link[\"rel\"] == \"next\"]\n\n        # Exit the loop if there are no other URLs\n        if not next:\n            break\n        \n        # Ensure the information gathered by other STAC API links associated with the collection are added to the original path\n        # \"href\" is the identifier for each of the tiles stored in the STAC API\n        items_url = next[0][\"href\"]\n\n    # Return the information about the total number of granules found associated with the collection (Air-Sea CO2 Flux ECCO-Darwin model)\n    return count\n\n\n# Apply the function created above \"get_item_count\" to the Air-Sea CO2 Flux ECCO-Darwin collection\nnumber_of_items = get_item_count(collection_name)\n\n# Get the information about the number of granules found in the collection\nitems = requests.get(f\"{STAC_API_URL}/collections/{collection_name}/items?limit={number_of_items}\").json()[\"features\"]\n\n# Print the total number of items (granules) found\nprint(f\"Found {len(items)} items\")\n\n\n# Examine the first item in the collection\n# Keep in mind that a list starts from 0, 1, 2... therefore items[0] is referring to the first item in the list/collection\nitems[0]",
    "crumbs": [
      "Data Usage Notebooks",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "Air-Sea CO₂ Flux, ECCO-Darwin Model v5"
    ]
  },
  {
    "objectID": "user_data_notebooks/eccodarwin-co2flux-monthgrid-v5_User_Notebook.html#exploring-changes-in-co₂-levels-using-the-raster-api",
    "href": "user_data_notebooks/eccodarwin-co2flux-monthgrid-v5_User_Notebook.html#exploring-changes-in-co₂-levels-using-the-raster-api",
    "title": "Air-Sea CO₂ Flux, ECCO-Darwin Model v5",
    "section": "Exploring Changes in CO₂ Levels Using the Raster API",
    "text": "Exploring Changes in CO₂ Levels Using the Raster API\nIn this notebook, we will explore the global changes of CO₂ flux over time in urban regions. We will visualize the outputs on a map using folium.\n\n# Now we create a dictionary where the start datetime values for each granule is queried more explicitly by year and month (e.g., 2020-02)\nitems = {item[\"properties\"][\"start_datetime\"]: item for item in items}\n\n# Next, we need to specify the asset name for this collection.\n# The asset name is referring to the raster band containing the pixel values for the parameter of interest.\n# For the case of the Air-Sea CO2 Flux ECCO-Darwin collection, the parameter of interest is “co2”.\nasset_name = \"co2\"\n\nBelow, we are entering the minimum and maximum values to provide our upper and lower bounds in the rescale_values.\n\n# Fetch the minimum and maximum values for the CO2 value range\nrescale_values = {\"max\":0.0007, \"min\":-0.0007}\n\nNow, we will pass the item id, collection name, asset name, and the rescaling factor to the Raster API endpoint. This step is done twice so that we can visualize two arbitrary events independently.\n\n# Choose a color map for displaying the first observation (event)\n# Please refer to matplotlib library if you'd prefer choosing a different color ramp.\n# For more information on Colormaps in Matplotlib, please visit https://matplotlib.org/stable/users/explain/colors/colormaps.html\ncolor_map = \"magma\"\n\n# Make a GET request to retrieve information for the December 2022 tile which is the 1st item in the collection\n# To retrieve the first item in the collection we use \"0\" in the \"(items.keys())[0]\" statement \n# If you want to select another item (granule) in the list (collection), you can refer to the Data Browser in the U.S. Greenhouse Gas Center website  \n# URL to the Air-Sea CO2 Flux ECCO-Darwin collection in the US GHG Center: https://dljsq618eotzp.cloudfront.net/browseui/#eccodarwin-co2flux-monthgrid-v5/\n\n# A GET request is made for the December 2022 tile\ndecember_2022_tile = requests.get(\n\n    # Pass the collection name, the item number in the list, and its ID\n    f\"{RASTER_API_URL}/stac/tilejson.json?collection={items[list(items.keys())[0]]['collection']}&item={items[list(items.keys())[0]]['id']}\"\n\n    # Pass the asset name\n    f\"&assets={asset_name}\"\n\n    # Pass the color formula and colormap for custom visualization\n    f\"&color_formula=gamma+r+1.05&colormap_name={color_map}\"\n\n    # Pass the minimum and maximum values for rescaling \n    f\"&rescale={rescale_values['min']},{rescale_values['max']}\", \n\n# Return the response in JSON format\n).json()\n\n# Print the properties of the retrieved granule to the console\ndecember_2022_tile\n\n\n# Make a GET request to retrieve information for the April 2021 tile which is the 21th item in the collection\n# To retrieve the 21st item in the collection we use \"20\" in the \"(items.keys())[20]\" statement \n# Keep in mind that a list starts from 0, therefore \"items[20]\" is referring to the 21st item in the list/collection\n\n# A GET request is made for the April 2021 tile\napril_2021_tile = requests.get(\n\n    # Pass the collection name, the item number in the list, and its ID\n    f\"{RASTER_API_URL}/stac/tilejson.json?collection={items[list(items.keys())[20]]['collection']}&item={items[list(items.keys())[20]]['id']}\"\n\n    # Pass the asset name\n    f\"&assets={asset_name}\"\n\n    # Pass the color formula and colormap for custom visualization\n    f\"&color_formula=gamma+r+1.05&colormap_name={color_map}\"\n\n    # Pass the minimum and maximum values for rescaling\n    f\"&rescale={rescale_values['min']},{rescale_values['max']}\", \n\n# Return the response in JSON format\n).json()\n\n# Print the properties of the retrieved granule to the console\napril_2021_tile",
    "crumbs": [
      "Data Usage Notebooks",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "Air-Sea CO₂ Flux, ECCO-Darwin Model v5"
    ]
  },
  {
    "objectID": "user_data_notebooks/eccodarwin-co2flux-monthgrid-v5_User_Notebook.html#visualizing-co₂-flux-emissions",
    "href": "user_data_notebooks/eccodarwin-co2flux-monthgrid-v5_User_Notebook.html#visualizing-co₂-flux-emissions",
    "title": "Air-Sea CO₂ Flux, ECCO-Darwin Model v5",
    "section": "Visualizing CO₂ flux Emissions",
    "text": "Visualizing CO₂ flux Emissions\n\n# For this study we are going to compare the CO2 level in 2021 and 2022 along the coast of California\n# To change the location, you can simply insert the latitude and longitude of the area of your interest in the \"location=(LAT, LONG)\" statement\n\n# Set the initial zoom level and center of map for both tiles\n# 'folium.plugins' allows mapping side-by-side\nmap_ = folium.plugins.DualMap(location=(34, -118), zoom_start=6)\n\n\n# Define the first map layer with the CO2 Flux data for December 2022\nmap_layer_1 = TileLayer(\n    tiles=december_2022_tile[\"tiles\"][0], # Path to retrieve the tile\n    attr=\"GHG\", # Set the attribution \n    name='December 2022 CO2 Flux', # Title for the layer\n    overlay=True, # The layer can be overlaid on the map\n    opacity=0.8, # Adjust the transparency of the layer\n)\n# Add the first layer to the Dual Map \nmap_layer_1.add_to(map_.m1)\n\n\n# Define the second map layer with the CO2 Flux data for April 2021\nmap_layer_2 = TileLayer(\n    tiles=april_2021_tile[\"tiles\"][0], # Path to retrieve the tile\n    attr=\"GHG\", # Set the attribution \n    name='April 2021 CO2 Flux', # Title for the layer\n    overlay=True, # The layer can be overlaid on the map\n    opacity=0.8, # Adjust the transparency of the layer\n)\n# Add the second layer to the Dual Map \nmap_layer_2.add_to(map_.m2)\n\n\n# Display data markers (titles) on both maps\nfolium.Marker((40, 5.0), tooltip=\"both\").add_to(map_)\n\n# Add a layer control to switch between map layers\nfolium.LayerControl(collapsed=False).add_to(map_)\n\n# Add a legend to the dual map using the 'branca' library\n# Note: the inserted legend is representing the minimum and maximum values for both tiles\n# Minimum value = -0.0007, maximum value = 0.0007\ncolormap = branca.colormap.LinearColormap(colors=[\"#0000FF\", \"#3399FF\", \"#66CCFF\", \"#FFFFFF\", \"#FF66CC\", \"#FF3399\", \"#FF0000\"], vmin=-0.0007, vmax=0.0007) \n\n# Add the data unit as caption \ncolormap.caption = 'Millimoles per meter squared per second (mmol m²/s)'\n\n# Define custom tick values for the legend bar\ntick_val = [-0.0007, -0.00035, 0, 0.00035, 0.0007]\n\n# Create a HTML representation\nlegend_html = colormap._repr_html_()\n\n# Create a customized HTML structure for the legend\nlegend_html = f'''\n&lt;div style=\"position: fixed; bottom: 50px; left: 50px; z-index: 1000; width: 400px; height: auto; background-color: rgba(255, 255, 255, 0.8);\n             border-radius: 5px; border: 1px solid grey; padding: 10px; font-size: 14px; color: black;\"&gt;\n    &lt;b&gt;{colormap.caption}&lt;/b&gt;&lt;br&gt;\n    &lt;div style=\"display: flex; justify-content: space-between;\"&gt;\n        &lt;div&gt;{tick_val[0]}&lt;/div&gt; \n        &lt;div&gt;{tick_val[1]}&lt;/div&gt; \n        &lt;div&gt;{tick_val[2]}&lt;/div&gt; \n        &lt;div&gt;{tick_val[3]}&lt;/div&gt; \n        &lt;div&gt;{tick_val[4]}&lt;/div&gt; \n    &lt;/div&gt;\n    &lt;div style=\"background: linear-gradient(to right,\n                {'#0000FF'}, {'#3399FF'} {20}%,\n                {'#3399FF'} {20}%, {'#66CCFF'} {40}%,\n                {'#66CCFF'} {40}%, {'#FFFFFF'} {50}%,\n                {'#FFFFFF'} {50}%, {'#FF66CC'} {80}%,\n                {'#FF66CC'} {80}%, {'#FF3399'}); height: 10px;\"&gt;&lt;/div&gt;\n&lt;/div&gt;\n'''\n\n# Display the legend and caption on the map\nmap_.get_root().html.add_child(folium.Element(legend_html))\n\n# Visualize the Dual Map\nmap_",
    "crumbs": [
      "Data Usage Notebooks",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "Air-Sea CO₂ Flux, ECCO-Darwin Model v5"
    ]
  },
  {
    "objectID": "user_data_notebooks/eccodarwin-co2flux-monthgrid-v5_User_Notebook.html#visualizing-the-data-as-a-time-series",
    "href": "user_data_notebooks/eccodarwin-co2flux-monthgrid-v5_User_Notebook.html#visualizing-the-data-as-a-time-series",
    "title": "Air-Sea CO₂ Flux, ECCO-Darwin Model v5",
    "section": "Visualizing the Data as a Time Series",
    "text": "Visualizing the Data as a Time Series\nWe can now explore the fossil fuel emission time series (January 2020 -December 2022) available for the Coastal California area of the U.S. We can plot the data set using the code below:\n\n# Sort the DataFrame by the datetime column so the plot is displaying the values from left to right (2020 -&gt; 2022)\ndf_sorted = df.sort_values(by=\"datetime\")\n\n# Plot the timeseries analysis of the monthly air-sea CO₂ flux changes along the coast of California\n# Figure size: 20 representing the width, 10 representing the height\nfig = plt.figure(figsize=(20, 10))\n\nplt.plot(\n    df_sorted[\"datetime\"],    # X-axis: sorted datetime\n    df_sorted[\"max\"],         # Y-axis: maximum CO₂ value\n    color=\"purple\",           # Line color\n    linestyle=\"-\",            # Line style\n    linewidth=1,              # Line width\n    label=\"CO2 Emissions\",    # Legend label\n)\n\n# Display legend\nplt.legend()\n\n# Insert label for the X-axis\nplt.xlabel(\"Years\")\n\n# Insert label for the Y-axis\nplt.ylabel(\"CO2 Emissions mmol m²/s\")\n\n# Insert title for the plot\nplt.title(\"CO2 Emission Values for Coastal California (2020-2022)\")\n\n# Rotate x-axis labels to avoid cramping\nplt.xticks(rotation=90)\n\n# Add data citation\nplt.text(\n    df_sorted[\"datetime\"].iloc[0],           # X-coordinate of the text (first datetime value)\n    df_sorted[\"max\"].min(),                  # Y-coordinate of the text (minimum CO2 value)\n\n    # Text to be displayed\n    \"Source: NASA Air-Sea CO₂ Flux, ECCO-Darwin Model v5\",                   \n    fontsize=12,                             # Font size\n    horizontalalignment=\"left\",              # Horizontal alignment\n    verticalalignment=\"bottom\",              # Vertical alignment\n    color=\"blue\",                            # Text color\n)\n\n# Plot the time series\nplt.show()\n\nLooking at the plot above, we notice that CO₂ emission level increases particularly around 2022-09-01 for the defined area of interest. To take a closer look at monthly CO₂ flux variability across this region, we are going to retrieve and display data collected during the September 2022 observation.\n\n# The 2022-09-01 observation is the 4th item in the list. \n# Considering that a list starts with \"0\", we need to insert \"3\" in the \"items[3]\" statement\nprint(items[3][\"properties\"][\"start_datetime\"])\n\n\n# A GET request is made for the September 2022 tile\nSeptember2022_co2_flux = requests.get(\n\n    # Pass the collection name, the item number in the list, and its ID\n    f\"{RASTER_API_URL}/stac/tilejson.json?collection={items[3]['collection']}&item={items[3]['id']}\"\n\n    # Pass the asset name\n    f\"&assets={asset_name}\"\n\n    # Pass the color formula and colormap for custom visualization\n    f\"&color_formula=gamma+r+1.05&colormap_name={color_map}\"\n\n    # Pass the minimum and maximum values for rescaling\n    f\"&rescale={rescale_values['min']},{rescale_values['max']}\",\n\n# Return the response in JSON format\n).json()\n\n# Print the properties of the retrieved granule to the console  \nSeptember2022_co2_flux\n\n\n# Create a new map to display the September 2022 tile\naoi_map_bbox = Map(\n\n    # Base map is set to OpenStreetMap\n    tiles=\"OpenStreetMap\",\n\n    # Set the center of the map\n    location=[\n        34, -120\n    ],\n\n    # Set the zoom value\n    zoom_start=5.5,\n)\n\n# Define the map layer with the CO2 flux data for September 2022\nmap_layer = TileLayer(\n    tiles=September2022_co2_flux[\"tiles\"][0], # Path to retrieve the tile\n    attr=\"GHG\", # Set the attribution\n    opacity = 0.7, # Adjust the transparency of the layer\n)\n\n# Add the layer to the map\nmap_layer.add_to(aoi_map_bbox)\n\n# Add a legend to the map\n# Minimum value = -0.0007, maximum value = 0.0007\ncolormap = branca.colormap.LinearColormap(colors=[\"#0000FF\", \"#3399FF\", \"#66CCFF\", \"#FFFFFF\", \"#FF66CC\", \"#FF3399\", \"#FF0000\"], vmin=-0.0007, vmax=0.0007) \n\n# Add the data unit as caption \ncolormap.caption = 'Millimoles per meter squared per second (mmol m²/s)'\n\n# Define custom tick values for the legend bar\ntick_val = [-0.0007, -0.00035, 0, 0.00035, 0.0007]\n\n# Create a HTML representation\nlegend_html = colormap._repr_html_()\n\n# Create a customized HTML structure for the legend\nlegend_html = f'''\n&lt;div style=\"position: fixed; bottom: 50px; left: 50px; z-index: 1000; width: 400px; height: auto; background-color: rgba(255, 255, 255, 0.8);\n             border-radius: 5px; border: 1px solid grey; padding: 10px; font-size: 14px; color: black;\"&gt;\n    &lt;b&gt;{colormap.caption}&lt;/b&gt;&lt;br&gt;\n    &lt;div style=\"display: flex; justify-content: space-between;\"&gt;\n        &lt;div&gt;{tick_val[0]}&lt;/div&gt; \n        &lt;div&gt;{tick_val[1]}&lt;/div&gt; \n        &lt;div&gt;{tick_val[2]}&lt;/div&gt; \n        &lt;div&gt;{tick_val[3]}&lt;/div&gt; \n        &lt;div&gt;{tick_val[4]}&lt;/div&gt; \n    &lt;/div&gt;\n    &lt;div style=\"background: linear-gradient(to right,\n                {'#0000FF'}, {'#3399FF'} {20}%,\n                {'#3399FF'} {20}%, {'#66CCFF'} {40}%,\n                {'#66CCFF'} {40}%, {'#FFFFFF'} {50}%,\n                {'#FFFFFF'} {50}%, {'#FF66CC'} {80}%,\n                {'#FF66CC'} {80}%, {'#FF3399'}); height: 10px;\"&gt;&lt;/div&gt;\n&lt;/div&gt;\n'''\n\n# Display the legend and caption on the map\naoi_map_bbox.get_root().html.add_child(folium.Element(legend_html))\n\n# Add the title to the map\ntitle_html = '''\n&lt;div style=\"position: fixed; top: 10px; right: 10px; z-index: 1000; background-color: rgba(255, 255, 255, 0.8); border-radius: 5px; border: 1px solid grey; padding: 10px;\"&gt;\n    &lt;b&gt;Air-Sea CO₂ Flux, ECCO-Darwin&lt;/b&gt;&lt;br&gt;\n    September 2022\n&lt;/div&gt;\n'''\n# Display the title on the map\naoi_map_bbox.get_root().html.add_child(folium.Element(title_html))\n\n# Visualize the map\naoi_map_bbox",
    "crumbs": [
      "Data Usage Notebooks",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "Air-Sea CO₂ Flux, ECCO-Darwin Model v5"
    ]
  },
  {
    "objectID": "user_data_notebooks/eccodarwin-co2flux-monthgrid-v5_User_Notebook.html#summary",
    "href": "user_data_notebooks/eccodarwin-co2flux-monthgrid-v5_User_Notebook.html#summary",
    "title": "Air-Sea CO₂ Flux, ECCO-Darwin Model v5",
    "section": "Summary",
    "text": "Summary\nIn this notebook we have successfully completed the following steps for the STAC collection for the NASA Air-Sea CO₂ Flux ECCO Darwin dataset: 1. Install and import the necessary libraries 2. Fetch the collection from STAC collections using the appropriate endpoints 3. Count the number of existing granules within the collection 4. Map and compare the CO₂ Flux levels over the Coastal California area for two distinctive months/years 5. Create a table that displays the minimum, maximum, and sum of the CO₂ Flux values for a specified region 6. Generate a time-series graph of the CO₂ Flux values for a specified region\nIf you have any questions regarding this user notebook, please contact us using the feedback form.",
    "crumbs": [
      "Data Usage Notebooks",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "Air-Sea CO₂ Flux, ECCO-Darwin Model v5"
    ]
  },
  {
    "objectID": "user_data_notebooks/oco2-mip-co2budget-yeargrid-v1_User_Notebook.html",
    "href": "user_data_notebooks/oco2-mip-co2budget-yeargrid-v1_User_Notebook.html",
    "title": "OCO-2 MIP Top-Down CO₂ Budgets",
    "section": "",
    "text": "Identify available dates and temporal frequency of observations for the given collection using the GHGC API /stac endpoint. The collection processed in this notebook is the OCO-2 MIP Top-Down CO₂ Budgets data product.\nPass the STAC item into the raster API /stac/tilejson.jsonendpoint.\nUsing folium.plugins.DualMap, we will visualize two tiles (side-by-side), allowing us to compare time points.\nAfter the visualization, we will perform zonal statistics for a given polygon.",
    "crumbs": [
      "Data Usage Notebooks",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "OCO-2 MIP Top-Down CO₂ Budgets"
    ]
  },
  {
    "objectID": "user_data_notebooks/oco2-mip-co2budget-yeargrid-v1_User_Notebook.html#approach",
    "href": "user_data_notebooks/oco2-mip-co2budget-yeargrid-v1_User_Notebook.html#approach",
    "title": "OCO-2 MIP Top-Down CO₂ Budgets",
    "section": "",
    "text": "Identify available dates and temporal frequency of observations for the given collection using the GHGC API /stac endpoint. The collection processed in this notebook is the OCO-2 MIP Top-Down CO₂ Budgets data product.\nPass the STAC item into the raster API /stac/tilejson.jsonendpoint.\nUsing folium.plugins.DualMap, we will visualize two tiles (side-by-side), allowing us to compare time points.\nAfter the visualization, we will perform zonal statistics for a given polygon.",
    "crumbs": [
      "Data Usage Notebooks",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "OCO-2 MIP Top-Down CO₂ Budgets"
    ]
  },
  {
    "objectID": "user_data_notebooks/oco2-mip-co2budget-yeargrid-v1_User_Notebook.html#about-the-data",
    "href": "user_data_notebooks/oco2-mip-co2budget-yeargrid-v1_User_Notebook.html#about-the-data",
    "title": "OCO-2 MIP Top-Down CO₂ Budgets",
    "section": "About the Data",
    "text": "About the Data\nThe Committee on Earth Observation Satellites (CEOS) Atmospheric Composition - Virtual Constellation (AC-VC) Greenhouse Gas (GHG) team has generated the CEOS CO₂ Budgets dataset, which provides annual top-down carbon dioxide (CO2) emissions and removals from 2015 - 2020 gridded globally at 1° resolution, and as national totals. Data is provided in units of grams of carbon dioxide per square meter per year (g CO2/m2/yr). Only a subset of the full dataset is displayed in the GHG Center explore view.",
    "crumbs": [
      "Data Usage Notebooks",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "OCO-2 MIP Top-Down CO₂ Budgets"
    ]
  },
  {
    "objectID": "user_data_notebooks/oco2-mip-co2budget-yeargrid-v1_User_Notebook.html#installing-the-required-libraries",
    "href": "user_data_notebooks/oco2-mip-co2budget-yeargrid-v1_User_Notebook.html#installing-the-required-libraries",
    "title": "OCO-2 MIP Top-Down CO₂ Budgets",
    "section": "Installing the required libraries",
    "text": "Installing the required libraries\nPlease run the cell below to install the libraries required to run this notebook.\n\n%pip install requests --quiet\n%pip install folium --quiet\n%pip install rasterstats --quiet\n%pip install pystac_client --quiet\n%pip install pandas --quiet\n%pip install matplotlib --quiet\n\nNote: you may need to restart the kernel to use updated packages.\nNote: you may need to restart the kernel to use updated packages.\nNote: you may need to restart the kernel to use updated packages.\nNote: you may need to restart the kernel to use updated packages.\nNote: you may need to restart the kernel to use updated packages.\nNote: you may need to restart the kernel to use updated packages.",
    "crumbs": [
      "Data Usage Notebooks",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "OCO-2 MIP Top-Down CO₂ Budgets"
    ]
  },
  {
    "objectID": "user_data_notebooks/oco2-mip-co2budget-yeargrid-v1_User_Notebook.html#querying-the-stac-api",
    "href": "user_data_notebooks/oco2-mip-co2budget-yeargrid-v1_User_Notebook.html#querying-the-stac-api",
    "title": "OCO-2 MIP Top-Down CO₂ Budgets",
    "section": "Querying the STAC API",
    "text": "Querying the STAC API\n\nimport requests\nfrom folium import Map, TileLayer\nfrom pystac_client import Client\n\n\n# Provide STAC and RASTER API endpoints\nSTAC_API_URL = \"http://ghg.center/api/stac\"\nRASTER_API_URL = \"https://ghg.center/api/raster\"\n\n# Please use the collection name similar to the one used in STAC collection.\n# Name of the collection for CEOS National Top-Down CO₂ Budgets dataset. \ncollection_name = \"oco2-mip-co2budget-yeargrid-v1\"\n\n\n# Fetching the collection from STAC collections using appropriate endpoint.\ncollection = requests.get(f\"{STAC_API_URL}/collections/{collection_name}\").json()\n#collection\n\nExamining the contents of our collection under the temporal variable, we see that the data is available from January 2015 to December 2020. By looking at the dashboard:time density, we observe that the periodic frequency of these observations is yearly.\n\ndef get_item_count(collection_id):\n    count = 0\n    items_url = f\"{STAC_API_URL}/collections/{collection_id}/items\"\n\n    while True:\n        response = requests.get(items_url)\n\n        if not response.ok:\n            print(\"error getting items\")\n            exit()\n\n        stac = response.json()\n        count += int(stac[\"context\"].get(\"returned\", 0))\n        next = [link for link in stac[\"links\"] if link[\"rel\"] == \"next\"]\n\n        if not next:\n            break\n        items_url = next[0][\"href\"]\n\n    return count\n\n\n# Check total number of items available\nnumber_of_items = get_item_count(collection_name)\nitems = requests.get(f\"{STAC_API_URL}/collections/{collection_name}/items?limit={number_of_items}\").json()[\"features\"]\nprint(f\"Found {len(items)} items\")\n\nFound 6 items\n\n\n\n# Examining the first item in the collection\n#items[0]\n\nBelow, we are entering the minimum and maximum values to provide our upper and lower bounds in rescale_values.",
    "crumbs": [
      "Data Usage Notebooks",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "OCO-2 MIP Top-Down CO₂ Budgets"
    ]
  },
  {
    "objectID": "user_data_notebooks/oco2-mip-co2budget-yeargrid-v1_User_Notebook.html#exploring-changes-in-co₂-levels-using-the-raster-api",
    "href": "user_data_notebooks/oco2-mip-co2budget-yeargrid-v1_User_Notebook.html#exploring-changes-in-co₂-levels-using-the-raster-api",
    "title": "OCO-2 MIP Top-Down CO₂ Budgets",
    "section": "Exploring Changes in CO₂ Levels Using the Raster API",
    "text": "Exploring Changes in CO₂ Levels Using the Raster API\nIn this notebook, we will explore the global changes of CO₂ budgets over time in urban regions. We will visualize the outputs on a map using folium.\n\n# to access the year value from each item more easily, this will let us query more explicity by year and month (e.g., 2020-02)\nitems = {item[\"properties\"][\"start_datetime\"]: item for item in items} \nasset_name = \"ff\" #fossil fuel\n\n\n# Fetching the min and max values for a specific item\nrescale_values = {\"max\":items[list(items.keys())[0]][\"assets\"][asset_name][\"raster:bands\"][0][\"histogram\"][\"max\"], \"min\":items[list(items.keys())[0]][\"assets\"][asset_name][\"raster:bands\"][0][\"histogram\"][\"min\"]}\n#Hardcoding the min and max values to match the scale in the GHG Center dashboard\nrescale_values = {\"max\": 450, \"min\": 0}\n\nNow, we will pass the item id, collection name, and rescaling_factor to the Raster API endpoint. We will do this twice, once for 2020 and again for 2019, so that we can visualize each event independently.\n\ncolor_map = \"purd\"\nco2_flux_1 = requests.get(\n    f\"{RASTER_API_URL}/stac/tilejson.json?collection={items[list(items.keys())[0]]['collection']}&item={items[list(items.keys())[0]]['id']}\"\n    f\"&assets={asset_name}\"\n    f\"&color_formula=gamma+r+1.05&colormap_name={color_map}\"\n    f\"&rescale={rescale_values['min']},{rescale_values['max']}\", \n).json()\nco2_flux_1\n\n{'tilejson': '2.2.0',\n 'version': '1.0.0',\n 'scheme': 'xyz',\n 'tiles': ['https://ghg.center/api/raster/stac/tiles/WebMercatorQuad/{z}/{x}/{y}@1x?collection=oco2-mip-co2budget-yeargrid-v1&item=oco2-mip-co2budget-yeargrid-v1-2020&assets=ff&color_formula=gamma+r+1.05&colormap_name=purd&rescale=0%2C450'],\n 'minzoom': 0,\n 'maxzoom': 24,\n 'bounds': [-180.0, -90.0, 180.0, 90.0],\n 'center': [0.0, 0.0, 0]}\n\n\n\nco2_flux_2 = requests.get(\n    f\"{RASTER_API_URL}/stac/tilejson.json?collection={items[list(items.keys())[1]]['collection']}&item={items[list(items.keys())[1]]['id']}\"\n    f\"&assets={asset_name}\"\n    f\"&color_formula=gamma+r+1.05&colormap_name={color_map}\"\n    f\"&rescale={rescale_values['min']},{rescale_values['max']}\", \n).json()\nco2_flux_2\n\n{'tilejson': '2.2.0',\n 'version': '1.0.0',\n 'scheme': 'xyz',\n 'tiles': ['https://ghg.center/api/raster/stac/tiles/WebMercatorQuad/{z}/{x}/{y}@1x?collection=oco2-mip-co2budget-yeargrid-v1&item=oco2-mip-co2budget-yeargrid-v1-2019&assets=ff&color_formula=gamma+r+1.05&colormap_name=purd&rescale=0%2C450'],\n 'minzoom': 0,\n 'maxzoom': 24,\n 'bounds': [-180.0, -90.0, 180.0, 90.0],\n 'center': [0.0, 0.0, 0]}",
    "crumbs": [
      "Data Usage Notebooks",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "OCO-2 MIP Top-Down CO₂ Budgets"
    ]
  },
  {
    "objectID": "user_data_notebooks/oco2-mip-co2budget-yeargrid-v1_User_Notebook.html#visualizing-co₂-emissions",
    "href": "user_data_notebooks/oco2-mip-co2budget-yeargrid-v1_User_Notebook.html#visualizing-co₂-emissions",
    "title": "OCO-2 MIP Top-Down CO₂ Budgets",
    "section": "Visualizing CO₂ Emissions",
    "text": "Visualizing CO₂ Emissions\n\n# We'll import folium to map and folium.plugins to allow mapping side-by-side\nimport folium\nimport folium.plugins\n\n# Set initial zoom and center of map for CO₂ Layer\n# Centre of map [latitude,longitude]\nmap_ = folium.plugins.DualMap(location=(34, -118), zoom_start=6)\n\n\nmap_layer_2020 = TileLayer(\n    tiles=co2_flux_1[\"tiles\"][0],\n    attr=\"GHG\",\n    opacity=0.5,\n)\nmap_layer_2020.add_to(map_.m1)\n\nmap_layer_2019 = TileLayer(\n    tiles=co2_flux_2[\"tiles\"][0],\n    attr=\"GHG\",\n    opacity=0.5,\n)\nmap_layer_2019.add_to(map_.m2)\n\n# visualising the map\nmap_\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook",
    "crumbs": [
      "Data Usage Notebooks",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "OCO-2 MIP Top-Down CO₂ Budgets"
    ]
  },
  {
    "objectID": "user_data_notebooks/oco2-mip-co2budget-yeargrid-v1_User_Notebook.html#visualizing-the-data-as-a-time-series",
    "href": "user_data_notebooks/oco2-mip-co2budget-yeargrid-v1_User_Notebook.html#visualizing-the-data-as-a-time-series",
    "title": "OCO-2 MIP Top-Down CO₂ Budgets",
    "section": "Visualizing the Data as a Time Series",
    "text": "Visualizing the Data as a Time Series\nWe can now explore the fossil fuel emission time series (January 2015 -December 2020) available for the Dallas, Texas area of the U.S. We can plot the data set using the code below:\n\nimport matplotlib.pyplot as plt\n\nfig = plt.figure(figsize=(20, 10))\n\n\nplt.plot(\n    df[\"datetime\"],\n    df[\"max\"],\n    color=\"red\",\n    linestyle=\"-\",\n    linewidth=0.5,\n    label=\"CO2 emissions\",\n)\n\nplt.legend()\nplt.xlabel(\"Years\")\nplt.ylabel(\"CO2 emissions gC/m2/year1\")\nplt.title(\"CO2 emission Values for Texas, Dallas (2015-2020)\")\n\nText(0.5, 1.0, 'CO2 emission Values for Texas, Dallas (2015-2020)')\n\n\n\n\n\n\n\n\n\n\nprint(items[2][\"properties\"][\"start_datetime\"])\n\n2018-01-01T00:00:00+00:00\n\n\n\nco2_flux_3 = requests.get(\n    f\"{RASTER_API_URL}/stac/tilejson.json?collection={items[2]['collection']}&item={items[2]['id']}\"\n    f\"&assets={asset_name}\"\n    f\"&color_formula=gamma+r+1.05&colormap_name={color_map}\"\n    f\"&rescale={rescale_values['min']},{rescale_values['max']}\",\n).json()\nco2_flux_3\n\n{'tilejson': '2.2.0',\n 'version': '1.0.0',\n 'scheme': 'xyz',\n 'tiles': ['https://ghg.center/api/raster/stac/tiles/WebMercatorQuad/{z}/{x}/{y}@1x?collection=oco2-mip-co2budget-yeargrid-v1&item=oco2-mip-co2budget-yeargrid-v1-2018&assets=ff&color_formula=gamma+r+1.05&colormap_name=purd&rescale=0%2C450'],\n 'minzoom': 0,\n 'maxzoom': 24,\n 'bounds': [-180.0, -90.0, 180.0, 90.0],\n 'center': [0.0, 0.0, 0]}\n\n\n\n# Use bbox initial zoom and map\n# Set up a map located w/in event bounds\nimport folium\n\naoi_map_bbox = Map(\n    tiles=\"OpenStreetMap\",\n    location=[\n        30,-100\n    ],\n    zoom_start=6.8,\n)\n\nmap_layer = TileLayer(\n    tiles=co2_flux_3[\"tiles\"][0],\n    attr=\"GHG\", opacity = 0.7\n)\n\nmap_layer.add_to(aoi_map_bbox)\n\naoi_map_bbox\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook",
    "crumbs": [
      "Data Usage Notebooks",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "OCO-2 MIP Top-Down CO₂ Budgets"
    ]
  },
  {
    "objectID": "user_data_notebooks/oco2-mip-co2budget-yeargrid-v1_User_Notebook.html#summary",
    "href": "user_data_notebooks/oco2-mip-co2budget-yeargrid-v1_User_Notebook.html#summary",
    "title": "OCO-2 MIP Top-Down CO₂ Budgets",
    "section": "Summary",
    "text": "Summary\nIn this notebook we have successfully explored, analyzed, and visualized the STAC collection for OCO-2 MIP Top-Down CO₂ Budgets.",
    "crumbs": [
      "Data Usage Notebooks",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "OCO-2 MIP Top-Down CO₂ Budgets"
    ]
  },
  {
    "objectID": "user_data_notebooks/casagfed-carbonflux-monthgrid-v3_User_Notebook.html",
    "href": "user_data_notebooks/casagfed-carbonflux-monthgrid-v3_User_Notebook.html",
    "title": "CASA-GFED3 Land Carbon Flux",
    "section": "",
    "text": "Identify available dates and temporal frequency of observations for a given collection using the GHGC API /stac endpoint. The collection processed in this notebook is the Land-Atmosphere Carbon Flux data product.\nPass the STAC item into the raster API /stac/tilejson.jsonendpoint.\nUsing folium.plugins.DualMap, visualize two tiles (side-by-side), allowing time point comparison.\nAfter the visualization, perform zonal statistics for a given polygon."
  },
  {
    "objectID": "user_data_notebooks/casagfed-carbonflux-monthgrid-v3_User_Notebook.html#approach",
    "href": "user_data_notebooks/casagfed-carbonflux-monthgrid-v3_User_Notebook.html#approach",
    "title": "CASA-GFED3 Land Carbon Flux",
    "section": "",
    "text": "Identify available dates and temporal frequency of observations for a given collection using the GHGC API /stac endpoint. The collection processed in this notebook is the Land-Atmosphere Carbon Flux data product.\nPass the STAC item into the raster API /stac/tilejson.jsonendpoint.\nUsing folium.plugins.DualMap, visualize two tiles (side-by-side), allowing time point comparison.\nAfter the visualization, perform zonal statistics for a given polygon."
  },
  {
    "objectID": "user_data_notebooks/casagfed-carbonflux-monthgrid-v3_User_Notebook.html#about-the-data",
    "href": "user_data_notebooks/casagfed-carbonflux-monthgrid-v3_User_Notebook.html#about-the-data",
    "title": "CASA-GFED3 Land Carbon Flux",
    "section": "About the Data",
    "text": "About the Data\nThis dataset presents a variety of carbon flux parameters derived from the Carnegie-Ames-Stanford-Approach – Global Fire Emissions Database version 3 (CASA-GFED3) model. The model’s input data includes air temperature, precipitation, incident solar radiation, a soil classification map, and a number of satellite derived products. All model calculations are driven by analyzed meteorological data from NASA’s Modern-Era Retrospective analysis for Research and Application, Version 2 (MERRA-2). The resulting product provides monthly, global data at 0.5 degree resolution from January 2003 through December 2017. It includes the following carbon flux variables expressed in units of kilograms of carbon per square meter per month (kg Carbon m²/mon) from the following sources: net primary production (NPP), net ecosystem exchange (NEE), heterotrophic respiration (Rh), wildfire emissions (FIRE), and fuel wood burning emissions (FUEL). This product and earlier versions of MERRA-driven CASA-GFED carbon fluxes have been used in a number of atmospheric CO₂ transport studies, and through the support of NASA’s Carbon Monitoring System (CMS), it helps characterize, quantify, understand and predict the evolution of global carbon sources and sinks."
  },
  {
    "objectID": "user_data_notebooks/casagfed-carbonflux-monthgrid-v3_User_Notebook.html#querying-the-stac-api",
    "href": "user_data_notebooks/casagfed-carbonflux-monthgrid-v3_User_Notebook.html#querying-the-stac-api",
    "title": "CASA-GFED3 Land Carbon Flux",
    "section": "Querying the STAC API",
    "text": "Querying the STAC API\nPlease run the next cell to import the required libraries.\n\nimport requests\nimport folium\nimport folium.plugins\nfrom folium import Map, TileLayer \nfrom pystac_client import Client \nimport branca \nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n\n# Provide STAC and RASTER API endpoints\nSTAC_API_URL = \"http://ghg.center/api/stac\"\nRASTER_API_URL = \"https://ghg.center/api/raster\"\n\n# Please use the collection name similar to the one used in the STAC collection.\n# Name of the collection for CASA GFED Land-Atmosphere Carbon Flux monthly emissions. \ncollection_name = \"casagfed-carbonflux-monthgrid-v3\"\n\n\n# Fetch the collection from STAC collections using the appropriate endpoint\n# the 'requests' library allows a HTTP request possible\ncollection = requests.get(f\"{STAC_API_URL}/collections/{collection_name}\").json()\ncollection\n\nExamining the contents of our collection under the temporal variable, we see that the data is available from January 2003 to December 2017. By looking at the dashboard:time density, we observe that the periodic frequency of these observations is monthly.\n\n# Create a function that would search for the above data collection in the STAC API\ndef get_item_count(collection_id):\n    count = 0\n    items_url = f\"{STAC_API_URL}/collections/{collection_id}/items\"\n\n    while True:\n        response = requests.get(items_url)\n\n        if not response.ok:\n            print(\"error getting items\")\n            exit()\n\n        stac = response.json()\n        count += int(stac[\"context\"].get(\"returned\", 0))\n        next = [link for link in stac[\"links\"] if link[\"rel\"] == \"next\"]\n\n        if not next:\n            break\n        items_url = next[0][\"href\"]\n\n    return count\n\n\n# Apply the above function and check the total number of items available within the collection\nnumber_of_items = get_item_count(collection_name)\nitems = requests.get(f\"{STAC_API_URL}/collections/{collection_name}/items?limit={number_of_items}\").json()[\"features\"]\nprint(f\"Found {len(items)} items\")\n\n\n# Examine the first item in the collection\nitems[0]"
  },
  {
    "objectID": "user_data_notebooks/casagfed-carbonflux-monthgrid-v3_User_Notebook.html#exploring-changes-in-carbon-flux-levels-using-the-raster-api",
    "href": "user_data_notebooks/casagfed-carbonflux-monthgrid-v3_User_Notebook.html#exploring-changes-in-carbon-flux-levels-using-the-raster-api",
    "title": "CASA-GFED3 Land Carbon Flux",
    "section": "Exploring Changes in Carbon Flux Levels Using the Raster API",
    "text": "Exploring Changes in Carbon Flux Levels Using the Raster API\nWe will explore changes in the land atmosphere Carbon flux Heterotrophic Respiration and examine their impacts over time. We’ll then visualize the outputs on a map using folium.\n\n# To access the year value from each item more easily, this will let us query more explicitly by year and month (e.g., 2020-02)\nitems = {item[\"properties\"][\"start_datetime\"][:7]: item for item in items} \n# rh = Heterotrophic Respiration\nasset_name = \"rh\"\n\nBelow, we are entering the minimum and maximum values to provide our upper and lower bounds in rescale_values.\n\nrescale_values = {\"max\":items[list(items.keys())[0]][\"assets\"][asset_name][\"raster:bands\"][0][\"histogram\"][\"max\"], \"min\":items[list(items.keys())[0]][\"assets\"][asset_name][\"raster:bands\"][0][\"histogram\"][\"min\"]}\n\nNow, we will pass the item id, collection name, and rescaling_factor to the Raster API endpoint. We will do this twice, once for December 2003 and again for December 2017, so that we can visualize each event independently.\n\ncolor_map = \"purd\" # please refer to matplotlib library if you'd prefer choosing a different color ramp.\n# For more information on Colormaps in Matplotlib, please visit https://matplotlib.org/stable/users/explain/colors/colormaps.html\n\n# To change the year and month of the observed parameter, you can modify the \"items['YYYY-MM']\" statement\n# For example, you can change the current statement \"items['2003-12']\" to \"items['2016-10']\" \ndecember_2003_tile = requests.get(\n    f\"{RASTER_API_URL}/stac/tilejson.json?collection={items['2003-12']['collection']}&item={items['2003-12']['id']}\"\n    f\"&assets={asset_name}\"\n    f\"&color_formula=gamma+r+1.05&colormap_name={color_map}\"\n    f\"&rescale={rescale_values['min']},{rescale_values['max']}\", \n).json()\ndecember_2003_tile\n\n\n# Now we apply the same process used in the previous step for the December 2017 tile\ndecember_2017_tile = requests.get(\n    f\"{RASTER_API_URL}/stac/tilejson.json?collection={items['2017-12']['collection']}&item={items['2017-12']['id']}\"\n    f\"&assets={asset_name}\"\n    f\"&color_formula=gamma+r+1.05&colormap_name={color_map}\"\n    f\"&rescale={rescale_values['min']},{rescale_values['max']}\", \n).json()\ndecember_2017_tile"
  },
  {
    "objectID": "user_data_notebooks/casagfed-carbonflux-monthgrid-v3_User_Notebook.html#visualizing-land-atmosphere-carbon-flux-heterotrophic-respiration",
    "href": "user_data_notebooks/casagfed-carbonflux-monthgrid-v3_User_Notebook.html#visualizing-land-atmosphere-carbon-flux-heterotrophic-respiration",
    "title": "CASA-GFED3 Land Carbon Flux",
    "section": "Visualizing Land-Atmosphere Carbon Flux (Heterotrophic Respiration)",
    "text": "Visualizing Land-Atmosphere Carbon Flux (Heterotrophic Respiration)\n\n# For this study we are going to compare the RH level in 2003 and 2017 over the State of Texas \n# To change the location, you can simply insert the latitude and longitude of the area of your interest in the \"location=(LAT, LONG)\" statement\n# For example, you can change the current statement \"location=(31.9, -99.9)\" to \"location=(34, -118)\" to monitor the RH level in California instead of Texas\n\n# Set initial zoom and center of map for CO₂ Layer\n# 'folium.plugins' allows mapping side-by-side\nmap_ = folium.plugins.DualMap(location=(31.9, -99.9), zoom_start=6)\n\n# The TileLayer library helps in manipulating and displaying raster layers on a map\n# December 2003\nmap_layer_2003 = TileLayer(\n    tiles=december_2003_tile[\"tiles\"][0],\n    attr=\"GHG\",\n    opacity=0.8,\n    name=\"December 2003 RH Level\",\n    overlay= True,\n    legendEnabled = True\n)\nmap_layer_2003.add_to(map_.m1)\n\n\n# December 2017\nmap_layer_2017 = TileLayer(\n    tiles=december_2017_tile[\"tiles\"][0],\n    attr=\"GHG\",\n    opacity=0.8,\n    name=\"December 2017 RH Level\",\n    overlay= True,\n    legendEnabled = True\n)\nmap_layer_2017.add_to(map_.m2)\n\n\n# Display data markers (titles) on both maps\nfolium.Marker((40, 5.0), tooltip=\"both\").add_to(map_)\nfolium.LayerControl(collapsed=False).add_to(map_)\n\n\n# Add a legend to the dual map using the 'branca' library. \n# Note: the inserted legend is representing the minimum and maximum values for both tiles.\ncolormap = branca.colormap.linear.PuRd_09.scale(0, 0.3) # minimum value = 0, maximum value = 0.3 (kg Carbon/m2/month)\ncolormap = colormap.to_step(index=[0, 0.07, 0.15, 0.22, 0.3])\ncolormap.caption = 'Rh Values (kg Carbon/m2/month)'\n\ncolormap.add_to(map_.m1)\n\n\n# Visualizing the map\nmap_"
  },
  {
    "objectID": "user_data_notebooks/casagfed-carbonflux-monthgrid-v3_User_Notebook.html#visualizing-the-data-as-a-time-series",
    "href": "user_data_notebooks/casagfed-carbonflux-monthgrid-v3_User_Notebook.html#visualizing-the-data-as-a-time-series",
    "title": "CASA-GFED3 Land Carbon Flux",
    "section": "Visualizing the Data as a Time Series",
    "text": "Visualizing the Data as a Time Series\nWe can now explore the Heterotrophic Respiration time series (January 2003 -December 2017) available for the Dallas, Texas area. We can plot the data set using the code below:\n\nfig = plt.figure(figsize=(20, 10)) #determine the width and height of the plot using the 'matplotlib' library\n\nplt.plot(\n    df[\"date\"],\n    df[\"max\"],\n    color=\"purple\",\n    linestyle=\"-\",\n    linewidth=0.5,\n    label=\"Max monthly Carbon emissions\",\n)\n\nplt.legend()\nplt.xlabel(\"Years\")\nplt.ylabel(\"kg Carbon/m2/month\")\nplt.title(\"Heterotrophic Respiration Values for Dallas, Texas (2003-2017)\")\n\n\n# Now let's examine the Rh level for the 3rd item in the collection for Dallas, Texas area\n# Keep in mind that a list starts from 0, 1, 2,... therefore items[2] is referring to the third item in the list/collection\nprint(items[2][\"properties\"][\"start_datetime\"]) #print the start Date Time of the third granule in the collection!\n\n\n# Fetch the third granule in the collection and set the color scheme and rescale values. \noctober_tile = requests.get(\n    f\"{RASTER_API_URL}/stac/tilejson.json?collection={items[2]['collection']}&item={items[2]['id']}\"\n    f\"&assets={asset_name}\"\n    f\"&color_formula=gamma+r+1.05&colormap_name={color_map}\"\n    f\"&rescale={rescale_values['min']},{rescale_values['max']}\",\n).json()\noctober_tile\n\n\n# Map the Rh level for the Dallas, Texas area for the October, 2017 timeframe\naoi_map_bbox = Map(\n    tiles=\"OpenStreetMap\",\n    location=[\n        32.8, # latitude\n        -96.79, # longitude\n    ],\n    zoom_start=9,\n)\n\nmap_layer = TileLayer(\n    tiles=october_tile[\"tiles\"][0],\n    attr=\"GHG\", opacity = 0.7, name=\"October 2017 RH Level\", overlay= True, legendEnabled = True\n)\n\nmap_layer.add_to(aoi_map_bbox)\n\n# Display data marker (title) on the map\nfolium.Marker((40, 5.9), tooltip=\"both\").add_to(aoi_map_bbox)\nfolium.LayerControl(collapsed=False).add_to(aoi_map_bbox)\n\n# Add a legend\ncolormap = branca.colormap.linear.PuRd_09.scale(0, 0.3) # minimum value = 0, maximum value = 0.3 (kg Carbon/m2/month)\ncolormap = colormap.to_step(index=[0, 0.07, 0.15, 0.22, 0.3])\ncolormap.caption = 'Rh Values (kg Carbon/m2/month)'\n\ncolormap.add_to(aoi_map_bbox)\n\naoi_map_bbox"
  },
  {
    "objectID": "user_data_notebooks/casagfed-carbonflux-monthgrid-v3_User_Notebook.html#summary",
    "href": "user_data_notebooks/casagfed-carbonflux-monthgrid-v3_User_Notebook.html#summary",
    "title": "CASA-GFED3 Land Carbon Flux",
    "section": "Summary",
    "text": "Summary\nIn this notebook we have successfully completed the following steps for the STAC collection for CASA GFED Land-Atmosphere Carbon Flux data: 1. Install and import the necessary libraries 2. Fetch the collection from STAC collections using the appropriate endpoints 3. Count the number of existing granules within the collection 4. Map and compare the Heterotrophic Respiration (Rh) levels over the Dallas, Texas area for two distinctive years 5. Create a table that displays the minimum, maximum, and sum of the Rh values for a specified region 6. Generate a time-series graph of the Rh values for a specified region\nIf you have any questions regarding this user notebook, please contact us using the feedback form."
  },
  {
    "objectID": "user_data_notebooks/sedac-popdensity-yeargrid5yr-v4.11_User_Notebook.html",
    "href": "user_data_notebooks/sedac-popdensity-yeargrid5yr-v4.11_User_Notebook.html",
    "title": "SEDAC Gridded World Population Density",
    "section": "",
    "text": "Identify available dates and temporal frequency of observations for the given collection using the GHGC API /stac endpoint. Collection processed in this notebook is SEDAC gridded population density.\nPass the STAC item into raster API /stac/tilejson.json endpoint\nWe’ll visualize two tiles (side-by-side) allowing for comparison of each of the time points using folium.plugins.DualMap\nAfter the visualization, we’ll perform zonal statistics for a given polygon.",
    "crumbs": [
      "Data Usage Notebooks",
      "Socioeconomic",
      "SEDAC Gridded World Population Density"
    ]
  },
  {
    "objectID": "user_data_notebooks/sedac-popdensity-yeargrid5yr-v4.11_User_Notebook.html#approach",
    "href": "user_data_notebooks/sedac-popdensity-yeargrid5yr-v4.11_User_Notebook.html#approach",
    "title": "SEDAC Gridded World Population Density",
    "section": "",
    "text": "Identify available dates and temporal frequency of observations for the given collection using the GHGC API /stac endpoint. Collection processed in this notebook is SEDAC gridded population density.\nPass the STAC item into raster API /stac/tilejson.json endpoint\nWe’ll visualize two tiles (side-by-side) allowing for comparison of each of the time points using folium.plugins.DualMap\nAfter the visualization, we’ll perform zonal statistics for a given polygon.",
    "crumbs": [
      "Data Usage Notebooks",
      "Socioeconomic",
      "SEDAC Gridded World Population Density"
    ]
  },
  {
    "objectID": "user_data_notebooks/sedac-popdensity-yeargrid5yr-v4.11_User_Notebook.html#about-the-data",
    "href": "user_data_notebooks/sedac-popdensity-yeargrid5yr-v4.11_User_Notebook.html#about-the-data",
    "title": "SEDAC Gridded World Population Density",
    "section": "About the Data",
    "text": "About the Data\nThe SEDAC Gridded Population of the World: Population Density, v4.11 dataset provides annual estimates of population density for the years 2000, 2005, 2010, 2015, and 2020 on a 30 arc-second (~1 km) grid. These data can be used for assessing disaster impacts, risk mapping, and any other applications that include a human dimension. This population density dataset is provided by NASA’s Socioeconomic Data and Applications Center (SEDAC) hosted by the Center for International Earth Science Information Network (CIESIN) at Columbia University. The population estimates are provided as a continuous raster for the entire globe.",
    "crumbs": [
      "Data Usage Notebooks",
      "Socioeconomic",
      "SEDAC Gridded World Population Density"
    ]
  },
  {
    "objectID": "user_data_notebooks/sedac-popdensity-yeargrid5yr-v4.11_User_Notebook.html#querying-the-stac-api",
    "href": "user_data_notebooks/sedac-popdensity-yeargrid5yr-v4.11_User_Notebook.html#querying-the-stac-api",
    "title": "SEDAC Gridded World Population Density",
    "section": "Querying the STAC API",
    "text": "Querying the STAC API\n\nimport requests\nfrom folium import Map, TileLayer\nfrom pystac_client import Client\n\n\n# Provide STAC and RASTER API endpoints\nSTAC_API_URL = \"http://ghg.center/api/stac\"\nRASTER_API_URL = \"https://ghg.center/api/raster\"\n\n#Please use the collection name similar to the one used in STAC collection.\n# Name of the collection for SEDAC population density dataset. \ncollection_name = \"sedac-popdensity-yeargrid5yr-v4.11\"\n\n\n# Fetching the collection from STAC collections using appropriate endpoint.\ncollection = requests.get(f\"{STAC_API_URL}/collections/{collection_name}\").json()\ncollection\n\nExamining the contents of our collection under summaries we see that the data is available from January 2000 to December 2020. By looking at the dashboard:time density we observe that the data is available for the years 2000, 2005, 2010, 2015, 2020.\n\ndef get_item_count(collection_id):\n    count = 0\n    items_url = f\"{STAC_API_URL}/collections/{collection_id}/items\"\n\n    while True:\n        response = requests.get(items_url)\n\n        if not response.ok:\n            print(\"error getting items\")\n            exit()\n\n        stac = response.json()\n        count += int(stac[\"context\"].get(\"returned\", 0))\n        next = [link for link in stac[\"links\"] if link[\"rel\"] == \"next\"]\n\n        if not next:\n            break\n        items_url = next[0][\"href\"]\n\n    return count\n\n\n# Check total number of items available\nnumber_of_items = get_item_count(collection_name)\nitems = requests.get(f\"{STAC_API_URL}/collections/{collection_name}/items?limit={number_of_items}\").json()[\"features\"]\nprint(f\"Found {len(items)} items\")\n\n\nitems[0]\n\nBelow, we are entering the minimum and maximum values to provide our upper and lower bounds in rescale_values.",
    "crumbs": [
      "Data Usage Notebooks",
      "Socioeconomic",
      "SEDAC Gridded World Population Density"
    ]
  },
  {
    "objectID": "user_data_notebooks/sedac-popdensity-yeargrid5yr-v4.11_User_Notebook.html#exploring-changes-in-the-world-population-density-using-the-raster-api",
    "href": "user_data_notebooks/sedac-popdensity-yeargrid5yr-v4.11_User_Notebook.html#exploring-changes-in-the-world-population-density-using-the-raster-api",
    "title": "SEDAC Gridded World Population Density",
    "section": "Exploring Changes in the World Population Density using the Raster API",
    "text": "Exploring Changes in the World Population Density using the Raster API\nWe will explore changes in population density in urban regions. In this notebook, we’ll explore the changes in population density over time. We’ll then visualize the outputs on a map using folium.\n\n# to access the year value from each item more easily, this will let us query more explicity by year and month (e.g., 2020-02)\nitems = {item[\"properties\"][\"start_datetime\"][:7]: item for item in items} \nasset_name = \"population-density\"\n\n\nrescale_values = {\"max\":items[list(items.keys())[0]][\"assets\"][asset_name][\"raster:bands\"][0][\"histogram\"][\"max\"], \"min\":items[list(items.keys())[0]][\"assets\"][asset_name][\"raster:bands\"][0][\"histogram\"][\"min\"]}\n\nNow we will pass the item id, collection name, and rescaling_factor to the Raster API endpoint. We will do this twice, once for January 2000 and again for January 2020, so that we can visualize each event independently.\n\ncolor_map = \"rainbow\" # please select the color ramp from matplotlib library.\njanuary_2020_tile = requests.get(\n    f\"{RASTER_API_URL}/stac/tilejson.json?collection={items['2020-01']['collection']}&item={items['2020-01']['id']}\"\n    f\"&assets={asset_name}\"\n    f\"&color_formula=gamma+r+1.05&colormap_name={color_map}\"\n    f\"&rescale={rescale_values['min']},{rescale_values['max']}\", \n).json()\njanuary_2020_tile\n\n\njanuary_2000_tile = requests.get(\n    f\"{RASTER_API_URL}/stac/tilejson.json?collection={items['2000-01']['collection']}&item={items['2000-01']['id']}\"\n    f\"&assets={asset_name}\"\n    f\"&color_formula=gamma+r+1.05&colormap_name={color_map}\"\n    f\"&rescale={rescale_values['min']},{rescale_values['max']}\", \n).json()\njanuary_2000_tile",
    "crumbs": [
      "Data Usage Notebooks",
      "Socioeconomic",
      "SEDAC Gridded World Population Density"
    ]
  },
  {
    "objectID": "user_data_notebooks/sedac-popdensity-yeargrid5yr-v4.11_User_Notebook.html#visualizing-population-density.",
    "href": "user_data_notebooks/sedac-popdensity-yeargrid5yr-v4.11_User_Notebook.html#visualizing-population-density.",
    "title": "SEDAC Gridded World Population Density",
    "section": "Visualizing Population Density.",
    "text": "Visualizing Population Density.\n\n# We'll import folium to map and folium.plugins to allow mapping side-by-side\nimport folium\nimport folium.plugins\n\n# Set initial zoom and center of map for population density Layer\nmap_ = folium.plugins.DualMap(location=(34, -118), zoom_start=6)\n\n# January 2020\nmap_layer_2020 = TileLayer(\n    tiles=january_2020_tile[\"tiles\"][0],\n    attr=\"GHG\",\n    opacity=1,\n)\nmap_layer_2020.add_to(map_.m1)\n\n# January 2000\nmap_layer_2000 = TileLayer(\n    tiles=january_2000_tile[\"tiles\"][0],\n    attr=\"GHG\",\n    opacity=1,\n)\nmap_layer_2000.add_to(map_.m2)\n\n# visualising the map\nmap_",
    "crumbs": [
      "Data Usage Notebooks",
      "Socioeconomic",
      "SEDAC Gridded World Population Density"
    ]
  },
  {
    "objectID": "user_data_notebooks/sedac-popdensity-yeargrid5yr-v4.11_User_Notebook.html#section",
    "href": "user_data_notebooks/sedac-popdensity-yeargrid5yr-v4.11_User_Notebook.html#section",
    "title": "SEDAC Gridded World Population Density",
    "section": "",
    "text": "# Texas, USA\ntexas_aoi = {\n    \"type\": \"Feature\",\n    \"properties\": {},\n    \"geometry\": {\n        \"coordinates\": [\n            [\n                # [13.686159004559698, -21.700046934333145],\n                # [13.686159004559698, -23.241974326585833],\n                # [14.753560168039911, -23.241974326585833],\n                # [14.753560168039911, -21.700046934333145],\n                # [13.686159004559698, -21.700046934333145],\n                [-95, 29],\n                [-95, 33],\n                [-104, 33],\n                [-104,29],\n                [-95, 29]\n            ]\n        ],\n        \"type\": \"Polygon\",\n    },\n}\n\n\n# We'll plug in the coordinates for a location\n# central to the study area and a reasonable zoom level\n\nimport folium\n\naoi_map = Map(\n    tiles=\"OpenStreetMap\",\n    location=[\n        30,-100\n    ],\n    zoom_start=6,\n)\n\nfolium.GeoJson(texas_aoi, name=\"Texas, USA\").add_to(aoi_map)\naoi_map\n\n\n# Check total number of items available\nitems = requests.get(\n    f\"{STAC_API_URL}/collections/{collection_name}/items?limit=300\"\n).json()[\"features\"]\nprint(f\"Found {len(items)} items\")\n\n\n# Explore one item to see what it contains\nitems[0]\n\n\n# the bounding box should be passed to the geojson param as a geojson Feature or FeatureCollection\ndef generate_stats(item, geojson):\n    result = requests.post(\n        f\"{RASTER_API_URL}/cog/statistics\",\n        params={\"url\": item[\"assets\"][asset_name][\"href\"]},\n        json=geojson,\n    ).json()\n    return {\n        **result[\"properties\"],\n        \"start_datetime\": item[\"properties\"][\"start_datetime\"],\n    }\n\nWith the function above we can generate the statistics for the AOI.\n\n%%time\nstats = [generate_stats(item, texas_aoi) for item in items]\n\n\nstats[0]\n\n\nimport pandas as pd\n\n\ndef clean_stats(stats_json) -&gt; pd.DataFrame:\n    df = pd.json_normalize(stats_json)\n    df.columns = [col.replace(\"statistics.b1.\", \"\") for col in df.columns]\n    df[\"date\"] = pd.to_datetime(df[\"start_datetime\"])\n    return df\n\n\ndf = clean_stats(stats)\ndf.head(5)",
    "crumbs": [
      "Data Usage Notebooks",
      "Socioeconomic",
      "SEDAC Gridded World Population Density"
    ]
  },
  {
    "objectID": "user_data_notebooks/sedac-popdensity-yeargrid5yr-v4.11_User_Notebook.html#visualizing-the-data-as-a-time-series",
    "href": "user_data_notebooks/sedac-popdensity-yeargrid5yr-v4.11_User_Notebook.html#visualizing-the-data-as-a-time-series",
    "title": "SEDAC Gridded World Population Density",
    "section": "Visualizing the Data as a Time Series",
    "text": "Visualizing the Data as a Time Series\nWe can now explore the SEDAC population density dataset time series available for the Texas, Dallas area of USA. We can plot the dataset using the code below:\n\nimport matplotlib.pyplot as plt\n\nfig = plt.figure(figsize=(20, 10))\n\n\nplt.plot(\n    df[\"date\"],\n    df[\"max\"],\n    color=\"red\",\n    linestyle=\"-\",\n    linewidth=0.5,\n    label=\"Population density over the years\",\n)\n\nplt.legend()\nplt.xlabel(\"Years\")\nplt.ylabel(\"Population density\")\nplt.title(\"Population density over Texas, Dallas (2000-2020)\")\n\n\nprint(items[2][\"properties\"][\"start_datetime\"])\n\n\noctober_tile = requests.get(\n    f\"{RASTER_API_URL}/stac/tilejson.json?collection={items[2]['collection']}&item={items[2]['id']}\"\n    f\"&assets={asset_name}\"\n    f\"&color_formula=gamma+r+1.05&colormap_name={color_map}\"\n    f\"&rescale={rescale_values['min']},{rescale_values['max']}\",\n).json()\noctober_tile\n\n\n# Use bbox initial zoom and map\n# Set up a map located w/in event bounds\nimport folium\n\naoi_map_bbox = Map(\n    tiles=\"OpenStreetMap\",\n    location=[\n        30,-100\n    ],\n    zoom_start=8,\n)\n\nmap_layer = TileLayer(\n    tiles=october_tile[\"tiles\"][0],\n    attr=\"GHG\", opacity = 0.5\n)\n\nmap_layer.add_to(aoi_map_bbox)\n\naoi_map_bbox",
    "crumbs": [
      "Data Usage Notebooks",
      "Socioeconomic",
      "SEDAC Gridded World Population Density"
    ]
  },
  {
    "objectID": "user_data_notebooks/sedac-popdensity-yeargrid5yr-v4.11_User_Notebook.html#summary",
    "href": "user_data_notebooks/sedac-popdensity-yeargrid5yr-v4.11_User_Notebook.html#summary",
    "title": "SEDAC Gridded World Population Density",
    "section": "Summary",
    "text": "Summary\nIn this notebook we have successfully explored, analyzed and visualized the STAC collection for the SEDAC Gridded World Population Density dataset.",
    "crumbs": [
      "Data Usage Notebooks",
      "Socioeconomic",
      "SEDAC Gridded World Population Density"
    ]
  },
  {
    "objectID": "data_workflow/noaa-insitu_Data_Flow.html",
    "href": "data_workflow/noaa-insitu_Data_Flow.html",
    "title": "Atmospheric Carbon Dioxide Concentrations from the NOAA Global Monitoring Laboratory",
    "section": "",
    "text": "Atmospheric Carbon Dioxide Concentrations from the NOAA Global Monitoring Laboratory\n\n\n\nData Flow Diagram Extending From Acquisition/Creation to User Delivery\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Data Workflow Documentation",
      "Greenhouse Gas Concentrations",
      "Atmospheric Carbon Dioxide Concentrations from the NOAA Global Monitoring Laboratory"
    ]
  },
  {
    "objectID": "data_workflow/emit-ch4plume-v1_Data_Flow.html",
    "href": "data_workflow/emit-ch4plume-v1_Data_Flow.html",
    "title": "EMIT methane point source plume complexes",
    "section": "",
    "text": "EMIT methane point source plume complexes\n\n\n\nData Flow Diagram Extending From Acquisition/Creation to User Delivery\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Data Workflow Documentation",
      "Large Emissions Events",
      "EMIT methane point source plume complexes"
    ]
  },
  {
    "objectID": "data_workflow/odiac-ffco2-monthgrid-v2022_Data_Flow.html",
    "href": "data_workflow/odiac-ffco2-monthgrid-v2022_Data_Flow.html",
    "title": "ODIAC Fossil Fuel CO₂ Emissions",
    "section": "",
    "text": "ODIAC Fossil Fuel CO₂ Emissions\n\n\n\nData Flow Diagram Extending From Acquisition/Creation to User Delivery\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Data Workflow Documentation",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "ODIAC Fossil Fuel CO₂ Emissions"
    ]
  },
  {
    "objectID": "data_workflow/lpjeosim-wetlandch4-daygrid-v1_Data_Flow.html",
    "href": "data_workflow/lpjeosim-wetlandch4-daygrid-v1_Data_Flow.html",
    "title": "Wetland Methane Emissions, LPJ-EOSIM Model",
    "section": "",
    "text": "Wetland Methane Emissions, LPJ-EOSIM Model\n\n\n\nData Flow Diagram Extending From Acquisition/Creation to User Delivery\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "data_workflow/eccodarwin-co2flux-monthgrid-v5_Data_Flow.html",
    "href": "data_workflow/eccodarwin-co2flux-monthgrid-v5_Data_Flow.html",
    "title": "Air-Sea CO₂ Flux, ECCO-Darwin Model v5",
    "section": "",
    "text": "Air-Sea CO₂ Flux, ECCO-Darwin Model v5\n\n\n\nData Flow Diagram Extending From Acquisition/Creation to User Delivery\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Data Workflow Documentation",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "Air-Sea CO₂ Flux, ECCO-Darwin Model v5"
    ]
  },
  {
    "objectID": "data_workflow/micasa-carbonflux-daygrid-v1_Data_Flow.html",
    "href": "data_workflow/micasa-carbonflux-daygrid-v1_Data_Flow.html",
    "title": "MiCASA Land Carbon Flux - Data Workflow",
    "section": "",
    "text": "MiCASA Land Carbon Flux - Data Workflow\n\n\n\nData Flow Diagram Extending From Acquisition/Creation to User Delivery\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Data Workflow Documentation",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "MiCASA Land Carbon Flux - Data Workflow"
    ]
  },
  {
    "objectID": "data_workflow/gosat-based-ch4budget-yeargrid-v1_Data_Flow.html",
    "href": "data_workflow/gosat-based-ch4budget-yeargrid-v1_Data_Flow.html",
    "title": "GOSAT-based Top-down Total and Natural Methane Emissions",
    "section": "",
    "text": "GOSAT-based Top-down Total and Natural Methane Emissions\n\n\n\nData Flow Diagram Extending From Acquisition/Creation to User Delivery\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Data Workflow Documentation",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "GOSAT-based Top-down Total and Natural Methane Emissions"
    ]
  },
  {
    "objectID": "data_workflow/oco2geos-co2-daygrid-v10r_Data_Flow.html",
    "href": "data_workflow/oco2geos-co2-daygrid-v10r_Data_Flow.html",
    "title": "OCO-2 GEOS Column CO₂ Concentrations",
    "section": "",
    "text": "OCO-2 GEOS Column CO₂ Concentrations\n\n\n\nData Flow Diagram Extending From Acquisition/Creation to User Delivery\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Data Workflow Documentation",
      "Greenhouse Gas Concentrations",
      "OCO-2 GEOS Column CO₂ Concentrations"
    ]
  },
  {
    "objectID": "data_workflow/epa-ch4emission-grid-v2express_Data_Flow.html",
    "href": "data_workflow/epa-ch4emission-grid-v2express_Data_Flow.html",
    "title": "Gridded Anthropogenic Methane Emissions Inventory",
    "section": "",
    "text": "Gridded Anthropogenic Methane Emissions Inventory\n\n\n\nData Flow Diagram Extending From Acquisition/Creation to User Delivery\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Data Workflow Documentation",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "Gridded Anthropogenic Methane Emissions Inventory"
    ]
  },
  {
    "objectID": "data_workflow/oco2-mip-co2budget-yeargrid-v1_Data_Flow.html",
    "href": "data_workflow/oco2-mip-co2budget-yeargrid-v1_Data_Flow.html",
    "title": "OCO-2 MIP Top-Down CO₂ Budgets",
    "section": "",
    "text": "OCO-2 MIP Top-Down CO₂ Budgets\n\n\n\nData Flow Diagram Extending From Acquisition/Creation to User Delivery\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Data Workflow Documentation",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "OCO-2 MIP Top-Down CO₂ Budgets"
    ]
  },
  {
    "objectID": "data_workflow/tm54dvar-ch4flux-monthgrid-v1_Data_Flow.html",
    "href": "data_workflow/tm54dvar-ch4flux-monthgrid-v1_Data_Flow.html",
    "title": "TM5-4DVar Isotopic CH₄ Inverse Fluxes",
    "section": "",
    "text": "TM5-4DVar Isotopic CH₄ Inverse Fluxes\n\n\n\nData Flow Diagram Extending From Acquisition/Creation to User Delivery\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Data Workflow Documentation",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "TM5-4DVar Isotopic CH₄ Inverse Fluxes"
    ]
  },
  {
    "objectID": "data_workflow/sedac-popdensity-yeargrid5yr-v4.11_Data_Flow.html",
    "href": "data_workflow/sedac-popdensity-yeargrid5yr-v4.11_Data_Flow.html",
    "title": "SEDAC Gridded World Population Data",
    "section": "",
    "text": "SEDAC Gridded World Population Data\n\n\n\nData Flow Diagram Extending From Acquisition/Creation to User Delivery\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Data Workflow Documentation",
      "Socioeconomic",
      "SEDAC Gridded World Population Data"
    ]
  },
  {
    "objectID": "user_data_notebooks/odiac-ffco2-monthgrid-v2022_User_Notebook.html",
    "href": "user_data_notebooks/odiac-ffco2-monthgrid-v2022_User_Notebook.html",
    "title": "ODIAC Fossil Fuel CO₂ Emissions",
    "section": "",
    "text": "Identify available dates and temporal frequency of observations for the given collection using the GHGC API /stac endpoint. Collection processed in this notebook is ODIAC CO₂ emissions version 2022.\nPass the STAC item into raster API /stac/tilejson.json endpoint\nWe’ll visualize two tiles (side-by-side) allowing for comparison of each of the time points using folium.plugins.DualMap\nAfter the visualization, we’ll perform zonal statistics for a given polygon.",
    "crumbs": [
      "Data Usage Notebooks",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "ODIAC Fossil Fuel CO₂ Emissions"
    ]
  },
  {
    "objectID": "user_data_notebooks/odiac-ffco2-monthgrid-v2022_User_Notebook.html#approach",
    "href": "user_data_notebooks/odiac-ffco2-monthgrid-v2022_User_Notebook.html#approach",
    "title": "ODIAC Fossil Fuel CO₂ Emissions",
    "section": "",
    "text": "Identify available dates and temporal frequency of observations for the given collection using the GHGC API /stac endpoint. Collection processed in this notebook is ODIAC CO₂ emissions version 2022.\nPass the STAC item into raster API /stac/tilejson.json endpoint\nWe’ll visualize two tiles (side-by-side) allowing for comparison of each of the time points using folium.plugins.DualMap\nAfter the visualization, we’ll perform zonal statistics for a given polygon.",
    "crumbs": [
      "Data Usage Notebooks",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "ODIAC Fossil Fuel CO₂ Emissions"
    ]
  },
  {
    "objectID": "user_data_notebooks/odiac-ffco2-monthgrid-v2022_User_Notebook.html#about-the-data",
    "href": "user_data_notebooks/odiac-ffco2-monthgrid-v2022_User_Notebook.html#about-the-data",
    "title": "ODIAC Fossil Fuel CO₂ Emissions",
    "section": "About the Data",
    "text": "About the Data\nThe Open-Data Inventory for Anthropogenic Carbon dioxide (ODIAC) is a high-spatial resolution global emission data product of CO₂ emissions from fossil fuel combustion (Oda and Maksyutov, 2011). ODIAC pioneered the combined use of space-based nighttime light data and individual power plant emission/location profiles to estimate the global spatial extent of fossil fuel CO₂ emissions. With the innovative emission modeling approach, ODIAC achieved the fine picture of global fossil fuel CO₂ emissions at a 1x1km.",
    "crumbs": [
      "Data Usage Notebooks",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "ODIAC Fossil Fuel CO₂ Emissions"
    ]
  },
  {
    "objectID": "user_data_notebooks/odiac-ffco2-monthgrid-v2022_User_Notebook.html#querying-the-stac-api",
    "href": "user_data_notebooks/odiac-ffco2-monthgrid-v2022_User_Notebook.html#querying-the-stac-api",
    "title": "ODIAC Fossil Fuel CO₂ Emissions",
    "section": "Querying the STAC API",
    "text": "Querying the STAC API\n\nimport requests\nfrom folium import Map, TileLayer\nfrom pystac_client import Client\n\n\n# Provide STAC and RASTER API endpoints\nSTAC_API_URL = \"http://ghg.center/api/stac\"\nRASTER_API_URL = \"https://ghg.center/api/raster\"\n\n#Please use the collection name similar to the one used in STAC collection.\n# Name of the collection for ODIAC dataset. \ncollection_name = \"odiac-ffco2-monthgrid-v2022\"\n\n\n# Fetching the collection from STAC collections using appropriate endpoint.\ncollection = requests.get(f\"{STAC_API_URL}/collections/{collection_name}\").json()\ncollection\n\nExamining the contents of our collection under summaries we see that the data is available from January 2000 to December 2021. By looking at the dashboard:time density we observe that the periodic frequency of these observations is monthly.\n\ndef get_item_count(collection_id):\n    count = 0\n    items_url = f\"{STAC_API_URL}/collections/{collection_id}/items\"\n\n    while True:\n        response = requests.get(items_url)\n\n        if not response.ok:\n            print(\"error getting items\")\n            exit()\n\n        stac = response.json()\n        count += int(stac[\"context\"].get(\"returned\", 0))\n        next = [link for link in stac[\"links\"] if link[\"rel\"] == \"next\"]\n\n        if not next:\n            break\n        items_url = next[0][\"href\"]\n\n    return count\n\n\n# Check total number of items available\nnumber_of_items = get_item_count(collection_name)\nitems = requests.get(f\"{STAC_API_URL}/collections/{collection_name}/items?limit={number_of_items}\").json()[\"features\"]\nprint(f\"Found {len(items)} items\")\n\n\nitems[0]\n\nThis makes sense as there are 22 years between 2000 - 2021, with 12 months per year, meaning 264 records in total.\nBelow, we are entering the minimum and maximum values to provide our upper and lower bounds in rescale_values.",
    "crumbs": [
      "Data Usage Notebooks",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "ODIAC Fossil Fuel CO₂ Emissions"
    ]
  },
  {
    "objectID": "user_data_notebooks/odiac-ffco2-monthgrid-v2022_User_Notebook.html#exploring-changes-in-carbon-dioxide-co₂-levels-using-the-raster-api",
    "href": "user_data_notebooks/odiac-ffco2-monthgrid-v2022_User_Notebook.html#exploring-changes-in-carbon-dioxide-co₂-levels-using-the-raster-api",
    "title": "ODIAC Fossil Fuel CO₂ Emissions",
    "section": "Exploring Changes in Carbon Dioxide (CO₂) levels using the Raster API",
    "text": "Exploring Changes in Carbon Dioxide (CO₂) levels using the Raster API\nWe will explore changes in fossil fuel emissions in urban egions. In this notebook, we’ll explore the impacts of these emissions and explore these changes over time. We’ll then visualize the outputs on a map using folium.\n\n# to access the year value from each item more easily, this will let us query more explicity by year and month (e.g., 2020-02)\nitems = {item[\"properties\"][\"start_datetime\"][:7]: item for item in items} \nasset_name = \"co2-emissions\"\n\n\nrescale_values = {\"max\":items[list(items.keys())[0]][\"assets\"][asset_name][\"raster:bands\"][0][\"histogram\"][\"max\"], \"min\":items[list(items.keys())[0]][\"assets\"][asset_name][\"raster:bands\"][0][\"histogram\"][\"min\"]}\n\nNow we will pass the item id, collection name, and rescaling_factor to the Raster API endpoint. We will do this twice, once for January 2020 and again for January 2000, so that we can visualize each event independently.\n\ncolor_map = \"rainbow\" # please select the color ramp from matplotlib library.\njanuary_2020_tile = requests.get(\n    f\"{RASTER_API_URL}/stac/tilejson.json?collection={items['2020-01']['collection']}&item={items['2020-01']['id']}\"\n    f\"&assets={asset_name}\"\n    f\"&color_formula=gamma+r+1.05&colormap_name={color_map}\"\n    f\"&rescale={rescale_values['min']},{rescale_values['max']}\", \n).json()\njanuary_2020_tile\n\n\njanuary_2000_tile = requests.get(\n    f\"{RASTER_API_URL}/stac/tilejson.json?collection={items['2000-01']['collection']}&item={items['2000-01']['id']}\"\n    f\"&assets={asset_name}\"\n    f\"&color_formula=gamma+r+1.05&colormap_name={color_map}\"\n    f\"&rescale={rescale_values['min']},{rescale_values['max']}\", \n).json()\njanuary_2000_tile",
    "crumbs": [
      "Data Usage Notebooks",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "ODIAC Fossil Fuel CO₂ Emissions"
    ]
  },
  {
    "objectID": "user_data_notebooks/odiac-ffco2-monthgrid-v2022_User_Notebook.html#visualizing-co₂-emissions",
    "href": "user_data_notebooks/odiac-ffco2-monthgrid-v2022_User_Notebook.html#visualizing-co₂-emissions",
    "title": "ODIAC Fossil Fuel CO₂ Emissions",
    "section": "Visualizing CO₂ emissions",
    "text": "Visualizing CO₂ emissions\n\n# We'll import folium to map and folium.plugins to allow mapping side-by-side\nimport folium\nimport folium.plugins\n\n# Set initial zoom and center of map for CO₂ Layer\nmap_ = folium.plugins.DualMap(location=(34, -118), zoom_start=6)\n\n# December 2001\nmap_layer_2020 = TileLayer(\n    tiles=january_2020_tile[\"tiles\"][0],\n    attr=\"GHG\",\n    opacity=0.8,\n)\nmap_layer_2020.add_to(map_.m1)\n\n# December 2021\nmap_layer_2000 = TileLayer(\n    tiles=january_2000_tile[\"tiles\"][0],\n    attr=\"GHG\",\n    opacity=0.8,\n)\nmap_layer_2000.add_to(map_.m2)\n\n# visualising the map\nmap_",
    "crumbs": [
      "Data Usage Notebooks",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "ODIAC Fossil Fuel CO₂ Emissions"
    ]
  },
  {
    "objectID": "user_data_notebooks/odiac-ffco2-monthgrid-v2022_User_Notebook.html#section",
    "href": "user_data_notebooks/odiac-ffco2-monthgrid-v2022_User_Notebook.html#section",
    "title": "ODIAC Fossil Fuel CO₂ Emissions",
    "section": "",
    "text": "# Texas, USA\ntexas_aoi = {\n    \"type\": \"Feature\",\n    \"properties\": {},\n    \"geometry\": {\n        \"coordinates\": [\n            [\n                # [13.686159004559698, -21.700046934333145],\n                # [13.686159004559698, -23.241974326585833],\n                # [14.753560168039911, -23.241974326585833],\n                # [14.753560168039911, -21.700046934333145],\n                # [13.686159004559698, -21.700046934333145],\n                [-95, 29],\n                [-95, 33],\n                [-104, 33],\n                [-104,29],\n                [-95, 29]\n            ]\n        ],\n        \"type\": \"Polygon\",\n    },\n}\n\n\n# We'll plug in the coordinates for a location\n# central to the study area and a reasonable zoom level\n\nimport folium\n\naoi_map = Map(\n    tiles=\"OpenStreetMap\",\n    location=[\n        30,-100\n    ],\n    zoom_start=6,\n)\n\nfolium.GeoJson(texas_aoi, name=\"Texas, USA\").add_to(aoi_map)\naoi_map\n\n\n# Check total number of items available\nitems = requests.get(\n    f\"{STAC_API_URL}/collections/{collection_name}/items?limit=300\"\n).json()[\"features\"]\nprint(f\"Found {len(items)} items\")\n\n\n# Explore one item to see what it contains\nitems[0]\n\n\n# the bounding box should be passed to the geojson param as a geojson Feature or FeatureCollection\ndef generate_stats(item, geojson):\n    result = requests.post(\n        f\"{RASTER_API_URL}/cog/statistics\",\n        params={\"url\": item[\"assets\"][asset_name][\"href\"]},\n        json=geojson,\n    ).json()\n    return {\n        **result[\"properties\"],\n        \"start_datetime\": item[\"properties\"][\"start_datetime\"][:7],\n    }\n\nWith the function above we can generate the statistics for the AOI.\n\n%%time\nstats = [generate_stats(item, texas_aoi) for item in items]\n\n\nstats[0]\n\n\nimport pandas as pd\n\n\ndef clean_stats(stats_json) -&gt; pd.DataFrame:\n    df = pd.json_normalize(stats_json)\n    df.columns = [col.replace(\"statistics.b1.\", \"\") for col in df.columns]\n    df[\"date\"] = pd.to_datetime(df[\"start_datetime\"])\n    return df\n\n\ndf = clean_stats(stats)\ndf.head(5)",
    "crumbs": [
      "Data Usage Notebooks",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "ODIAC Fossil Fuel CO₂ Emissions"
    ]
  },
  {
    "objectID": "user_data_notebooks/odiac-ffco2-monthgrid-v2022_User_Notebook.html#visualizing-the-data-as-a-time-series",
    "href": "user_data_notebooks/odiac-ffco2-monthgrid-v2022_User_Notebook.html#visualizing-the-data-as-a-time-series",
    "title": "ODIAC Fossil Fuel CO₂ Emissions",
    "section": "Visualizing the Data as a Time Series",
    "text": "Visualizing the Data as a Time Series\nWe can now explore the ODIAC fossil fuel emission time series available (January 2000 -December 2021) for the Texas, Dallas area of USA. We can plot the data set using the code below:\n\nimport matplotlib.pyplot as plt\n\nfig = plt.figure(figsize=(20, 10))\n\n\nplt.plot(\n    df[\"date\"],\n    df[\"max\"],\n    color=\"red\",\n    linestyle=\"-\",\n    linewidth=0.5,\n    label=\"Max monthly CO₂ emissions\",\n)\n\nplt.legend()\nplt.xlabel(\"Years\")\nplt.ylabel(\"CO2 emissions gC/m2/d\")\nplt.title(\"CO2 emission Values for Texas, Dallas (2000-2021)\")\n\n\nprint(items[2][\"properties\"][\"start_datetime\"])\n\n\noctober_tile = requests.get(\n    f\"{RASTER_API_URL}/stac/tilejson.json?collection={items[2]['collection']}&item={items[2]['id']}\"\n    f\"&assets={asset_name}\"\n    f\"&color_formula=gamma+r+1.05&colormap_name={color_map}\"\n    f\"&rescale={rescale_values['min']},{rescale_values['max']}\",\n).json()\noctober_tile\n\n\n# Use bbox initial zoom and map\n# Set up a map located w/in event bounds\nimport folium\n\naoi_map_bbox = Map(\n    tiles=\"OpenStreetMap\",\n    location=[\n        30,-100\n    ],\n    zoom_start=8,\n)\n\nmap_layer = TileLayer(\n    tiles=october_tile[\"tiles\"][0],\n    attr=\"GHG\", opacity = 0.5\n)\n\nmap_layer.add_to(aoi_map_bbox)\n\naoi_map_bbox",
    "crumbs": [
      "Data Usage Notebooks",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "ODIAC Fossil Fuel CO₂ Emissions"
    ]
  },
  {
    "objectID": "user_data_notebooks/odiac-ffco2-monthgrid-v2022_User_Notebook.html#summary",
    "href": "user_data_notebooks/odiac-ffco2-monthgrid-v2022_User_Notebook.html#summary",
    "title": "ODIAC Fossil Fuel CO₂ Emissions",
    "section": "Summary",
    "text": "Summary\nIn this notebook we have successfully explored, analysed and visualized STAC collecetion for ODIAC C02 fossisl fuel emission (2022).",
    "crumbs": [
      "Data Usage Notebooks",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "ODIAC Fossil Fuel CO₂ Emissions"
    ]
  },
  {
    "objectID": "user_data_notebooks/tm54dvar-ch4flux-monthgrid-v1_User_Notebook.html",
    "href": "user_data_notebooks/tm54dvar-ch4flux-monthgrid-v1_User_Notebook.html",
    "title": "TM5-4DVar Isotopic CH₄ Inverse Fluxes",
    "section": "",
    "text": "Identify available dates and temporal frequency of observations for the given collection using the GHGC API /stac endpoint. The collection processed in this notebook is the TM5-4DVar Isotopic CH₄ Inverse Fluxes Data product.\nPass the STAC item into the raster API /stac/tilejson.jsonendpoint.\nUsing folium.plugins.DualMap, we will visualize two tiles (side-by-side), allowing us to compare time points.\nAfter the visualization, we will perform zonal statistics for a given polygon.",
    "crumbs": [
      "Data Usage Notebooks",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "TM5-4DVar Isotopic CH₄ Inverse Fluxes"
    ]
  },
  {
    "objectID": "user_data_notebooks/tm54dvar-ch4flux-monthgrid-v1_User_Notebook.html#approach",
    "href": "user_data_notebooks/tm54dvar-ch4flux-monthgrid-v1_User_Notebook.html#approach",
    "title": "TM5-4DVar Isotopic CH₄ Inverse Fluxes",
    "section": "",
    "text": "Identify available dates and temporal frequency of observations for the given collection using the GHGC API /stac endpoint. The collection processed in this notebook is the TM5-4DVar Isotopic CH₄ Inverse Fluxes Data product.\nPass the STAC item into the raster API /stac/tilejson.jsonendpoint.\nUsing folium.plugins.DualMap, we will visualize two tiles (side-by-side), allowing us to compare time points.\nAfter the visualization, we will perform zonal statistics for a given polygon.",
    "crumbs": [
      "Data Usage Notebooks",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "TM5-4DVar Isotopic CH₄ Inverse Fluxes"
    ]
  },
  {
    "objectID": "user_data_notebooks/tm54dvar-ch4flux-monthgrid-v1_User_Notebook.html#about-the-data",
    "href": "user_data_notebooks/tm54dvar-ch4flux-monthgrid-v1_User_Notebook.html#about-the-data",
    "title": "TM5-4DVar Isotopic CH₄ Inverse Fluxes",
    "section": "About the Data",
    "text": "About the Data\nSurface methane (CH₄) emissions are derived from atmospheric measurements of methane and its ¹³C carbon isotope content. Different sources of methane contain different ratios of the two stable isotopologues, ¹²CH₄ and ¹³CH₄. This makes normally indistinguishable collocated sources of methane, say from agriculture and oil and gas exploration, distinguishable. The National Oceanic and Atmospheric Administration (NOAA) collects whole air samples from its global cooperative network of flasks (https://gml.noaa.gov/ccgg/about.html), which are then analyzed for methane and other trace gasses. A subset of those flasks are also analyzed for ¹³C of methane in collaboration with the Institute of Arctic and Alpine Research at the University of Colorado Boulder. Scientists at the National Aeronautics and Space Administration (NASA) and NOAA used those measurements of methane and ¹³C of methane in conjunction with a model of atmospheric circulation to estimate emissions of methane separated by three source types, microbial, fossil and pyrogenic.",
    "crumbs": [
      "Data Usage Notebooks",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "TM5-4DVar Isotopic CH₄ Inverse Fluxes"
    ]
  },
  {
    "objectID": "user_data_notebooks/tm54dvar-ch4flux-monthgrid-v1_User_Notebook.html#installing-the-required-libraries",
    "href": "user_data_notebooks/tm54dvar-ch4flux-monthgrid-v1_User_Notebook.html#installing-the-required-libraries",
    "title": "TM5-4DVar Isotopic CH₄ Inverse Fluxes",
    "section": "Installing the required libraries",
    "text": "Installing the required libraries\nPlease run the cell below to install the libraries required to run this notebook.\n%pip install requests %pip install folium %pip install rasterstats %pip install pystac_client",
    "crumbs": [
      "Data Usage Notebooks",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "TM5-4DVar Isotopic CH₄ Inverse Fluxes"
    ]
  },
  {
    "objectID": "user_data_notebooks/tm54dvar-ch4flux-monthgrid-v1_User_Notebook.html#querying-the-stac-api",
    "href": "user_data_notebooks/tm54dvar-ch4flux-monthgrid-v1_User_Notebook.html#querying-the-stac-api",
    "title": "TM5-4DVar Isotopic CH₄ Inverse Fluxes",
    "section": "Querying the STAC API",
    "text": "Querying the STAC API\n\nimport requests\nfrom folium import Map, TileLayer\nfrom pystac_client import Client\n\n\n# Provide STAC and RASTER API endpoints\nSTAC_API_URL = \"http://ghg.center/api/stac\"\nRASTER_API_URL = \"https://ghg.center/api/raster\"\n\n# Please use the collection name similar to the one used in STAC collection.\n# Name of the collection for TM5 CH₄ inverse flux dataset. \ncollection_name = \"tm54dvar-ch4flux-monthgrid-v1\"\n\n\n# Fetching the collection from STAC collections using appropriate endpoint.\ncollection = requests.get(f\"{STAC_API_URL}/collections/{collection_name}\").json()\ncollection\n\nExamining the contents of our collection under the temporal variable, we see that the data is available from January 1999 to December 2016. By looking at the dashboard:time density, we observe that the data is periodic with monthly time density.\n\ndef get_item_count(collection_id):\n    count = 0\n    items_url = f\"{STAC_API_URL}/collections/{collection_id}/items\"\n\n    while True:\n        response = requests.get(items_url)\n\n        if not response.ok:\n            print(\"error getting items\")\n            exit()\n\n        stac = response.json()\n        count += int(stac[\"context\"].get(\"returned\", 0))\n        next = [link for link in stac[\"links\"] if link[\"rel\"] == \"next\"]\n\n        if not next:\n            break\n        items_url = next[0][\"href\"]\n\n    return count\n\n\n# Check total number of items available\nnumber_of_items = get_item_count(collection_name)\nitems = requests.get(f\"{STAC_API_URL}/collections/{collection_name}/items?limit={number_of_items}\").json()[\"features\"]\nprint(f\"Found {len(items)} items\")\n\n\n# Examining the first item in the collection\nitems[0]\n\nBelow, we are entering the minimum and maximum values to provide our upper and lower bounds in rescale_values.",
    "crumbs": [
      "Data Usage Notebooks",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "TM5-4DVar Isotopic CH₄ Inverse Fluxes"
    ]
  },
  {
    "objectID": "user_data_notebooks/tm54dvar-ch4flux-monthgrid-v1_User_Notebook.html#exploring-changes-in-ch₄-flux-levels-using-the-raster-api",
    "href": "user_data_notebooks/tm54dvar-ch4flux-monthgrid-v1_User_Notebook.html#exploring-changes-in-ch₄-flux-levels-using-the-raster-api",
    "title": "TM5-4DVar Isotopic CH₄ Inverse Fluxes",
    "section": "Exploring Changes in CH₄ flux Levels Using the Raster API",
    "text": "Exploring Changes in CH₄ flux Levels Using the Raster API\nIn this notebook, we will explore the global changes of CH₄ flux over time in urban regions. We will visualize the outputs on a map using folium.\n\n# to access the year value from each item more easily, this will let us query more explicity by year and month (e.g., 2020-02)\nitems = {item[\"properties\"][\"start_datetime\"][:10]: item for item in items} \nasset_name = \"fossil\" #fossil fuel\n\n\n# Fetching the min and max values for a specific item\nrescale_values = {\"max\":items[list(items.keys())[0]][\"assets\"][asset_name][\"raster:bands\"][0][\"histogram\"][\"max\"], \"min\":items[list(items.keys())[0]][\"assets\"][asset_name][\"raster:bands\"][0][\"histogram\"][\"min\"]}\n\nNow, we will pass the item id, collection name, and rescaling_factor to the Raster API endpoint. We will do this twice, once for 2020 and again for 2019, so that we can visualize each event independently.\n\ncolor_map = \"purd\"\nco2_flux_1 = requests.get(\n    f\"{RASTER_API_URL}/stac/tilejson.json?collection={items['2016-12-01']['collection']}&item={items['2016-12-01']['id']}\"\n    f\"&assets={asset_name}\"\n    f\"&color_formula=gamma+r+1.05&colormap_name={color_map}\"\n    f\"&rescale={rescale_values['min']},{rescale_values['max']}\", \n).json()\nco2_flux_1\n\n\nco2_flux_2 = requests.get(\n    f\"{RASTER_API_URL}/stac/tilejson.json?collection={items['1999-12-01']['collection']}&item={items['1999-12-01']['id']}\"\n    f\"&assets={asset_name}\"\n    f\"&color_formula=gamma+r+1.05&colormap_name={color_map}\"\n    f\"&rescale={rescale_values['min']},{rescale_values['max']}\", \n).json()\nco2_flux_2",
    "crumbs": [
      "Data Usage Notebooks",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "TM5-4DVar Isotopic CH₄ Inverse Fluxes"
    ]
  },
  {
    "objectID": "user_data_notebooks/tm54dvar-ch4flux-monthgrid-v1_User_Notebook.html#visualizing-ch₄-flux-emissions-from-fossil-fuel",
    "href": "user_data_notebooks/tm54dvar-ch4flux-monthgrid-v1_User_Notebook.html#visualizing-ch₄-flux-emissions-from-fossil-fuel",
    "title": "TM5-4DVar Isotopic CH₄ Inverse Fluxes",
    "section": "Visualizing CH₄ flux Emissions from Fossil Fuel",
    "text": "Visualizing CH₄ flux Emissions from Fossil Fuel\n\n# We'll import folium to map and folium.plugins to allow mapping side-by-side\nimport folium\nimport folium.plugins\n\n# Set initial zoom and center of map for CO₂ Layer\n# Centre of map [latitude,longitude]\nmap_ = folium.plugins.DualMap(location=(34, -118), zoom_start=6)\n\n\nmap_layer_2016 = TileLayer(\n    tiles=co2_flux_1[\"tiles\"][0],\n    attr=\"GHG\",\n    opacity=0.8,\n)\nmap_layer_2016.add_to(map_.m1)\n\nmap_layer_1999 = TileLayer(\n    tiles=co2_flux_2[\"tiles\"][0],\n    attr=\"GHG\",\n    opacity=0.8,\n)\nmap_layer_1999.add_to(map_.m2)\n\n# visualising the map\nmap_",
    "crumbs": [
      "Data Usage Notebooks",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "TM5-4DVar Isotopic CH₄ Inverse Fluxes"
    ]
  },
  {
    "objectID": "user_data_notebooks/tm54dvar-ch4flux-monthgrid-v1_User_Notebook.html#visualizing-the-data-as-a-time-series",
    "href": "user_data_notebooks/tm54dvar-ch4flux-monthgrid-v1_User_Notebook.html#visualizing-the-data-as-a-time-series",
    "title": "TM5-4DVar Isotopic CH₄ Inverse Fluxes",
    "section": "Visualizing the Data as a Time Series",
    "text": "Visualizing the Data as a Time Series\nWe can now explore the fossil fuel emission time series (January 1999 -December 2016) available for the Dallas, Texas area of the U.S. We can plot the data set using the code below:\n\nimport matplotlib.pyplot as plt\n\nfig = plt.figure(figsize=(20, 10))\n\n\nplt.plot(\n    df[\"datetime\"],\n    df[\"max\"],\n    color=\"red\",\n    linestyle=\"-\",\n    linewidth=0.5,\n    label=\"CH4 emissions\",\n)\n\nplt.legend()\nplt.xlabel(\"Years\")\nplt.ylabel(\"g CH₄/m²/year\")\nplt.xticks(rotation = 90)\nplt.title(\"CH4 emission Values for Texas, Dallas (2015-2020)\")\n\n\nprint(items[2][\"properties\"][\"start_datetime\"])\n\n\nco2_flux_3 = requests.get(\n    f\"{RASTER_API_URL}/stac/tilejson.json?collection={items[2]['collection']}&item={items[2]['id']}\"\n    f\"&assets={asset_name}\"\n    f\"&color_formula=gamma+r+1.05&colormap_name={color_map}\"\n    f\"&rescale={rescale_values['min']},{rescale_values['max']}\",\n).json()\nco2_flux_3\n\n\n# Use bbox initial zoom and map\n# Set up a map located w/in event bounds\nimport folium\n\naoi_map_bbox = Map(\n    tiles=\"OpenStreetMap\",\n    location=[\n        30,-100\n    ],\n    zoom_start=6.8,\n)\n\nmap_layer = TileLayer(\n    tiles=co2_flux_3[\"tiles\"][0],\n    attr=\"GHG\", opacity = 0.7\n)\n\nmap_layer.add_to(aoi_map_bbox)\n\naoi_map_bbox",
    "crumbs": [
      "Data Usage Notebooks",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "TM5-4DVar Isotopic CH₄ Inverse Fluxes"
    ]
  },
  {
    "objectID": "user_data_notebooks/tm54dvar-ch4flux-monthgrid-v1_User_Notebook.html#summary",
    "href": "user_data_notebooks/tm54dvar-ch4flux-monthgrid-v1_User_Notebook.html#summary",
    "title": "TM5-4DVar Isotopic CH₄ Inverse Fluxes",
    "section": "Summary",
    "text": "Summary\nIn this notebook we have successfully explored, analyzed, and visualized the STAC collection for TM5-4DVar Isotopic CH₄ Inverse Fluxes dataset.",
    "crumbs": [
      "Data Usage Notebooks",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "TM5-4DVar Isotopic CH₄ Inverse Fluxes"
    ]
  },
  {
    "objectID": "user_data_notebooks/emit-ch4plume-v1_User_Notebook.html",
    "href": "user_data_notebooks/emit-ch4plume-v1_User_Notebook.html",
    "title": "EMIT Methane Point Source Plume Complexes",
    "section": "",
    "text": "Identify available dates and temporal frequency of observations for the given collection using the GHGC API /stac endpoint. The collection processed in this notebook is the Earth Surface Mineral Dust Source Investigation (EMIT) methane emission plumes data product.\nPass the STAC item into the raster API /stac/tilejson.json endpoint.\nUsing folium.Map, visualize the plumes.\nAfter the visualization, perform zonal statistics for a given polygon.",
    "crumbs": [
      "Data Usage Notebooks",
      "Large Emissions Events",
      "EMIT Methane Point Source Plume Complexes"
    ]
  },
  {
    "objectID": "user_data_notebooks/emit-ch4plume-v1_User_Notebook.html#approach",
    "href": "user_data_notebooks/emit-ch4plume-v1_User_Notebook.html#approach",
    "title": "EMIT Methane Point Source Plume Complexes",
    "section": "",
    "text": "Identify available dates and temporal frequency of observations for the given collection using the GHGC API /stac endpoint. The collection processed in this notebook is the Earth Surface Mineral Dust Source Investigation (EMIT) methane emission plumes data product.\nPass the STAC item into the raster API /stac/tilejson.json endpoint.\nUsing folium.Map, visualize the plumes.\nAfter the visualization, perform zonal statistics for a given polygon.",
    "crumbs": [
      "Data Usage Notebooks",
      "Large Emissions Events",
      "EMIT Methane Point Source Plume Complexes"
    ]
  },
  {
    "objectID": "user_data_notebooks/emit-ch4plume-v1_User_Notebook.html#about-the-data",
    "href": "user_data_notebooks/emit-ch4plume-v1_User_Notebook.html#about-the-data",
    "title": "EMIT Methane Point Source Plume Complexes",
    "section": "About the Data",
    "text": "About the Data\nThe EMIT instrument builds upon NASA’s long history of developing advanced imaging spectrometers for new science and applications. EMIT launched to the International Space Station (ISS) on July 14, 2022. The data shows high-confidence research grade methane plumes from point source emitters - updated as they are identified - in keeping with JPL Open Science and Open Data policy.",
    "crumbs": [
      "Data Usage Notebooks",
      "Large Emissions Events",
      "EMIT Methane Point Source Plume Complexes"
    ]
  },
  {
    "objectID": "user_data_notebooks/emit-ch4plume-v1_User_Notebook.html#querying-the-stac-api",
    "href": "user_data_notebooks/emit-ch4plume-v1_User_Notebook.html#querying-the-stac-api",
    "title": "EMIT Methane Point Source Plume Complexes",
    "section": "Querying the STAC API",
    "text": "Querying the STAC API\n\nimport requests\nfrom folium import Map, TileLayer\nfrom pystac_client import Client\n\n\n# Provide STAC and RASTER API endpoints\nSTAC_API_URL = \"http://ghg.center/api/stac\"\nRASTER_API_URL = \"https://ghg.center/api/raster\"\n\n#Please use the collection name similar to the one used in STAC collection.\n\n# Name of the collection for methane emission plumes. \ncollection_name = \"emit-ch4plume-v1\"\n\n\n# Fetching the collection from STAC collections using appropriate endpoint.\ncollection = requests.get(f\"{STAC_API_URL}/collections/{collection_name}\").json()\ncollection\n\nExamining the contents of our collection under the temporal variable, we note that data is available from August 2022 to May 2023. By looking at the dashboard: time density, we can see that observations are conducted daily and non-periodically (i.e., there are plumes emissions for multiple places on the same dates).\n\ndef get_item_count(collection_id):\n    count = 0\n    items_url = f\"{STAC_API_URL}/collections/{collection_id}/items\"\n\n    while True:\n        response = requests.get(items_url)\n\n        if not response.ok:\n            print(\"error getting items\")\n            exit()\n\n        stac = response.json()\n        count += int(stac[\"context\"].get(\"returned\", 0))\n        next = [link for link in stac[\"links\"] if link[\"rel\"] == \"next\"]\n\n        if not next:\n            break\n        items_url = next[0][\"href\"]\n\n    return count\n\n\n# Check total number of items available\nnumber_of_items = get_item_count(collection_name)\nitems = requests.get(f\"{STAC_API_URL}/collections/{collection_name}/items?limit={number_of_items}\").json()[\"features\"]\nprint(f\"Found {len(items)} items\")\n\n\n# Examining the first item in the collection\nitems[0]\n\nBelow, we are entering the minimum and maximum values to provide our upper and lower bounds in rescale_values.",
    "crumbs": [
      "Data Usage Notebooks",
      "Large Emissions Events",
      "EMIT Methane Point Source Plume Complexes"
    ]
  },
  {
    "objectID": "user_data_notebooks/emit-ch4plume-v1_User_Notebook.html#exploring-methane-emission-plumes-ch₄-using-the-raster-api",
    "href": "user_data_notebooks/emit-ch4plume-v1_User_Notebook.html#exploring-methane-emission-plumes-ch₄-using-the-raster-api",
    "title": "EMIT Methane Point Source Plume Complexes",
    "section": "Exploring Methane Emission Plumes (CH₄) using the Raster API",
    "text": "Exploring Methane Emission Plumes (CH₄) using the Raster API\nIn this notebook, we will explore global methane emission plumes from point sources. We will visualize the outputs on a map using folium.\n\n# To access the year value from each item more easily, this will let us query more explicity by year and month (e.g., 2020-02)\nitems = {item[\"id\"][20:]: item for item in items} \nasset_name = \"ch4-plume-emissions\"\n\n\n# Fetching the min and max values for a specific item\nrescale_values = {\"max\":items[list(items.keys())[0]][\"assets\"][asset_name][\"raster:bands\"][0][\"histogram\"][\"max\"], \"min\":items[list(items.keys())[0]][\"assets\"][asset_name][\"raster:bands\"][0][\"histogram\"][\"min\"]}\n\nNow we will pass the item id, collection name, and rescaling_factor to the Raster API endpoint. We will do this for only one item so that we can visualize the event.\n\n# Select the item ID which you want to visualize. Item ID is in the format yyyymmdd followed by the timestamp. This ID can be extracted from the COG name as well.\nitem_id = \"20230418T200118_000829\"\ncolor_map = \"magma\"\nmethane_plume_tile = requests.get(\n    f\"{RASTER_API_URL}/stac/tilejson.json?collection={items[item_id]['collection']}&item={items[item_id]['id']}\"\n    f\"&assets={asset_name}\"\n    f\"&color_formula=gamma+r+1.05&colormap_name={color_map}\"\n    f\"&rescale={rescale_values['min']},{rescale_values['max']}\", \n).json()\nmethane_plume_tile",
    "crumbs": [
      "Data Usage Notebooks",
      "Large Emissions Events",
      "EMIT Methane Point Source Plume Complexes"
    ]
  },
  {
    "objectID": "user_data_notebooks/emit-ch4plume-v1_User_Notebook.html#visualizing-ch₄-emission-plume",
    "href": "user_data_notebooks/emit-ch4plume-v1_User_Notebook.html#visualizing-ch₄-emission-plume",
    "title": "EMIT Methane Point Source Plume Complexes",
    "section": "Visualizing CH₄ Emission Plume",
    "text": "Visualizing CH₄ Emission Plume\n\n# We will import folium to map and folium.plugins to allow side-by-side mapping\nimport folium\nimport folium.plugins\n\n# Set initial zoom and center of map for plume Layer\nmap_ = folium.Map(location=(methane_plume_tile[\"center\"][1], methane_plume_tile[\"center\"][0]), zoom_start=13)\n\n# December 2001\nmap_layer = TileLayer(\n    tiles=methane_plume_tile[\"tiles\"][0],\n    attr=\"GHG\",\n    opacity=1,\n)\nmap_layer.add_to(map_)\n\n# visualising the map\nmap_",
    "crumbs": [
      "Data Usage Notebooks",
      "Large Emissions Events",
      "EMIT Methane Point Source Plume Complexes"
    ]
  },
  {
    "objectID": "user_data_notebooks/emit-ch4plume-v1_User_Notebook.html#summary",
    "href": "user_data_notebooks/emit-ch4plume-v1_User_Notebook.html#summary",
    "title": "EMIT Methane Point Source Plume Complexes",
    "section": "Summary",
    "text": "Summary\nIn this notebook we have successfully explored, analyzed, and visualized the STAC collection for EMIT methane emission plumes.",
    "crumbs": [
      "Data Usage Notebooks",
      "Large Emissions Events",
      "EMIT Methane Point Source Plume Complexes"
    ]
  },
  {
    "objectID": "user_data_notebooks/oco2geos-co2-daygrid-v10r_User_Notebook.html",
    "href": "user_data_notebooks/oco2geos-co2-daygrid-v10r_User_Notebook.html",
    "title": "OCO-2 GEOS Column CO₂ Concentrations",
    "section": "",
    "text": "Identify available dates and temporal frequency of observations for the given collection using the GHGC API /stac endpoint. The collection processed in this notebook is the OCO-2 GEOS Column CO₂ Concentrations data product.\nPass the STAC item into the raster API /stac/tilejson.json endpoint.\nUsing folium.plugins.DualMap, visualize two tiles (side-by-side), allowing time point comparison.\nAfter the visualization, perform zonal statistics for a given polygon.",
    "crumbs": [
      "Data Usage Notebooks",
      "Greenhouse Gas Concentrations",
      "OCO-2 GEOS Column CO₂ Concentrations"
    ]
  },
  {
    "objectID": "user_data_notebooks/oco2geos-co2-daygrid-v10r_User_Notebook.html#approach",
    "href": "user_data_notebooks/oco2geos-co2-daygrid-v10r_User_Notebook.html#approach",
    "title": "OCO-2 GEOS Column CO₂ Concentrations",
    "section": "",
    "text": "Identify available dates and temporal frequency of observations for the given collection using the GHGC API /stac endpoint. The collection processed in this notebook is the OCO-2 GEOS Column CO₂ Concentrations data product.\nPass the STAC item into the raster API /stac/tilejson.json endpoint.\nUsing folium.plugins.DualMap, visualize two tiles (side-by-side), allowing time point comparison.\nAfter the visualization, perform zonal statistics for a given polygon.",
    "crumbs": [
      "Data Usage Notebooks",
      "Greenhouse Gas Concentrations",
      "OCO-2 GEOS Column CO₂ Concentrations"
    ]
  },
  {
    "objectID": "user_data_notebooks/oco2geos-co2-daygrid-v10r_User_Notebook.html#about-the-data",
    "href": "user_data_notebooks/oco2geos-co2-daygrid-v10r_User_Notebook.html#about-the-data",
    "title": "OCO-2 GEOS Column CO₂ Concentrations",
    "section": "About the Data",
    "text": "About the Data\nIn July 2014, NASA successfully launched the first dedicated Earth remote sensing satellite to study atmospheric carbon dioxide (CO₂) from space. The Orbiting Carbon Observatory-2 (OCO-2) is an exploratory science mission designed to collect space-based global measurements of atmospheric CO₂ with the precision, resolution, and coverage needed to characterize sources and sinks (fluxes) on regional scales (≥1000 km). This dataset provides global gridded, daily column-averaged carbon dioxide (XCO₂) concentrations from January 1, 2015 - February 28, 2022. The data are derived from OCO-2 observations that were input to the Goddard Earth Observing System (GEOS) Constituent Data Assimilation System (CoDAS), a modeling and data assimilation system maintained by NASA’s Global Modeling and Assimilation Office (GMAO). Concentrations are measured in moles of carbon dioxide per mole of dry air (mol CO₂/mol dry) at a spatial resolution of 0.5° x 0.625°. Data assimilation synthesizes simulations and observations, adjusting modeled atmospheric constituents like CO₂ to reflect observed values. With the support of NASA’s Carbon Monitoring System (CMS) Program and the OCO Science Team, this dataset was produced as part of the OCO-2 mission which provides the highest quality space-based XCO₂ retrievals to date.",
    "crumbs": [
      "Data Usage Notebooks",
      "Greenhouse Gas Concentrations",
      "OCO-2 GEOS Column CO₂ Concentrations"
    ]
  },
  {
    "objectID": "user_data_notebooks/oco2geos-co2-daygrid-v10r_User_Notebook.html#querying-the-stac-api",
    "href": "user_data_notebooks/oco2geos-co2-daygrid-v10r_User_Notebook.html#querying-the-stac-api",
    "title": "OCO-2 GEOS Column CO₂ Concentrations",
    "section": "Querying the STAC API",
    "text": "Querying the STAC API\n\n# Provide STAC and RASTER API endpoints\nSTAC_API_URL = \"http://ghg.center/api/stac\"\nRASTER_API_URL = \"https://ghg.center/api/raster\"\n\n# Please use the collection name similar to the one used in STAC collection.\n# Name of the collection for OCO-2 GEOS Column CO₂ Concentrations. \ncollection_name = \"oco2geos-co2-daygrid-v10r\"\n\n\n# Fetching the collection from STAC collections using appropriate endpoint.\ncollection = requests.get(f\"{STAC_API_URL}/collections/{collection_name}\").json()\ncollection\n\nExamining the contents of our collection under the temporal variable, we see that the data is available from January 2015 to February 2022. By looking at the dashboard:time density, we can see that these observations are collected daily.\n\ndef get_item_count(collection_id):\n    count = 0\n    items_url = f\"{STAC_API_URL}/collections/{collection_id}/items\"\n\n    while True:\n        response = requests.get(items_url)\n\n        if not response.ok:\n            print(\"error getting items\")\n            exit()\n\n        stac = response.json()\n        count += int(stac[\"context\"].get(\"returned\", 0))\n        next = [link for link in stac[\"links\"] if link[\"rel\"] == \"next\"]\n\n        if not next:\n            break\n        items_url = next[0][\"href\"]\n\n    return count\n\n\n# Check total number of items available\nnumber_of_items = get_item_count(collection_name)\nitems = requests.get(f\"{STAC_API_URL}/collections/{collection_name}/items?limit={number_of_items}\").json()[\"features\"]\nprint(f\"Found {len(items)} items\")\n\n\n# Examining the first item in the collection\nitems[0]\n\nBelow, we enter minimum and maximum values to provide our upper and lower bounds in rescale_values.",
    "crumbs": [
      "Data Usage Notebooks",
      "Greenhouse Gas Concentrations",
      "OCO-2 GEOS Column CO₂ Concentrations"
    ]
  },
  {
    "objectID": "user_data_notebooks/oco2geos-co2-daygrid-v10r_User_Notebook.html#exploring-changes-in-column-averaged-xco₂-concentrations-levels-using-the-raster-api",
    "href": "user_data_notebooks/oco2geos-co2-daygrid-v10r_User_Notebook.html#exploring-changes-in-column-averaged-xco₂-concentrations-levels-using-the-raster-api",
    "title": "OCO-2 GEOS Column CO₂ Concentrations",
    "section": "Exploring Changes in Column-Averaged XCO₂ Concentrations Levels Using the Raster API",
    "text": "Exploring Changes in Column-Averaged XCO₂ Concentrations Levels Using the Raster API\nIn this notebook, we will explore the temporal impacts of CO₂ emissions. We will visualize the outputs on a map using folium.\n\n# To access the year value from each item more easily, this will let us query more explicitly by year and month (e.g., 2020-02)\nitems = {item[\"properties\"][\"datetime\"]: item for item in items} \nasset_name = \"xco2\" #fossil fuel\n\n\n# Fetching the min and max values for a specific item\nrescale_values = {\"max\":items[list(items.keys())[0]][\"assets\"][asset_name][\"raster:bands\"][0][\"histogram\"][\"max\"], \"min\":items[list(items.keys())[0]][\"assets\"][asset_name][\"raster:bands\"][0][\"histogram\"][\"min\"]}\n\nNow, we will pass the item id, collection name, and rescaling_factor to the Raster API endpoint. We will do this twice, once for 2022-02-08 and again for 2022-01-27, so that we can visualize each event independently.\n\ncolor_map = \"magma\"\noco2_1 = requests.get(\n    f\"{RASTER_API_URL}/stac/tilejson.json?collection={items[list(items.keys())[0]]['collection']}&item={items[list(items.keys())[0]]['id']}\"\n    f\"&assets={asset_name}\"\n    f\"&color_formula=gamma+r+1.05&colormap_name={color_map}\"\n    f\"&rescale={rescale_values['min']},{rescale_values['max']}\", \n).json()\noco2_1\n\n\noco2_2 = requests.get(\n    f\"{RASTER_API_URL}/stac/tilejson.json?collection={items[list(items.keys())[1]]['collection']}&item={items[list(items.keys())[1]]['id']}\"\n    f\"&assets={asset_name}\"\n    f\"&color_formula=gamma+r+1.05&colormap_name={color_map}\"\n    f\"&rescale={rescale_values['min']},{rescale_values['max']}\", \n).json()\noco2_2",
    "crumbs": [
      "Data Usage Notebooks",
      "Greenhouse Gas Concentrations",
      "OCO-2 GEOS Column CO₂ Concentrations"
    ]
  },
  {
    "objectID": "user_data_notebooks/oco2geos-co2-daygrid-v10r_User_Notebook.html#visualizing-daily-column-averaged-xco₂-concentrations",
    "href": "user_data_notebooks/oco2geos-co2-daygrid-v10r_User_Notebook.html#visualizing-daily-column-averaged-xco₂-concentrations",
    "title": "OCO-2 GEOS Column CO₂ Concentrations",
    "section": "Visualizing Daily Column-Averaged XCO₂ Concentrations",
    "text": "Visualizing Daily Column-Averaged XCO₂ Concentrations\n\n# We will import folium to map and folium.plugins to allow mapping side-by-side\nimport folium\nimport folium.plugins\n\n# Set initial zoom and center of map for XCO₂ Layer\n# Centre of map [latitude,longitude]\nmap_ = folium.plugins.DualMap(location=(34, -118), zoom_start=6)\n\n\nmap_layer_2020 = TileLayer(\n    tiles=oco2_1[\"tiles\"][0],\n    attr=\"GHG\",\n    opacity=0.5,\n)\nmap_layer_2020.add_to(map_.m1)\n\nmap_layer_2019 = TileLayer(\n    tiles=oco2_2[\"tiles\"][0],\n    attr=\"GHG\",\n    opacity=0.5,\n)\nmap_layer_2019.add_to(map_.m2)\n\n# visualising the map\nmap_",
    "crumbs": [
      "Data Usage Notebooks",
      "Greenhouse Gas Concentrations",
      "OCO-2 GEOS Column CO₂ Concentrations"
    ]
  },
  {
    "objectID": "user_data_notebooks/oco2geos-co2-daygrid-v10r_User_Notebook.html#visualizing-the-data-as-a-time-series",
    "href": "user_data_notebooks/oco2geos-co2-daygrid-v10r_User_Notebook.html#visualizing-the-data-as-a-time-series",
    "title": "OCO-2 GEOS Column CO₂ Concentrations",
    "section": "Visualizing the Data as a Time Series",
    "text": "Visualizing the Data as a Time Series\nWe can now explore the XCO₂ concentrations time series (January 1, 2015 - February 28, 2022) available for the Dallas, Texas area of the U.S. We can plot the data set using the code below:\n\nimport matplotlib.pyplot as plt\n\nfig = plt.figure(figsize=(20, 10))\n\n\nplt.plot(\n    df[\"datetime\"],\n    df[\"max\"],\n    color=\"red\",\n    linestyle=\"-\",\n    linewidth=0.5,\n    label=\"CO₂ concentrations\",\n)\n\nplt.legend()\nplt.xlabel(\"Years\")\nplt.ylabel(\"CO2 concentrations ppm\")\nplt.title(\"CO₂ concentrations Values for Texas, Dallas (Jan 2015- Feb 2022)\")\n\n\nprint(items[2][\"properties\"][\"datetime\"])\n\n\noco2_3 = requests.get(\n    f\"{RASTER_API_URL}/stac/tilejson.json?collection={items[2]['collection']}&item={items[2]['id']}\"\n    f\"&assets={asset_name}\"\n    f\"&color_formula=gamma+r+1.05&colormap_name={color_map}\"\n    f\"&rescale={rescale_values['min']},{rescale_values['max']}\",\n).json()\noco2_3\n\n\n# Use bbox initial zoom and map\n# Set up a map located w/in event bounds\nimport folium\n\naoi_map_bbox = Map(\n    tiles=\"OpenStreetMap\",\n    location=[\n        30,-100\n    ],\n    zoom_start=6.8,\n)\n\nmap_layer = TileLayer(\n    tiles=oco2_3[\"tiles\"][0],\n    attr=\"GHG\", opacity = 0.7\n)\n\nmap_layer.add_to(aoi_map_bbox)\n\naoi_map_bbox",
    "crumbs": [
      "Data Usage Notebooks",
      "Greenhouse Gas Concentrations",
      "OCO-2 GEOS Column CO₂ Concentrations"
    ]
  },
  {
    "objectID": "user_data_notebooks/oco2geos-co2-daygrid-v10r_User_Notebook.html#summary",
    "href": "user_data_notebooks/oco2geos-co2-daygrid-v10r_User_Notebook.html#summary",
    "title": "OCO-2 GEOS Column CO₂ Concentrations",
    "section": "Summary",
    "text": "Summary\nIn this notebook, we have successfully explored, analyzed, and visualized the STAC collection for OCO-2 GEOS Column CO₂ Concentrations.",
    "crumbs": [
      "Data Usage Notebooks",
      "Greenhouse Gas Concentrations",
      "OCO-2 GEOS Column CO₂ Concentrations"
    ]
  },
  {
    "objectID": "user_data_notebooks/epa-ch4emission-grid-v2express_User_Notebook.html",
    "href": "user_data_notebooks/epa-ch4emission-grid-v2express_User_Notebook.html",
    "title": "U.S. Gridded Anthropogenic Methane Emissions Inventory",
    "section": "",
    "text": "Identify available dates and temporal frequency of observations for the given collection using the GHGC API /stac endpoint. The collection processed in this notebook is the gridded methane emissions data product.\nPass the STAC item into the raster API /stac/tilejson.jsonendpoint.\nUsing folium.plugins.DualMap, we will visualize two tiles (side-by-side), allowing us to compare time points.\nAfter the visualization, we will perform zonal statistics for a given polygon.",
    "crumbs": [
      "Data Usage Notebooks",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "U.S. Gridded Anthropogenic Methane Emissions Inventory"
    ]
  },
  {
    "objectID": "user_data_notebooks/epa-ch4emission-grid-v2express_User_Notebook.html#approach",
    "href": "user_data_notebooks/epa-ch4emission-grid-v2express_User_Notebook.html#approach",
    "title": "U.S. Gridded Anthropogenic Methane Emissions Inventory",
    "section": "",
    "text": "Identify available dates and temporal frequency of observations for the given collection using the GHGC API /stac endpoint. The collection processed in this notebook is the gridded methane emissions data product.\nPass the STAC item into the raster API /stac/tilejson.jsonendpoint.\nUsing folium.plugins.DualMap, we will visualize two tiles (side-by-side), allowing us to compare time points.\nAfter the visualization, we will perform zonal statistics for a given polygon.",
    "crumbs": [
      "Data Usage Notebooks",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "U.S. Gridded Anthropogenic Methane Emissions Inventory"
    ]
  },
  {
    "objectID": "user_data_notebooks/epa-ch4emission-grid-v2express_User_Notebook.html#about-the-data",
    "href": "user_data_notebooks/epa-ch4emission-grid-v2express_User_Notebook.html#about-the-data",
    "title": "U.S. Gridded Anthropogenic Methane Emissions Inventory",
    "section": "About the Data",
    "text": "About the Data\nThe gridded EPA U.S. anthropogenic methane greenhouse gas inventory (gridded GHGI) includes spatially disaggregated (0.1 deg x 0.1 deg or approximately 10 x 10 km resolution) maps of annual anthropogenic methane emissions (for the contiguous United States (CONUS), consistent with national annual U.S. anthropogenic methane emissions reported in the U.S. EPA Inventory of U.S. Greenhouse Gas Emissions and Sinks (U.S. GHGI). This V2 Express Extension dataset contains methane emissions provided as fluxes, in units of molecules of methane per square cm per second, for over 25 individual emission source categories, including those from agriculture, petroleum and natural gas systems, coal mining, and waste. The data have been converted from their original NetCDF format to Cloud-Optimized GeoTIFF (COG) for use in the US GHG Center, thereby enabling user exploration of spatial anthropogenic methane emissions and their trends.",
    "crumbs": [
      "Data Usage Notebooks",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "U.S. Gridded Anthropogenic Methane Emissions Inventory"
    ]
  },
  {
    "objectID": "user_data_notebooks/epa-ch4emission-grid-v2express_User_Notebook.html#querying-the-stac-api",
    "href": "user_data_notebooks/epa-ch4emission-grid-v2express_User_Notebook.html#querying-the-stac-api",
    "title": "U.S. Gridded Anthropogenic Methane Emissions Inventory",
    "section": "Querying the STAC API",
    "text": "Querying the STAC API\n\nimport requests\nfrom folium import Map, TileLayer\nfrom pystac_client import Client\n\n\n# Provide STAC and RASTER API endpoints\nSTAC_API_URL = \"http://ghg.center/api/stac\"\nRASTER_API_URL = \"https://ghg.center/api/raster\"\n\n# Please use the collection name similar to the one used in STAC collection.\n\n# Name of the collection for gridded methane dataset. \ncollection_name = \"epa-ch4emission-yeargrid-v2\"\n\n\n# Fetching the collection from STAC collections using appropriate endpoint.\ncollection = requests.get(f\"{STAC_API_URL}/collections/{collection_name}\").json()\ncollection\n\nExamining the contents of our collection under the temporal variable, we see that the data is available from January 2012 to December 2020. By looking at the dashboard:time density, we observe that the periodic frequency of these observations is yearly.\n\ndef get_item_count(collection_id):\n    count = 0\n    items_url = f\"{STAC_API_URL}/collections/{collection_id}/items\"\n\n    while True:\n        response = requests.get(items_url)\n\n        if not response.ok:\n            print(\"error getting items\")\n            exit()\n\n        stac = response.json()\n        count += int(stac[\"context\"].get(\"returned\", 0))\n        next = [link for link in stac[\"links\"] if link[\"rel\"] == \"next\"]\n\n        if not next:\n            break\n        items_url = next[0][\"href\"]\n\n    return count\n\n\n# Check total number of items available\nnumber_of_items = get_item_count(collection_name)\nitems = requests.get(f\"{STAC_API_URL}/collections/{collection_name}/items?limit={number_of_items}\").json()[\"features\"]\nprint(f\"Found {len(items)} items\")\n\n\n# Examining the first item in the collection\nitems[0]\n\nThis makes sense as there are 9 years between 2012 - 2020, meaning 9 records in total.\nBelow, we enter minimum and maximum values to provide our upper and lower bounds in rescale_values.",
    "crumbs": [
      "Data Usage Notebooks",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "U.S. Gridded Anthropogenic Methane Emissions Inventory"
    ]
  },
  {
    "objectID": "user_data_notebooks/epa-ch4emission-grid-v2express_User_Notebook.html#exploring-changes-in-methane-ch4-levels-using-the-raster-api",
    "href": "user_data_notebooks/epa-ch4emission-grid-v2express_User_Notebook.html#exploring-changes-in-methane-ch4-levels-using-the-raster-api",
    "title": "U.S. Gridded Anthropogenic Methane Emissions Inventory",
    "section": "Exploring Changes in Methane (CH4) Levels Using the Raster API",
    "text": "Exploring Changes in Methane (CH4) Levels Using the Raster API\nIn this notebook, we will explore the impacts of methane emissions and by examining changes over time in urban regions. We will visualize the outputs on a map using folium.\n\n# To access the year value from each item more easily, this will let us query more explicity by year and month (e.g., 2020-02)\nitems = {item[\"properties\"][\"datetime\"][:7]: item for item in items} \nasset_name = \"surface-coal\"\n\n\n# Fetching the min and max values for a specific item\nrescale_values = {\"max\":items[list(items.keys())[0]][\"assets\"][asset_name][\"raster:bands\"][0][\"histogram\"][\"max\"], \"min\":items[list(items.keys())[0]][\"assets\"][asset_name][\"raster:bands\"][0][\"histogram\"][\"min\"]}\n\n\nitems\n\nNow, we will pass the item id, collection name, and rescaling_factor to the Raster API endpoint. We will do this twice, once for January 2018 and again for January 2012, so that we can visualize each event independently.\n\ncolor_map = \"rainbow\" # please select the color ramp from matplotlib library.\njanuary_2018_tile = requests.get(\n    f\"{RASTER_API_URL}/stac/tilejson.json?collection={items['2018-01']['collection']}&item={items['2018-01']['id']}\"\n    f\"&assets={asset_name}\"\n    f\"&color_formula=gamma+r+1.05&colormap_name={color_map}\"\n    f\"&rescale={rescale_values['min']},{rescale_values['max']}\", \n).json()\njanuary_2018_tile\n\n\njanuary_2012_tile = requests.get(\n    f\"{RASTER_API_URL}/stac/tilejson.json?collection={items['2012-01']['collection']}&item={items['2012-01']['id']}\"\n    f\"&assets={asset_name}\"\n    f\"&color_formula=gamma+r+1.05&colormap_name={color_map}\"\n    f\"&rescale={rescale_values['min']},{rescale_values['max']}\", \n).json()\njanuary_2012_tile",
    "crumbs": [
      "Data Usage Notebooks",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "U.S. Gridded Anthropogenic Methane Emissions Inventory"
    ]
  },
  {
    "objectID": "user_data_notebooks/epa-ch4emission-grid-v2express_User_Notebook.html#visualizing-ch₄-emissions",
    "href": "user_data_notebooks/epa-ch4emission-grid-v2express_User_Notebook.html#visualizing-ch₄-emissions",
    "title": "U.S. Gridded Anthropogenic Methane Emissions Inventory",
    "section": "Visualizing CH₄ emissions",
    "text": "Visualizing CH₄ emissions\n\n# We will import folium to map and folium.plugins to allow side-by-side mapping\nimport folium\nimport folium.plugins\n\n# Set initial zoom and center of map for CH₄ Layer\n# Centre of map [latitude,longitude]\nmap_ = folium.plugins.DualMap(location=(34, -118), zoom_start=6)\n\n# January 2018\nmap_layer_2018 = TileLayer(\n    tiles=january_2018_tile[\"tiles\"][0],\n    attr=\"GHG\",\n    opacity=0.7,\n)\nmap_layer_2018.add_to(map_.m1)\n\n# January 2012\nmap_layer_2012 = TileLayer(\n    tiles=january_2012_tile[\"tiles\"][0],\n    attr=\"GHG\",\n    opacity=0.7,\n)\nmap_layer_2012.add_to(map_.m2)\n\n# visualising the map\nmap_",
    "crumbs": [
      "Data Usage Notebooks",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "U.S. Gridded Anthropogenic Methane Emissions Inventory"
    ]
  },
  {
    "objectID": "user_data_notebooks/epa-ch4emission-grid-v2express_User_Notebook.html#visualizing-the-data-as-a-time-series",
    "href": "user_data_notebooks/epa-ch4emission-grid-v2express_User_Notebook.html#visualizing-the-data-as-a-time-series",
    "title": "U.S. Gridded Anthropogenic Methane Emissions Inventory",
    "section": "Visualizing the Data as a Time Series",
    "text": "Visualizing the Data as a Time Series\nWe can now explore the gridded methane emission (Domestic Wastewater Treatment & Discharge (5D)) time series (January 2000 -December 2021) available for the Dallas, Texas area of the U.S. We can plot the data set using the code below:\n\nimport matplotlib.pyplot as plt\n\nfig = plt.figure(figsize=(20, 10))\n\n\nplt.plot(\n    df[\"date\"],\n    df[\"max\"],\n    color=\"red\",\n    linestyle=\"-\",\n    linewidth=0.5,\n    label=\"Max monthly CO₂ emissions\",\n)\n\nplt.legend()\nplt.xlabel(\"Years\")\nplt.ylabel(\"CH4 emissions Molecules CH₄/cm²/s\")\nplt.title(\"CH4 gridded methane emission from Domestic Wastewater Treatment & Discharge (5D) for Texas, Dallas (2012-202)\")\n\n\nprint(items[2][\"properties\"][\"datetime\"])\n\n\ntile_2016 = requests.get(\n    f\"{RASTER_API_URL}/stac/tilejson.json?collection={items[2]['collection']}&item={items[2]['id']}\"\n    f\"&assets={asset_name}\"\n    f\"&color_formula=gamma+r+1.05&colormap_name={color_map}\"\n    f\"&rescale={rescale_values['min']},{rescale_values['max']}\",\n).json()\ntile_2016\n\n\n# Use bbox initial zoom and map\n# Set up a map located w/in event bounds\nimport folium\n\naoi_map_bbox = Map(\n    tiles=\"OpenStreetMap\",\n    location=[\n        30,-100\n    ],\n    zoom_start=8,\n)\n\nmap_layer = TileLayer(\n    tiles=tile_2016[\"tiles\"][0],\n    attr=\"GHG\", opacity = 0.5\n)\n\nmap_layer.add_to(aoi_map_bbox)\n\naoi_map_bbox",
    "crumbs": [
      "Data Usage Notebooks",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "U.S. Gridded Anthropogenic Methane Emissions Inventory"
    ]
  },
  {
    "objectID": "user_data_notebooks/epa-ch4emission-grid-v2express_User_Notebook.html#summary",
    "href": "user_data_notebooks/epa-ch4emission-grid-v2express_User_Notebook.html#summary",
    "title": "U.S. Gridded Anthropogenic Methane Emissions Inventory",
    "section": "Summary",
    "text": "Summary\nIn this notebook we have successfully explored, analyzed, and visualized the STAC collection for gridded methane emissions.",
    "crumbs": [
      "Data Usage Notebooks",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "U.S. Gridded Anthropogenic Methane Emissions Inventory"
    ]
  },
  {
    "objectID": "user_data_notebooks/lpjwsl-wetlandch4-grid-v1_User_Notebook.html",
    "href": "user_data_notebooks/lpjwsl-wetlandch4-grid-v1_User_Notebook.html",
    "title": "Wetland Methane Emissions, LPJ-EOSIM Model",
    "section": "",
    "text": "Identify available dates and temporal frequency of observations for the given collection using the GHGC API /stac endpoint. The collection processed in this notebook is the Wetland Methane Emissions, LPJ-EOSIM Model data product.\nPass the STAC item into the raster API /stac/tilejson.jsonendpoint.\nUsing folium.plugins.DualMap, visualize two tiles (side-by-side), allowing time point comparison.\nAfter the visualization, perform zonal statistics for a given polygon.",
    "crumbs": [
      "Data Usage Notebooks",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "Wetland Methane Emissions, LPJ-EOSIM Model"
    ]
  },
  {
    "objectID": "user_data_notebooks/lpjwsl-wetlandch4-grid-v1_User_Notebook.html#approach",
    "href": "user_data_notebooks/lpjwsl-wetlandch4-grid-v1_User_Notebook.html#approach",
    "title": "Wetland Methane Emissions, LPJ-EOSIM Model",
    "section": "",
    "text": "Identify available dates and temporal frequency of observations for the given collection using the GHGC API /stac endpoint. The collection processed in this notebook is the Wetland Methane Emissions, LPJ-EOSIM Model data product.\nPass the STAC item into the raster API /stac/tilejson.jsonendpoint.\nUsing folium.plugins.DualMap, visualize two tiles (side-by-side), allowing time point comparison.\nAfter the visualization, perform zonal statistics for a given polygon.",
    "crumbs": [
      "Data Usage Notebooks",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "Wetland Methane Emissions, LPJ-EOSIM Model"
    ]
  },
  {
    "objectID": "user_data_notebooks/lpjwsl-wetlandch4-grid-v1_User_Notebook.html#about-the-data",
    "href": "user_data_notebooks/lpjwsl-wetlandch4-grid-v1_User_Notebook.html#about-the-data",
    "title": "Wetland Methane Emissions, LPJ-EOSIM Model",
    "section": "About the Data",
    "text": "About the Data\nMethane (CH₄) emissions from vegetated wetlands are estimated to be the largest natural source of methane in the global CH₄ budget, contributing to roughly one third of the total of natural and anthropogenic emissions. Wetland CH₄ is produced by microbes breaking down organic matter in the oxygen deprived environment of inundated soils. Due to limited data availability, the details of the role of wetland CH₄ emissions have thus far been underrepresented. Using the Earth Observation SIMulator version (LPJ-EOSIM) of the Lund-Potsdam-Jena Dynamic Global Vegetation Model (LPJ-DGVM) global CH₄ emissions from wetlands are estimated at 0.5° x 0.5 degree spatial resolution. By simulating wetland extent and using characteristics of inundated areas, such as wetland soil moisture, temperature, and carbon content, the model provides estimates of CH₄ quantities emitted into the atmosphere. This dataset shows concentrated methane sources from tropical and high latitude ecosystems. The LPJ-EOSIM Wetland Methane Emissions dataset consists of global daily model estimates of terrestrial wetland methane emissions from 1990 to the present, with data added bimonthly. The estimates are regularly used in conjunction with NASA’s Goddard Earth Observing System (GEOS) model to simulate the impact of wetlands and other methane sources on atmospheric methane concentrations, to compare against satellite and airborne data, and to improve understanding and prediction of wetland emissions.\nFor more information regarding this dataset, please visit the U.S. Greenhouse Gas Center.",
    "crumbs": [
      "Data Usage Notebooks",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "Wetland Methane Emissions, LPJ-EOSIM Model"
    ]
  },
  {
    "objectID": "user_data_notebooks/lpjwsl-wetlandch4-grid-v1_User_Notebook.html#query-the-stac-api",
    "href": "user_data_notebooks/lpjwsl-wetlandch4-grid-v1_User_Notebook.html#query-the-stac-api",
    "title": "Wetland Methane Emissions, LPJ-EOSIM Model",
    "section": "Query the STAC API",
    "text": "Query the STAC API\nFirst, we are going to import the required libraries. Once imported, they allow better executing a query in the GHG Center Spatio Temporal Asset Catalog (STAC) Application Programming Interface (API) where the granules for this collection are stored.\n\n# Import the following libraries\nimport requests\nimport folium\nimport folium.plugins\nfrom folium import Map, TileLayer\nfrom pystac_client import Client\nimport branca\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n\n# Provide the STAC and RASTER API endpoints\n# The endpoint is referring to a location within the API that executes a request on a data collection nesting on the server.\n\n# The STAC API is a catalog of all the existing data collections that are stored in the GHG Center.\nSTAC_API_URL = \"http://dev.ghg.center/ghgcenter/api/stac\"\n\n# The RASTER API is used to fetch collections for visualization\nRASTER_API_URL = \"https://dev.ghg.center/ghgcenter/api/raster\"\n\n# The collection name is used to fetch the dataset from the STAC API. First, we define the collection name as a variable\n# Name of the collection for the wetland methane emissions LPJ-EOSIM Model\ncollection_name = \"lpjeosim-wetlandch4-daygrid-v2\"\n\n# Next, we need to specify the asset name for this collection\n# The asset name is referring to the raster band containing the pixel values for the parameter of interest\nasset_name = \"ensemble-mean-ch4-wetlands-emissions\"\n\n\n# Fetch the collection from the STAC API using the appropriate endpoint\n# The 'requests' library allows a HTTP request possible\ncollection = requests.get(f\"{STAC_API_URL}/collections/{collection_name}\").json()\n\n# Print the properties of the collection to the console\ncollection\n\n{'id': 'lpjeosim-wetlandch4-daygrid-v2',\n 'type': 'Collection',\n 'links': [{'rel': 'items',\n   'type': 'application/geo+json',\n   'href': 'https://dev.ghg.center/ghgcenter/api/stac/collections/lpjeosim-wetlandch4-daygrid-v2/items'},\n  {'rel': 'parent',\n   'type': 'application/json',\n   'href': 'https://dev.ghg.center/ghgcenter/api/stac/'},\n  {'rel': 'root',\n   'type': 'application/json',\n   'href': 'https://dev.ghg.center/ghgcenter/api/stac/'},\n  {'rel': 'self',\n   'type': 'application/json',\n   'href': 'https://dev.ghg.center/ghgcenter/api/stac/collections/lpjeosim-wetlandch4-daygrid-v2'}],\n 'title': 'Wetland Methane Emissions, LPJ-EOSIM Model v2',\n 'assets': None,\n 'extent': {'spatial': {'bbox': [[-180, -90, 180, 90]]},\n  'temporal': {'interval': [['1990-01-01T00:00:00+00:00',\n     '1990-02-21T00:00:00+00:00']]}},\n 'license': 'CC0 1.0',\n 'keywords': None,\n 'providers': [{'url': None,\n   'name': 'NASA',\n   'roles': None,\n   'description': None}],\n 'summaries': {'datetime': ['1990-01-01T00:00:00Z', '1990-02-21T00:00:00Z']},\n 'description': 'Global, daily estimates of methane (CH4) emissions from terrestrial wetlands at 0.5 x 0.5 degree spatial resolution using the Earth Observation SIMulator version (LPJ-EOSIM) of the Lund-Potsdam-Jena Dynamic Global Vegetation Model (LPJ-DGVM). Methane emissions from vegetated wetlands are estimated to be the largest natural source of methane in the global CH4 budget, contributing to roughly one third of the total of natural and anthropogenic emissions. Wetland CH4 is produced by microbes breaking down organic matter in the oxygen deprived environment of inundated soils. Due to limited data availability, the details of the role of wetland CH4 emissions have thus far been underrepresented. The LPJ-EOSIM model estimates wetland methane emissions by simulating wetland extent and using characteristics of these inundated areas such as soil moisture, temperature, and carbon content to estimate CH4 quantities emitted into the atmosphere. Input climate forcing data comes from Modern-Era Retrospective analysis for Research and Applications Version 2 (MERRA-2) data and ECMWF Re-Analysis data (ERA5). An ensemble layer provides the result of the mean of the MERRA-2 and ERA5 layers.',\n 'item_assets': {'era5-ch4-wetlands-emissions': {'type': 'image/tiff; application=geotiff; profile=cloud-optimized',\n   'roles': ['data', 'layer'],\n   'title': 'Wetland Methane Emissions, ERA5 LPJ-EOSIM Model v2',\n   'description': 'Methane emissions from wetlands in units of grams of methane per meter squared per day. ECMWF Re-Analysis (ERA5) as input to LPJ-EOSIM model.'},\n  'merra2-ch4-wetlands-emissions': {'type': 'image/tiff; application=geotiff; profile=cloud-optimized',\n   'roles': ['data', 'layer'],\n   'title': 'Wetland Methane Emissions, MERRA-2 LPJ-EOSIM Model v2',\n   'description': 'Methane emissions from wetlands in units of grams of methane per meter squared per day. Modern-Era Retrospective analysis for Research and Applications Version 2 (MERRA-2) data as input to LPJ-EOSIM model.'},\n  'ensemble-mean-ch4-wetlands-emissions': {'type': 'image/tiff; application=geotiff; profile=cloud-optimized',\n   'roles': ['data', 'layer'],\n   'title': 'Wetland Methane Emissions, Ensemble Mean LPJ-EOSIM Model v2',\n   'description': 'Methane emissions from wetlands in units of grams of methane per meter squared per day. Ensemble of multiple climate forcing data sources input to LPJ-EOSIM model.'}},\n 'stac_version': '1.0.0',\n 'stac_extensions': None,\n 'dashboard:is_periodic': True,\n 'dashboard:time_density': 'day'}\n\n\nExamining the contents of our collection under summaries, we see that the data is available from January 1980 to December 2021. By looking at dashboard: time density, we can see that these observations are collected monthly.\n\n# Create a function that would search for a data collection in the US GHG Center STAC API\n\n# First, we need to define the function\n# The name of the function = \"get_item_count\"\n# The argument that will be passed through the defined function = \"collection_id\"\n\ndef get_item_count(collection_id):\n\n    # Set a counter for the number of items existing in the collection\n    count = 0\n\n    # Define the path to retrieve the granules (items) of the collection of interest in the STAC API\n    items_url = f\"{STAC_API_URL}/collections/{collection_id}/items\"\n\n    # Run a while loop to make HTTP requests until there are no more URLs associated with the collection in the STAC API\n    while True:\n\n        # Retrieve information about the granules by sending a \"get\" request to the STAC API using the defined collection path\n        response = requests.get(items_url)\n\n        # If the items do not exist, print an error message and quit the loop\n        if not response.ok:\n            print(\"error getting items\")\n            exit()\n\n        # Return the results of the HTTP response as JSON\n        stac = response.json()\n\n        # Increase the \"count\" by the number of items (granules) returned in the response\n        count += int(stac[\"context\"].get(\"returned\", 0))\n\n        # Retrieve information about the next URL associated with the collection in the STAC API (if applicable)\n        next = [link for link in stac[\"links\"] if link[\"rel\"] == \"next\"]\n\n        # Exit the loop if there are no other URLs\n        if not next:\n            break\n        \n        # Ensure the information gathered by other STAC API links associated with the collection are added to the original path\n        # \"href\" is the identifier for each of the tiles stored in the STAC API\n        items_url = next[0][\"href\"]\n        temp = items_url.split('/')\n        temp.insert(3, 'ghgcenter')\n        temp.insert(4, 'api')\n        temp.insert(5, 'stac')\n        items_url = '/'.join(temp)\n\n    # Return the information about the total number of granules found associated with the collection\n    return count\n\n\n# Apply the function created above \"get_item_count\" to the data collection\nnumber_of_items = get_item_count(collection_name)\n\n# Get the information about the number of granules found in the collection\nitems = requests.get(f\"{STAC_API_URL}/collections/{collection_name}/items?limit=600\"\n).json()[\"features\"]\n\n# Print the total number of items (granules) found\nprint(f\"Found {len(items)} items\")\n\nFound 52 items\n\n\n\n# Examine the first item in the collection\n# Keep in mind that a list starts from 0, 1, 2... therefore items[0] is referring to the first item in the list/collection\nitems[0]\n\n{'id': 'lpjeosim-wetlandch4-daygrid-v2-19900221day',\n 'bbox': [-180.0, -90.0, 180.0, 90.0],\n 'type': 'Feature',\n 'links': [{'rel': 'collection',\n   'type': 'application/json',\n   'href': 'https://dev.ghg.center/ghgcenter/api/stac/collections/lpjeosim-wetlandch4-daygrid-v2'},\n  {'rel': 'parent',\n   'type': 'application/json',\n   'href': 'https://dev.ghg.center/ghgcenter/api/stac/collections/lpjeosim-wetlandch4-daygrid-v2'},\n  {'rel': 'root',\n   'type': 'application/json',\n   'href': 'https://dev.ghg.center/ghgcenter/api/stac/'},\n  {'rel': 'self',\n   'type': 'application/geo+json',\n   'href': 'https://dev.ghg.center/ghgcenter/api/stac/collections/lpjeosim-wetlandch4-daygrid-v2/items/lpjeosim-wetlandch4-daygrid-v2-19900221day'}],\n 'assets': {'era5-ch4-wetlands-emissions': {'href': 's3://ghgc-data-store-dev/lpjwsl-wetlandch4-daygrid-v2-new-units/ERA5/LPJ_EOSIM_L2_CH4e_dch4e_ERA5_19900221day.tif',\n   'type': 'image/tiff; application=geotiff; profile=cloud-optimized',\n   'roles': ['data', 'layer'],\n   'title': 'Wetland Methane Emissions, ERA5 LPJ-EOSIM Model v2',\n   'proj:bbox': [-180.0, 90.0, 180.0, -90.0],\n   'proj:epsg': 4326.0,\n   'proj:shape': [360.0, 720.0],\n   'description': 'Methane emissions from wetlands in units of grams of methane per meter squared per day. ECMWF Re-Analysis (ERA5) as input to LPJ-EOSIM model.',\n   'raster:bands': [{'scale': 1.0,\n     'nodata': -9999.0,\n     'offset': 0.0,\n     'sampling': 'area',\n     'data_type': 'float32',\n     'histogram': {'max': 0.00022204435663297772,\n      'min': 0.0,\n      'count': 11.0,\n      'buckets': [61122.0,\n       653.0,\n       283.0,\n       124.0,\n       91.0,\n       61.0,\n       55.0,\n       49.0,\n       16.0,\n       5.0]},\n     'statistics': {'mean': 1.94791880305756e-06,\n      'stddev': 1.0934216774964025e-05,\n      'maximum': 0.00022204435663297772,\n      'minimum': 0.0,\n      'valid_percent': 24.09683641975309}}],\n   'proj:geometry': {'type': 'Polygon',\n    'coordinates': [[[-180.0, 90.0],\n      [180.0, 90.0],\n      [180.0, -90.0],\n      [-180.0, -90.0],\n      [-180.0, 90.0]]]},\n   'proj:projjson': {'id': {'code': 4326.0, 'authority': 'EPSG'},\n    'name': 'WGS 84',\n    'type': 'GeographicCRS',\n    'datum': {'name': 'World Geodetic System 1984',\n     'type': 'GeodeticReferenceFrame',\n     'ellipsoid': {'name': 'WGS 84',\n      'semi_major_axis': 6378137.0,\n      'inverse_flattening': 298.257223563}},\n    '$schema': 'https://proj.org/schemas/v0.4/projjson.schema.json',\n    'coordinate_system': {'axis': [{'name': 'Geodetic latitude',\n       'unit': 'degree',\n       'direction': 'north',\n       'abbreviation': 'Lat'},\n      {'name': 'Geodetic longitude',\n       'unit': 'degree',\n       'direction': 'east',\n       'abbreviation': 'Lon'}],\n     'subtype': 'ellipsoidal'}},\n   'proj:transform': [0.5, 0.0, -180.0, 0.0, 0.5, -90.0, 0.0, 0.0, 1.0]},\n  'merra2-ch4-wetlands-emissions': {'href': 's3://ghgc-data-store-dev/lpjwsl-wetlandch4-daygrid-v2-new-units/MERRA2/LPJ_EOSIM_L2_CH4e_dch4e_MERRA2_19900221day.tif',\n   'type': 'image/tiff; application=geotiff; profile=cloud-optimized',\n   'roles': ['data', 'layer'],\n   'title': 'Wetland Methane Emissions, MERRA-2 LPJ-EOSIM Model v2',\n   'proj:bbox': [-180.0, 90.0, 180.0, -90.0],\n   'proj:epsg': 4326.0,\n   'proj:shape': [360.0, 720.0],\n   'description': 'Methane emissions from wetlands in units of grams of methane per meter squared per day. Modern-Era Retrospective analysis for Research and Applications Version 2 (MERRA-2) data as input to LPJ-EOSIM model.',\n   'raster:bands': [{'scale': 1.0,\n     'nodata': -9999.0,\n     'offset': 0.0,\n     'sampling': 'area',\n     'data_type': 'float32',\n     'histogram': {'max': 0.00023568027245346457,\n      'min': 0.0,\n      'count': 11.0,\n      'buckets': [61189.0,\n       637.0,\n       248.0,\n       117.0,\n       66.0,\n       68.0,\n       45.0,\n       38.0,\n       30.0,\n       18.0]},\n     'statistics': {'mean': 1.993146024150978e-06,\n      'stddev': 1.1805217172619661e-05,\n      'maximum': 0.00023568027245346457,\n      'minimum': 0.0,\n      'valid_percent': 24.095679012345677}}],\n   'proj:geometry': {'type': 'Polygon',\n    'coordinates': [[[-180.0, 90.0],\n      [180.0, 90.0],\n      [180.0, -90.0],\n      [-180.0, -90.0],\n      [-180.0, 90.0]]]},\n   'proj:projjson': {'id': {'code': 4326.0, 'authority': 'EPSG'},\n    'name': 'WGS 84',\n    'type': 'GeographicCRS',\n    'datum': {'name': 'World Geodetic System 1984',\n     'type': 'GeodeticReferenceFrame',\n     'ellipsoid': {'name': 'WGS 84',\n      'semi_major_axis': 6378137.0,\n      'inverse_flattening': 298.257223563}},\n    '$schema': 'https://proj.org/schemas/v0.4/projjson.schema.json',\n    'coordinate_system': {'axis': [{'name': 'Geodetic latitude',\n       'unit': 'degree',\n       'direction': 'north',\n       'abbreviation': 'Lat'},\n      {'name': 'Geodetic longitude',\n       'unit': 'degree',\n       'direction': 'east',\n       'abbreviation': 'Lon'}],\n     'subtype': 'ellipsoidal'}},\n   'proj:transform': [0.5, 0.0, -180.0, 0.0, 0.5, -90.0, 0.0, 0.0, 1.0]},\n  'ensemble-mean-ch4-wetlands-emissions': {'href': 's3://ghgc-data-store-dev/lpjwsl-wetlandch4-daygrid-v2-new-units/ensemble_mean/LPJ_EOSIM_L2_CH4e_dch4e_ensemble_mean_19900221day.tif',\n   'type': 'image/tiff; application=geotiff; profile=cloud-optimized',\n   'roles': ['data', 'layer'],\n   'title': 'Wetland Methane Emissions, Ensemble Mean LPJ-EOSIM Model v2',\n   'proj:bbox': [-180.0, 90.0, 180.0, -90.0],\n   'proj:epsg': 4326.0,\n   'proj:shape': [360.0, 720.0],\n   'description': 'Methane emissions from wetlands in units of grams of methane per meter squared per day. Ensemble of multiple climate forcing data sources input to LPJ-EOSIM model.',\n   'raster:bands': [{'scale': 1.0,\n     'nodata': -9999.0,\n     'offset': 0.0,\n     'sampling': 'area',\n     'data_type': 'float32',\n     'histogram': {'max': 0.0002132821100531146,\n      'min': 0.0,\n      'count': 11.0,\n      'buckets': [61056.0,\n       702.0,\n       260.0,\n       136.0,\n       87.0,\n       62.0,\n       55.0,\n       49.0,\n       32.0,\n       17.0]},\n     'statistics': {'mean': 1.9705793158840367e-06,\n      'stddev': 1.123134260556708e-05,\n      'maximum': 0.0002132821100531146,\n      'minimum': 0.0,\n      'valid_percent': 24.095679012345677}}],\n   'proj:geometry': {'type': 'Polygon',\n    'coordinates': [[[-180.0, 90.0],\n      [180.0, 90.0],\n      [180.0, -90.0],\n      [-180.0, -90.0],\n      [-180.0, 90.0]]]},\n   'proj:projjson': {'id': {'code': 4326.0, 'authority': 'EPSG'},\n    'name': 'WGS 84',\n    'type': 'GeographicCRS',\n    'datum': {'name': 'World Geodetic System 1984',\n     'type': 'GeodeticReferenceFrame',\n     'ellipsoid': {'name': 'WGS 84',\n      'semi_major_axis': 6378137.0,\n      'inverse_flattening': 298.257223563}},\n    '$schema': 'https://proj.org/schemas/v0.4/projjson.schema.json',\n    'coordinate_system': {'axis': [{'name': 'Geodetic latitude',\n       'unit': 'degree',\n       'direction': 'north',\n       'abbreviation': 'Lat'},\n      {'name': 'Geodetic longitude',\n       'unit': 'degree',\n       'direction': 'east',\n       'abbreviation': 'Lon'}],\n     'subtype': 'ellipsoidal'}},\n   'proj:transform': [0.5, 0.0, -180.0, 0.0, 0.5, -90.0, 0.0, 0.0, 1.0]}},\n 'geometry': {'type': 'Polygon',\n  'coordinates': [[[-180, -90],\n    [180, -90],\n    [180, 90],\n    [-180, 90],\n    [-180, -90]]]},\n 'collection': 'lpjeosim-wetlandch4-daygrid-v2',\n 'properties': {'datetime': '1990-02-21T00:00:00+00:00'},\n 'stac_version': '1.0.0',\n 'stac_extensions': []}\n\n\nBelow, we are entering the minimum and maximum values to provide our upper and lower bounds in the rescale_values.\n\n# Fetch the minimum and maximum values for rescaling\nrescale_values = {'max': 0.0003, 'min': 0.0}",
    "crumbs": [
      "Data Usage Notebooks",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "Wetland Methane Emissions, LPJ-EOSIM Model"
    ]
  },
  {
    "objectID": "user_data_notebooks/lpjwsl-wetlandch4-grid-v1_User_Notebook.html#explore-changes-in-methane-ch4-emission-levels-using-the-raster-api",
    "href": "user_data_notebooks/lpjwsl-wetlandch4-grid-v1_User_Notebook.html#explore-changes-in-methane-ch4-emission-levels-using-the-raster-api",
    "title": "Wetland Methane Emissions, LPJ-EOSIM Model",
    "section": "Explore Changes in Methane (CH4) Emission Levels Using the Raster API",
    "text": "Explore Changes in Methane (CH4) Emission Levels Using the Raster API\nIn this notebook, we will explore the temporal impacts of methane emissions. We will visualize the outputs on a map using folium.\n\n# Now we create a dictionary where the start datetime values for each granule is queried more explicitly by year and month (e.g., 2020-02)\nitems = {item[\"properties\"][\"datetime\"][:10]: item for item in items} \n\nNow, we will pass the item id, collection name, and rescaling_factor to the Raster API endpoint. We will do this twice, once for December 2001 and again for December 2021, so we can visualize each event independently.\n\n# Choose a color for displaying the tiles\n# Please refer to matplotlib library if you'd prefer choosing a different color ramp.\n# For more information on Colormaps in Matplotlib, please visit https://matplotlib.org/stable/users/explain/colors/colormaps.html\ncolor_map = \"magma\" \n\n# Make a GET request to retrieve information for the date mentioned below\ndate1 = '1990-01-01'\ndate1_tile = requests.get(\n\n    # Pass the collection name, collection date, and its ID\n    # To change the year and month of the observed parameter, you can modify the \"items['YYYY-MM-DD']\" statement\n    f\"{RASTER_API_URL}/stac/tilejson.json?collection={items[date1]['collection']}&item={items[date1]['id']}\"\n\n    # Pass the asset name\n    f\"&assets={asset_name}\"\n\n    # Pass the color formula and colormap for custom visualization\n    f\"&color_formula=gamma+r+1.05&colormap_name={color_map}\"\n\n    # Pass the minimum and maximum values for rescaling\n    f\"&rescale={rescale_values['min']},{rescale_values['max']}\", \n\n# Return response in JSON format\n).json()\n\n# Print the properties of the retrieved granule to the console\ndate1_tile\n\n{'tilejson': '2.2.0',\n 'version': '1.0.0',\n 'scheme': 'xyz',\n 'tiles': ['https://dev.ghg.center/ghgcenter/api/raster/stac/tiles/WebMercatorQuad/{z}/{x}/{y}@1x?collection=lpjeosim-wetlandch4-daygrid-v2&item=lpjeosim-wetlandch4-daygrid-v2-19900101day&assets=ensemble-mean-ch4-wetlands-emissions&color_formula=gamma+r+1.05&colormap_name=magma&rescale=0.0%2C0.0003'],\n 'minzoom': 0,\n 'maxzoom': 24,\n 'bounds': [-180.0, -90.0, 180.0, 90.0],\n 'center': [0.0, 0.0, 0]}\n\n\n\n# Make a GET request to retrieve information for date mentioned below\ndate2 = '1990-01-30'\ndate2_tile = requests.get(\n\n    # Pass the collection name, collection date, and its ID\n    # To change the year and month of the observed parameter, you can modify the \"items['YYYY-MM-DD']\" statement\n    f\"{RASTER_API_URL}/stac/tilejson.json?collection={items[date2]['collection']}&item={items[date2]['id']}\"\n\n    # Pass the asset name\n    f\"&assets={asset_name}\"\n\n    # Pass the color formula and colormap for custom visualization\n    f\"&color_formula=gamma+r+1.05&colormap_name={color_map}\"\n\n    # Pass the minimum and maximum values for rescaling\n    f\"&rescale={rescale_values['min']},{rescale_values['max']}\",\n\n# Return response in JSON format \n).json()\n\n# Print the properties of the retrieved granule to the console\ndate2_tile\n\n{'tilejson': '2.2.0',\n 'version': '1.0.0',\n 'scheme': 'xyz',\n 'tiles': ['https://dev.ghg.center/ghgcenter/api/raster/stac/tiles/WebMercatorQuad/{z}/{x}/{y}@1x?collection=lpjeosim-wetlandch4-daygrid-v2&item=lpjeosim-wetlandch4-daygrid-v2-19900130day&assets=ensemble-mean-ch4-wetlands-emissions&color_formula=gamma+r+1.05&colormap_name=magma&rescale=0.0%2C0.0003'],\n 'minzoom': 0,\n 'maxzoom': 24,\n 'bounds': [-180.0, -90.0, 180.0, 90.0],\n 'center': [0.0, 0.0, 0]}",
    "crumbs": [
      "Data Usage Notebooks",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "Wetland Methane Emissions, LPJ-EOSIM Model"
    ]
  },
  {
    "objectID": "user_data_notebooks/lpjwsl-wetlandch4-grid-v1_User_Notebook.html#visualize-ch₄-emissions",
    "href": "user_data_notebooks/lpjwsl-wetlandch4-grid-v1_User_Notebook.html#visualize-ch₄-emissions",
    "title": "Wetland Methane Emissions, LPJ-EOSIM Model",
    "section": "Visualize CH₄ Emissions",
    "text": "Visualize CH₄ Emissions\n\n# For this study we are going to compare the CH₄ Emissions in 2001 and 2021 along the coast of California\n# To change the location, you can simply insert the latitude and longitude of the area of your interest in the \"location=(LAT, LONG)\" statement\n\n# Set initial zoom and center of map\n# 'folium.plugins' allows mapping side-by-side\nmap_ = folium.plugins.DualMap(location=(34, -118), zoom_start=6)\n\n# Define the first map layer for tile fetched for date 1\n# The TileLayer library helps in manipulating and displaying raster layers on a map\nmap_layer_date1 = TileLayer(\n    tiles=date1_tile[\"tiles\"][0], # Path to retrieve the tile\n    attr=\"GHG\", # Set the attribution\n    opacity=0.5, # Adjust the transparency of the layer\n)\n\n# Add the first layer to the Dual Map\nmap_layer_date1.add_to(map_.m1)\n\n\n# Define the second map layer for the tile fetched for date 2\nmap_layer_date2 = TileLayer(\n    tiles=date2_tile[\"tiles\"][0], # Path to retrieve the tile\n    attr=\"GHG\", # Set the attribution\n    opacity=0.5, # Adjust the transparency of the layer\n)\n\n# Add the second layer to the Dual Map\nmap_layer_date2.add_to(map_.m2)\n\n# Visualize the Dual Map\nmap_\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook",
    "crumbs": [
      "Data Usage Notebooks",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "Wetland Methane Emissions, LPJ-EOSIM Model"
    ]
  },
  {
    "objectID": "user_data_notebooks/lpjwsl-wetlandch4-grid-v1_User_Notebook.html#visualize-the-data-as-a-time-series",
    "href": "user_data_notebooks/lpjwsl-wetlandch4-grid-v1_User_Notebook.html#visualize-the-data-as-a-time-series",
    "title": "Wetland Methane Emissions, LPJ-EOSIM Model",
    "section": "Visualize the Data as a Time Series",
    "text": "Visualize the Data as a Time Series\nWe can now explore the wetland methane emissions time series (January 1980 – December 2021) available for the Texas area of the U.S. We can plot the data set using the code below:\n\n# Determine the width and height of the plot using the 'matplotlib' library\n# Figure size: 20 representing the width, 10 representing the height\nfig = plt.figure(figsize=(20, 10))\n\n# Plot the time series\nplt.plot(\n    df[\"date\"], # X-axis: date\n    df[\"max\"], # Y-axis: CH₄ value\n    color=\"red\", # Line color\n    linestyle=\"-\", # Line style\n    linewidth=0.5, # Line width\n    label=\"Max monthly CH₄ emissions\", # Legend label\n)\n\n# Display legend\nplt.legend()\n\n# Insert label for the X-axis\nplt.xlabel(\"Years\")\n\n# Insert label for the Y-axis\nplt.ylabel(\"CH4 emissions g/m2\")\n\n# Insert title for the plot\nplt.title(\"CH4 emission Values for Texas, 1980-2021)\")\n\nTo take a closer look at the CH4 variability across this region, we are going to retrieve and display data collected during the October, 1990 observation.\n\n# The 1990-10 observation is the 3rd item in the list\n# Considering that a list starts with \"0\", we need to insert \"2\" in the \"items[2]\" statement\n# Print the start Date Time of the third granule in the collection\nprint(items[2][\"properties\"][\"start_datetime\"])\n\n\n# A GET request is made for the 3rd item in the collection\noctober_tile = requests.get(\n\n    # Pass the collection name, the item number in the list, and its ID\n    f\"{RASTER_API_URL}/stac/tilejson.json?collection={items[2]['collection']}&item={items[2]['id']}\"\n    \"&assets=ch4-wetlands-emissions\"\n\n    # Pass the color formula and colormap for custom visualization\n    f\"&color_formula=gamma+r+1.05&colormap_name={color_map}\"\n\n    # Pass the minimum and maximum values for rescaling\n    f\"&rescale={rescale_values['min']},{rescale_values['max']}\",\n\n# Return the response in JSON format\n).json()\n\n# Print the properties of the retrieved granule to the console\noctober_tile\n\n\n# Create a new map to display the CH4 variability for the Texas region for the October, 1990 timeframe\naoi_map_bbox = Map(\n\n    # Base map is set to OpenStreetMap\n    tiles=\"OpenStreetMap\",\n\n    # Set the center of the map\n    location=[\n        30,-100\n    ],\n\n    # Set the zoom value\n    zoom_start=8,\n)\n\n# Define the map layer\nmap_layer = TileLayer(\n    tiles=october_tile[\"tiles\"][0], # Path to retrieve the tile\n    attr=\"GHG\", opacity = 0.5 # Set the attribution and transparency\n)\n\n# Add the layer to the map\nmap_layer.add_to(aoi_map_bbox)\n\n# Visualize the map\naoi_map_bbox",
    "crumbs": [
      "Data Usage Notebooks",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "Wetland Methane Emissions, LPJ-EOSIM Model"
    ]
  },
  {
    "objectID": "user_data_notebooks/lpjwsl-wetlandch4-grid-v1_User_Notebook.html#summary",
    "href": "user_data_notebooks/lpjwsl-wetlandch4-grid-v1_User_Notebook.html#summary",
    "title": "Wetland Methane Emissions, LPJ-EOSIM Model",
    "section": "Summary",
    "text": "Summary\nIn this notebook we have successfully completed the following steps for the STAC collection for the Wetland Methane Emissions, LPJ-EOSIM Model data: 1. Install and import the necessary libraries 2. Fetch the collection from STAC collections using the appropriate endpoints 3. Count the number of existing granules within the collection 4. Map and compare the CH4 levels over the Texas region for two distinctive years 5. Create a table that displays the minimum, maximum, and sum of the CH4 levels for a specified region 6. Generate a time-series graph of the CH4 levels for a specified region\nIf you have any questions regarding this user notebook, please contact us using the feedback form.",
    "crumbs": [
      "Data Usage Notebooks",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "Wetland Methane Emissions, LPJ-EOSIM Model"
    ]
  },
  {
    "objectID": "user_data_notebooks/gosat-based-ch4budget-yeargrid-v1_User_Notebook.html",
    "href": "user_data_notebooks/gosat-based-ch4budget-yeargrid-v1_User_Notebook.html",
    "title": "GOSAT-based Top-down Total and Natural Methane Emissions",
    "section": "",
    "text": "Identify available dates and temporal frequency of observations for the given collection using the GHGC API /stac endpoint. The collection processed in this notebook is the gridded methane emissions data product.\nPass the STAC item into the raster API /stac/tilejson.jsonendpoint.\nUsing folium.plugins.DualMap, we will visualize two tiles (side-by-side), allowing us to compare time points.\nAfter the visualization, we will perform zonal statistics for a given polygon.",
    "crumbs": [
      "Data Usage Notebooks",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "GOSAT-based Top-down Total and Natural Methane Emissions"
    ]
  },
  {
    "objectID": "user_data_notebooks/gosat-based-ch4budget-yeargrid-v1_User_Notebook.html#approach",
    "href": "user_data_notebooks/gosat-based-ch4budget-yeargrid-v1_User_Notebook.html#approach",
    "title": "GOSAT-based Top-down Total and Natural Methane Emissions",
    "section": "",
    "text": "Identify available dates and temporal frequency of observations for the given collection using the GHGC API /stac endpoint. The collection processed in this notebook is the gridded methane emissions data product.\nPass the STAC item into the raster API /stac/tilejson.jsonendpoint.\nUsing folium.plugins.DualMap, we will visualize two tiles (side-by-side), allowing us to compare time points.\nAfter the visualization, we will perform zonal statistics for a given polygon.",
    "crumbs": [
      "Data Usage Notebooks",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "GOSAT-based Top-down Total and Natural Methane Emissions"
    ]
  },
  {
    "objectID": "user_data_notebooks/gosat-based-ch4budget-yeargrid-v1_User_Notebook.html#about-the-data",
    "href": "user_data_notebooks/gosat-based-ch4budget-yeargrid-v1_User_Notebook.html#about-the-data",
    "title": "GOSAT-based Top-down Total and Natural Methane Emissions",
    "section": "About the Data",
    "text": "About the Data\nThe NASA Carbon Monitoring System Flux (CMS-Flux) team analyzed remote sensing observations from Japan’s Greenhouse gases Observing SATellite (GOSAT) to produce the global Committee on Earth Observation Satellites (CEOS) CH₄ Emissions data product. They used an analytic Bayesian inversion approach and the GEOS-Chem global chemistry transport model to quantify annual methane (CH₄) emissions and their uncertainties at a spatial resolution of 1° by 1° and then projected these to each country for 2019.",
    "crumbs": [
      "Data Usage Notebooks",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "GOSAT-based Top-down Total and Natural Methane Emissions"
    ]
  },
  {
    "objectID": "user_data_notebooks/gosat-based-ch4budget-yeargrid-v1_User_Notebook.html#querying-the-stac-api",
    "href": "user_data_notebooks/gosat-based-ch4budget-yeargrid-v1_User_Notebook.html#querying-the-stac-api",
    "title": "GOSAT-based Top-down Total and Natural Methane Emissions",
    "section": "Querying the STAC API",
    "text": "Querying the STAC API\n\nimport requests\nfrom folium import Map, TileLayer\nfrom pystac_client import Client\n\n\n# Provide STAC and RASTER API endpoints\nSTAC_API_URL = \"http://ghg.center/api/stac\"\nRASTER_API_URL = \"https://ghg.center/api/raster\"\n\n# Please use the collection name similar to the one used in STAC collection.\n\n# Name of the collection for gosat budget methane. \ncollection_name = \"gosat-based-ch4budget-yeargrid-v1\"\n\n\n# Fetching the collection from STAC collections using appropriate endpoint.\ncollection = requests.get(f\"{STAC_API_URL}/collections/{collection_name}\").json()\ncollection\n\nExamining the contents of our collection under the temporal variable, we see that the data is available from January 2012 to December 2018. By looking at the dashboard:time density, we observe that the data is available for only one year, i.e. 2019.\n\ndef get_item_count(collection_id):\n    count = 0\n    items_url = f\"{STAC_API_URL}/collections/{collection_id}/items\"\n\n    while True:\n        response = requests.get(items_url)\n\n        if not response.ok:\n            print(\"error getting items\")\n            exit()\n\n        stac = response.json()\n        count += int(stac[\"context\"].get(\"returned\", 0))\n        next = [link for link in stac[\"links\"] if link[\"rel\"] == \"next\"]\n\n        if not next:\n            break\n        items_url = next[0][\"href\"]\n\n    return count\n\n\n# Check total number of items available\nnumber_of_items = get_item_count(collection_name)\nitems = requests.get(f\"{STAC_API_URL}/collections/{collection_name}/items?limit={number_of_items}\").json()[\"features\"]\nprint(f\"Found {len(items)} items\")\n\n\n# Examining the first item in the collection\nitems[0]\n\nBelow, we enter minimum and maximum values to provide our upper and lower bounds in rescale_values.",
    "crumbs": [
      "Data Usage Notebooks",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "GOSAT-based Top-down Total and Natural Methane Emissions"
    ]
  },
  {
    "objectID": "user_data_notebooks/gosat-based-ch4budget-yeargrid-v1_User_Notebook.html#exploring-changes-in-gosat-methane-budgets-ch4-levels-using-the-raster-api",
    "href": "user_data_notebooks/gosat-based-ch4budget-yeargrid-v1_User_Notebook.html#exploring-changes-in-gosat-methane-budgets-ch4-levels-using-the-raster-api",
    "title": "GOSAT-based Top-down Total and Natural Methane Emissions",
    "section": "Exploring Changes in GOSAT Methane budgets (CH4) Levels Using the Raster API",
    "text": "Exploring Changes in GOSAT Methane budgets (CH4) Levels Using the Raster API\nIn this notebook, we will explore the impacts of methane emissions and by examining changes over time in urban regions. We will visualize the outputs on a map using folium.\n\n# To access the year value from each item more easily, this will let us query more explicity by year and month (e.g., 2020-02)\nitems = {item[\"properties\"][\"start_datetime\"][:10]: item for item in items} \nasset_name = \"prior-total\"\n\n\n# Fetching the min and max values for a specific item\nrescale_values = {\"max\":items[list(items.keys())[0]][\"assets\"][asset_name][\"raster:bands\"][0][\"histogram\"][\"max\"], \"min\":items[list(items.keys())[0]][\"assets\"][asset_name][\"raster:bands\"][0][\"histogram\"][\"min\"]}\n\n\nitems.keys()\n\nNow, we will pass the item id, collection name, and rescaling_factor to the Raster API endpoint. We will do this for first January 2019.\n\ncolor_map = \"rainbow\" # please select the color ramp from matplotlib library.\njanuary_2019_tile = requests.get(\n    f\"{RASTER_API_URL}/stac/tilejson.json?collection={items['2019-01-01']['collection']}&item={items['2019-01-01']['id']}\"\n    f\"&assets={asset_name}\"\n    f\"&color_formula=gamma+r+1.05&colormap_name={color_map}\"\n    f\"&rescale={rescale_values['min']},{rescale_values['max']}\", \n).json()\njanuary_2019_tile",
    "crumbs": [
      "Data Usage Notebooks",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "GOSAT-based Top-down Total and Natural Methane Emissions"
    ]
  },
  {
    "objectID": "user_data_notebooks/gosat-based-ch4budget-yeargrid-v1_User_Notebook.html#visualizing-ch₄-emissions",
    "href": "user_data_notebooks/gosat-based-ch4budget-yeargrid-v1_User_Notebook.html#visualizing-ch₄-emissions",
    "title": "GOSAT-based Top-down Total and Natural Methane Emissions",
    "section": "Visualizing CH₄ Emissions",
    "text": "Visualizing CH₄ Emissions\n\n# We will import folium to map and folium.plugins to allow side-by-side mapping\nimport folium\nimport folium.plugins\n\n# Set initial zoom and center of map for CH₄ Layer\n# Centre of map [latitude,longitude]\nmap_ = folium.Map(location=(34, -118), zoom_start=6)\n\n# January 2019\nmap_layer_2019 = TileLayer(\n    tiles=january_2019_tile[\"tiles\"][0],\n    attr=\"GHG\",\n    opacity=0.7,\n)\nmap_layer_2019.add_to(map_)\n\n# # January 2012\n# map_layer_2012 = TileLayer(\n#     tiles=january_2012_tile[\"tiles\"][0],\n#     attr=\"GHG\",\n#     opacity=0.7,\n# )\n# map_layer_2012.add_to(map_.m2)\n\n# visualising the map\nmap_",
    "crumbs": [
      "Data Usage Notebooks",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "GOSAT-based Top-down Total and Natural Methane Emissions"
    ]
  },
  {
    "objectID": "user_data_notebooks/gosat-based-ch4budget-yeargrid-v1_User_Notebook.html#summary",
    "href": "user_data_notebooks/gosat-based-ch4budget-yeargrid-v1_User_Notebook.html#summary",
    "title": "GOSAT-based Top-down Total and Natural Methane Emissions",
    "section": "Summary",
    "text": "Summary\nIn this notebook we have successfully explored, analyzed, and visualized the STAC collection for GOSAT-based Top-down Total and Natural Methane Emissions.",
    "crumbs": [
      "Data Usage Notebooks",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "GOSAT-based Top-down Total and Natural Methane Emissions"
    ]
  },
  {
    "objectID": "services/jupyterhub.html",
    "href": "services/jupyterhub.html",
    "title": "JupyterHub",
    "section": "",
    "text": "The US GHG Center promotes the use of JupyterHub environments for interactive data science. JupyterHub enables you to analyze massive archives of Earth science data in the cloud in an interactive environment that alleviates the complexities of managing compute resources (virtual machines, roles and permissions, etc).\nUsers affiliated with the US GHG Center can get access to a dedicated JupyterHub service, provided in collaboration with 2i2c: hub.ghg.center. Please find instructions for requesting access below.\nIf you are a scientist affiliated with other NASA projects such as VEDA, EIS, and MAAP, you can also keep using the resources provided by these projects. Through the use of open-source technology, we make sure our services are interoperable and exchangeable.",
    "crumbs": [
      "User Services",
      "JupyterHub"
    ]
  },
  {
    "objectID": "services/jupyterhub.html#to-get-us-ghg-center-jupyterhub-access",
    "href": "services/jupyterhub.html#to-get-us-ghg-center-jupyterhub-access",
    "title": "JupyterHub",
    "section": "To Get US GHG Center JupyterHub access:",
    "text": "To Get US GHG Center JupyterHub access:\nThe US GHG Center notebook environment is available to authorized users on an as-need basis. If you are a user affiliated with the US GHG Center, you can gain access by using our Hub Access Request form.\n\nMake sure you have a GitHub Account. Take note of your GitHub username.\nFill out the request form and provide needed information.\nWatch your email for notification of authorization and the invite to join the US GHG Center Hub Access GitHub Team.\nOnce you accept the invitation, you can go to hub.ghg.center and login using your GitHub credentials.",
    "crumbs": [
      "User Services",
      "JupyterHub"
    ]
  },
  {
    "objectID": "services/jupyterhub.html#to-access-user-notebooks",
    "href": "services/jupyterhub.html#to-access-user-notebooks",
    "title": "JupyterHub",
    "section": "To access User Notebooks",
    "text": "To access User Notebooks\nThis site provides Jupyter notebooks showing how to load and analyze Earth data in the interactive cloud computing environment.\nFurther instructions are included in each notebook.\nIf you have any questions, please use the feedback form to contact the US GHG Center user support team.",
    "crumbs": [
      "User Services",
      "JupyterHub"
    ]
  },
  {
    "objectID": "processing_and_verification_reports/emit-ch4plume-v1_Processing and Verification Report.html",
    "href": "processing_and_verification_reports/emit-ch4plume-v1_Processing and Verification Report.html",
    "title": "EMIT Methane Point Source Plume Complexes",
    "section": "",
    "text": "This browser does not support PDFs. Please download the PDF to view it: Download PDF.\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Processing and Verification Reports",
      "Large Emissions Events",
      "EMIT Methane Point Source Plume Complexes"
    ]
  },
  {
    "objectID": "processing_and_verification_reports/gosat-based-ch4budget-yeargrid-v1_Processing and Verification Report.html",
    "href": "processing_and_verification_reports/gosat-based-ch4budget-yeargrid-v1_Processing and Verification Report.html",
    "title": "GOSAT-based Top-down Total and Natural Methane Emissions",
    "section": "",
    "text": "This browser does not support PDFs. Please download the PDF to view it: Download PDF.\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Processing and Verification Reports",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "GOSAT-based Top-down Total and Natural Methane Emissions"
    ]
  },
  {
    "objectID": "processing_and_verification_reports/casagfed-carbonflux-monthgrid-v3_Processing and Verification Report.html",
    "href": "processing_and_verification_reports/casagfed-carbonflux-monthgrid-v3_Processing and Verification Report.html",
    "title": "MiCASA Land Carbon Flux",
    "section": "",
    "text": "This browser does not support PDFs. Please download the PDF to view it: Download PDF.\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Processing and Verification Reports",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "MiCASA Land Carbon Flux"
    ]
  },
  {
    "objectID": "processing_and_verification_reports/oco2-mip-co2budget-yeargrid-v1_Processing and Verification Report.html",
    "href": "processing_and_verification_reports/oco2-mip-co2budget-yeargrid-v1_Processing and Verification Report.html",
    "title": "OCO-2 MIP Top-Down CO₂ Budgets",
    "section": "",
    "text": "This browser does not support PDFs. Please download the PDF to view it: Download PDF.\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Processing and Verification Reports",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "OCO-2 MIP Top-Down CO₂ Budgets"
    ]
  },
  {
    "objectID": "processing_and_verification_reports/sedac-popdensity-yeargrid5yr-v4.11_Processing and Verification Report.html",
    "href": "processing_and_verification_reports/sedac-popdensity-yeargrid5yr-v4.11_Processing and Verification Report.html",
    "title": "SEDAC Gridded World Population Density",
    "section": "",
    "text": "This browser does not support PDFs. Please download the PDF to view it: Download PDF.\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Processing and Verification Reports",
      "Socioeconomic",
      "SEDAC Gridded World Population Density"
    ]
  },
  {
    "objectID": "cog_transformation/lpjwsl-wetlandch4-daygrid-v1.html",
    "href": "cog_transformation/lpjwsl-wetlandch4-daygrid-v1.html",
    "title": "Wetland Methane Emissions, LPJ-wsl Model",
    "section": "",
    "text": "This script was used to transform the Wetland Methane Emissions, LPJ-wsl Model dataset from netCDF to Cloud Optimized GeoTIFF (COG) format for display in the Greenhouse Gas (GHG) Center.\n\nimport os\nimport xarray\nimport re\nimport pandas as pd\nimport json\nimport tempfile\nimport boto3\nfrom datetime import datetime, timedelta\n\n\nsession = boto3.session.Session()\ns3_client = session.client(\"s3\")\nbucket_name = (\n    \"ghgc-data-store-dev\"  # S3 bucket where the COGs are stored after transformation\n)\nFOLDER_NAME = \"NASA_GSFC_ch4_wetlands_daily\"\ndirectory = \"ch4_wetlands_daily\"\n\nfiles_processed = pd.DataFrame(\n    columns=[\"file_name\", \"COGs_created\"]\n)  # A dataframe to keep track of the files that we have transformed into COGs\n\n# Reading the raw netCDF files from local machine\nfor name in os.listdir(directory):\n    xds = xarray.open_dataset(\n        f\"{directory}/{name}\", engine=\"netcdf4\", decode_times=False\n    )\n    xds = xds.assign_coords(longitude=(((xds.longitude + 180) % 360) - 180)).sortby(\n        \"longitude\"\n    )\n    variable = [var for var in xds.data_vars]\n    filename = name.split(\"/ \")[-1]\n    filename_elements = re.split(\"[_ .]\", filename)\n    start_time = datetime(int(filename_elements[-2]), 1, 1)\n\n    for time_increment in range(0, len(xds.time)):\n        for var in variable:\n            filename = name.split(\"/ \")[-1]\n            filename_elements = re.split(\"[_ .]\", filename)\n            data = getattr(xds.isel(time=time_increment), var)\n            data = data.isel(latitude=slice(None, None, -1))\n            data = data * 1000\n            data.rio.set_spatial_dims(\"longitude\", \"latitude\", inplace=True)\n            data.rio.write_crs(\"epsg:4326\", inplace=True)\n            date = start_time + timedelta(hours=data.time.item(0))\n\n            # # insert date of generated COG into filename\n            filename_elements.pop()\n            filename_elements[-1] = date.strftime(\"%Y%m%d\")\n            filename_elements.insert(2, var)\n            cog_filename = \"_\".join(filename_elements)\n            # # add extension\n            cog_filename = f\"{cog_filename}.tif\"\n\n            with tempfile.NamedTemporaryFile() as temp_file:\n                data.rio.to_raster(\n                    temp_file.name,\n                    driver=\"COG\",\n                )\n                s3_client.upload_file(\n                    Filename=temp_file.name,\n                    Bucket=bucket_name,\n                    Key=f\"{FOLDER_NAME}/{cog_filename}\",\n                )\n\n            files_processed = files_processed._append(\n                {\"file_name\": name, \"COGs_created\": cog_filename},\n                ignore_index=True,\n            )\n\n            print(f\"Generated and saved COG: {cog_filename}\")\n\n# Generate the json file with the metadata that is present in the netCDF files.\nwith tempfile.NamedTemporaryFile(mode=\"w+\") as fp:\n    json.dump(xds.attrs, fp)\n    json.dump({\"data_dimensions\": dict(xds.dims)}, fp)\n    json.dump({\"data_variables\": list(xds.data_vars)}, fp)\n    fp.flush()\n\n    s3_client.upload_file(\n        Filename=fp.name,\n        Bucket=bucket_name,\n        Key=f\"{FOLDER_NAME}/metadata.json\",\n    )\n\n# creating the csv file with the names of files transformed.\nfiles_processed.to_csv(\n    f\"s3://{bucket_name}/{FOLDER_NAME}/files_converted.csv\",\n)\nprint(\"Done generating COGs\")\n\n\n\n\n Back to top"
  },
  {
    "objectID": "cog_transformation/sedac-popdensity-yeargrid5yr-v4.11.html",
    "href": "cog_transformation/sedac-popdensity-yeargrid5yr-v4.11.html",
    "title": "SEDAC Gridded World Population Data",
    "section": "",
    "text": "This script was used to transform SEDAC Gridded World Population Data from netCDF to Cloud Optimized GeoTIFF (COG) format for display in the Greenhouse Gas (GHG) Center.\n\nimport os\nimport xarray\nimport re\nimport pandas as pd\n\nimport tempfile\nimport boto3\n\n\nsession = boto3.session.Session()\ns3_client = session.client(\"s3\")\nbucket_name = (\n    \"ghgc-data-store-dev\"  # S3 bucket where the COGs are stored after transformation\n)\n\nfold_names = os.listdir(\"gpw\")\n\nfiles_processed = pd.DataFrame(\n    columns=[\"file_name\", \"COGs_created\"]\n)  # A dataframe to keep track of the files that we have transformed into COGs\n\n# Reading the raw netCDF files from local machine\nfor fol_ in fold_names:\n    for name in os.listdir(f\"gpw/{fol_}\"):\n        if name.endswith(\".tif\"):\n            xds = xarray.open_dataarray(f\"gpw/{fol_}/{name}\")\n\n            filename = name.split(\"/ \")[-1]\n            filename_elements = re.split(\"[_ .]\", filename)\n            # # insert date of generated COG into filename\n            filename_elements.pop()\n            filename_elements.append(filename_elements[-3])\n\n            xds.rio.set_spatial_dims(\"x\", \"y\", inplace=True)\n            xds.rio.write_crs(\"epsg:4326\", inplace=True)\n\n            cog_filename = \"_\".join(filename_elements)\n            # # add extension\n            cog_filename = f\"{cog_filename}.tif\"\n\n            with tempfile.NamedTemporaryFile() as temp_file:\n                xds.rio.to_raster(temp_file.name, driver=\"COG\")\n                s3_client.upload_file(\n                    Filename=temp_file.name,\n                    Bucket=bucket_name,\n                    Key=f\"gridded_population_cog/{cog_filename}\",\n                )\n\n            files_processed = files_processed._append(\n                {\"file_name\": name, \"COGs_created\": cog_filename},\n                ignore_index=True,\n            )\n\n            print(f\"Generated and saved COG: {cog_filename}\")\n\n\n# creating the csv file with the names of files transformed.\nfiles_processed.to_csv(\n    f\"s3://{bucket_name}/gridded_population_cog/files_converted.csv\",\n)\nprint(\"Done generating COGs\")\n\n\n\n\n Back to top",
    "crumbs": [
      "Data Transformation Notebooks",
      "Socioeconomic",
      "SEDAC Gridded World Population Data"
    ]
  },
  {
    "objectID": "cog_transformation/lpjwsl-wetlandch4-monthgrid-v1.html",
    "href": "cog_transformation/lpjwsl-wetlandch4-monthgrid-v1.html",
    "title": "Wetland Methane Emissions, LPJ-wsl Model",
    "section": "",
    "text": "This script was used to transform the Wetland Methane Emissions, LPJ-wsl Model dataset from netCDF to Cloud Optimized GeoTIFF (COG) format for display in the Greenhouse Gas (GHG) Center.\n\nimport os\nimport xarray\nimport re\nimport pandas as pd\nimport json\nimport tempfile\nimport boto3\n\n\nsession = boto3.session.Session()\ns3_client = session.client(\"s3\")\nbucket_name = (\n    \"ghgc-data-store-dev\"  # S3 bucket where the COGs are stored after transformation\n)\nFOLDER_NAME = \"NASA_GSFC_ch4_wetlands_monthly\"\ndirectory = \"ch4_wetlands_monthly\"\n\nfiles_processed = pd.DataFrame(\n    columns=[\"file_name\", \"COGs_created\"]\n)  # A dataframe to keep track of the files that we have transformed into COGs\n\n# Reading the raw netCDF files from local machine\nfor name in os.listdir(directory):\n    xds = xarray.open_dataset(\n        f\"{directory}/{name}\", engine=\"netcdf4\", decode_times=False\n    )\n    xds = xds.assign_coords(longitude=(((xds.longitude + 180) % 360) - 180)).sortby(\n        \"longitude\"\n    )\n    variable = [var for var in xds.data_vars]\n    filename = name.split(\"/ \")[-1]\n    filename_elements = re.split(\"[_ .]\", filename)\n\n    for time_increment in range(0, len(xds.time)):\n        for var in variable:\n            filename = name.split(\"/ \")[-1]\n            filename_elements = re.split(\"[_ .]\", filename)\n            data = getattr(xds.isel(time=time_increment), var)\n            data = data.isel(latitude=slice(None, None, -1))\n            data = data * 1000\n            data.rio.set_spatial_dims(\"longitude\", \"latitude\", inplace=True)\n            data.rio.write_crs(\"epsg:4326\", inplace=True)\n\n            date = (\n                f\"0{int((data.time.item(0)/732)+1)}\"\n                if len(str(int((data.time.item(0) / 732) + 1))) == 1\n                else f\"{int((data.time.item(0)/732)+1)}\"\n            )\n            # # insert date of generated COG into filename\n            filename_elements.pop()\n            filename_elements[-1] = filename_elements[-1] + date\n            filename_elements.insert(2, var)\n            cog_filename = \"_\".join(filename_elements)\n            # # add extension\n            cog_filename = f\"{cog_filename}.tif\"\n\n            with tempfile.NamedTemporaryFile() as temp_file:\n                data.rio.to_raster(\n                    temp_file.name,\n                    driver=\"COG\",\n                )\n                s3_client.upload_file(\n                    Filename=temp_file.name,\n                    Bucket=bucket_name,\n                    Key=f\"{FOLDER_NAME}/{cog_filename}\",\n                )\n\n            files_processed = files_processed._append(\n                {\"file_name\": name, \"COGs_created\": cog_filename},\n                ignore_index=True,\n            )\n\n            print(f\"Generated and saved COG: {cog_filename}\")\n\n# Generate the json file with the metadata that is present in the netCDF files.\nwith tempfile.NamedTemporaryFile(mode=\"w+\") as fp:\n    json.dump(xds.attrs, fp)\n    json.dump({\"data_dimensions\": dict(xds.dims)}, fp)\n    json.dump({\"data_variables\": list(xds.data_vars)}, fp)\n    fp.flush()\n\n    s3_client.upload_file(\n        Filename=fp.name,\n        Bucket=bucket_name,\n        Key=f\"{FOLDER_NAME}/metadata.json\",\n    )\n\n# creating the csv file with the names of files transformed.\nfiles_processed.to_csv(\n    f\"s3://{bucket_name}/{FOLDER_NAME}/files_converted.csv\",\n)\nprint(\"Done generating COGs\")\n\n\n\n\n Back to top"
  },
  {
    "objectID": "cog_transformation/emit-ch4plume-v1.html",
    "href": "cog_transformation/emit-ch4plume-v1.html",
    "title": "EMIT Methane Point Source Plume Complexes",
    "section": "",
    "text": "This script was used to read the EMIT Methane Point Source Plume Complexes dataset provided in Cloud Optimized GeoTIFF (COG) format for display in the Greenhouse Gas (GHG) Center.\n\nimport re\nimport pandas as pd\nimport json\nimport tempfile\nimport boto3\n\n\nsession_ghgc = boto3.session.Session(profile_name=\"ghg_user\")\ns3_client_ghgc = session_ghgc.client(\"s3\")\nsession_veda_smce = boto3.session.Session()\ns3_client_veda_smce = session_veda_smce.client(\"s3\")\n\n# Since the plume emissions were already COGs, we just had to transform their naming convention to be stored in the STAC collection.\nSOURCE_BUCKET_NAME = \"ghgc-data-staging-uah\"\nTARGET_BUCKET_NAME = \"ghgc-data-store-dev\"\n\n\nkeys = []\nresp = s3_client_ghgc.list_objects_v2(Bucket=SOURCE_BUCKET_NAME)\nfor obj in resp[\"Contents\"]:\n    if \"l3\" in obj[\"Key\"]:\n        keys.append(obj[\"Key\"])\n\nfor key in keys:\n    s3_obj = s3_client_ghgc.get_object(Bucket=SOURCE_BUCKET_NAME, Key=key)[\n        \"Body\"\n    ]\n    filename = key.split(\"/\")[-1]\n    filename_elements = re.split(\"[_ .]\", filename)\n\n    date = re.search(\"t\\d\\d\\d\\d\\d\\d\\d\\dt\", key).group(0)\n    filename_elements.insert(-1, date[1:-1])\n    filename_elements.pop()\n\n    cog_filename = \"_\".join(filename_elements)\n    # # add extension\n    cog_filename = f\"{cog_filename}.tif\"\n    s3_client_veda_smce.upload_fileobj(\n        Fileobj=s3_obj,\n        Bucket=TARGET_BUCKET_NAME,\n        Key=f\"plum_data/{cog_filename}\",\n    )\n\n\n\n\n Back to top",
    "crumbs": [
      "Data Transformation Notebooks",
      "Large Emissions Events",
      "EMIT Methane Point Source Plume Complexes"
    ]
  },
  {
    "objectID": "cog_transformation/oco2geos-co2-daygrid-v10r.html",
    "href": "cog_transformation/oco2geos-co2-daygrid-v10r.html",
    "title": "OCO-2 GEOS Column CO₂ Concentrations",
    "section": "",
    "text": "This script was used to transform the OCO-2 GEOS Column CO₂ Concentrations dataset from netCDF to Cloud Optimized GeoTIFF (COG) format for display in the Greenhouse Gas (GHG) Center.\n\nimport xarray\nimport re\nimport pandas as pd\nimport json\nimport tempfile\nimport boto3\nimport os\n\n\nsession = boto3.Session()\ns3_client = session.client(\"s3\")\nbucket_name = (\n    \"ghgc-data-store-dev\"  # S3 bucket where the COGs are stored after transformation\n)\nFOLDER_NAME = \"earth_data/geos_oco2\"\ns3_folder_name = \"geos-oco2\"\n\nerror_files = []\ncount = 0\nfiles_processed = pd.DataFrame(\n    columns=[\"file_name\", \"COGs_created\"]\n)  # A dataframe to keep track of the files that we have transformed into COGs\n\n# Reading the raw netCDF files from local machine\nfor name in os.listdir(FOLDER_NAME):\n    try:\n        xds = xarray.open_dataset(f\"{FOLDER_NAME}/{name}\", engine=\"netcdf4\")\n        xds = xds.assign_coords(lon=(((xds.lon + 180) % 360) - 180)).sortby(\"lon\")\n        variable = [var for var in xds.data_vars]\n        filename = name.split(\"/ \")[-1]\n        filename_elements = re.split(\"[_ .]\", filename)\n\n        for time_increment in range(0, len(xds.time)):\n            for var in variable:\n                filename = name.split(\"/ \")[-1]\n                filename_elements = re.split(\"[_ .]\", filename)\n                data = getattr(xds.isel(time=time_increment), var)\n                data = data.isel(lat=slice(None, None, -1))\n                data.rio.set_spatial_dims(\"lon\", \"lat\", inplace=True)\n                data.rio.write_crs(\"epsg:4326\", inplace=True)\n\n                # # insert date of generated COG into filename\n                filename_elements[-1] = filename_elements[-3]\n                filename_elements.insert(2, var)\n                filename_elements.pop(-3)\n                cog_filename = \"_\".join(filename_elements)\n                # # add extension\n                cog_filename = f\"{cog_filename}.tif\"\n\n                with tempfile.NamedTemporaryFile() as temp_file:\n                    data.rio.to_raster(\n                        temp_file.name,\n                        driver=\"COG\",\n                    )\n                    s3_client.upload_file(\n                        Filename=temp_file.name,\n                        Bucket=bucket_name,\n                        Key=f\"{s3_folder_name}/{cog_filename}\",\n                    )\n\n                files_processed = files_processed._append(\n                    {\"file_name\": name, \"COGs_created\": cog_filename},\n                    ignore_index=True,\n                )\n        count += 1\n        print(f\"Generated and saved COG: {cog_filename}\")\n    except OSError:\n        error_files.append(name)\n        pass\n\n# Generate the json file with the metadata that is present in the netCDF files.\nwith tempfile.NamedTemporaryFile(mode=\"w+\") as fp:\n    json.dump(xds.attrs, fp)\n    json.dump({\"data_dimensions\": dict(xds.dims)}, fp)\n    json.dump({\"data_variables\": list(xds.data_vars)}, fp)\n    fp.flush()\n\n    s3_client.upload_file(\n        Filename=fp.name,\n        Bucket=bucket_name,\n        Key=f\"{s3_folder_name}/metadata.json\",\n    )\n\n# creating the csv file with the names of files transformed.\nfiles_processed.to_csv(\n    f\"s3://{bucket_name}/{s3_folder_name}/files_converted.csv\",\n)\nprint(\"Done generating COGs\")\n\n\n\n\n Back to top",
    "crumbs": [
      "Data Transformation Notebooks",
      "Greenhouse Gas Concentrations",
      "OCO-2 GEOS Column CO₂ Concentrations"
    ]
  },
  {
    "objectID": "cog_transformation/eccodarwin-co2flux-monthgrid-v5.html",
    "href": "cog_transformation/eccodarwin-co2flux-monthgrid-v5.html",
    "title": "Air-Sea CO₂ Flux, ECCO-Darwin Model v5",
    "section": "",
    "text": "This script was used to transform the Air-Sea CO₂ Flux, ECCO-Darwin Mode dataset from netCDF to Cloud Optimized GeoTIFF (COG) format for display in the Greenhouse Gas (GHG) Center.\n\nimport os\nimport xarray\nimport re\nimport pandas as pd\nimport json\nimport tempfile\nimport boto3\nimport rasterio\nfrom datetime import datetime\nfrom dateutil.relativedelta import relativedelta\n\n\nsession = boto3.session.Session()\ns3_client = session.client(\"s3\")\n\nbucket_name = (\n    \"ghgc-data-store-dev\"  # S3 bucket where the COGs are stored after transformation\n)\nFOLDER_NAME = \"ecco-darwin\"\ns3_fol_name = \"ecco_darwin\"\n\n# Reading the raw netCDF files from local machine\nfiles_processed = pd.DataFrame(\n    columns=[\"file_name\", \"COGs_created\"]\n)  # A dataframe to keep track of the files that we have transformed into COGs\nfor name in os.listdir(FOLDER_NAME):\n    xds = xarray.open_dataset(\n        f\"{FOLDER_NAME}/{name}\",\n        engine=\"netcdf4\",\n    )\n    xds = xds.rename({\"y\": \"latitude\", \"x\": \"longitude\"})\n    xds = xds.assign_coords(longitude=((xds.longitude / 1440) * 360) - 180).sortby(\n        \"longitude\"\n    )\n    xds = xds.assign_coords(latitude=((xds.latitude / 721) * 180) - 90).sortby(\n        \"latitude\"\n    )\n\n    variable = [var for var in xds.data_vars]\n\n    for time_increment in xds.time.values:\n        for var in variable[2:]:\n            filename = name.split(\"/ \")[-1]\n            filename_elements = re.split(\"[_ .]\", filename)\n            data = xds[var]\n\n            data = data.reindex(latitude=list(reversed(data.latitude)))\n            data.rio.set_spatial_dims(\"longitude\", \"latitude\", inplace=True)\n            data.rio.write_crs(\"epsg:4326\", inplace=True)\n\n            # generate COG\n            COG_PROFILE = {\"driver\": \"COG\", \"compress\": \"DEFLATE\"}\n\n            filename_elements.pop()\n            filename_elements[-1] = filename_elements[-2] + filename_elements[-1]\n            filename_elements.pop(-2)\n            # # insert date of generated COG into filename\n            cog_filename = \"_\".join(filename_elements)\n            # # add extension\n            cog_filename = f\"{cog_filename}.tif\"\n\n            with tempfile.NamedTemporaryFile() as temp_file:\n                data.rio.to_raster(temp_file.name, **COG_PROFILE)\n                s3_client.upload_file(\n                    Filename=temp_file.name,\n                    Bucket=bucket_name,\n                    Key=f\"{s3_fol_name}/{cog_filename}\",\n                )\n\n            files_processed = files_processed._append(\n                {\"file_name\": name, \"COGs_created\": cog_filename},\n                ignore_index=True,\n            )\n            del data\n\n            print(f\"Generated and saved COG: {cog_filename}\")\n\n# Generate the json file with the metadata that is present in the netCDF files.\nwith tempfile.NamedTemporaryFile(mode=\"w+\") as fp:\n    json.dump(xds.attrs, fp)\n    json.dump({\"data_dimensions\": dict(xds.dims)}, fp)\n    json.dump({\"data_variables\": list(xds.data_vars)}, fp)\n    fp.flush()\n\n    s3_client.upload_file(\n        Filename=fp.name,\n        Bucket=bucket_name,\n        Key=\"s3_fol_name/metadata.json\",\n    )\n\n# A csv file to store the names of all the files converted.\nfiles_processed.to_csv(\n    f\"s3://{bucket_name}/{s3_fol_name}/files_converted.csv\",\n)\nprint(\"Done generating COGs\")\n\n\n\n\n Back to top",
    "crumbs": [
      "Data Transformation Notebooks",
      "Natural Greenhouse Gas Sources Emissions and Sinks",
      "Air-Sea CO₂ Flux, ECCO-Darwin Model v5"
    ]
  },
  {
    "objectID": "cog_transformation/epa-ch4emission-grid-v2express.html",
    "href": "cog_transformation/epa-ch4emission-grid-v2express.html",
    "title": "U.S. Gridded Anthropogenic Methane Emissions Inventory",
    "section": "",
    "text": "This script was used to transform the Gridded Anthropogenic Methane Emissions Inventory monthly dataset from netCDF to Cloud Optimized GeoTIFF (COG) format for display in the Greenhouse Gas (GHG) Center.\n\nimport os\nimport xarray\nimport re\nimport pandas as pd\nimport json\nimport tempfile\nimport boto3\nfrom datetime import datetime\nimport numpy as np\n\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\nTrue\n\n\n\n# session = boto3.session.Session()\nsession = boto3.Session(\n    aws_access_key_id=os.environ.get(\"AWS_ACCESS_KEY_ID\"),\n    aws_secret_access_key=os.environ.get(\"AWS_SECRET_ACCESS_KEY\"),\n    aws_session_token=os.environ.get(\"AWS_SESSION_TOKEN\"),\n)\ns3_client = session.client(\"s3\")\nbucket_name = \"ghgc-data-store-dev\" # S3 bucket where the COGs are stored after transformation\nFOLDER_NAME = \"../data/epa_emissions_express_extension\"\ns3_folder_name = \"epa_express_extension_Mg_km2_yr\"\n# raw gridded data [molec/cm2/s] * 1/6.022x10^23 [molec/mol] * 16.04x10^-6 [ Mg/mol] * 366 [days/yr] * 1x10^10 [cm2/km2]\n\nfiles_processed = pd.DataFrame(columns=[\"file_name\", \"COGs_created\"])   # A dataframe to keep track of the files that we have transformed into COGs\n\n# Reading the raw netCDF files from local machine\nfor name in os.listdir(FOLDER_NAME):\n    xds = xarray.open_dataset(f\"{FOLDER_NAME}/{name}\", engine=\"netcdf4\")\n    xds = xds.assign_coords(lon=(((xds.lon + 180) % 360) - 180)).sortby(\"lon\")\n    variable = [var for var in xds.data_vars]\n    filename = name.split(\"/ \")[-1]\n    filename_elements = re.split(\"[_ .]\", filename)\n    start_time = datetime(int(filename_elements[-2]), 1, 1)\n\n    for time_increment in range(0, len(xds.time)):\n        for var in variable:\n            filename = name.split(\"/ \")[-1]\n            filename_elements = re.split(\"[_ .]\", filename)\n            data = getattr(xds.isel(time=time_increment), var)\n            data.values[data.values==0] = np.nan\n            data = data*((1/(6.022*pow(10,23)))*(16.04*pow(10,-6))*366*pow(10,10)*86400)\n            data = data.fillna(-9999)\n            data = data.isel(lat=slice(None, None, -1))\n            data.rio.set_spatial_dims(\"lon\", \"lat\", inplace=True)\n            data.rio.write_crs(\"epsg:4326\", inplace=True)\n\n            # # insert date of generated COG into filename\n            filename_elements.pop()\n            filename_elements[-1] = start_time.strftime(\"%Y\")\n            filename_elements.insert(2, var)\n            cog_filename = \"_\".join(filename_elements)\n            # # add extension\n            cog_filename = f\"{cog_filename}.tif\"\n\n            with tempfile.NamedTemporaryFile() as temp_file:\n                data.rio.to_raster(\n                    temp_file.name,\n                    driver=\"COG\",\n                )\n                s3_client.upload_file(\n                    Filename=temp_file.name,\n                    Bucket=bucket_name,\n                    Key=f\"{s3_folder_name}/{cog_filename}\",\n                )\n\n            files_processed = files_processed._append(\n                {\"file_name\": name, \"COGs_created\": cog_filename},\n                ignore_index=True,\n            )\n\n            print(f\"Generated and saved COG: {cog_filename}\")\n\n# Generate the json file with the metadata that is present in the netCDF files.\nwith tempfile.NamedTemporaryFile(mode=\"w+\") as fp:\n    json.dump(xds.attrs, fp)\n    json.dump({\"data_dimensions\": dict(xds.dims)}, fp)\n    json.dump({\"data_variables\": list(xds.data_vars)}, fp)\n    fp.flush()\n\n    s3_client.upload_file(\n        Filename=fp.name,\n        Bucket=bucket_name,\n        Key=f\"{s3_folder_name}/metadata.json\",\n    )\n\n# creating the csv file with the names of files transformed.\nfiles_processed.to_csv(\n    f\"s3://{bucket_name}/{s3_folder_name}/files_converted.csv\",\n)\nprint(\"Done generating COGs\")\n\nGenerated and saved COG: Express_Extension_emi_ch4_1A_Combustion_Mobile_Gridded_GHGI_Methane_v2_2015.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1A_Combustion_Stationary_Gridded_GHGI_Methane_v2_2015.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B1a_Abandoned_Coal_Gridded_GHGI_Methane_v2_2015.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B1a_Surface_Coal_Gridded_GHGI_Methane_v2_2015.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B1a_Underground_Coal_Gridded_GHGI_Methane_v2_2015.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2a_Petroleum_Systems_Exploration_Gridded_GHGI_Methane_v2_2015.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2a_Petroleum_Systems_Production_Gridded_GHGI_Methane_v2_2015.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2a_Petroleum_Systems_Refining_Gridded_GHGI_Methane_v2_2015.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2a_Petroleum_Systems_Transport_Gridded_GHGI_Methane_v2_2015.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2ab_Abandoned_Oil_Gas_Gridded_GHGI_Methane_v2_2015.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_Distribution_Gridded_GHGI_Methane_v2_2015.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_Exploration_Gridded_GHGI_Methane_v2_2015.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_Processing_Gridded_GHGI_Methane_v2_2015.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_Production_Gridded_GHGI_Methane_v2_2015.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_TransmissionStorage_Gridded_GHGI_Methane_v2_2015.tif\nGenerated and saved COG: Express_Extension_emi_ch4_2B8_Industry_Petrochemical_Gridded_GHGI_Methane_v2_2015.tif\nGenerated and saved COG: Express_Extension_emi_ch4_2C2_Industry_Ferroalloy_Gridded_GHGI_Methane_v2_2015.tif\nGenerated and saved COG: Express_Extension_emi_ch4_3A_Enteric_Fermentation_Gridded_GHGI_Methane_v2_2015.tif\nGenerated and saved COG: Express_Extension_emi_ch4_3B_Manure_Management_Gridded_GHGI_Methane_v2_2015.tif\nGenerated and saved COG: Express_Extension_emi_ch4_3C_Rice_Cultivation_Gridded_GHGI_Methane_v2_2015.tif\nGenerated and saved COG: Express_Extension_emi_ch4_3F_Field_Burning_Gridded_GHGI_Methane_v2_2015.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5A1_Landfills_Industrial_Gridded_GHGI_Methane_v2_2015.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5A1_Landfills_MSW_Gridded_GHGI_Methane_v2_2015.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5B1_Composting_Gridded_GHGI_Methane_v2_2015.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5D_Wastewater_Treatment_Domestic_Gridded_GHGI_Methane_v2_2015.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5D_Wastewater_Treatment_Industrial_Gridded_GHGI_Methane_v2_2015.tif\nGenerated and saved COG: Express_Extension_emi_ch4_Supp_1B2b_PostMeter_Gridded_GHGI_Methane_v2_2015.tif\nGenerated and saved COG: Express_Extension_grid_cell_area_Gridded_GHGI_Methane_v2_2015.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1A_Combustion_Mobile_Gridded_GHGI_Methane_v2_2020.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1A_Combustion_Stationary_Gridded_GHGI_Methane_v2_2020.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B1a_Abandoned_Coal_Gridded_GHGI_Methane_v2_2020.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B1a_Surface_Coal_Gridded_GHGI_Methane_v2_2020.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B1a_Underground_Coal_Gridded_GHGI_Methane_v2_2020.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2a_Petroleum_Systems_Exploration_Gridded_GHGI_Methane_v2_2020.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2a_Petroleum_Systems_Production_Gridded_GHGI_Methane_v2_2020.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2a_Petroleum_Systems_Refining_Gridded_GHGI_Methane_v2_2020.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2a_Petroleum_Systems_Transport_Gridded_GHGI_Methane_v2_2020.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2ab_Abandoned_Oil_Gas_Gridded_GHGI_Methane_v2_2020.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_Distribution_Gridded_GHGI_Methane_v2_2020.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_Exploration_Gridded_GHGI_Methane_v2_2020.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_Processing_Gridded_GHGI_Methane_v2_2020.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_Production_Gridded_GHGI_Methane_v2_2020.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_TransmissionStorage_Gridded_GHGI_Methane_v2_2020.tif\nGenerated and saved COG: Express_Extension_emi_ch4_2B8_Industry_Petrochemical_Gridded_GHGI_Methane_v2_2020.tif\nGenerated and saved COG: Express_Extension_emi_ch4_2C2_Industry_Ferroalloy_Gridded_GHGI_Methane_v2_2020.tif\nGenerated and saved COG: Express_Extension_emi_ch4_3A_Enteric_Fermentation_Gridded_GHGI_Methane_v2_2020.tif\nGenerated and saved COG: Express_Extension_emi_ch4_3B_Manure_Management_Gridded_GHGI_Methane_v2_2020.tif\nGenerated and saved COG: Express_Extension_emi_ch4_3C_Rice_Cultivation_Gridded_GHGI_Methane_v2_2020.tif\nGenerated and saved COG: Express_Extension_emi_ch4_3F_Field_Burning_Gridded_GHGI_Methane_v2_2020.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5A1_Landfills_Industrial_Gridded_GHGI_Methane_v2_2020.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5A1_Landfills_MSW_Gridded_GHGI_Methane_v2_2020.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5B1_Composting_Gridded_GHGI_Methane_v2_2020.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5D_Wastewater_Treatment_Domestic_Gridded_GHGI_Methane_v2_2020.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5D_Wastewater_Treatment_Industrial_Gridded_GHGI_Methane_v2_2020.tif\nGenerated and saved COG: Express_Extension_emi_ch4_Supp_1B2b_PostMeter_Gridded_GHGI_Methane_v2_2020.tif\nGenerated and saved COG: Express_Extension_grid_cell_area_Gridded_GHGI_Methane_v2_2020.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1A_Combustion_Mobile_Gridded_GHGI_Methane_v2_2014.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1A_Combustion_Stationary_Gridded_GHGI_Methane_v2_2014.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B1a_Abandoned_Coal_Gridded_GHGI_Methane_v2_2014.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B1a_Surface_Coal_Gridded_GHGI_Methane_v2_2014.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B1a_Underground_Coal_Gridded_GHGI_Methane_v2_2014.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2a_Petroleum_Systems_Exploration_Gridded_GHGI_Methane_v2_2014.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2a_Petroleum_Systems_Production_Gridded_GHGI_Methane_v2_2014.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2a_Petroleum_Systems_Refining_Gridded_GHGI_Methane_v2_2014.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2a_Petroleum_Systems_Transport_Gridded_GHGI_Methane_v2_2014.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2ab_Abandoned_Oil_Gas_Gridded_GHGI_Methane_v2_2014.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_Distribution_Gridded_GHGI_Methane_v2_2014.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_Exploration_Gridded_GHGI_Methane_v2_2014.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_Processing_Gridded_GHGI_Methane_v2_2014.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_Production_Gridded_GHGI_Methane_v2_2014.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_TransmissionStorage_Gridded_GHGI_Methane_v2_2014.tif\nGenerated and saved COG: Express_Extension_emi_ch4_2B8_Industry_Petrochemical_Gridded_GHGI_Methane_v2_2014.tif\nGenerated and saved COG: Express_Extension_emi_ch4_2C2_Industry_Ferroalloy_Gridded_GHGI_Methane_v2_2014.tif\nGenerated and saved COG: Express_Extension_emi_ch4_3A_Enteric_Fermentation_Gridded_GHGI_Methane_v2_2014.tif\nGenerated and saved COG: Express_Extension_emi_ch4_3B_Manure_Management_Gridded_GHGI_Methane_v2_2014.tif\nGenerated and saved COG: Express_Extension_emi_ch4_3C_Rice_Cultivation_Gridded_GHGI_Methane_v2_2014.tif\nGenerated and saved COG: Express_Extension_emi_ch4_3F_Field_Burning_Gridded_GHGI_Methane_v2_2014.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5A1_Landfills_Industrial_Gridded_GHGI_Methane_v2_2014.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5A1_Landfills_MSW_Gridded_GHGI_Methane_v2_2014.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5B1_Composting_Gridded_GHGI_Methane_v2_2014.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5D_Wastewater_Treatment_Domestic_Gridded_GHGI_Methane_v2_2014.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5D_Wastewater_Treatment_Industrial_Gridded_GHGI_Methane_v2_2014.tif\nGenerated and saved COG: Express_Extension_emi_ch4_Supp_1B2b_PostMeter_Gridded_GHGI_Methane_v2_2014.tif\nGenerated and saved COG: Express_Extension_grid_cell_area_Gridded_GHGI_Methane_v2_2014.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1A_Combustion_Mobile_Gridded_GHGI_Methane_v2_2013.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1A_Combustion_Stationary_Gridded_GHGI_Methane_v2_2013.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B1a_Abandoned_Coal_Gridded_GHGI_Methane_v2_2013.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B1a_Surface_Coal_Gridded_GHGI_Methane_v2_2013.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B1a_Underground_Coal_Gridded_GHGI_Methane_v2_2013.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2a_Petroleum_Systems_Exploration_Gridded_GHGI_Methane_v2_2013.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2a_Petroleum_Systems_Production_Gridded_GHGI_Methane_v2_2013.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2a_Petroleum_Systems_Refining_Gridded_GHGI_Methane_v2_2013.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2a_Petroleum_Systems_Transport_Gridded_GHGI_Methane_v2_2013.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2ab_Abandoned_Oil_Gas_Gridded_GHGI_Methane_v2_2013.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_Distribution_Gridded_GHGI_Methane_v2_2013.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_Exploration_Gridded_GHGI_Methane_v2_2013.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_Processing_Gridded_GHGI_Methane_v2_2013.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_Production_Gridded_GHGI_Methane_v2_2013.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_TransmissionStorage_Gridded_GHGI_Methane_v2_2013.tif\nGenerated and saved COG: Express_Extension_emi_ch4_2B8_Industry_Petrochemical_Gridded_GHGI_Methane_v2_2013.tif\nGenerated and saved COG: Express_Extension_emi_ch4_2C2_Industry_Ferroalloy_Gridded_GHGI_Methane_v2_2013.tif\nGenerated and saved COG: Express_Extension_emi_ch4_3A_Enteric_Fermentation_Gridded_GHGI_Methane_v2_2013.tif\nGenerated and saved COG: Express_Extension_emi_ch4_3B_Manure_Management_Gridded_GHGI_Methane_v2_2013.tif\nGenerated and saved COG: Express_Extension_emi_ch4_3C_Rice_Cultivation_Gridded_GHGI_Methane_v2_2013.tif\nGenerated and saved COG: Express_Extension_emi_ch4_3F_Field_Burning_Gridded_GHGI_Methane_v2_2013.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5A1_Landfills_Industrial_Gridded_GHGI_Methane_v2_2013.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5A1_Landfills_MSW_Gridded_GHGI_Methane_v2_2013.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5B1_Composting_Gridded_GHGI_Methane_v2_2013.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5D_Wastewater_Treatment_Domestic_Gridded_GHGI_Methane_v2_2013.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5D_Wastewater_Treatment_Industrial_Gridded_GHGI_Methane_v2_2013.tif\nGenerated and saved COG: Express_Extension_emi_ch4_Supp_1B2b_PostMeter_Gridded_GHGI_Methane_v2_2013.tif\nGenerated and saved COG: Express_Extension_grid_cell_area_Gridded_GHGI_Methane_v2_2013.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1A_Combustion_Mobile_Gridded_GHGI_Methane_v2_2017.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1A_Combustion_Stationary_Gridded_GHGI_Methane_v2_2017.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B1a_Abandoned_Coal_Gridded_GHGI_Methane_v2_2017.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B1a_Surface_Coal_Gridded_GHGI_Methane_v2_2017.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B1a_Underground_Coal_Gridded_GHGI_Methane_v2_2017.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2a_Petroleum_Systems_Exploration_Gridded_GHGI_Methane_v2_2017.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2a_Petroleum_Systems_Production_Gridded_GHGI_Methane_v2_2017.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2a_Petroleum_Systems_Refining_Gridded_GHGI_Methane_v2_2017.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2a_Petroleum_Systems_Transport_Gridded_GHGI_Methane_v2_2017.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2ab_Abandoned_Oil_Gas_Gridded_GHGI_Methane_v2_2017.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_Distribution_Gridded_GHGI_Methane_v2_2017.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_Exploration_Gridded_GHGI_Methane_v2_2017.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_Processing_Gridded_GHGI_Methane_v2_2017.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_Production_Gridded_GHGI_Methane_v2_2017.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_TransmissionStorage_Gridded_GHGI_Methane_v2_2017.tif\nGenerated and saved COG: Express_Extension_emi_ch4_2B8_Industry_Petrochemical_Gridded_GHGI_Methane_v2_2017.tif\nGenerated and saved COG: Express_Extension_emi_ch4_2C2_Industry_Ferroalloy_Gridded_GHGI_Methane_v2_2017.tif\nGenerated and saved COG: Express_Extension_emi_ch4_3A_Enteric_Fermentation_Gridded_GHGI_Methane_v2_2017.tif\nGenerated and saved COG: Express_Extension_emi_ch4_3B_Manure_Management_Gridded_GHGI_Methane_v2_2017.tif\nGenerated and saved COG: Express_Extension_emi_ch4_3C_Rice_Cultivation_Gridded_GHGI_Methane_v2_2017.tif\nGenerated and saved COG: Express_Extension_emi_ch4_3F_Field_Burning_Gridded_GHGI_Methane_v2_2017.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5A1_Landfills_Industrial_Gridded_GHGI_Methane_v2_2017.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5A1_Landfills_MSW_Gridded_GHGI_Methane_v2_2017.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5B1_Composting_Gridded_GHGI_Methane_v2_2017.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5D_Wastewater_Treatment_Domestic_Gridded_GHGI_Methane_v2_2017.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5D_Wastewater_Treatment_Industrial_Gridded_GHGI_Methane_v2_2017.tif\nGenerated and saved COG: Express_Extension_emi_ch4_Supp_1B2b_PostMeter_Gridded_GHGI_Methane_v2_2017.tif\nGenerated and saved COG: Express_Extension_grid_cell_area_Gridded_GHGI_Methane_v2_2017.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1A_Combustion_Mobile_Gridded_GHGI_Methane_v2_2016.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1A_Combustion_Stationary_Gridded_GHGI_Methane_v2_2016.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B1a_Abandoned_Coal_Gridded_GHGI_Methane_v2_2016.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B1a_Surface_Coal_Gridded_GHGI_Methane_v2_2016.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B1a_Underground_Coal_Gridded_GHGI_Methane_v2_2016.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2a_Petroleum_Systems_Exploration_Gridded_GHGI_Methane_v2_2016.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2a_Petroleum_Systems_Production_Gridded_GHGI_Methane_v2_2016.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2a_Petroleum_Systems_Refining_Gridded_GHGI_Methane_v2_2016.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2a_Petroleum_Systems_Transport_Gridded_GHGI_Methane_v2_2016.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2ab_Abandoned_Oil_Gas_Gridded_GHGI_Methane_v2_2016.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_Distribution_Gridded_GHGI_Methane_v2_2016.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_Exploration_Gridded_GHGI_Methane_v2_2016.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_Processing_Gridded_GHGI_Methane_v2_2016.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_Production_Gridded_GHGI_Methane_v2_2016.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_TransmissionStorage_Gridded_GHGI_Methane_v2_2016.tif\nGenerated and saved COG: Express_Extension_emi_ch4_2B8_Industry_Petrochemical_Gridded_GHGI_Methane_v2_2016.tif\nGenerated and saved COG: Express_Extension_emi_ch4_2C2_Industry_Ferroalloy_Gridded_GHGI_Methane_v2_2016.tif\nGenerated and saved COG: Express_Extension_emi_ch4_3A_Enteric_Fermentation_Gridded_GHGI_Methane_v2_2016.tif\nGenerated and saved COG: Express_Extension_emi_ch4_3B_Manure_Management_Gridded_GHGI_Methane_v2_2016.tif\nGenerated and saved COG: Express_Extension_emi_ch4_3C_Rice_Cultivation_Gridded_GHGI_Methane_v2_2016.tif\nGenerated and saved COG: Express_Extension_emi_ch4_3F_Field_Burning_Gridded_GHGI_Methane_v2_2016.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5A1_Landfills_Industrial_Gridded_GHGI_Methane_v2_2016.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5A1_Landfills_MSW_Gridded_GHGI_Methane_v2_2016.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5B1_Composting_Gridded_GHGI_Methane_v2_2016.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5D_Wastewater_Treatment_Domestic_Gridded_GHGI_Methane_v2_2016.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5D_Wastewater_Treatment_Industrial_Gridded_GHGI_Methane_v2_2016.tif\nGenerated and saved COG: Express_Extension_emi_ch4_Supp_1B2b_PostMeter_Gridded_GHGI_Methane_v2_2016.tif\nGenerated and saved COG: Express_Extension_grid_cell_area_Gridded_GHGI_Methane_v2_2016.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1A_Combustion_Mobile_Gridded_GHGI_Methane_v2_2012.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1A_Combustion_Stationary_Gridded_GHGI_Methane_v2_2012.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B1a_Abandoned_Coal_Gridded_GHGI_Methane_v2_2012.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B1a_Surface_Coal_Gridded_GHGI_Methane_v2_2012.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B1a_Underground_Coal_Gridded_GHGI_Methane_v2_2012.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2a_Petroleum_Systems_Exploration_Gridded_GHGI_Methane_v2_2012.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2a_Petroleum_Systems_Production_Gridded_GHGI_Methane_v2_2012.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2a_Petroleum_Systems_Refining_Gridded_GHGI_Methane_v2_2012.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2a_Petroleum_Systems_Transport_Gridded_GHGI_Methane_v2_2012.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2ab_Abandoned_Oil_Gas_Gridded_GHGI_Methane_v2_2012.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_Distribution_Gridded_GHGI_Methane_v2_2012.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_Exploration_Gridded_GHGI_Methane_v2_2012.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_Processing_Gridded_GHGI_Methane_v2_2012.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_Production_Gridded_GHGI_Methane_v2_2012.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_TransmissionStorage_Gridded_GHGI_Methane_v2_2012.tif\nGenerated and saved COG: Express_Extension_emi_ch4_2B8_Industry_Petrochemical_Gridded_GHGI_Methane_v2_2012.tif\nGenerated and saved COG: Express_Extension_emi_ch4_2C2_Industry_Ferroalloy_Gridded_GHGI_Methane_v2_2012.tif\nGenerated and saved COG: Express_Extension_emi_ch4_3A_Enteric_Fermentation_Gridded_GHGI_Methane_v2_2012.tif\nGenerated and saved COG: Express_Extension_emi_ch4_3B_Manure_Management_Gridded_GHGI_Methane_v2_2012.tif\nGenerated and saved COG: Express_Extension_emi_ch4_3C_Rice_Cultivation_Gridded_GHGI_Methane_v2_2012.tif\nGenerated and saved COG: Express_Extension_emi_ch4_3F_Field_Burning_Gridded_GHGI_Methane_v2_2012.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5A1_Landfills_Industrial_Gridded_GHGI_Methane_v2_2012.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5A1_Landfills_MSW_Gridded_GHGI_Methane_v2_2012.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5B1_Composting_Gridded_GHGI_Methane_v2_2012.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5D_Wastewater_Treatment_Domestic_Gridded_GHGI_Methane_v2_2012.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5D_Wastewater_Treatment_Industrial_Gridded_GHGI_Methane_v2_2012.tif\nGenerated and saved COG: Express_Extension_emi_ch4_Supp_1B2b_PostMeter_Gridded_GHGI_Methane_v2_2012.tif\nGenerated and saved COG: Express_Extension_grid_cell_area_Gridded_GHGI_Methane_v2_2012.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1A_Combustion_Mobile_Gridded_GHGI_Methane_v2_2019.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1A_Combustion_Stationary_Gridded_GHGI_Methane_v2_2019.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B1a_Abandoned_Coal_Gridded_GHGI_Methane_v2_2019.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B1a_Surface_Coal_Gridded_GHGI_Methane_v2_2019.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B1a_Underground_Coal_Gridded_GHGI_Methane_v2_2019.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2a_Petroleum_Systems_Exploration_Gridded_GHGI_Methane_v2_2019.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2a_Petroleum_Systems_Production_Gridded_GHGI_Methane_v2_2019.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2a_Petroleum_Systems_Refining_Gridded_GHGI_Methane_v2_2019.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2a_Petroleum_Systems_Transport_Gridded_GHGI_Methane_v2_2019.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2ab_Abandoned_Oil_Gas_Gridded_GHGI_Methane_v2_2019.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_Distribution_Gridded_GHGI_Methane_v2_2019.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_Exploration_Gridded_GHGI_Methane_v2_2019.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_Processing_Gridded_GHGI_Methane_v2_2019.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_Production_Gridded_GHGI_Methane_v2_2019.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_TransmissionStorage_Gridded_GHGI_Methane_v2_2019.tif\nGenerated and saved COG: Express_Extension_emi_ch4_2B8_Industry_Petrochemical_Gridded_GHGI_Methane_v2_2019.tif\nGenerated and saved COG: Express_Extension_emi_ch4_2C2_Industry_Ferroalloy_Gridded_GHGI_Methane_v2_2019.tif\nGenerated and saved COG: Express_Extension_emi_ch4_3A_Enteric_Fermentation_Gridded_GHGI_Methane_v2_2019.tif\nGenerated and saved COG: Express_Extension_emi_ch4_3B_Manure_Management_Gridded_GHGI_Methane_v2_2019.tif\nGenerated and saved COG: Express_Extension_emi_ch4_3C_Rice_Cultivation_Gridded_GHGI_Methane_v2_2019.tif\nGenerated and saved COG: Express_Extension_emi_ch4_3F_Field_Burning_Gridded_GHGI_Methane_v2_2019.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5A1_Landfills_Industrial_Gridded_GHGI_Methane_v2_2019.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5A1_Landfills_MSW_Gridded_GHGI_Methane_v2_2019.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5B1_Composting_Gridded_GHGI_Methane_v2_2019.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5D_Wastewater_Treatment_Domestic_Gridded_GHGI_Methane_v2_2019.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5D_Wastewater_Treatment_Industrial_Gridded_GHGI_Methane_v2_2019.tif\nGenerated and saved COG: Express_Extension_emi_ch4_Supp_1B2b_PostMeter_Gridded_GHGI_Methane_v2_2019.tif\nGenerated and saved COG: Express_Extension_grid_cell_area_Gridded_GHGI_Methane_v2_2019.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1A_Combustion_Mobile_Gridded_GHGI_Methane_v2_2018.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1A_Combustion_Stationary_Gridded_GHGI_Methane_v2_2018.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B1a_Abandoned_Coal_Gridded_GHGI_Methane_v2_2018.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B1a_Surface_Coal_Gridded_GHGI_Methane_v2_2018.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B1a_Underground_Coal_Gridded_GHGI_Methane_v2_2018.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2a_Petroleum_Systems_Exploration_Gridded_GHGI_Methane_v2_2018.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2a_Petroleum_Systems_Production_Gridded_GHGI_Methane_v2_2018.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2a_Petroleum_Systems_Refining_Gridded_GHGI_Methane_v2_2018.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2a_Petroleum_Systems_Transport_Gridded_GHGI_Methane_v2_2018.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2ab_Abandoned_Oil_Gas_Gridded_GHGI_Methane_v2_2018.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_Distribution_Gridded_GHGI_Methane_v2_2018.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_Exploration_Gridded_GHGI_Methane_v2_2018.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_Processing_Gridded_GHGI_Methane_v2_2018.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_Production_Gridded_GHGI_Methane_v2_2018.tif\nGenerated and saved COG: Express_Extension_emi_ch4_1B2b_Natural_Gas_TransmissionStorage_Gridded_GHGI_Methane_v2_2018.tif\nGenerated and saved COG: Express_Extension_emi_ch4_2B8_Industry_Petrochemical_Gridded_GHGI_Methane_v2_2018.tif\nGenerated and saved COG: Express_Extension_emi_ch4_2C2_Industry_Ferroalloy_Gridded_GHGI_Methane_v2_2018.tif\nGenerated and saved COG: Express_Extension_emi_ch4_3A_Enteric_Fermentation_Gridded_GHGI_Methane_v2_2018.tif\nGenerated and saved COG: Express_Extension_emi_ch4_3B_Manure_Management_Gridded_GHGI_Methane_v2_2018.tif\nGenerated and saved COG: Express_Extension_emi_ch4_3C_Rice_Cultivation_Gridded_GHGI_Methane_v2_2018.tif\nGenerated and saved COG: Express_Extension_emi_ch4_3F_Field_Burning_Gridded_GHGI_Methane_v2_2018.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5A1_Landfills_Industrial_Gridded_GHGI_Methane_v2_2018.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5A1_Landfills_MSW_Gridded_GHGI_Methane_v2_2018.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5B1_Composting_Gridded_GHGI_Methane_v2_2018.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5D_Wastewater_Treatment_Domestic_Gridded_GHGI_Methane_v2_2018.tif\nGenerated and saved COG: Express_Extension_emi_ch4_5D_Wastewater_Treatment_Industrial_Gridded_GHGI_Methane_v2_2018.tif\nGenerated and saved COG: Express_Extension_emi_ch4_Supp_1B2b_PostMeter_Gridded_GHGI_Methane_v2_2018.tif\nGenerated and saved COG: Express_Extension_grid_cell_area_Gridded_GHGI_Methane_v2_2018.tif\nDone generating COGs\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Data Transformation Notebooks",
      "Gridded Anthropogenic Greenhouse Gas Emissions",
      "U.S. Gridded Anthropogenic Methane Emissions Inventory"
    ]
  },
  {
    "objectID": "processingreport.html",
    "href": "processingreport.html",
    "title": "U.S. Greenhouse Gas Center: Processing and Verification Reports",
    "section": "",
    "text": "Welcome to the U.S. Greenhouse Gas (GHG) Center processing and verification reports. These reports verify that the accuracy and integrity of each dataset in the US GHG Center is maintained once it is processed into the Center.\nThe reports are grouped topically and labeled by dataset name. Click on a dataset name to view the processing and verification report for that dataset.\nExamples of processing that may occur include transforming data from its source format into a could-optimized format, converting the units of the source data into a more common or standard unit, and flagging “nodata” values to ensure accurate data visualization. We strive to handle all data with extreme care, and share these reports to provide transparency and insight into any processing that is applied, while ensuring accuracy and reliability every step of the way.\nJoin us in our mission to make data-driven environmental solutions accessible. Explore, analyze, and make a difference with the US GHG Center.\nView the US GHG Center Data Catalog",
    "crumbs": [
      "Processing and Verification Reports"
    ]
  },
  {
    "objectID": "processingreport.html#gridded-anthropogenic-greenhouse-gas-emissions",
    "href": "processingreport.html#gridded-anthropogenic-greenhouse-gas-emissions",
    "title": "U.S. Greenhouse Gas Center: Processing and Verification Reports",
    "section": "Gridded Anthropogenic Greenhouse Gas Emissions",
    "text": "Gridded Anthropogenic Greenhouse Gas Emissions\n\nOCO-2 MIP Top-Down CO₂ Budgets Processing and Verification Report\nODIAC Fossil Fuel CO₂ Emissions Processing and Verification Report\nTM5-4DVar Isotopic CH₄ Inverse Fluxes Processing and Verification Report\nU.S. Gridded Anthropogenic Methane Emissions Inventory Processing and Verification Report",
    "crumbs": [
      "Processing and Verification Reports"
    ]
  },
  {
    "objectID": "processingreport.html#natural-greenhouse-gas-emissions-and-sinks",
    "href": "processingreport.html#natural-greenhouse-gas-emissions-and-sinks",
    "title": "U.S. Greenhouse Gas Center: Processing and Verification Reports",
    "section": "Natural Greenhouse Gas Emissions and Sinks",
    "text": "Natural Greenhouse Gas Emissions and Sinks\n\nAir-Sea CO₂ Flux, ECCO-Darwin Model v5 Processing and Verification Report\nMiCASA Land Carbon Flux Processing and Verification Report\nGOSAT-based Top-down Total and Natural Methane Emissions Processing and Verification Report\nOCO-2 MIP Top-Down CO₂ Budgets Processing and Verification Report\nTM5-4DVar Isotopic CH₄ Inverse Fluxes Processing and Verification Report\nWetland Methane Emissions, LPJ-EOSIM model Processing and Verification Report",
    "crumbs": [
      "Processing and Verification Reports"
    ]
  },
  {
    "objectID": "processingreport.html#large-emissions-events",
    "href": "processingreport.html#large-emissions-events",
    "title": "U.S. Greenhouse Gas Center: Processing and Verification Reports",
    "section": "Large Emissions Events",
    "text": "Large Emissions Events\n\nEMIT Methane Point Source Plume Complexes Processing and Verification Report",
    "crumbs": [
      "Processing and Verification Reports"
    ]
  },
  {
    "objectID": "processingreport.html#greenhouse-gas-concentrations",
    "href": "processingreport.html#greenhouse-gas-concentrations",
    "title": "U.S. Greenhouse Gas Center: Processing and Verification Reports",
    "section": "Greenhouse Gas Concentrations",
    "text": "Greenhouse Gas Concentrations\n\nOCO-2 GEOS Column CO₂ Concentrations Processing and Verification Report",
    "crumbs": [
      "Processing and Verification Reports"
    ]
  },
  {
    "objectID": "processingreport.html#socioeconomic",
    "href": "processingreport.html#socioeconomic",
    "title": "U.S. Greenhouse Gas Center: Processing and Verification Reports",
    "section": "Socioeconomic",
    "text": "Socioeconomic\n\nSEDAC Gridded World Population Density Processing and Verification Report",
    "crumbs": [
      "Processing and Verification Reports"
    ]
  },
  {
    "objectID": "processingreport.html#contact",
    "href": "processingreport.html#contact",
    "title": "U.S. Greenhouse Gas Center: Processing and Verification Reports",
    "section": "Contact",
    "text": "Contact\nFor technical help or general questions, please contact the support team using the feedback form.",
    "crumbs": [
      "Processing and Verification Reports"
    ]
  },
  {
    "objectID": "advanceduser.html",
    "href": "advanceduser.html",
    "title": "U.S. Greenhouse Gas Center: Advanced User Notebook",
    "section": "",
    "text": "Welcome to the U.S. Greenhouse Gas (GHG) Center: Advanced User Notebook, your gateway to exploring and analyzing curated datasets on greenhouse gas emissions. Our cloud-based system offers seamless access to Greenhouse Gas curated datasets. Dive into the data with our data usage Jupyter notebooks, designed for efficient exploration, visualization, and analysis. Whether you are focused on specific focus areas or product types, our dataset usage notebooks provide invaluable insights to drive informed decision-making.\nJoin us in our mission to make data-driven environmental solutions. Explore, analyze, and make a difference with the US GHG Center.\nView the US GHG Center Data Catalogg"
  },
  {
    "objectID": "advanceduser.html#gridded-anthropogenic-greenhouse-gas-emissions",
    "href": "advanceduser.html#gridded-anthropogenic-greenhouse-gas-emissions",
    "title": "U.S. Greenhouse Gas Center: Advanced User Notebook",
    "section": "Gridded Anthropogenic Greenhouse Gas Emissions",
    "text": "Gridded Anthropogenic Greenhouse Gas Emissions\n\nOCO-2 MIP Top-Down National CO₂ Budgets - Model Output"
  },
  {
    "objectID": "advanceduser.html#natural-greenhouse-gas-emissions-and-sinks",
    "href": "advanceduser.html#natural-greenhouse-gas-emissions-and-sinks",
    "title": "U.S. Greenhouse Gas Center: Advanced User Notebook",
    "section": "Natural Greenhouse Gas Emissions and Sinks",
    "text": "Natural Greenhouse Gas Emissions and Sinks"
  },
  {
    "objectID": "advanceduser.html#large-emissions-events",
    "href": "advanceduser.html#large-emissions-events",
    "title": "U.S. Greenhouse Gas Center: Advanced User Notebook",
    "section": "Large Emissions Events",
    "text": "Large Emissions Events"
  },
  {
    "objectID": "advanceduser.html#greenhouse-gas-concentrations",
    "href": "advanceduser.html#greenhouse-gas-concentrations",
    "title": "U.S. Greenhouse Gas Center: Advanced User Notebook",
    "section": "Greenhouse Gas Concentrations",
    "text": "Greenhouse Gas Concentrations"
  },
  {
    "objectID": "advanceduser.html#socioeconomic",
    "href": "advanceduser.html#socioeconomic",
    "title": "U.S. Greenhouse Gas Center: Advanced User Notebook",
    "section": "Socioeconomic",
    "text": "Socioeconomic"
  },
  {
    "objectID": "advanceduser.html#contact",
    "href": "advanceduser.html#contact",
    "title": "U.S. Greenhouse Gas Center: Advanced User Notebook",
    "section": "Contact",
    "text": "Contact\nFor technical help or general questions, please contact the support team using the feedback form."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "U.S. Greenhouse Gas Center: Documentation",
    "section": "",
    "text": "The U.S. Greenhouse Gas (GHG) Center provides a cloud-based system for exploring and analyzing U.S. government and other curated greenhouse gas datasets.\nOn this site, you can find the technical documentation for the services the center provides, how to load the datasets, and how the datasets were transformed from their source formats (eg. netCDF, HDF, etc.) into cloud-optimized formats that enable efficient cloud data access and visualization.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#welcome",
    "href": "index.html#welcome",
    "title": "U.S. Greenhouse Gas Center: Documentation",
    "section": "",
    "text": "The U.S. Greenhouse Gas (GHG) Center provides a cloud-based system for exploring and analyzing U.S. government and other curated greenhouse gas datasets.\nOn this site, you can find the technical documentation for the services the center provides, how to load the datasets, and how the datasets were transformed from their source formats (eg. netCDF, HDF, etc.) into cloud-optimized formats that enable efficient cloud data access and visualization.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#contents",
    "href": "index.html#contents",
    "title": "U.S. Greenhouse Gas Center: Documentation",
    "section": "Contents",
    "text": "Contents\n\nServices provided for accessing and analyzing the US GHG Center datasets, such as the JupyterHub environment for interactive computing.\nDataset usage examples, e.g. for the Wetland Methane Emissions from the LPJ-wsl model dataset, that shows how to load the dataset in Python in JupyterHub.\nDataset transformation scripts, such as the CASA-GFED3 Land Carbon Flux dataset.\nData processing and verification reports that openly present the process we used to check and verify that any transformation did not alter the original source data. An example is the CEOS CH₄ budget yearly dataset.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#contact",
    "href": "index.html#contact",
    "title": "U.S. Greenhouse Gas Center: Documentation",
    "section": "Contact",
    "text": "Contact\nFor technical help or general questions, please contact the support team using the feedback form.",
    "crumbs": [
      "Welcome"
    ]
  }
]